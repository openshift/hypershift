---
################# IMPORTANT ########################
# Document generated by `make api-docs`. DO NOT EDIT
################# IMPORTANT ########################
title: API
---
# HyperShift API Reference
<p>Packages:</p>
<ul>
<li>
<a href="#hypershift.openshift.io%2fv1beta1">hypershift.openshift.io/v1beta1</a>
</li>
</ul>
<h2 id="hypershift.openshift.io/v1beta1">hypershift.openshift.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains the HyperShift API.</p>
<p>The HyperShift API enables creating and managing lightweight, flexible, heterogeneous
OpenShift clusters at scale.</p>
<p>HyperShift clusters are deployed in a topology which isolates the &ldquo;control plane&rdquo;
(e.g. etcd, the API server, controller manager, etc.) from the &ldquo;data plane&rdquo; (e.g.
worker nodes and their kubelets, and the infrastructure on which they run). This
enables &ldquo;hosted control plane as a service&rdquo; use cases.</p>
</p>
##CertificateSigningRequestApproval { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval }
<p>
<p>CertificateSigningRequestApproval defines the desired state of CertificateSigningRequestApproval</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>CertificateSigningRequestApproval</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec">
CertificateSigningRequestApprovalSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus">
CertificateSigningRequestApprovalStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
##HostedCluster { #hypershift.openshift.io/v1beta1.HostedCluster }
<p>
<p>HostedCluster is the primary representation of a HyperShift cluster and encapsulates
the control plane and common data plane configuration. Creating a HostedCluster
results in a fully functional OpenShift control plane with no attached nodes.
To support workloads (e.g. pods), a HostedCluster may have one or more associated
NodePool resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>HostedCluster</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">
HostedClusterSpec
</a>
</em>
</td>
<td>
<p>Spec is the desired behavior of the HostedCluster.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the desired OCP release payload for all the hosted cluster components.
This includes those components running management side like the Kube API Server and the CVO but also the operands which land in the hosted cluster data plane like the ingress controller, ovn agents, etc.
The maximum and minimum supported release versions are determined by the running Hypersfhit Operator.
Attempting to use an unsupported version will result in the HostedCluster being degraded and the validateReleaseImage condition being false.
Attempting to use a release with a skew against a NodePool release bigger than N-2 for the y-stream will result in leaving the NodePool in an unsupported state.
Changing this field will trigger a rollout of the control plane components.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneRelease</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneRelease is like spec.release but only for the components running on the management cluster.
This excludes any operand which will land in the hosted cluster data plane.
It is useful when you need to apply patch management side like a CVE, transparently for the hosted cluster.
Version input for this field is free, no validation is performed against spec.release or maximum and minimum is performed.
If defined, it will dicate the version of the components running management side, while spec.release will dictate the version of the components landing in the hosted cluster data plane.
If not defined, spec.release is used for both.
Changing this field will trigger a rollout of the control plane.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterID uniquely identifies this cluster. This is expected to be an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in hexadecimal digits).
As with a Kubernetes metadata.uid, this ID uniquely identifies this cluster in space and time.
This value identifies the cluster in metrics pushed to telemetry and metrics produced by the control plane operators.
If a value is not specified, a random clusterID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>infraID is a globally unique identifier for the cluster.
It must consist of lowercase alphanumeric characters and hyphens (&lsquo;-&rsquo;) only, and start and end with an alphanumeric character.
It must be no more than 253 characters in length.
This identifier will be used to associate various cloud resources with the HostedCluster and its associated NodePools.
infraID is used to compute and tag created resources with &ldquo;kubernetes.io/cluster/&rdquo;+hcluster.Spec.InfraID which has contractual meaning for the cloud provider implementations.
If a value is not specified, a random infraID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
If omitted we will use the appropriate update service for the cluster and region.
This is used by the control plane operator to determine and signal the appropriate available upgrades in the hostedCluster.status.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster.
If omitted no particular upgrades are suggested.
TODO(alberto): Consider the backend to use the default channel by default. Default channel will contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components like the Kube API Server.
Possible values are HighlyAvailable and SingleReplica. The default value is HighlyAvailable.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on the hosted cluster data plane like the ingress controller and image registry controller.
Possible values are HighlyAvailable and SingleReplica. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>dns specifies the DNS configuration for the hosted cluster ingress.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<p>networking specifies network configuration for the hosted cluster.
Defaults to OVNKubernetes with a cluster network of cidr: &ldquo;10.132.0.0/14&rdquo; and a service network of cidr: &ldquo;172.31.0.0/16&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with this HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<p>autoNode specifies the configuration for the autoNode feature.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>etcd specifies configuration for the control plane etcd cluster. The
default managementType is Managed. Once set, the managementType cannot be
changed.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>services specifies how individual control plane services endpoints are published for consumption.
This requires APIServer;OAuthServer;Konnectivity;Ignition.
This field is immutable for all platforms but IBMCloud.
Max is 6 to account for OIDC;OVNSbDb for backward compatibility though they are no-op.</p>
<p>-kubebuilder:validation:XValidation:rule=&ldquo;self.all(s, !(s.service == &lsquo;APIServer&rsquo; &amp;&amp; s.servicePublishingStrategy.type == &lsquo;Route&rsquo;) || has(s.servicePublishingStrategy.route.hostname))&rdquo;,message=&ldquo;If serviceType is &lsquo;APIServer&rsquo; and publishing strategy is &lsquo;Route&rsquo;, then hostname must be set&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;[&lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, &lsquo;Ignition&rsquo;].all(requiredType, self.exists(s, s.service == requiredType))&rdquo;,message=&ldquo;Services list must contain at least &lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, and &lsquo;Ignition&rsquo; service types&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; has(s.servicePublishingStrategy.route) &amp;&amp; has(s.servicePublishingStrategy.route.hostname)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.route) &amp;&amp; has(y.servicePublishingStrategy.route.hostname) &amp;&amp; y.servicePublishingStrategy.route.hostname == x.servicePublishingStrategy.route.hostname)).size() &lt;= 1)&rdquo;,message=&ldquo;Each route publishingStrategy &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; has(s.servicePublishingStrategy.nodePort) &amp;&amp; has(s.servicePublishingStrategy.nodePort.address) &amp;&amp; has(s.servicePublishingStrategy.nodePort.port)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.nodePort) &amp;&amp; has(y.servicePublishingStrategy.nodePort.address) &amp;&amp; y.servicePublishingStrategy.nodePort.address == x.servicePublishingStrategy.nodePort.address &amp;&amp; has(y.servicePublishingStrategy.nodePort.port) &amp;&amp; y.servicePublishingStrategy.nodePort.port == x.servicePublishingStrategy.nodePort.port )).size() &lt;= 1)&rdquo;,message=&ldquo;Each nodePort publishingStrategy &lsquo;nodePort&rsquo; and &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
TODO(alberto): this breaks the cost budget for &lt; 4.17. We should figure why and enable it back. And If not fixable, consider imposing a minimum version on the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>pullSecret is a local reference to a Secret that must have a &ldquo;.dockerconfigjson&rdquo; key whose content must be a valid Openshift pull secret JSON.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This pull secret will be part of every payload generated by the controllers for any NodePool of the HostedCluster
and it will be injected into the container runtime of all NodePools.
Changing this value will trigger a rollout for all existing NodePools in the cluster.
Changing the content of the secret inplace will not trigger a rollout and might result in unpredictable behaviour.
TODO(alberto): have our own local reference type to include our opinions and avoid transparent changes.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>sshKey is a local reference to a Secret that must have a &ldquo;id_rsa.pub&rdquo; key whose content must be the public part of 1..N SSH keys.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
When sshKey is set, the controllers will generate a machineConfig with the sshAuthorizedKeys <a href="https://coreos.github.io/ignition/configuration-v3_2/">https://coreos.github.io/ignition/configuration-v3_2/</a> populated with this value.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>issuerURL is an OIDC issuer URL which will be used as the issuer in all
ServiceAccount tokens generated by the control plane API server via &ndash;service-account-issuer kube api server flag.
<a href="https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/">https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/</a>
<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection</a>
The default value is kubernetes.default.svc, which only works for in-cluster
validation.
If the platform is AWS and this value is set, the controller will update an s3 object with the appropriate OIDC documents (using the serviceAccountSigningKey info) into that issuerURL.
The expectation is for this s3 url to be backed by an OIDC provider in the AWS IAM.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAccountSigningKey is a local reference to a secret that must have a &ldquo;key&rdquo; key whose content must be the private key
used by the service account token issuer.
If not specified, a service account signing key will
be generated automatically for the cluster.
When specifying a service account signing key, an IssuerURL must also be specified.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
</td>
</tr>
<tr>
<td>
<code>operatorConfiguration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">
OperatorConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditWebhook contains metadata for configuring an audit webhook endpoint
for a cluster to process cluster audit events. It references a secret that
contains the webhook information for the audit webhook endpoint. It is a
secret because if the endpoint has mTLS the kubeconfig will contain client
keys. The kubeconfig needs to be stored in the secret with a secret key
name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageContentSources specifies image mirrors that can be used by cluster
nodes to pull content.
When imageContentSources is set, the controllers will generate a machineConfig.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalTrustBundle is a local reference to a ConfigMap that must have a &ldquo;ca-bundle.crt&rdquo; key
whose content must be a PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>secretEncryption specifies a Kubernetes secret encryption strategy for the
control plane.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>fips indicates whether this cluster&rsquo;s nodes will be running in FIPS mode.
If set to true, the control plane&rsquo;s ignition server will be configured to
expect that nodes joining the cluster will be FIPS-enabled.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the HostedCluster controller, resulting in any change to the HostedCluster being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OLMCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector when specified, is propagated to all control plane Deployments and Stateful sets running management side.
It must be satisfied by the management Nodes for the pods to be scheduled. Otherwise the HostedCluster will enter a degraded state.
Changes to this field will propagate to existing Deployments and StatefulSets.
TODO(alberto): add additional validation for the map key/values.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tolerations when specified, define what custom tolerations are added to the hcp pods.</p>
</td>
</tr>
<tr>
<td>
<code>labels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>labels when specified, define what custom labels are added to the hcp pods.
Changing this day 2 will cause a rollout of all hcp pods.
Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
<p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=&ldquo;label key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (<em>), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)&rdquo;
-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=&ldquo;label value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores (</em>) or dots (.), and begin and end with an alphanumeric character&rdquo;
TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p>
</td>
</tr>
<tr>
<td>
<code>capabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">
Capabilities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capabilities allows for disabling optional components at cluster install time.
This field is optional and once set cannot be changed.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">
HostedClusterStatus
</a>
</em>
</td>
<td>
<p>Status is the latest observed status of the HostedCluster.</p>
</td>
</tr>
</tbody>
</table>
##NodePool { #hypershift.openshift.io/v1beta1.NodePool }
<p>
<p>NodePool is a scalable set of worker nodes attached to a HostedCluster.
NodePool machine architectures are uniform within a given pool, and are
independent of the control planeâ€™s underlying machine architecture.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>NodePool</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">
NodePoolSpec
</a>
</em>
</td>
<td>
<p>Spec is the desired behavior of the NodePool.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>clusterName</code></br>
<em>
string
</em>
</td>
<td>
<p>clusterName is the name of the HostedCluster this NodePool belongs to.
If a HostedCluster with this name doesn&rsquo;t exist, the controller will no-op until it exists.</p>
</td>
</tr>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the OCP release used for the NodePool. This informs the
ignition configuration for machines which includes the kubelet version, as well as other platform specific
machine properties (e.g. an AMI on the AWS platform).
It&rsquo;s not supported to use a release in a NodePool which minor version skew against the Control Plane release is bigger than N-2. Although there&rsquo;s no enforcement that prevents this from happening.
Attempting to use a release with a bigger skew might result in unpredictable behaviour.
Attempting to use a release higher than the HosterCluster one will result in the NodePool being degraded and the ValidReleaseImage condition being false.
Attempting to use a release lower than the current NodePool y-stream will result in the NodePool being degraded and the ValidReleaseImage condition being false.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">
NodePoolPlatform
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the NodePool
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>replicas is the desired number of nodes the pool should maintain. If unset, the controller default value is 0.
replicas is mutually exclusive with autoscaling. If autoscaling is configured, replicas must be omitted and autoscaling will control the NodePool size internally.</p>
</td>
</tr>
<tr>
<td>
<code>management</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">
NodePoolManagement
</a>
</em>
</td>
<td>
<p>management specifies behavior for managing nodes in the pool, such as
upgrade strategies and auto-repair behaviors.</p>
</td>
</tr>
<tr>
<td>
<code>autoScaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolAutoScaling">
NodePoolAutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior for the NodePool.
autoscaling is mutually exclusive with replicas. If replicas is set, this field must be omitted.</p>
</td>
</tr>
<tr>
<td>
<code>config</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>config is a list of references to ConfigMaps containing serialized
MachineConfig resources to be injected into the ignition configurations of
nodes in the NodePool. The MachineConfig API schema is defined here:</p>
<p><a href="https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185">https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</a></p>
<p>Each ConfigMap must have a single key named &ldquo;config&rdquo; whose value is the YML
with one or more serialized machineconfiguration.openshift.io resources:</p>
<ul>
<li>KubeletConfig</li>
<li>ContainerRuntimeConfig</li>
<li>MachineConfig</li>
<li>ClusterImagePolicy</li>
<li>ImageContentSourcePolicy</li>
<li>ImageDigestMirrorSet</li>
</ul>
<p>This is validated in the backend and signaled back via validMachineConfig condition.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeDrainTimeout is the maximum amount of time that the controller will spend on retrying to drain a node until it succeeds.
The default value is 0, meaning that the node can retry drain without any time limitations.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeVolumeDetachTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node.
The default value is 0, meaning that the volumes will be detached from the node without any time limitations.
After the timeout, any remaining attached volumes will be ignored and the removal of the machine will continue.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeLabels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeLabels propagates a list of labels to Nodes, only once on creation.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
</td>
</tr>
<tr>
<td>
<code>taints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Taint">
[]Taint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>taints if specified, propagates a list of taints to Nodes, only once on creation.
These taints are additive to the ones applied by other controllers</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the NodePool controller. Resulting in any change to the NodePool being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>tuningConfig is a list of references to ConfigMaps containing serialized
Tuned or PerformanceProfile resources to define the tuning configuration to be applied to
nodes in the NodePool. The Tuned API is defined here:</p>
<p><a href="https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go">https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</a></p>
<p>The PerformanceProfile API is defined here:
<a href="https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2">https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</a></p>
<p>Each ConfigMap must have a single key named &ldquo;tuning&rdquo; whose value is the
JSON or YAML of a serialized Tuned or PerformanceProfile.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>arch</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>arch is the preferred processor architecture for the NodePool. Different platforms might have different supported architectures.
TODO: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery:
<a href="https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215">https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</a>
TODO Add s390x to enum validation once the architecture is supported</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">
NodePoolStatus
</a>
</em>
</td>
<td>
<p>Status is the latest observed status of the NodePool.</p>
</td>
</tr>
</tbody>
</table>
###AESCBCSpec { #hypershift.openshift.io/v1beta1.AESCBCSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>AESCBCSpec defines metadata about the AESCBC secret encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ActiveKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>BackupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
</tbody>
</table>
###APIServerNetworking { #hypershift.openshift.io/v1beta1.APIServerNetworking }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>APIServerNetworking specifies how the APIServer is exposed inside a cluster
node.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>advertiseAddress</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>advertiseAddress is the address that pods within the nodes will use to talk to the API
server. This is an address associated with the loopback adapter of each
node. If not specified, the controller will take default values.
The default values will be set as 172.20.0.1 or fd00::1.
This value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>port</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>port is the port at which the APIServer is exposed inside a node. Other
pods using host networking cannot listen on this port.
If omitted 6443 is used.
This is useful to choose a port other than the default one which might interfere with customer environments e.g. <a href="https://github.com/openshift/hypershift/pull/356">https://github.com/openshift/hypershift/pull/356</a>.
Setting this to 443 is possible only for backward compatibility reasons and it&rsquo;s discouraged.
Doing so, it would result in the controller overriding the KAS endpoint in the guest cluster having a discrepancy with the KAS Pod and potentially causing temporarily network failures.
This value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>allowedCIDRBlocks</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CIDRBlock">
[]CIDRBlock
</a>
</em>
</td>
<td>
<p>allowedCIDRBlocks is an allow list of CIDR blocks that can access the APIServer
If not specified, traffic is allowed from all addresses.
This depends on underlying support by the cloud provider for Service LoadBalancerSourceRanges</p>
</td>
</tr>
</tbody>
</table>
###AWSCloudProviderConfig { #hypershift.openshift.io/v1beta1.AWSCloudProviderConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSCloudProviderConfig specifies AWS networking configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is the subnet to use for control plane cloud resources.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Zone is the availability zone where control plane cloud resources are
created.</p>
</td>
</tr>
<tr>
<td>
<code>vpc</code></br>
<em>
string
</em>
</td>
<td>
<p>VPC is the VPC to use for control plane cloud resources.</p>
</td>
</tr>
</tbody>
</table>
###AWSEndpointAccessType { #hypershift.openshift.io/v1beta1.AWSEndpointAccessType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSEndpointAccessType specifies the publishing scope of cluster endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Private&#34;</p></td>
<td><p>Private endpoint access allows only private API server access and private
node communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;Public&#34;</p></td>
<td><p>Public endpoint access allows public API server access and public node
communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;PublicAndPrivate&#34;</p></td>
<td><p>PublicAndPrivate endpoint access allows public API server access and
private node communication with the control plane.</p>
</td>
</tr></tbody>
</table>
###AWSKMSAuthSpec { #hypershift.openshift.io/v1beta1.AWSKMSAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">AWSKMSSpec</a>)
</p>
<p>
<p>AWSKMSAuthSpec defines metadata about the management of credentials used to interact and encrypt data via AWS KMS key.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>awsKms</code></br>
<em>
string
</em>
</td>
<td>
<p>The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]
}</p>
<p>AWSKMSARN is an ARN value referencing a role appropriate for managing the auth via the AWS KMS key.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Encrypt&rdquo;,
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt<em>&rdquo;,
&ldquo;kms:GenerateDataKey</em>&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: %q
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AWSKMSKeyEntry { #hypershift.openshift.io/v1beta1.AWSKMSKeyEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">AWSKMSSpec</a>)
</p>
<p>
<p>AWSKMSKeyEntry defines metadata to locate the encryption key in AWS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code></br>
<em>
string
</em>
</td>
<td>
<p>ARN is the Amazon Resource Name for the encryption key</p>
</td>
</tr>
</tbody>
</table>
###AWSKMSSpec { #hypershift.openshift.io/v1beta1.AWSKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>AWSKMSSpec defines metadata about the configuration of the AWS KMS Secret Encryption provider</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region contains the AWS region</p>
</td>
</tr>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry">
AWSKMSKeyEntry
</a>
</em>
</td>
<td>
<p>ActiveKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry">
AWSKMSKeyEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>BackupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
<tr>
<td>
<code>auth</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSAuthSpec">
AWSKMSAuthSpec
</a>
</em>
</td>
<td>
<p>Auth defines metadata about the management of credentials used to interact with AWS KMS</p>
</td>
</tr>
</tbody>
</table>
###AWSNodePoolPlatform { #hypershift.openshift.io/v1beta1.AWSNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AWSNodePoolPlatform specifies the configuration of a NodePool when operating
on AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceType</code></br>
<em>
string
</em>
</td>
<td>
<p>InstanceType is an ec2 instance type for node instances (e.g. m5.large).</p>
</td>
</tr>
<tr>
<td>
<code>instanceProfile</code></br>
<em>
string
</em>
</td>
<td>
<p>InstanceProfile is the AWS EC2 instance profile, which is a container for an IAM role that the EC2 instance uses.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<p>Subnet is the subnet to use for node instances.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMI is the image id to use for node instances. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroups</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroups is an optional set of security groups to associate with node
instances.</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume specifies configuration for the root volume of node instances.</p>
</td>
</tr>
<tr>
<td>
<code>resourceTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceTag">
[]AWSResourceTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ResourceTags is an optional list of additional tags to apply to AWS node
instances.</p>
<p>These will be merged with HostedCluster scoped tags, and HostedCluster tags
take precedence in case of conflicts.</p>
<p>See <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html</a> for
information on tagging AWS resources. AWS supports a maximum of 50 tags per
resource. OpenShift reserves 25 tags for its use, leaving 25 tags available
for the user.</p>
</td>
</tr>
<tr>
<td>
<code>placement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlacementOptions">
PlacementOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>placement specifies the placement options for the EC2 instances.</p>
</td>
</tr>
</tbody>
</table>
###AWSPlatformSpec { #hypershift.openshift.io/v1beta1.AWSPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AWSPlatformSpec specifies configuration for clusters running on Amazon Web Services.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the AWS region in which the cluster resides. This configures the
OCP control plane cloud integrations, and is used by NodePool to resolve
the correct boot AMI for a given release.</p>
</td>
</tr>
<tr>
<td>
<code>cloudProviderConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig">
AWSCloudProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudProviderConfig specifies AWS networking configuration for the control
plane.
This is mainly used for cloud provider controller config:
<a href="https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364">https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364</a>
TODO(dan): should this be named AWSNetworkConfig?</p>
</td>
</tr>
<tr>
<td>
<code>serviceEndpoints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSServiceEndpoint">
[]AWSServiceEndpoint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceEndpoints specifies optional custom endpoints which will override
the default service endpoint of specific AWS Services.</p>
<p>There must be only one ServiceEndpoint for a given service name.</p>
</td>
</tr>
<tr>
<td>
<code>rolesRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSRolesRef">
AWSRolesRef
</a>
</em>
</td>
<td>
<p>RolesRef contains references to various AWS IAM roles required to enable
integrations such as OIDC.</p>
</td>
</tr>
<tr>
<td>
<code>resourceTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceTag">
[]AWSResourceTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ResourceTags is a list of additional tags to apply to AWS resources created
for the cluster. See
<a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html</a> for
information on tagging AWS resources. AWS supports a maximum of 50 tags per
resource. OpenShift reserves 25 tags for its use, leaving 25 tags available
for the user.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSEndpointAccessType">
AWSEndpointAccessType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EndpointAccess specifies the publishing scope of cluster endpoints. The
default is Public.</p>
</td>
</tr>
<tr>
<td>
<code>additionalAllowedPrincipals</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalAllowedPrincipals specifies a list of additional allowed principal ARNs
to be added to the hosted control plane&rsquo;s VPC Endpoint Service to enable additional
VPC Endpoint connection requests to be automatically accepted.
See <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html">https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html</a>
for more details around VPC Endpoint Service allowed principals.</p>
</td>
</tr>
<tr>
<td>
<code>multiArch</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>MultiArch specifies whether the Hosted Cluster will be expected to support NodePools with different
CPU architectures, i.e., supporting arm64 NodePools and supporting amd64 NodePools on the same Hosted Cluster.
Deprecated: This field is no longer used. The HyperShift Operator now performs multi-arch validations
automatically despite the platform type. The HyperShift Operator will set HostedCluster.Status.PayloadArch based
on the HostedCluster release image. This field is used by the NodePool controller to validate the
NodePool.Spec.Arch is supported.</p>
</td>
</tr>
<tr>
<td>
<code>sharedVPC</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPC">
AWSSharedVPC
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SharedVPC contains fields that must be specified if the HostedCluster must use a VPC that is
created in a different AWS account and is shared with the AWS account where the HostedCluster
will be created.</p>
</td>
</tr>
</tbody>
</table>
###AWSPlatformStatus { #hypershift.openshift.io/v1beta1.AWSPlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">PlatformStatus</a>)
</p>
<p>
<p>AWSPlatformStatus contains status specific to the AWS platform</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>defaultWorkerSecurityGroupID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DefaultWorkerSecurityGroupID is the ID of a security group created by
the control plane operator. It is always added to worker machines in
addition to any security groups specified in the NodePool.</p>
</td>
</tr>
</tbody>
</table>
###AWSResourceReference { #hypershift.openshift.io/v1beta1.AWSResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig">AWSCloudProviderConfig</a>, 
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>AWSResourceReference is a reference to a specific AWS resource by ID or filters.
Only one of ID or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>filters</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Filter">
[]Filter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filters is a set of key/value pairs used to identify a resource
They are applied according to the rules defined by the AWS API:
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html</a></p>
</td>
</tr>
</tbody>
</table>
###AWSResourceTag { #hypershift.openshift.io/v1beta1.AWSResourceTag }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSResourceTag is a tag to apply to AWS resources created for the cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>Key is the key of the tag.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<p>Value is the value of the tag.</p>
<p>Some AWS service do not support empty values. Since tags are added to
resources in many services, the length of the tag value must meet the
requirements of all services.</p>
</td>
</tr>
</tbody>
</table>
###AWSRoleCredentials { #hypershift.openshift.io/v1beta1.AWSRoleCredentials }
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>namespace</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
###AWSRolesRef { #hypershift.openshift.io/v1beta1.AWSRolesRef }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSRolesRef contains references to various AWS IAM roles required for operators to make calls against the AWS API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code></br>
<em>
string
</em>
</td>
<td>
<p>The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]
}</p>
<p>IngressARN is an ARN value referencing a role appropriate for the Ingress Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:aws:route53:::PUBLIC_ZONE_ID&rdquo;,
&ldquo;arn:aws:route53:::PRIVATE_ZONE_ID&rdquo;
]
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ImageRegistryARN is an ARN value referencing a role appropriate for the Image Registry Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;s3:CreateBucket&rdquo;,
&ldquo;s3:DeleteBucket&rdquo;,
&ldquo;s3:PutBucketTagging&rdquo;,
&ldquo;s3:GetBucketTagging&rdquo;,
&ldquo;s3:PutBucketPublicAccessBlock&rdquo;,
&ldquo;s3:GetBucketPublicAccessBlock&rdquo;,
&ldquo;s3:PutEncryptionConfiguration&rdquo;,
&ldquo;s3:GetEncryptionConfiguration&rdquo;,
&ldquo;s3:PutLifecycleConfiguration&rdquo;,
&ldquo;s3:GetLifecycleConfiguration&rdquo;,
&ldquo;s3:GetBucketLocation&rdquo;,
&ldquo;s3:ListBucket&rdquo;,
&ldquo;s3:GetObject&rdquo;,
&ldquo;s3:PutObject&rdquo;,
&ldquo;s3:DeleteObject&rdquo;,
&ldquo;s3:ListBucketMultipartUploads&rdquo;,
&ldquo;s3:AbortMultipartUpload&rdquo;,
&ldquo;s3:ListMultipartUploadParts&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>storageARN</code></br>
<em>
string
</em>
</td>
<td>
<p>StorageARN is an ARN value referencing a role appropriate for the Storage Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:CreateSnapshot&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:DeleteSnapshot&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeSnapshots&rdquo;,
&ldquo;ec2:DescribeTags&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DescribeVolumesModifications&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>networkARN</code></br>
<em>
string
</em>
</td>
<td>
<p>NetworkARN is an ARN value referencing a role appropriate for the Network Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInstanceStatus&rdquo;,
&ldquo;ec2:DescribeInstanceTypes&rdquo;,
&ldquo;ec2:UnassignPrivateIpAddresses&rdquo;,
&ldquo;ec2:AssignPrivateIpAddresses&rdquo;,
&ldquo;ec2:UnassignIpv6Addresses&rdquo;,
&ldquo;ec2:AssignIpv6Addresses&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerARN</code></br>
<em>
string
</em>
</td>
<td>
<p>KubeCloudControllerARN is an ARN value referencing a role appropriate for the KCM/KCC.
Source: <a href="https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies">https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies</a></p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;autoscaling:DescribeAutoScalingGroups&rdquo;,
&ldquo;autoscaling:DescribeLaunchConfigurations&rdquo;,
&ldquo;autoscaling:DescribeTags&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeRegions&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;,
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:DeleteRoute&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:AttachLoadBalancerToSubnets&rdquo;,
&ldquo;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerPolicy&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:ConfigureHealthCheck&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:DetachLoadBalancerFromSubnets&rdquo;,
&ldquo;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:ModifyLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:RegisterInstancesWithLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:CreateListener&rdquo;,
&ldquo;elasticloadbalancing:CreateTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeleteListener&rdquo;,
&ldquo;elasticloadbalancing:DeleteTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeregisterTargets&rdquo;,
&ldquo;elasticloadbalancing:DescribeListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerPolicies&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetGroups&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetHealth&rdquo;,
&ldquo;elasticloadbalancing:ModifyListener&rdquo;,
&ldquo;elasticloadbalancing:ModifyTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:RegisterTargets&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&rdquo;,
&ldquo;iam:CreateServiceLinkedRole&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;*&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementARN</code></br>
<em>
string
</em>
</td>
<td>
<p>NodePoolManagementARN is an ARN value referencing a role appropriate for the CAPI Controller.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;ec2:AssociateRouteTable&rdquo;,
&ldquo;ec2:AttachInternetGateway&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateInternetGateway&rdquo;,
&ldquo;ec2:CreateNatGateway&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:CreateRouteTable&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateSubnet&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:DeleteInternetGateway&rdquo;,
&ldquo;ec2:DeleteNatGateway&rdquo;,
&ldquo;ec2:DeleteRouteTable&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteSubnet&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DescribeAccountAttributes&rdquo;,
&ldquo;ec2:DescribeAddresses&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInternetGateways&rdquo;,
&ldquo;ec2:DescribeNatGateways&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;ec2:DescribeVpcAttribute&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DetachInternetGateway&rdquo;,
&ldquo;ec2:DisassociateRouteTable&rdquo;,
&ldquo;ec2:DisassociateAddress&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:ModifySubnetAttribute&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RunInstances&rdquo;,
&ldquo;ec2:TerminateInstances&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;ec2:CreateLaunchTemplate&rdquo;,
&ldquo;ec2:CreateLaunchTemplateVersion&rdquo;,
&ldquo;ec2:DescribeLaunchTemplates&rdquo;,
&ldquo;ec2:DescribeLaunchTemplateVersions&rdquo;,
&ldquo;ec2:DeleteLaunchTemplate&rdquo;,
&ldquo;ec2:DeleteLaunchTemplateVersions&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;<em>&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Condition&rdquo;: {
&ldquo;StringLike&rdquo;: {
&ldquo;iam:AWSServiceName&rdquo;: &ldquo;elasticloadbalancing.amazonaws.com&rdquo;
}
},
&ldquo;Action&rdquo;: [
&ldquo;iam:CreateServiceLinkedRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Action&rdquo;: [
&ldquo;iam:PassRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/</em>-worker-role&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt&rdquo;,
&ldquo;kms:GenerateDataKeyWithoutPlainText&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;<em>&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:CreateGrant&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;</em>&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;Bool&rdquo;: {
&ldquo;kms:GrantIsForAWSResource&rdquo;: true
}
}
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneOperatorARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneOperatorARN  is an ARN value referencing a role appropriate for the Control Plane Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;arn:aws:route53:::%s&rdquo;
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AWSServiceEndpoint { #hypershift.openshift.io/v1beta1.AWSServiceEndpoint }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSServiceEndpoint stores the configuration for services to
override existing defaults of AWS Services.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the AWS service.
This must be provided and cannot be empty.</p>
</td>
</tr>
<tr>
<td>
<code>url</code></br>
<em>
string
</em>
</td>
<td>
<p>URL is fully qualified URI with scheme https, that overrides the default generated
endpoint for a client.
This must be provided and cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###AWSSharedVPC { #hypershift.openshift.io/v1beta1.AWSSharedVPC }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSSharedVPC contains fields needed to create a HostedCluster using a VPC that has been
created and shared from a different AWS account than the AWS account where the cluster
is getting created.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rolesRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef">
AWSSharedVPCRolesRef
</a>
</em>
</td>
<td>
<p>RolesRef contains references to roles in the VPC owner account that enable a
HostedCluster on a shared VPC.</p>
</td>
</tr>
<tr>
<td>
<code>localZoneID</code></br>
<em>
string
</em>
</td>
<td>
<p>LocalZoneID is the ID of the route53 hosted zone for [cluster-name].hypershift.local that is
associated with the HostedCluster&rsquo;s VPC and exists in the VPC owner account.</p>
</td>
</tr>
</tbody>
</table>
###AWSSharedVPCRolesRef { #hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPC">AWSSharedVPC</a>)
</p>
<p>
<p>AWSSharedVPCRolesRef contains references to AWS IAM roles required for a shared VPC hosted cluster.
These roles must exist in the VPC owner&rsquo;s account.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code></br>
<em>
string
</em>
</td>
<td>
<p>IngressARN is an ARN value referencing the role in the VPC owner account that allows the
ingress operator in the cluster account to create and manage records in the private DNS
hosted zone.</p>
<p>The referenced role must have a trust relationship that allows it to be assumed by the
ingress operator role in the VPC creator account.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Sid&rdquo;: &ldquo;Statement1&rdquo;,
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;AWS&rdquo;: &ldquo;arn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-openshift-ingress&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRole&rdquo;
}
]
}</p>
<p>The following is an example of the policy document for this role.
(Based on <a href="https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config">https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config</a>)</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;route53:ListHostedZonesByName&rdquo;,
&ldquo;route53:ChangeTagsForResource&rdquo;,
&ldquo;route53:GetAccountLimit&rdquo;,
&ldquo;route53:GetChange&rdquo;,
&ldquo;route53:GetHostedZone&rdquo;,
&ldquo;route53:ListTagsForResource&rdquo;,
&ldquo;route53:UpdateHostedZoneComment&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;tag:UntagResources&rdquo;
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneARN is an ARN value referencing the role in the VPC owner account that allows
the control plane operator in the cluster account to create and manage a VPC endpoint, its
corresponding Security Group, and DNS records in the hypershift local hosted zone.</p>
<p>The referenced role must have a trust relationship that allows it to be assumed by the
control plane operator role in the VPC creator account.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Sid&rdquo;: &ldquo;Statement1&rdquo;,
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;AWS&rdquo;: &ldquo;arn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-control-plane-operator&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRole&rdquo;
}
]
}</p>
<p>The following is an example of the policy document for this role.</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AddressPair { #hypershift.openshift.io/v1beta1.AddressPair }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PortSpec">PortSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ipAddress</code></br>
<em>
string
</em>
</td>
<td>
<p>IPAddress is the IP address of the allowed address pair. Depending on
the configuration of Neutron, it may be supported to specify a CIDR
instead of a specific IP address.</p>
</td>
</tr>
</tbody>
</table>
###AgentNodePoolPlatform { #hypershift.openshift.io/v1beta1.AgentNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AgentNodePoolPlatform specifies the configuration of a NodePool when operating
on the Agent platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>agentLabelSelector</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AgentLabelSelector contains labels that must be set on an Agent in order to
be selected for a Machine.</p>
</td>
</tr>
</tbody>
</table>
###AgentPlatformSpec { #hypershift.openshift.io/v1beta1.AgentPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AgentPlatformSpec specifies configuration for agent-based installations.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>agentNamespace</code></br>
<em>
string
</em>
</td>
<td>
<p>AgentNamespace is the namespace where to search for Agents for this cluster</p>
</td>
</tr>
</tbody>
</table>
###AllocationPool { #hypershift.openshift.io/v1beta1.AllocationPool }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SubnetSpec">SubnetSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>start</code></br>
<em>
string
</em>
</td>
<td>
<p>Start represents the start of the AllocationPool, that is the lowest IP of the pool.</p>
</td>
</tr>
<tr>
<td>
<code>end</code></br>
<em>
string
</em>
</td>
<td>
<p>End represents the end of the AlloctionPool, that is the highest IP of the pool.</p>
</td>
</tr>
</tbody>
</table>
###AutoNode { #hypershift.openshift.io/v1beta1.AutoNode }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>We expose here internal configuration knobs that won&rsquo;t be exposed to the service.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provisionerConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ProvisionerConfig">
ProvisionerConfig
</a>
</em>
</td>
<td>
<p>provisioner is the implementation used for Node auto provisioning.</p>
</td>
</tr>
</tbody>
</table>
###AvailabilityPolicy { #hypershift.openshift.io/v1beta1.AvailabilityPolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>availabilityPolicy specifies a high level availability policy for components.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;HighlyAvailable&#34;</p></td>
<td><p>HighlyAvailable means components should be resilient to problems across
fault boundaries as defined by the component to which the policy is
attached. This usually means running critical workloads with 3 replicas and
with little or no toleration of disruption of the component.</p>
</td>
</tr><tr><td><p>&#34;SingleReplica&#34;</p></td>
<td><p>SingleReplica means components are not expected to be resilient to problems
across most fault boundaries associated with high availability. This
usually means running critical workloads with just 1 replica and with
toleration of full disruption of the component.</p>
</td>
</tr></tbody>
</table>
###AzureDiagnosticsStorageAccountType { #hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">Diagnostics</a>)
</p>
<p>
<p>AzureDiagnosticsStorageAccountType specifies the type of storage account for storing Azure VM diagnostics data.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disabled&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Managed&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;UserManaged&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###AzureDiskPersistence { #hypershift.openshift.io/v1beta1.AzureDiskPersistence }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk">AzureNodePoolOSDisk</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Ephemeral&#34;</p></td>
<td><p>EphemeralDiskPersistence is the ephemeral disk type.</p>
</td>
</tr><tr><td><p>&#34;Persistent&#34;</p></td>
<td><p>PersistentDiskPersistence is the persistent disk type.</p>
</td>
</tr></tbody>
</table>
###AzureDiskStorageAccountType { #hypershift.openshift.io/v1beta1.AzureDiskStorageAccountType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk">AzureNodePoolOSDisk</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Premium_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesPremiumLRS - Premium SSD locally redundant storage. Best for production and performance sensitive
workloads.</p>
</td>
</tr><tr><td><p>&#34;PremiumV2_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesPremiumV2LRS - Premium SSD v2 locally redundant storage. Best for production and performance-sensitive
workloads that consistently require low latency and high IOPS and throughput.</p>
</td>
</tr><tr><td><p>&#34;Standard_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesStandardLRS - Standard HDD locally redundant storage. Best for backup, non-critical, and infrequent
access.</p>
</td>
</tr><tr><td><p>&#34;StandardSSD_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesStandardSSDLRS - Standard SSD locally redundant storage. Best for web servers, lightly used enterprise
applications and dev/test.</p>
</td>
</tr><tr><td><p>&#34;UltraSSD_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesUltraSSDLRS - Ultra SSD locally redundant storage. Best for IO-intensive workloads such as SAP HANA,
top tier databases (for example, SQL, Oracle), and other transaction-heavy workloads.</p>
</td>
</tr></tbody>
</table>
###AzureKMSKey { #hypershift.openshift.io/v1beta1.AzureKMSKey }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">AzureKMSSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>keyVaultName</code></br>
<em>
string
</em>
</td>
<td>
<p>KeyVaultName is the name of the keyvault. Must match criteria specified at <a href="https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name">https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name</a>
Your Microsoft Entra application used to create the cluster must be authorized to access this keyvault, e.g using the AzureCLI:
<code>az keyvault set-policy -n $KEYVAULT_NAME --key-permissions decrypt encrypt --spn &lt;YOUR APPLICATION CLIENT ID&gt;</code></p>
</td>
</tr>
<tr>
<td>
<code>keyName</code></br>
<em>
string
</em>
</td>
<td>
<p>KeyName is the name of the keyvault key used for encrypt/decrypt</p>
</td>
</tr>
<tr>
<td>
<code>keyVersion</code></br>
<em>
string
</em>
</td>
<td>
<p>KeyVersion contains the version of the key to use</p>
</td>
</tr>
</tbody>
</table>
###AzureKMSSpec { #hypershift.openshift.io/v1beta1.AzureKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>AzureKMSSpec defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSKey">
AzureKMSKey
</a>
</em>
</td>
<td>
<p>ActiveKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSKey">
AzureKMSKey
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>BackupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
<tr>
<td>
<code>kms</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>kms is a pre-existing managed identity used to authenticate with Azure KMS.</p>
</td>
</tr>
</tbody>
</table>
###AzureMarketplaceImage { #hypershift.openshift.io/v1beta1.AzureMarketplaceImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">AzureVMImage</a>)
</p>
<p>
<p>AzureMarketplaceImage specifies the information needed to create an Azure VM from an Azure Marketplace image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>publisher</code></br>
<em>
string
</em>
</td>
<td>
<p>publisher is the name of the organization that created the image.
It must be between 3 and 50 characters in length, and consist of only lowercase letters, numbers, and hyphens (-) and underscores (_).
It must start with a lowercase letter or a number.
TODO: Can we explain where a user might find this value, or provide an example of one they might want to use</p>
</td>
</tr>
<tr>
<td>
<code>offer</code></br>
<em>
string
</em>
</td>
<td>
<p>offer specifies the name of a group of related images created by the publisher.
TODO: What is the valid character set for this field? What about minimum and maximum lengths?</p>
</td>
</tr>
<tr>
<td>
<code>sku</code></br>
<em>
string
</em>
</td>
<td>
<p>sku specifies an instance of an offer, such as a major release of a distribution.
For example, 22<em>04-lts-gen2, 8-lvm-gen2.
The value must consist only of lowercase letters, numbers, and hyphens (-) and underscores (</em>).
TODO: What about length limits?</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<p>version specifies the version of an image sku. The allowed formats are Major.Minor.Build or &lsquo;latest&rsquo;. Major,
Minor, and Build are decimal numbers, e.g. &lsquo;1.2.0&rsquo;. Specify &lsquo;latest&rsquo; to use the latest version of an image available at
deployment time. Even if you use &lsquo;latest&rsquo;, the VM image will not automatically update after deploy time even if a
new version becomes available.</p>
</td>
</tr>
</tbody>
</table>
###AzureNodePoolOSDisk { #hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>sizeGiB</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>SizeGiB is the size in GiB (1024^3 bytes) to assign to the OS disk.
This should be between 16 and 65,536 when using the UltraSSD_LRS storage account type and between 16 and 32,767 when using any other storage account type.
When not set, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.
The current default is 30.</p>
</td>
</tr>
<tr>
<td>
<code>diskStorageAccountType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiskStorageAccountType">
AzureDiskStorageAccountType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>diskStorageAccountType is the disk storage account type to use.
Valid values are Premium_LRS, PremiumV2_LRS, Standard_LRS, StandardSSD_LRS, UltraSSD_LRS.
Note that Standard means a HDD.
The disk performance is tied to the disk type, please refer to the Azure documentation for further details
<a href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison">https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison</a>.
When omitted this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.
The current default is Premium SSD LRS.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionSetID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>encryptionSetID is the ID of the DiskEncryptionSet resource to use to encrypt the OS disks for the VMs.
Configuring a DiskEncyptionSet allows greater control over the encryption of the VM OS disk at rest.
Can be used with either platform (Azure) managed, or customer managed encryption keys.
This needs to exist in the same subscription id listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.SubscriptionID.
DiskEncryptionSetID should also exist in a resource group under the same subscription id and the same location
listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.Location.
The encryptionSetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Copmute/diskEncryptionSets/{resourceName}</code>.
The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12.
The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character.
The resourceName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores.
TODO: Are there other encryption related options we may want to expose, should this be in a struct as well?</p>
</td>
</tr>
<tr>
<td>
<code>persistence</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiskPersistence">
AzureDiskPersistence
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>persistence determines whether the OS disk should be persisted beyond the life of the VM.
Valid values are Persistent and Ephemeral.
When set to Ephmeral, the OS disk will not be persisted to Azure storage and implies restrictions to the VM size and caching type.
Full details can be found in the Azure documentation <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ephemeral-os-disks">https://learn.microsoft.com/en-us/azure/virtual-machines/ephemeral-os-disks</a>.
Ephmeral disks are primarily used for stateless applications, provide lower latency than Persistent disks and also incur no storage costs.
When not set, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.</p>
</td>
</tr>
</tbody>
</table>
###AzureNodePoolPlatform { #hypershift.openshift.io/v1beta1.AzureNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AzureNodePoolPlatform is the platform specific configuration for an Azure node pool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>vmSize</code></br>
<em>
string
</em>
</td>
<td>
<p>vmSize is the Azure VM instance type to use for the nodes being created in the nodepool.
The size naming convention is documented here <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions">https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions</a>.
Size names should start with a Family name, which is represented by one of more capital letters, and then be followed by the CPU count.
This is followed by 0 or more additional features, represented by a, b, d, i, l, m, p, t, s, C, and NP, refer to the Azure documentation for an explanation of these features.
Optionally an accelerator such as a GPU can be added, prefixed by an underscore, for example A100, H100 or MI300X.
The size may also be versioned, in which case it should be suffixed with _v<version> where the version is a number.
For example, &ldquo;D32ads_v5&rdquo; would be a suitable general purpose VM size, or &ldquo;ND96_MI300X_v5&rdquo; would represent a GPU accelerated VM.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">
AzureVMImage
</a>
</em>
</td>
<td>
<p>image is used to configure the VM boot image. If unset, the default image at the location below will be used and
is expected to exist: subscription/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Compute/images/rhcos.x86_64.vhd.
The <subscriptionID> and the <resourceGroupName> are expected to be the same resource group documented in the
Hosted Cluster specification respectively, HostedCluster.Spec.Platform.Azure.SubscriptionID and
HostedCluster.Spec.Platform.Azure.ResourceGroupName.</p>
</td>
</tr>
<tr>
<td>
<code>osDisk</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk">
AzureNodePoolOSDisk
</a>
</em>
</td>
<td>
<p>osDisk provides configuration for the OS disk for the nodepool.
This can be used to configure the size, storage account type, encryption options and whether the disk is persistent or ephemeral.
When not provided, the platform will choose reasonable defaults which are subject to change over time.
Review the fields within the osDisk for more details.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>availabilityZone is the failure domain identifier where the VM should be attached to. This must not be specified
for clusters in a location that does not support AvailabilityZone because it would cause a failure from Azure API.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionAtHost</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>encryptionAtHost enables encryption at host on virtual machines. According to Microsoft documentation, this
means data stored on the VM host is encrypted at rest and flows encrypted to the Storage service. See
<a href="https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-host-based-encryption-portal?tabs=azure-powershell">https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-host-based-encryption-portal?tabs=azure-powershell</a>
for more information.</p>
</td>
</tr>
<tr>
<td>
<code>subnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>subnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a
different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must
exist in the same network, HostedCluster.Spec.Platform.Azure.VnetID, and must exist under the same subscription ID,
HostedCluster.Spec.Platform.Azure.SubscriptionID.
subnetID is immutable once set.
The subnetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}</code>.
The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12.
The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character.
The vnetName should be between 2 and 64 characters, consisting only of alphanumeric characters, hyphens, underscores and periods and must not end with either a period (.) or hyphen (-) character.
The subnetName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores and must start with an alphanumeric character and must not end with a period (.) or hyphen (-) character.</p>
</td>
</tr>
<tr>
<td>
<code>diagnostics</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">
Diagnostics
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>diagnostics specifies the diagnostics settings for a virtual machine.
If not specified, then Boot diagnostics will be disabled.</p>
</td>
</tr>
</tbody>
</table>
###AzurePlatformSpec { #hypershift.openshift.io/v1beta1.AzurePlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AzurePlatformSpec specifies configuration for clusters running on Azure. Generally, the HyperShift API assumes bring
your own (BYO) cloud infrastructure resources. For example, resources like a resource group, a subnet, or a vnet
would be pre-created and then their names would be used respectively in the ResourceGroupName, SubnetName, VnetName
fields of the Hosted Cluster CR. An existing cloud resource is expected to exist under the same SubscriptionID.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cloud</code></br>
<em>
string
</em>
</td>
<td>
<p>Cloud is the cloud environment identifier, valid values could be found here: <a href="https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33">https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33</a></p>
</td>
</tr>
<tr>
<td>
<code>location</code></br>
<em>
string
</em>
</td>
<td>
<p>Location is the Azure region in where all the cloud infrastructure resources will be created.</p>
<p>Example: eastus</p>
</td>
</tr>
<tr>
<td>
<code>resourceGroup</code></br>
<em>
string
</em>
</td>
<td>
<p>ResourceGroupName is the name of an existing resource group where all cloud resources created by the Hosted
Cluster are to be placed. The resource group is expected to exist under the same subscription as SubscriptionID.</p>
<p>In ARO HCP, this will be the managed resource group where customer cloud resources will be created.</p>
<p>Resource group naming requirements can be found here: <a href="https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/">https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/</a>.</p>
<p>Example: if your resource group ID is /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>, your
ResourceGroupName is <resourceGroupName>.</p>
</td>
</tr>
<tr>
<td>
<code>vnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>VnetID is the ID of an existing VNET to use in creating VMs. The VNET can exist in a different resource group
other than the one specified in ResourceGroupName, but it must exist under the same subscription as
SubscriptionID.</p>
<p>In ARO HCP, this will be the ID of the customer provided VNET.</p>
<p>Example: /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Network/virtualNetworks/<vnetName></p>
</td>
</tr>
<tr>
<td>
<code>subnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>subnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a
different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must
exist in the same network, HostedCluster.Spec.Platform.Azure.VnetID, and must exist under the same subscription ID,
HostedCluster.Spec.Platform.Azure.SubscriptionID.
subnetID is immutable once set.
The subnetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}</code>.
The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12.
The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character.
The vnetName should be between 2 and 64 characters, consisting only of alphanumeric characters, hyphens, underscores and periods and must not end with either a period (.) or hyphen (-) character.
The subnetName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores and must start with an alphanumeric character and must not end with a period (.) or hyphen (-) character.</p>
</td>
</tr>
<tr>
<td>
<code>subscriptionID</code></br>
<em>
string
</em>
</td>
<td>
<p>SubscriptionID is a unique identifier for an Azure subscription used to manage resources.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupID</code></br>
<em>
string
</em>
</td>
<td>
<p>SecurityGroupID is the ID of an existing security group on the SubnetID. This field is provided as part of the
configuration for the Azure cloud provider, aka Azure cloud controller manager (CCM). This security group is
expected to exist under the same subscription as SubscriptionID.</p>
</td>
</tr>
<tr>
<td>
<code>managedIdentities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities">
AzureResourceManagedIdentities
</a>
</em>
</td>
<td>
<p>managedIdentities contains the managed identities needed for HCP control plane and data plane components that
authenticate with Azure&rsquo;s API.</p>
</td>
</tr>
<tr>
<td>
<code>tenantID</code></br>
<em>
string
</em>
</td>
<td>
<p>tenantID is a unique identifier for the tenant where Azure resources will be created and managed in.</p>
</td>
</tr>
</tbody>
</table>
###AzureResourceManagedIdentities { #hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzurePlatformSpec">AzurePlatformSpec</a>)
</p>
<p>
<p>AzureResourceManagedIdentities contains the managed identities needed for HCP control plane and data plane components
that authenticate with Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>controlPlane</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities">
ControlPlaneManagedIdentities
</a>
</em>
</td>
<td>
<p>controlPlane contains the client IDs of all the managed identities on the HCP control plane needing to
authenticate with Azure&rsquo;s API.</p>
</td>
</tr>
<tr>
<td>
<code>dataPlane</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DataPlaneManagedIdentities">
DataPlaneManagedIdentities
</a>
</em>
</td>
<td>
<p>dataPlane contains the client IDs of all the managed identities on the data plane needing to authenticate with
Azure&rsquo;s API.</p>
</td>
</tr>
</tbody>
</table>
###AzureVMImage { #hypershift.openshift.io/v1beta1.AzureVMImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
<p>AzureVMImage represents the different types of boot image sources that can be provided for an Azure VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImageType">
AzureVMImageType
</a>
</em>
</td>
<td>
<p>type is the type of image data that will be provided to the Azure VM.
Valid values are &ldquo;ImageID&rdquo; and &ldquo;AzureMarketplace&rdquo;.
ImageID means is used for legacy managed VM images. This is where the user uploads a VM image directly to their resource group.
AzureMarketplace means the VM will boot from an Azure Marketplace image.
Marketplace images are preconfigured and published by the OS vendors and may include preconfigured software for the VM.</p>
</td>
</tr>
<tr>
<td>
<code>imageID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageID is the Azure resource ID of a VHD image to use to boot the Azure VMs from.
TODO: What is the valid character set for this field? What about minimum and maximum lengths?</p>
</td>
</tr>
<tr>
<td>
<code>azureMarketplace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureMarketplaceImage">
AzureMarketplaceImage
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>azureMarketplace contains the Azure Marketplace image info to use to boot the Azure VMs from.</p>
</td>
</tr>
</tbody>
</table>
###AzureVMImageType { #hypershift.openshift.io/v1beta1.AzureVMImageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">AzureVMImage</a>)
</p>
<p>
<p>AzureVMImageType is used to specify the source of the Azure VM boot image.
Valid values are ImageID and AzureMarketplace.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AzureMarketplace&#34;</p></td>
<td><p>AzureMarketplace is used to specify the Azure Marketplace image info to use to boot the Azure VMs from.</p>
</td>
</tr><tr><td><p>&#34;ImageID&#34;</p></td>
<td><p>ImageID is the used to specify that an Azure resource ID of a VHD image is used to boot the Azure VMs from.</p>
</td>
</tr></tbody>
</table>
###CIDRBlock { #hypershift.openshift.io/v1beta1.CIDRBlock }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.APIServerNetworking">APIServerNetworking</a>)
</p>
<p>
</p>
###Capabilities { #hypershift.openshift.io/v1beta1.Capabilities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>capabilities allows disabling optional components at install time.
Once set, it cannot be changed.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disabledCapabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OptionalCapability">
[]OptionalCapability
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>disabledCapabilities when specified, sets the cluster version baselineCapabilitySet to None
and sets all additionalEnabledCapabilities BUT the ones supplied in disabledCapabilities.
This effectively disables that capability on the hosted cluster.</p>
<p>When this is not supplied, the cluster will use the DefaultCapabilitySet defined for the respective
OpenShift version.</p>
<p>Once set, this field cannot be changed.</p>
</td>
</tr>
</tbody>
</table>
###CertificateSigningRequestApprovalSpec { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval">CertificateSigningRequestApproval</a>)
</p>
<p>
<p>CertificateSigningRequestApprovalSpec defines the desired state of CertificateSigningRequestApproval</p>
</p>
###CertificateSigningRequestApprovalStatus { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval">CertificateSigningRequestApproval</a>)
</p>
<p>
<p>CertificateSigningRequestApprovalStatus defines the observed state of CertificateSigningRequestApproval</p>
</p>
###ClusterAutoscaling { #hypershift.openshift.io/v1beta1.ClusterAutoscaling }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterAutoscaling specifies auto-scaling behavior that applies to all
NodePools associated with a control plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxNodesTotal</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxNodesTotal is the maximum allowable number of nodes for the Autoscaler scale out to be operational.
The autoscaler will not grow the cluster beyond this number.
If omitted, the autoscaler will not have a maximum limit.
number.</p>
</td>
</tr>
<tr>
<td>
<code>maxPodGracePeriod</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxPodGracePeriod is the maximum seconds to wait for graceful pod
termination before scaling down a NodePool. The default is 600 seconds.</p>
</td>
</tr>
<tr>
<td>
<code>maxNodeProvisionTime</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxNodeProvisionTime is the maximum time to wait for node provisioning
before considering the provisioning to be unsuccessful, expressed as a Go
duration string. The default is 15 minutes.</p>
</td>
</tr>
<tr>
<td>
<code>podPriorityThreshold</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>podPriorityThreshold enables users to schedule &ldquo;best-effort&rdquo; pods, which
shouldn&rsquo;t trigger autoscaler actions, but only run when there are spare
resources available. The default is -10.</p>
<p>See the following for more details:
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption</a></p>
</td>
</tr>
</tbody>
</table>
###ClusterConfiguration { #hypershift.openshift.io/v1beta1.ClusterConfiguration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterConfiguration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
<p>The API for individual configuration items is at:
<a href="https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html">https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiServer</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.APIServerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIServer holds configuration (like serving certificates, client CA and CORS domains)
shared by all API servers in the system, among them especially kube-apiserver
and openshift-apiserver.</p>
</td>
</tr>
<tr>
<td>
<code>authentication</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.AuthenticationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Authentication specifies cluster-wide settings for authentication (like OAuth and
webhook token authenticators).</p>
</td>
</tr>
<tr>
<td>
<code>featureGate</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.FeatureGateSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>FeatureGate holds cluster-wide information about feature gates.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.ImageSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Image governs policies related to imagestream imports and runtime configuration
for external registries. It allows cluster admins to configure which registries
OpenShift is allowed to import images from, extra CA trust bundles for external
registries, and policies to block or allow registry hostnames.
When exposing OpenShift&rsquo;s image registry to the public, this also lets cluster
admins specify the external hostname.
Changing this value will trigger a rollout for all existing NodePools in the cluster.
TODO(alberto): elaborate why.</p>
</td>
</tr>
<tr>
<td>
<code>ingress</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.IngressSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ingress holds cluster-wide information about ingress, including the default ingress domain
used for routes.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.NetworkSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network holds cluster-wide information about the network. It is used to configure the desired network configuration, such as: IP address pools for services/pod IPs, network plugin, etc.
Please view network.spec for an explanation on what applies when configuring this resource.
TODO (csrwng): Add validation here to exclude changes that conflict with networking settings in the HostedCluster.Spec.Networking field.</p>
</td>
</tr>
<tr>
<td>
<code>oauth</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.OAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OAuth holds cluster-wide information about OAuth.
It is used to configure the integrated OAuth server.
This configuration is only honored when the top level Authentication config has type set to IntegratedOAuth.</p>
</td>
</tr>
<tr>
<td>
<code>operatorhub</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.OperatorHubSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OperatorHub specifies the configuration for the Operator Lifecycle Manager in the HostedCluster. This is only configured at deployment time but the controller are not reconcilling over it.
The OperatorHub configuration will be constantly reconciled if catalog placement is management, but only on cluster creation otherwise.</p>
</td>
</tr>
<tr>
<td>
<code>scheduler</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.SchedulerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scheduler holds cluster-wide config information to run the Kubernetes Scheduler
and influence its placement decisions. The canonical name for this config is <code>cluster</code>.</p>
</td>
</tr>
<tr>
<td>
<code>proxy</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.ProxySpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
<tr>
<td>
<code>node</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.NodeSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSpec holds cluster-wide details for the node configuration object.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworkEntry { #hypershift.openshift.io/v1beta1.ClusterNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>ClusterNetworkEntry is a single IP address block for pod IP blocks. IP blocks
are allocated with size 2^HostSubnetLength.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>cidr is the IP block address pool.</p>
</td>
</tr>
<tr>
<td>
<code>hostPrefix</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>hostPrefix is the prefix size to allocate to each node from the CIDR.
For example, 24 would allocate 2^(32-24)=2^8=256 addresses to each node. If this
field is not used by the plugin, it can be left unset.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworking { #hypershift.openshift.io/v1beta1.ClusterNetworking }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>clusterNetworking specifies network configuration for a cluster.
All CIDRs must be unique. Additional validation to check for CIDRs overlap and consistent network stack is performed by the controllers.
Failing that validation will result in the HostedCluster being degraded and the validConfiguration condition being false.
TODO this is available in vanilla kube from 1.31 API servers and in Openshift from 4.16.
TODO(alberto): Use CEL cidr library for all these validation when all management clusters are &gt;= 1.31.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>machineNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MachineNetworkEntry">
[]MachineNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>machineNetwork is the list of IP address pools for machines.
This might be used among other things to generate appropriate networking security groups in some clouds providers.
Currently only one entry or two for dual stack is supported.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>clusterNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworkEntry">
[]ClusterNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterNetwork is the list of IP address pools for pods.
Defaults to cidr: &ldquo;10.132.0.0/14&rdquo;.
Currently only one entry is supported.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>serviceNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServiceNetworkEntry">
[]ServiceNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceNetwork is the list of IP address pools for services.
Defaults to cidr: &ldquo;172.31.0.0/16&rdquo;.
Currently only one entry is supported.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>networkType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkType">
NetworkType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>networkType specifies the SDN provider used for cluster networking.
Defaults to OVNKubernetes.
This field is required and immutable.
kubebuilder:validation:XValidation:rule=&ldquo;self == oldSelf&rdquo;, message=&ldquo;networkType is immutable&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>apiServer</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIServerNetworking">
APIServerNetworking
</a>
</em>
</td>
<td>
<p>apiServer contains advanced network settings for the API server that affect
how the APIServer is exposed inside a hosted cluster node.</p>
</td>
</tr>
</tbody>
</table>
###ClusterVersionOperatorSpec { #hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">OperatorConfiguration</a>)
</p>
<p>
<p>ClusterVersionOperatorSpec is the specification of the desired behavior of the Cluster Version Operator.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>operatorLogLevel</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.LogLevel">
LogLevel
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorLogLevel is an intent based logging for the operator itself. It does not give fine-grained control,
but it is a simple way to manage coarse grained logging choices that operators have to interpret for themselves.</p>
<p>Valid values are: &ldquo;Normal&rdquo;, &ldquo;Debug&rdquo;, &ldquo;Trace&rdquo;, &ldquo;TraceAll&rdquo;.
Defaults to &ldquo;Normal&rdquo;.</p>
</td>
</tr>
</tbody>
</table>
###ClusterVersionStatus { #hypershift.openshift.io/v1beta1.ClusterVersionStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>ClusterVersionStatus reports the status of the cluster versioning,
including any upgrades that are in progress. The current field will
be set to whichever version the cluster is reconciling to, and the
conditions array will report whether the update succeeded, is in
progress, or is failing.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>desired</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.Release
</a>
</em>
</td>
<td>
<p>desired is the version that the cluster is reconciling towards.
If the cluster is not yet fully initialized desired will be set
with the information available, which may be an image or a tag.</p>
</td>
</tr>
<tr>
<td>
<code>history</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.UpdateHistory
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>history contains a list of the most recent versions applied to the cluster.
This value may be empty during cluster startup, and then will be updated
when a new update is being applied. The newest update is first in the
list and it is ordered by recency. Updates in the history have state
Completed if the rollout completed - if an update was failing or halfway
applied the state will be Partial. Only a limited amount of update history
is preserved.</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code></br>
<em>
int64
</em>
</td>
<td>
<p>observedGeneration reports which version of the spec is being synced.
If this value is not equal to metadata.generation, then the desired
and conditions fields may represent a previous version.</p>
</td>
</tr>
<tr>
<td>
<code>availableUpdates</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.Release
</a>
</em>
</td>
<td>
<p>availableUpdates contains updates recommended for this
cluster. Updates which appear in conditionalUpdates but not in
availableUpdates may expose this cluster to known issues. This list
may be empty if no updates are recommended, if the update service
is unavailable, or if an invalid channel has been specified.</p>
</td>
</tr>
<tr>
<td>
<code>conditionalUpdates</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.ConditionalUpdate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditionalUpdates contains the list of updates that may be
recommended for this cluster if it meets specific required
conditions. Consumers interested in the set of updates that are
actually recommended for this cluster should use
availableUpdates. This list may be empty if no updates are
recommended, if the update service is unavailable, or if an empty
or invalid channel has been specified.</p>
</td>
</tr>
</tbody>
</table>
###ComponentResource { #hypershift.openshift.io/v1beta1.ComponentResource }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus">ControlPlaneComponentStatus</a>)
</p>
<p>
<p>ComponentResource defines a resource reconciled by a ControlPlaneComponent.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kind</code></br>
<em>
string
</em>
</td>
<td>
<p>kind is the name of the resource schema.</p>
</td>
</tr>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<p>group is the API group for this resource type.</p>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of this resource.</p>
</td>
</tr>
</tbody>
</table>
###ConditionType { #hypershift.openshift.io/v1beta1.ConditionType }
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWSDefaultSecurityGroupCreated&#34;</p></td>
<td><p>AWSDefaultSecurityGroupCreated indicates whether the default security group
for AWS workers has been created.
A failure here indicates that NodePools without a security group will be
blocked from creating machines.</p>
</td>
</tr><tr><td><p>&#34;AWSDefaultSecurityGroupDeleted&#34;</p></td>
<td><p>AWSDefaultSecurityGroupDeleted indicates whether the default security group
for AWS workers has been deleted.
A failure here indicates that the Security Group has some dependencies that
there are still pending cloud resources to be deleted that are using that SG.</p>
</td>
</tr><tr><td><p>&#34;AWSEndpointAvailable&#34;</p></td>
<td><p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint has been
created in the guest VPC</p>
</td>
</tr><tr><td><p>&#34;AWSEndpointServiceAvailable&#34;</p></td>
<td><p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint Service
has been created for the specified NLB in the management VPC</p>
</td>
</tr><tr><td><p>&#34;CVOScaledDown&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;CloudResourcesDestroyed&#34;</p></td>
<td><p>CloudResourcesDestroyed bubbles up the same condition from HCP. It signals if the cloud provider infrastructure created by Kubernetes
in the consumer cloud provider account was destroyed.
A failure here may require external user intervention to resolve. E.g. cloud provider perms were corrupted. E.g. the guest cluster was broken
and kube resource deletion that affects cloud infra like service type load balancer can&rsquo;t succeed.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionAvailable&#34;</p></td>
<td><p>ClusterVersionAvailable bubbles up Failing configv1.OperatorAvailable from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionFailing&#34;</p></td>
<td><p>ClusterVersionFailing bubbles up Failing from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionProgressing&#34;</p></td>
<td><p>ClusterVersionProgressing bubbles up configv1.OperatorProgressing from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionReleaseAccepted&#34;</p></td>
<td><p>ClusterVersionReleaseAccepted bubbles up Failing ReleaseAccepted from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionRetrievedUpdates&#34;</p></td>
<td><p>ClusterVersionRetrievedUpdates bubbles up RetrievedUpdates from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionSucceeding&#34;</p></td>
<td><p>ClusterVersionSucceeding indicates the current status of the desired release
version of the HostedCluster as indicated by the Failing condition in the
underlying cluster&rsquo;s ClusterVersion.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionUpgradeable&#34;</p></td>
<td><p>ClusterVersionUpgradeable indicates the Upgradeable condition in the
underlying cluster&rsquo;s ClusterVersion.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td><p>ControlPlaneComponentAvailable indicates whether the ControlPlaneComponent is available.</p>
</td>
</tr><tr><td><p>&#34;Progressing&#34;</p></td>
<td><p>ControlPlaneComponentProgressing indicates whether the ControlPlaneComponent is progressing.</p>
</td>
</tr><tr><td><p>&#34;EtcdAvailable&#34;</p></td>
<td><p>EtcdAvailable bubbles up the same condition from HCP. It signals if etcd is available.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;EtcdRecoveryActive&#34;</p></td>
<td><p>EtcdRecoveryActive indicates that the Etcd cluster is failing and the
recovery job was triggered.</p>
</td>
</tr><tr><td><p>&#34;EtcdSnapshotRestored&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ExternalDNSReachable&#34;</p></td>
<td><p>ExternalDNSReachable bubbles up the same condition from HCP. It signals if the configured external DNS is reachable.
A failure here requires external user intervention to resolve. E.g. changing the external DNS domain or making sure the domain is created
and registered correctly.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td><p>HostedClusterAvailable indicates whether the HostedCluster has a healthy
control plane.
When this is false for too long and there&rsquo;s no clear indication in the &ldquo;Reason&rdquo;, please check the remaining more granular conditions.</p>
</td>
</tr><tr><td><p>&#34;Degraded&#34;</p></td>
<td><p>HostedClusterDegraded indicates whether the HostedCluster is encountering
an error that may require user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;HostedClusterDestroyed&#34;</p></td>
<td><p>HostedClusterDestroyed indicates that a hosted has finished destroying and that it is waiting for a destroy grace period to go away.
The grace period is determined by the hypershift.openshift.io/destroy-grace-period annotation in the HostedCluster if present.</p>
</td>
</tr><tr><td><p>&#34;Progressing&#34;</p></td>
<td><p>HostedClusterProgressing indicates whether the HostedCluster is attempting
an initial deployment or upgrade.
When this is false for too long and there&rsquo;s no clear indication in the &ldquo;Reason&rdquo;, please check the remaining more granular conditions.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Degraded&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;IgnitionEndpointAvailable&#34;</p></td>
<td><p>IgnitionEndpointAvailable indicates whether the ignition server for the
HostedCluster is available to handle ignition requests.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;IgnitionServerValidReleaseInfo&#34;</p></td>
<td><p>IgnitionServerValidReleaseInfo indicates if the release contains all the images used by the local ignition provider
and reports missing images if any.</p>
</td>
</tr><tr><td><p>&#34;InfrastructureReady&#34;</p></td>
<td><p>InfrastructureReady bubbles up the same condition from HCP. It signals if the infrastructure for a control plane to be operational,
e.g. load balancers were created successfully.
A failure here may require external user intervention to resolve. E.g. hitting quotas on the cloud provider.</p>
</td>
</tr><tr><td><p>&#34;KubeAPIServerAvailable&#34;</p></td>
<td><p>KubeAPIServerAvailable bubbles up the same condition from HCP. It signals if the kube API server is available.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;KubeVirtNodesLiveMigratable&#34;</p></td>
<td><p>KubeVirtNodesLiveMigratable indicates if all nodes (VirtualMachines) of the kubevirt
hosted cluster can be live migrated without experiencing a node restart</p>
</td>
</tr><tr><td><p>&#34;PlatformCredentialsFound&#34;</p></td>
<td><p>PlatformCredentialsFound indicates that credentials required for the
desired platform are valid.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ReconciliationActive&#34;</p></td>
<td><p>ReconciliationActive indicates if reconciliation of the HostedCluster is
active or paused hostedCluster.spec.pausedUntil.</p>
</td>
</tr><tr><td><p>&#34;ReconciliationSucceeded&#34;</p></td>
<td><p>ReconciliationSucceeded indicates if the HostedCluster reconciliation
succeeded.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;SupportedHostedCluster&#34;</p></td>
<td><p>SupportedHostedCluster indicates whether a HostedCluster is supported by
the current configuration of the hypershift-operator.
e.g. If HostedCluster requests endpointAcess Private but the hypershift-operator
is running on a management cluster outside AWS or is not configured with AWS
credentials, the HostedCluster is not supported.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;UnmanagedEtcdAvailable&#34;</p></td>
<td><p>UnmanagedEtcdAvailable indicates whether a user-managed etcd cluster is
healthy.</p>
</td>
</tr><tr><td><p>&#34;ValidAWSIdentityProvider&#34;</p></td>
<td><p>ValidAWSIdentityProvider indicates if the Identity Provider referenced
in the cloud credentials is healthy. E.g. for AWS the idp ARN is referenced in the iam roles.
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]</p>
<p>A failure here may require external user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;ValidAWSKMSConfig&#34;</p></td>
<td><p>ValidAWSKMSConfig indicates whether the AWS KMS role and encryption key are valid and operational
A failure here indicates that the role or the key are invalid, or the role doesn&rsquo;t have access to use the key.</p>
</td>
</tr><tr><td><p>&#34;ValidAzureKMSConfig&#34;</p></td>
<td><p>ValidAzureKMSConfig indicates whether the given KMS input for the Azure platform is valid and operational
A failure here indicates that the input is invalid, or permissions are missing to use the encryption key.</p>
</td>
</tr><tr><td><p>&#34;ValidConfiguration&#34;</p></td>
<td><p>ValidHostedClusterConfiguration signals if the hostedCluster input is valid and
supported by the underlying management cluster.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidHostedControlPlaneConfiguration&#34;</p></td>
<td><p>ValidHostedControlPlaneConfiguration bubbles up the same condition from HCP. It signals if the hostedControlPlane input is valid and
supported by the underlying management cluster.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidIDPConfiguration&#34;</p></td>
<td><p>ValidIDPConfiguration indicates if the Identity Provider configuration is valid.
A failure here may require external user intervention to resolve
e.g. the user-provided IDP configuration provided is invalid or the IDP is not reachable.</p>
</td>
</tr><tr><td><p>&#34;ValidKubeVirtInfraNetworkMTU&#34;</p></td>
<td><p>ValidKubeVirtInfraNetworkMTU indicates if the MTU configured on an infra cluster
hosting a guest cluster utilizing kubevirt platform is a sufficient value that will avoid
performance degradation due to fragmentation of the double encapsulation in ovn-kubernetes</p>
</td>
</tr><tr><td><p>&#34;ValidOIDCConfiguration&#34;</p></td>
<td><p>ValidOIDCConfiguration indicates if an AWS cluster&rsquo;s OIDC condition is
detected as invalid.
A failure here may require external user intervention to resolve. E.g. oidc was deleted out of band.</p>
</td>
</tr><tr><td><p>&#34;ValidReleaseImage&#34;</p></td>
<td><p>ValidReleaseImage indicates if the release image set in the spec is valid
for the HostedCluster. For example, this can be set false if the
HostedCluster itself attempts an unsupported version before 4.9 or an
unsupported upgrade e.g y-stream upgrade before 4.11.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidReleaseInfo&#34;</p></td>
<td><p>ValidReleaseInfo bubbles up the same condition from HCP. It indicates if the release contains all the images used by hypershift
and reports missing images if any.</p>
</td>
</tr></tbody>
</table>
###ControlPlaneComponent { #hypershift.openshift.io/v1beta1.ControlPlaneComponent }
<p>
<p>ControlPlaneComponent specifies the state of a ControlPlane Component</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponentSpec">
ControlPlaneComponentSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus">
ControlPlaneComponentStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
###ControlPlaneComponentSpec { #hypershift.openshift.io/v1beta1.ControlPlaneComponentSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponent">ControlPlaneComponent</a>)
</p>
<p>
<p>ControlPlaneComponentSpec defines the desired state of ControlPlaneComponent</p>
</p>
###ControlPlaneComponentStatus { #hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponent">ControlPlaneComponent</a>)
</p>
<p>
<p>ControlPlaneComponentStatus defines the observed state of ControlPlaneComponent</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>version reports the current version of this component.</p>
</td>
</tr>
<tr>
<td>
<code>resources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ComponentResource">
[]ComponentResource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>resources is a list of the resources reconciled by this component.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions contains details for the current state of the ControlPlane Component.
If there is an error, then the Available condition will be false.</p>
<p>Current condition types are: &ldquo;Available&rdquo;</p>
</td>
</tr>
</tbody>
</table>
###ControlPlaneManagedIdentities { #hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities">AzureResourceManagedIdentities</a>)
</p>
<p>
<p>ControlPlaneManagedIdentities contains the managed identities on the HCP control plane needing to authenticate with
Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>managedIdentitiesKeyVault</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedAzureKeyVault">
ManagedAzureKeyVault
</a>
</em>
</td>
<td>
<p>managedIdentitiesKeyVault contains information on the management cluster&rsquo;s managed identities Azure Key Vault.
This Key Vault is where the managed identities certificates are stored. These certificates are pulled out of the
Key Vault by the Secrets Store CSI driver and mounted into a volume on control plane pods requiring
authentication with Azure API.</p>
<p>More information on how the Secrets Store CSI driver works to do this can be found here:
<a href="https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver">https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver</a>.</p>
</td>
</tr>
<tr>
<td>
<code>cloudProvider</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>cloudProvider is a pre-existing managed identity associated with the azure cloud provider, aka cloud controller
manager.</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>nodePoolManagement is a pre-existing managed identity associated with the operator managing the NodePools.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneOperator</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>controlPlaneOperator is a pre-existing managed identity associated with the control plane operator.</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistry</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>imageRegistry is a pre-existing managed identity associated with the cluster-image-registry-operator.</p>
</td>
</tr>
<tr>
<td>
<code>ingress</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>ingress is a pre-existing managed identity associated with the cluster-ingress-operator.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>network is a pre-existing managed identity associated with the cluster-network-operator.</p>
</td>
</tr>
<tr>
<td>
<code>disk</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>diskClientID is a pre-existing managed identity associated with the azure-disk-controller.</p>
</td>
</tr>
<tr>
<td>
<code>file</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>fileClientID is a pre-existing managed identity associated with the azure-disk-controller.</p>
</td>
</tr>
</tbody>
</table>
###DNSSpec { #hypershift.openshift.io/v1beta1.DNSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>DNSSpec specifies the DNS configuration for the hosted cluster ingress.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>baseDomain</code></br>
<em>
string
</em>
</td>
<td>
<p>baseDomain is the base domain of the hosted cluster.
It will be used to configure ingress in the hosted cluster through the subdomain baseDomainPrefix.baseDomain.
If baseDomainPrefix is omitted, the hostedCluster.name will be used as the subdomain.
Once set, this field is immutable.
When the value is the empty string &ldquo;&rdquo;, the controller might default to a value depending on the platform.</p>
</td>
</tr>
<tr>
<td>
<code>baseDomainPrefix</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>baseDomainPrefix is the base domain prefix for the hosted cluster ingress.
It will be used to configure ingress in the hosted cluster through the subdomain baseDomainPrefix.baseDomain.
If baseDomainPrefix is omitted, the hostedCluster.name will be used as the subdomain.
Set baseDomainPrefix to an empty string &ldquo;&rdquo;, if you don&rsquo;t want a prefix at all (not even hostedCluster.name) to be prepended to baseDomain.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>publicZoneID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>publicZoneID is the Hosted Zone ID where all the DNS records that are publicly accessible to the internet exist.
This field is optional and mainly leveraged in cloud environments where the DNS records for the .baseDomain are created by controllers in this zone.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>privateZoneID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>privateZoneID is the Hosted Zone ID where all the DNS records that are only available internally to the cluster exist.
This field is optional and mainly leveraged in cloud environments where the DNS records for the .baseDomain are created by controllers in this zone.
Once set, this value is immutable.</p>
</td>
</tr>
</tbody>
</table>
###DataPlaneManagedIdentities { #hypershift.openshift.io/v1beta1.DataPlaneManagedIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities">AzureResourceManagedIdentities</a>)
</p>
<p>
<p>DataPlaneManagedIdentities contains the client IDs of all the managed identities on the data plane needing to
authenticate with Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>imageRegistryMSIClientID</code></br>
<em>
string
</em>
</td>
<td>
<p>imageRegistryMSIClientID is the client ID of a pre-existing managed identity ID associated with the image
registry controller.</p>
</td>
</tr>
<tr>
<td>
<code>diskMSIClientID</code></br>
<em>
string
</em>
</td>
<td>
<p>diskMSIClientID is the client ID of a pre-existing managed identity ID associated with the CSI Disk driver.</p>
</td>
</tr>
<tr>
<td>
<code>fileMSIClientID</code></br>
<em>
string
</em>
</td>
<td>
<p>fileMSIClientID is the client ID of a pre-existing managed identity ID associated with the CSI File driver.</p>
</td>
</tr>
</tbody>
</table>
###Diagnostics { #hypershift.openshift.io/v1beta1.Diagnostics }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
<p>Diagnostics specifies the diagnostics settings for a virtual machine.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageAccountType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType">
AzureDiagnosticsStorageAccountType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageAccountType determines if the storage account for storing the diagnostics data
should be disabled (Disabled), provisioned by Azure (Managed) or by the user (UserManaged).</p>
</td>
</tr>
<tr>
<td>
<code>userManaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UserManagedDiagnostics">
UserManagedDiagnostics
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>userManaged specifies the diagnostics settings for a virtual machine when the storage account is managed by the user.</p>
</td>
</tr>
</tbody>
</table>
###EtcdManagementType { #hypershift.openshift.io/v1beta1.EtcdManagementType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>EtcdManagementType is a enum specifying the strategy for managing the cluster&rsquo;s etcd instance</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>Managed means HyperShift should provision and operator the etcd cluster
automatically.</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>Unmanaged means HyperShift will not provision or manage the etcd cluster,
and the user is responsible for doing so.</p>
</td>
</tr></tbody>
</table>
###EtcdSpec { #hypershift.openshift.io/v1beta1.EtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>EtcdSpec specifies configuration for a control plane etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>managementType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdManagementType">
EtcdManagementType
</a>
</em>
</td>
<td>
<p>managementType defines how the etcd cluster is managed.
This can be either Managed or Unmanaged.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>managed</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdSpec">
ManagedEtcdSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>managed specifies the behavior of an etcd cluster managed by HyperShift.</p>
</td>
</tr>
<tr>
<td>
<code>unmanaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec">
UnmanagedEtcdSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>unmanaged specifies configuration which enables the control plane to
integrate with an externally managed etcd cluster.</p>
</td>
</tr>
</tbody>
</table>
###EtcdTLSConfig { #hypershift.openshift.io/v1beta1.EtcdTLSConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec">UnmanagedEtcdSpec</a>)
</p>
<p>
<p>EtcdTLSConfig specifies TLS configuration for HTTPS etcd client endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ClientSecret refers to a secret for client mTLS authentication with the etcd cluster. It
may have the following key/value pairs:</p>
<pre><code>etcd-client-ca.crt: Certificate Authority value
etcd-client.crt: Client certificate value
etcd-client.key: Client certificate key value
</code></pre>
</td>
</tr>
</tbody>
</table>
###Filter { #hypershift.openshift.io/v1beta1.Filter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">AWSResourceReference</a>)
</p>
<p>
<p>Filter is a filter used to identify an AWS resource</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name of the filter. Filter names are case-sensitive.</p>
</td>
</tr>
<tr>
<td>
<code>values</code></br>
<em>
[]string
</em>
</td>
<td>
<p>Values includes one or more filter values. Filter values are case-sensitive.</p>
</td>
</tr>
</tbody>
</table>
###FilterByNeutronTags { #hypershift.openshift.io/v1beta1.FilterByNeutronTags }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NetworkFilter">NetworkFilter</a>, 
<a href="#hypershift.openshift.io/v1beta1.RouterFilter">RouterFilter</a>, 
<a href="#hypershift.openshift.io/v1beta1.SubnetFilter">SubnetFilter</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>tags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tags is a list of tags to filter by. If specified, the resource must
have all of the tags specified to be included in the result.</p>
</td>
</tr>
<tr>
<td>
<code>tagsAny</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>TagsAny is a list of tags to filter by. If specified, the resource
must have at least one of the tags specified to be included in the
result.</p>
</td>
</tr>
<tr>
<td>
<code>notTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NotTags is a list of tags to filter by. If specified, resources which
contain all of the given tags will be excluded from the result.</p>
</td>
</tr>
<tr>
<td>
<code>notTagsAny</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NotTagsAny is a list of tags to filter by. If specified, resources
which contain any of the given tags will be excluded from the result.</p>
</td>
</tr>
</tbody>
</table>
###HostedClusterSpec { #hypershift.openshift.io/v1beta1.HostedClusterSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedCluster">HostedCluster</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the desired OCP release payload for all the hosted cluster components.
This includes those components running management side like the Kube API Server and the CVO but also the operands which land in the hosted cluster data plane like the ingress controller, ovn agents, etc.
The maximum and minimum supported release versions are determined by the running Hypersfhit Operator.
Attempting to use an unsupported version will result in the HostedCluster being degraded and the validateReleaseImage condition being false.
Attempting to use a release with a skew against a NodePool release bigger than N-2 for the y-stream will result in leaving the NodePool in an unsupported state.
Changing this field will trigger a rollout of the control plane components.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneRelease</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneRelease is like spec.release but only for the components running on the management cluster.
This excludes any operand which will land in the hosted cluster data plane.
It is useful when you need to apply patch management side like a CVE, transparently for the hosted cluster.
Version input for this field is free, no validation is performed against spec.release or maximum and minimum is performed.
If defined, it will dicate the version of the components running management side, while spec.release will dictate the version of the components landing in the hosted cluster data plane.
If not defined, spec.release is used for both.
Changing this field will trigger a rollout of the control plane.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterID uniquely identifies this cluster. This is expected to be an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in hexadecimal digits).
As with a Kubernetes metadata.uid, this ID uniquely identifies this cluster in space and time.
This value identifies the cluster in metrics pushed to telemetry and metrics produced by the control plane operators.
If a value is not specified, a random clusterID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>infraID is a globally unique identifier for the cluster.
It must consist of lowercase alphanumeric characters and hyphens (&lsquo;-&rsquo;) only, and start and end with an alphanumeric character.
It must be no more than 253 characters in length.
This identifier will be used to associate various cloud resources with the HostedCluster and its associated NodePools.
infraID is used to compute and tag created resources with &ldquo;kubernetes.io/cluster/&rdquo;+hcluster.Spec.InfraID which has contractual meaning for the cloud provider implementations.
If a value is not specified, a random infraID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
If omitted we will use the appropriate update service for the cluster and region.
This is used by the control plane operator to determine and signal the appropriate available upgrades in the hostedCluster.status.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster.
If omitted no particular upgrades are suggested.
TODO(alberto): Consider the backend to use the default channel by default. Default channel will contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components like the Kube API Server.
Possible values are HighlyAvailable and SingleReplica. The default value is HighlyAvailable.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on the hosted cluster data plane like the ingress controller and image registry controller.
Possible values are HighlyAvailable and SingleReplica. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>dns specifies the DNS configuration for the hosted cluster ingress.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<p>networking specifies network configuration for the hosted cluster.
Defaults to OVNKubernetes with a cluster network of cidr: &ldquo;10.132.0.0/14&rdquo; and a service network of cidr: &ldquo;172.31.0.0/16&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with this HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<p>autoNode specifies the configuration for the autoNode feature.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>etcd specifies configuration for the control plane etcd cluster. The
default managementType is Managed. Once set, the managementType cannot be
changed.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>services specifies how individual control plane services endpoints are published for consumption.
This requires APIServer;OAuthServer;Konnectivity;Ignition.
This field is immutable for all platforms but IBMCloud.
Max is 6 to account for OIDC;OVNSbDb for backward compatibility though they are no-op.</p>
<p>-kubebuilder:validation:XValidation:rule=&ldquo;self.all(s, !(s.service == &lsquo;APIServer&rsquo; &amp;&amp; s.servicePublishingStrategy.type == &lsquo;Route&rsquo;) || has(s.servicePublishingStrategy.route.hostname))&rdquo;,message=&ldquo;If serviceType is &lsquo;APIServer&rsquo; and publishing strategy is &lsquo;Route&rsquo;, then hostname must be set&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;[&lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, &lsquo;Ignition&rsquo;].all(requiredType, self.exists(s, s.service == requiredType))&rdquo;,message=&ldquo;Services list must contain at least &lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, and &lsquo;Ignition&rsquo; service types&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; has(s.servicePublishingStrategy.route) &amp;&amp; has(s.servicePublishingStrategy.route.hostname)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.route) &amp;&amp; has(y.servicePublishingStrategy.route.hostname) &amp;&amp; y.servicePublishingStrategy.route.hostname == x.servicePublishingStrategy.route.hostname)).size() &lt;= 1)&rdquo;,message=&ldquo;Each route publishingStrategy &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; has(s.servicePublishingStrategy.nodePort) &amp;&amp; has(s.servicePublishingStrategy.nodePort.address) &amp;&amp; has(s.servicePublishingStrategy.nodePort.port)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.nodePort) &amp;&amp; has(y.servicePublishingStrategy.nodePort.address) &amp;&amp; y.servicePublishingStrategy.nodePort.address == x.servicePublishingStrategy.nodePort.address &amp;&amp; has(y.servicePublishingStrategy.nodePort.port) &amp;&amp; y.servicePublishingStrategy.nodePort.port == x.servicePublishingStrategy.nodePort.port )).size() &lt;= 1)&rdquo;,message=&ldquo;Each nodePort publishingStrategy &lsquo;nodePort&rsquo; and &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
TODO(alberto): this breaks the cost budget for &lt; 4.17. We should figure why and enable it back. And If not fixable, consider imposing a minimum version on the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>pullSecret is a local reference to a Secret that must have a &ldquo;.dockerconfigjson&rdquo; key whose content must be a valid Openshift pull secret JSON.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This pull secret will be part of every payload generated by the controllers for any NodePool of the HostedCluster
and it will be injected into the container runtime of all NodePools.
Changing this value will trigger a rollout for all existing NodePools in the cluster.
Changing the content of the secret inplace will not trigger a rollout and might result in unpredictable behaviour.
TODO(alberto): have our own local reference type to include our opinions and avoid transparent changes.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>sshKey is a local reference to a Secret that must have a &ldquo;id_rsa.pub&rdquo; key whose content must be the public part of 1..N SSH keys.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
When sshKey is set, the controllers will generate a machineConfig with the sshAuthorizedKeys <a href="https://coreos.github.io/ignition/configuration-v3_2/">https://coreos.github.io/ignition/configuration-v3_2/</a> populated with this value.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>issuerURL is an OIDC issuer URL which will be used as the issuer in all
ServiceAccount tokens generated by the control plane API server via &ndash;service-account-issuer kube api server flag.
<a href="https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/">https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/</a>
<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection</a>
The default value is kubernetes.default.svc, which only works for in-cluster
validation.
If the platform is AWS and this value is set, the controller will update an s3 object with the appropriate OIDC documents (using the serviceAccountSigningKey info) into that issuerURL.
The expectation is for this s3 url to be backed by an OIDC provider in the AWS IAM.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAccountSigningKey is a local reference to a secret that must have a &ldquo;key&rdquo; key whose content must be the private key
used by the service account token issuer.
If not specified, a service account signing key will
be generated automatically for the cluster.
When specifying a service account signing key, an IssuerURL must also be specified.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
</td>
</tr>
<tr>
<td>
<code>operatorConfiguration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">
OperatorConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditWebhook contains metadata for configuring an audit webhook endpoint
for a cluster to process cluster audit events. It references a secret that
contains the webhook information for the audit webhook endpoint. It is a
secret because if the endpoint has mTLS the kubeconfig will contain client
keys. The kubeconfig needs to be stored in the secret with a secret key
name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageContentSources specifies image mirrors that can be used by cluster
nodes to pull content.
When imageContentSources is set, the controllers will generate a machineConfig.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalTrustBundle is a local reference to a ConfigMap that must have a &ldquo;ca-bundle.crt&rdquo; key
whose content must be a PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>secretEncryption specifies a Kubernetes secret encryption strategy for the
control plane.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>fips indicates whether this cluster&rsquo;s nodes will be running in FIPS mode.
If set to true, the control plane&rsquo;s ignition server will be configured to
expect that nodes joining the cluster will be FIPS-enabled.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the HostedCluster controller, resulting in any change to the HostedCluster being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OLMCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector when specified, is propagated to all control plane Deployments and Stateful sets running management side.
It must be satisfied by the management Nodes for the pods to be scheduled. Otherwise the HostedCluster will enter a degraded state.
Changes to this field will propagate to existing Deployments and StatefulSets.
TODO(alberto): add additional validation for the map key/values.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tolerations when specified, define what custom tolerations are added to the hcp pods.</p>
</td>
</tr>
<tr>
<td>
<code>labels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>labels when specified, define what custom labels are added to the hcp pods.
Changing this day 2 will cause a rollout of all hcp pods.
Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
<p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=&ldquo;label key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (<em>), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)&rdquo;
-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=&ldquo;label value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores (</em>) or dots (.), and begin and end with an alphanumeric character&rdquo;
TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p>
</td>
</tr>
<tr>
<td>
<code>capabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">
Capabilities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capabilities allows for disabling optional components at cluster install time.
This field is optional and once set cannot be changed.</p>
</td>
</tr>
</tbody>
</table>
###HostedClusterStatus { #hypershift.openshift.io/v1beta1.HostedClusterStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedCluster">HostedCluster</a>)
</p>
<p>
<p>HostedClusterStatus is the latest observed status of a HostedCluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>version</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionStatus">
ClusterVersionStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version is the status of the release version applied to the
HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>kubeconfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeConfig is a reference to the secret containing the default kubeconfig
for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>kubeadminPassword</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeadminPassword is a reference to the secret that contains the initial
kubeadmin user password for the guest cluster.</p>
</td>
</tr>
<tr>
<td>
<code>ignitionEndpoint</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IgnitionEndpoint is the endpoint injected in the ign config userdata.
It exposes the config for instances to become kubernetes nodes.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIEndpoint">
APIEndpoint
</a>
</em>
</td>
<td>
<p>ControlPlaneEndpoint contains the endpoint information by which
external clients can access the control plane. This is populated
after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>oauthCallbackURLTemplate</code></br>
<em>
string
</em>
</td>
<td>
<p>OAuthCallbackURLTemplate contains a template for the URL to use as a callback
for identity providers. The [identity-provider-name] placeholder must be replaced
with the name of an identity provider defined on the HostedCluster.
This is populated after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions represents the latest available observations of a control
plane&rsquo;s current state.</p>
</td>
</tr>
<tr>
<td>
<code>payloadArch</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PayloadArchType">
PayloadArchType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>payloadArch represents the CPU architecture type of the HostedCluster.Spec.Release.Image. The valid values are:
Multi, ARM64, AMD64, S390X, or PPC64LE.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">
PlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Platform contains platform-specific status of the HostedCluster</p>
</td>
</tr>
<tr>
<td>
<code>oldestKubeletVersion</code></br>
<em>
string
</em>
</td>
<td>
<p>OldestKubeletVersion tracks the oldest kubelet version in a hosted cluster</p>
</td>
</tr>
</tbody>
</table>
###HostedControlPlaneSpec { #hypershift.openshift.io/v1beta1.HostedControlPlaneSpec }
<p>
<p>HostedControlPlaneSpec defines the desired state of HostedControlPlane</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>releaseImage</code></br>
<em>
string
</em>
</td>
<td>
<p>ReleaseImage is the release image applied to the hosted control plane.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneReleaseImage</code></br>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneReleaseImage specifies the desired OCP release payload for
control plane components running on the management cluster.
If not defined, ReleaseImage is used</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
By default it will use the appropriate update service for the cluster and region.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default
set of updates be applied to this cluster. The default channel will be
contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<p>IssuerURL is an OIDC issuer URL which is used as the issuer in all
ServiceAccount tokens generated by the control plane API server. The
default value is kubernetes.default.svc, which only works for in-cluster
validation.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Networking specifies network configuration for the cluster.
Temporarily optional for backward compatibility, required in future releases.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClusterID is the unique id that identifies the cluster externally.
Making it optional here allows us to keep compatibility with previous
versions of the control-plane-operator that have no knowledge of this
field.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountSigningKey is a reference to a secret containing the private key
used by the service account token issuer. The secret is expected to contain
a single key named &ldquo;key&rdquo;. If not specified, a service account signing key will
be generated automatically for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControllerAvailabilityPolicy specifies the availability policy applied to
critical control plane components. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfrastructureAvailabilityPolicy specifies the availability policy applied
to infrastructure services which run on cluster nodes. The default value is
SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>FIPS specifies if the nodes for the cluster will be running in FIPS mode</p>
</td>
</tr>
<tr>
<td>
<code>kubeconfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeConfig specifies the name and key for the kubeconfig secret</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>Services defines metadata about how control plane services are published
in the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditWebhook contains metadata for configuring an audit webhook
endpoint for a cluster to process cluster audit events. It references
a secret that contains the webhook information for the audit webhook endpoint.
It is a secret because if the endpoint has MTLS the kubeconfig will contain client
keys. This is currently only supported in IBM Cloud. The kubeconfig needs to be stored
in the secret with a secret key name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>Etcd contains metadata about the etcd cluster the hypershift managed Openshift control plane components
use to store data.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<p>Configuration embeds resources that correspond to the openshift configuration API:
<a href="https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html">https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</a></p>
</td>
</tr>
<tr>
<td>
<code>operatorConfiguration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">
OperatorConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageContentSources lists sources/repositories for the release-image content.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTrustBundle references a ConfigMap containing a PEM-encoded X.509 certificate bundle</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretEncryption contains metadata about the kubernetes secret encryption strategy being used for the
cluster when applicable.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OLMCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<p>autoNode specifies the configuration for the autoNode feature.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector when specified, must be true for the pods managed by the HostedCluster to be scheduled.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tolerations when specified, define what custom tolerations are added to the hcp pods.</p>
</td>
</tr>
<tr>
<td>
<code>labels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>labels when specified, define what custom labels are added to the hcp pods.
Changing this day 2 will cause a rollout of all hcp pods.
Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
<p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=&ldquo;label key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (<em>), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)&rdquo;
-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=&ldquo;label value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores (</em>) or dots (.), and begin and end with an alphanumeric character&rdquo;
TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p>
</td>
</tr>
<tr>
<td>
<code>capabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">
Capabilities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capabilities allows for disabling optional components at cluster install time.
This field is optional and once set cannot be changed.</p>
</td>
</tr>
</tbody>
</table>
###HostedControlPlaneStatus { #hypershift.openshift.io/v1beta1.HostedControlPlaneStatus }
<p>
<p>HostedControlPlaneStatus defines the observed state of HostedControlPlane</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code></br>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the HostedControlPlane API Server is ready to
receive requests
This satisfies CAPI contract <a href="https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230">https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230</a></p>
</td>
</tr>
<tr>
<td>
<code>initialized</code></br>
<em>
bool
</em>
</td>
<td>
<p>Initialized denotes whether or not the control plane has
provided a kubeadm-config.
Once this condition is marked true, its value is never changed. See the Ready condition for an indication of
the current readiness of the cluster&rsquo;s control plane.
This satisfies CAPI contract <a href="https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252">https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252</a></p>
</td>
</tr>
<tr>
<td>
<code>externalManagedControlPlane</code></br>
<em>
bool
</em>
</td>
<td>
<p>ExternalManagedControlPlane indicates to cluster-api that the control plane
is managed by an external service.
<a href="https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468">https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468</a></p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIEndpoint">
APIEndpoint
</a>
</em>
</td>
<td>
<p>ControlPlaneEndpoint contains the endpoint information by which
external clients can access the control plane.  This is populated
after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>oauthCallbackURLTemplate</code></br>
<em>
string
</em>
</td>
<td>
<p>OAuthCallbackURLTemplate contains a template for the URL to use as a callback
for identity providers. The [identity-provider-name] placeholder must be replaced
with the name of an identity provider defined on the HostedCluster.
This is populated after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>versionStatus</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionStatus">
ClusterVersionStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>versionStatus is the status of the release version applied by the
hosted control plane operator.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<p>Version is the semantic version of the release applied by
the hosted control plane operator</p>
<p>Deprecated: Use versionStatus.desired.version instead.</p>
</td>
</tr>
<tr>
<td>
<code>releaseImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ReleaseImage is the release image applied to the hosted control plane.</p>
<p>Deprecated: Use versionStatus.desired.image instead.</p>
</td>
</tr>
<tr>
<td>
<code>lastReleaseImageTransitionTime</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>lastReleaseImageTransitionTime is the time of the last update to the current
releaseImage property.</p>
<p>Deprecated: Use versionStatus.history[0].startedTime instead.</p>
</td>
</tr>
<tr>
<td>
<code>kubeConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<p>KubeConfig is a reference to the secret containing the default kubeconfig
for this control plane.</p>
</td>
</tr>
<tr>
<td>
<code>kubeadminPassword</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeadminPassword is a reference to the secret containing the initial kubeadmin password
for the guest cluster.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Condition contains details for one aspect of the current state of the HostedControlPlane.
Current condition types are: &ldquo;Available&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">
PlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Platform contains platform-specific status of the HostedCluster</p>
</td>
</tr>
<tr>
<td>
<code>nodeCount</code></br>
<em>
int
</em>
</td>
<td>
<p>NodeCount tracks the number of nodes in the HostedControlPlane.</p>
</td>
</tr>
<tr>
<td>
<code>oldestKubeletVersion</code></br>
<em>
string
</em>
</td>
<td>
<p>OldestKubeletVersion tracks the oldest kubelet version in a hosted cluster</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">IBMCloudKMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSAuthSpec defines metadata for how authentication is done with IBM Cloud KMS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType">
IBMCloudKMSAuthType
</a>
</em>
</td>
<td>
<p>Type defines the IBM Cloud KMS authentication strategy</p>
</td>
</tr>
<tr>
<td>
<code>unmanaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec">
IBMCloudKMSUnmanagedAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Unmanaged defines the auth metadata the customer provides to interact with IBM Cloud KMS</p>
</td>
</tr>
<tr>
<td>
<code>managed</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec">
IBMCloudKMSManagedAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Managed defines metadata around the service to service authentication strategy for the IBM Cloud
KMS system (all provider managed).</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSAuthType { #hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSAuthType defines the IBM Cloud KMS authentication strategy</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>IBMCloudKMSManagedAuth defines the KMS authentication strategy where the IKS/ROKS platform uses
service to service auth to call IBM Cloud KMS APIs (no customer credentials required)</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>IBMCloudKMSUnmanagedAuth defines the KMS authentication strategy where a customer supplies IBM Cloud
authentication to interact with IBM Cloud KMS APIs</p>
</td>
</tr></tbody>
</table>
###IBMCloudKMSKeyEntry { #hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">IBMCloudKMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSKeyEntry defines metadata for an IBM Cloud KMS encryption key</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>crkID</code></br>
<em>
string
</em>
</td>
<td>
<p>CRKID is the customer rook key id</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code></br>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the id for the key protect instance</p>
</td>
</tr>
<tr>
<td>
<code>correlationID</code></br>
<em>
string
</em>
</td>
<td>
<p>CorrelationID is an identifier used to track all api call usage from hypershift</p>
</td>
</tr>
<tr>
<td>
<code>url</code></br>
<em>
string
</em>
</td>
<td>
<p>URL is the url to call key protect apis over</p>
</td>
</tr>
<tr>
<td>
<code>keyVersion</code></br>
<em>
int
</em>
</td>
<td>
<p>KeyVersion is a unique number associated with the key. The number increments whenever a new
key is enabled for data encryption.</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSManagedAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSManagedAuthSpec defines metadata around the service to service authentication strategy for the IBM Cloud
KMS system (all provider managed).</p>
</p>
###IBMCloudKMSSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSSpec defines metadata for the IBM Cloud KMS encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the IBM Cloud region</p>
</td>
</tr>
<tr>
<td>
<code>auth</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">
IBMCloudKMSAuthSpec
</a>
</em>
</td>
<td>
<p>Auth defines metadata for how authentication is done with IBM Cloud KMS</p>
</td>
</tr>
<tr>
<td>
<code>keyList</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry">
[]IBMCloudKMSKeyEntry
</a>
</em>
</td>
<td>
<p>KeyList defines the list of keys used for data encryption</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSUnmanagedAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSUnmanagedAuthSpec defines the auth metadata the customer provides to interact with IBM Cloud KMS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>Credentials should reference a secret with a key field of IBMCloudIAMAPIKeySecretKey that contains a apikey to
call IBM Cloud KMS APIs</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudPlatformSpec { #hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>IBMCloudPlatformSpec defines IBMCloud specific settings for components</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerType</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.IBMCloudProviderType
</a>
</em>
</td>
<td>
<p>ProviderType is a specific supported infrastructure provider within IBM Cloud.</p>
</td>
</tr>
</tbody>
</table>
###ImageContentSource { #hypershift.openshift.io/v1beta1.ImageContentSource }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ImageContentSource specifies image mirrors that can be used by cluster nodes
to pull content. For cluster workloads, if a container image registry host of
the pullspec matches Source then one of the Mirrors are substituted as hosts
in the pullspec and tried in order to fetch the image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>source</code></br>
<em>
string
</em>
</td>
<td>
<p>Source is the repository that users refer to, e.g. in image pull
specifications.</p>
</td>
</tr>
<tr>
<td>
<code>mirrors</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Mirrors are one or more repositories that may also contain the same images.</p>
</td>
</tr>
</tbody>
</table>
###InPlaceUpgrade { #hypershift.openshift.io/v1beta1.InPlaceUpgrade }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>InPlaceUpgrade specifies an upgrade strategy which upgrades nodes in-place
without any new nodes being created or any old nodes being deleted.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxUnavailable is the maximum number of nodes that can be unavailable
during the update.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding down.</p>
<p>Defaults to 1.</p>
<p>Example: when this is set to 30%, a max of 30% of the nodes can be made
unschedulable/unavailable immediately when the update starts. Once a set
of nodes is updated, more nodes can be made unschedulable for update,
ensuring that the total number of nodes schedulable at all times during
the update is at least 70% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
###KMSProvider { #hypershift.openshift.io/v1beta1.KMSProvider }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>KMSProvider defines the supported KMS providers</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWS&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Azure&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;IBMCloud&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###KMSSpec { #hypershift.openshift.io/v1beta1.KMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>KMSSpec defines metadata about the kms secret encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provider</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KMSProvider">
KMSProvider
</a>
</em>
</td>
<td>
<p>Provider defines the KMS provider</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">
IBMCloudKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IBMCloud defines metadata for the IBM Cloud KMS encryption strategy</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">
AWSKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS defines metadata about the configuration of the AWS KMS Secret Encryption provider</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">
AzureKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Azure defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>
</td>
</tr>
</tbody>
</table>
###KarpenterAWSConfig { #hypershift.openshift.io/v1beta1.KarpenterAWSConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterConfig">KarpenterConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>roleARN</code></br>
<em>
string
</em>
</td>
<td>
<p>arn specifies the ARN of the Karpenter provisioner.</p>
</td>
</tr>
</tbody>
</table>
###KarpenterConfig { #hypershift.openshift.io/v1beta1.KarpenterConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ProvisionerConfig">ProvisionerConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>platform specifies the platform-specific configuration for Karpenter.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterAWSConfig">
KarpenterAWSConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aws specifies the AWS-specific configuration for Karpenter.</p>
</td>
</tr>
</tbody>
</table>
###KubeVirtNodePoolStatus { #hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus">NodePoolPlatformStatus</a>)
</p>
<p>
<p>KubeVirtNodePoolStatus contains the KubeVirt platform statuses</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cacheName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CacheName holds the name of the cache DataVolume, if exists</p>
</td>
</tr>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials">
KubevirtPlatformCredentials
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Credentials shows the client credentials used when creating KubeVirt virtual machines.
This filed is only exists when the KubeVirt virtual machines are being placed
on a cluster separate from the one hosting the Hosted Control Plane components.</p>
<p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on
the same cluster and namespace as the Hosted Control Plane.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtCachingStrategy { #hypershift.openshift.io/v1beta1.KubevirtCachingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtCachingStrategy defines the boot image caching strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType">
KubevirtCachingStrategyType
</a>
</em>
</td>
<td>
<p>Type is the type of the caching strategy</p>
</td>
</tr>
</tbody>
</table>
###KubevirtCachingStrategyType { #hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy">KubevirtCachingStrategy</a>)
</p>
<p>
<p>KubevirtCachingStrategyType is the type of the boot image caching mechanism for the KubeVirt provider</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;None&#34;</p></td>
<td><p>KubevirtCachingStrategyNone means that hypershift will not cache the boot image</p>
</td>
</tr><tr><td><p>&#34;PVC&#34;</p></td>
<td><p>KubevirtCachingStrategyPVC means that hypershift will cache the boot image into a PVC; only relevant when using
a QCOW boot image, and is ignored when using a container image</p>
</td>
</tr></tbody>
</table>
###KubevirtCompute { #hypershift.openshift.io/v1beta1.KubevirtCompute }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtCompute contains values associated with the virtual compute hardware requested for the VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>memory</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Memory represents how much guest memory the VM should have</p>
</td>
</tr>
<tr>
<td>
<code>cores</code></br>
<em>
uint32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Cores represents how many cores the guest VM should have</p>
</td>
</tr>
<tr>
<td>
<code>qosClass</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.QoSClass">
QoSClass
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>QosClass If set to &ldquo;Guaranteed&rdquo;, requests the scheduler to place the VirtualMachineInstance on a node with
limit memory and CPU, equal to be the requested values, to set the VMI as a Guaranteed QoS Class;
See here for more details:
<a href="https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances">https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances</a></p>
</td>
</tr>
</tbody>
</table>
###KubevirtDiskImage { #hypershift.openshift.io/v1beta1.KubevirtDiskImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtDiskImage contains values representing where the rhcos image is located</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>containerDiskImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerDiskImage is a string representing the container image that holds the root disk</p>
</td>
</tr>
</tbody>
</table>
###KubevirtHostDevice { #hypershift.openshift.io/v1beta1.KubevirtHostDevice }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code></br>
<em>
string
</em>
</td>
<td>
<p>DeviceName is the name of the host device that is desired to be utilized in the HostedCluster&rsquo;s NodePool
The device can be any supported PCI device, including GPU, either as a passthrough or a vGPU slice.</p>
</td>
</tr>
<tr>
<td>
<code>count</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>Count is the number of instances the specified host device will be attached to each of the
NodePool&rsquo;s nodes. Default is 1.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtManualStorageDriverConfig { #hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">KubevirtStorageDriverSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageClassMapping</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping">
[]KubevirtStorageClassMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageClassMapping maps StorageClasses on the infra cluster hosting
the KubeVirt VMs to StorageClasses that are made available within the
Guest Cluster.</p>
<p>NOTE: It is possible that not all capabilities of an infra cluster&rsquo;s
storageclass will be present for the corresponding guest clusters storageclass.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSnapshotClassMapping</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping">
[]KubevirtVolumeSnapshotClassMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
</tbody>
</table>
###KubevirtNetwork { #hypershift.openshift.io/v1beta1.KubevirtNetwork }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtNetwork specifies the configuration for a virtual machine
network interface</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name specify the network attached to the nodes
it is a value with the format &ldquo;[namespace]/[name]&rdquo; to reference the
multus network attachment definition</p>
</td>
</tr>
</tbody>
</table>
###KubevirtNodePoolPlatform { #hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>KubevirtNodePoolPlatform specifies the configuration of a NodePool when operating
on KubeVirt platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rootVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">
KubevirtRootVolume
</a>
</em>
</td>
<td>
<p>RootVolume represents values associated with the VM volume that will host rhcos</p>
</td>
</tr>
<tr>
<td>
<code>compute</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCompute">
KubevirtCompute
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Compute contains values representing the virtual hardware requested for the VM</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceMultiqueue</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MultiQueueSetting">
MultiQueueSetting
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaceMultiQueue If set to &ldquo;Enable&rdquo;, virtual network interfaces configured with a virtio bus will also
enable the vhost multiqueue feature for network devices. The number of queues created depends on additional
factors of the VirtualMachineInstance, like the number of guest CPUs.</p>
</td>
</tr>
<tr>
<td>
<code>additionalNetworks</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNetwork">
[]KubevirtNetwork
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalNetworks specify the extra networks attached to the nodes</p>
</td>
</tr>
<tr>
<td>
<code>attachDefaultNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>AttachDefaultNetwork specify if the default pod network should be attached to the nodes
this can only be set to false if AdditionalNetworks are configured</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector is a selector which must be true for the kubevirt VirtualMachine to fit on a node.
Selector which must match a node&rsquo;s labels for the VM to be scheduled on that node. More info:
<a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></p>
</td>
</tr>
<tr>
<td>
<code>hostDevices</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtHostDevice">
[]KubevirtHostDevice
</a>
</em>
</td>
<td>
<p>KubevirtHostDevices specifies the host devices (e.g. GPU devices) to be passed
from the management cluster, to the nodepool nodes</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPersistentVolume { #hypershift.openshift.io/v1beta1.KubevirtPersistentVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">KubevirtVolume</a>)
</p>
<p>
<p>KubevirtPersistentVolume contains the values involved with provisioning persistent storage for a KubeVirt VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>size</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Size is the size of the persistent storage volume</p>
</td>
</tr>
<tr>
<td>
<code>storageClass</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageClass is the storageClass used for the underlying PVC that hosts the volume</p>
</td>
</tr>
<tr>
<td>
<code>accessModes</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode">
[]PersistentVolumeAccessMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessModes is an array that contains the desired Access Modes the root volume should have.
More info: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a></p>
</td>
</tr>
<tr>
<td>
<code>volumeMode</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#persistentvolumemode-v1-core">
Kubernetes core/v1.PersistentVolumeMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VolumeMode defines what type of volume is required by the claim.
Value of Filesystem is implied when not included in claim spec.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPlatformCredentials { #hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus">KubeVirtNodePoolStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">KubevirtPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>infraKubeConfigSecret</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<p>InfraKubeConfigSecret is a reference to a secret that contains the kubeconfig for the external infra cluster
that will be used to host the KubeVirt virtual machines for this cluster.</p>
</td>
</tr>
<tr>
<td>
<code>infraNamespace</code></br>
<em>
string
</em>
</td>
<td>
<p>InfraNamespace defines the namespace on the external infra cluster that is used to host the KubeVirt
virtual machines. This namespace must already exist before creating the HostedCluster and the kubeconfig
referenced in the InfraKubeConfigSecret must have access to manage the required resources within this
namespace.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPlatformSpec { #hypershift.openshift.io/v1beta1.KubevirtPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>KubevirtPlatformSpec specifies configuration for kubevirt guest cluster installations</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>baseDomainPassthrough</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>BaseDomainPassthrough toggles whether or not an automatically
generated base domain for the guest cluster should be used that
is a subdomain of the management cluster&rsquo;s *.apps DNS.</p>
<p>For the KubeVirt platform, the basedomain can be autogenerated using
the *.apps domain of the management/infra hosting cluster
This makes the guest cluster&rsquo;s base domain a subdomain of the
hypershift infra/mgmt cluster&rsquo;s base domain.</p>
<p>Example:
Infra/Mgmt cluster&rsquo;s DNS
Base: example.com
Cluster: mgmt-cluster.example.com
Apps:    *.apps.mgmt-cluster.example.com
KubeVirt Guest cluster&rsquo;s DNS
Base: apps.mgmt-cluster.example.com
Cluster: guest.apps.mgmt-cluster.example.com
Apps: *.apps.guest.apps.mgmt-cluster.example.com</p>
<p>This is possible using OCP wildcard routes</p>
</td>
</tr>
<tr>
<td>
<code>generateID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>GenerateID is used to uniquely apply a name suffix to resources associated with
kubevirt infrastructure resources</p>
</td>
</tr>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials">
KubevirtPlatformCredentials
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Credentials defines the client credentials used when creating KubeVirt virtual machines.
Defining credentials is only necessary when the KubeVirt virtual machines are being placed
on a cluster separate from the one hosting the Hosted Control Plane components.</p>
<p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on
the same cluster and namespace as the Hosted Control Plane.</p>
</td>
</tr>
<tr>
<td>
<code>storageDriver</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">
KubevirtStorageDriverSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageDriver defines how the KubeVirt CSI driver exposes StorageClasses on
the infra cluster (hosting the VMs) to the guest cluster.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtRootVolume { #hypershift.openshift.io/v1beta1.KubevirtRootVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtRootVolume represents the volume that the rhcos disk will be stored and run from.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>diskImage</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtDiskImage">
KubevirtDiskImage
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Image represents what rhcos image to use for the node pool</p>
</td>
</tr>
<tr>
<td>
<code>KubevirtVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">
KubevirtVolume
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubevirtVolume</code> are embedded into this type.)
</p>
<p>KubevirtVolume represents of type of storage to run the image on</p>
</td>
</tr>
<tr>
<td>
<code>cacheStrategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy">
KubevirtCachingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CacheStrategy defines the boot image caching strategy. Default - no caching</p>
</td>
</tr>
</tbody>
</table>
###KubevirtStorageClassMapping { #hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">KubevirtManualStorageDriverConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<p>Group contains which group this mapping belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>infraStorageClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>InfraStorageClassName is the name of the infra cluster storage class that
will be exposed to the guest.</p>
</td>
</tr>
<tr>
<td>
<code>guestStorageClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>GuestStorageClassName is the name that the corresponding storageclass will
be called within the guest cluster</p>
</td>
</tr>
</tbody>
</table>
###KubevirtStorageDriverConfigType { #hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">KubevirtStorageDriverSpec</a>)
</p>
<p>
<p>KubevirtStorageDriverConfigType defines how the kubevirt storage driver is configured.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Default&#34;</p></td>
<td><p>DefaultKubevirtStorageDriverConfigType means the kubevirt storage driver maps to the
underlying infra cluster&rsquo;s default storageclass</p>
</td>
</tr><tr><td><p>&#34;Manual&#34;</p></td>
<td><p>ManualKubevirtStorageDriverConfigType means the kubevirt storage driver mapping is
explicitly defined.</p>
</td>
</tr><tr><td><p>&#34;None&#34;</p></td>
<td><p>NoneKubevirtStorageDriverConfigType means no kubevirt storage driver is used</p>
</td>
</tr></tbody>
</table>
###KubevirtStorageDriverSpec { #hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">KubevirtPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType">
KubevirtStorageDriverConfigType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type represents the type of kubevirt csi driver configuration to use</p>
</td>
</tr>
<tr>
<td>
<code>manual</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">
KubevirtManualStorageDriverConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Manual is used to explicitly define how the infra storageclasses are
mapped to guest storageclasses</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolume { #hypershift.openshift.io/v1beta1.KubevirtVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtVolume represents what kind of storage to use for a KubeVirt VM volume</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolumeType">
KubevirtVolumeType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type represents the type of storage to associate with the kubevirt VMs.</p>
</td>
</tr>
<tr>
<td>
<code>persistent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume">
KubevirtPersistentVolume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Persistent volume type means the VM&rsquo;s storage is backed by a PVC
VMs that use persistent volumes can survive disruption events like restart and eviction
This is the default type used when no storage type is defined.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolumeSnapshotClassMapping { #hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">KubevirtManualStorageDriverConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<p>Group contains which group this mapping belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>infraVolumeSnapshotClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>InfraStorageClassName is the name of the infra cluster volume snapshot class that
will be exposed to the guest.</p>
</td>
</tr>
<tr>
<td>
<code>guestVolumeSnapshotClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>GuestVolumeSnapshotClassName is the name that the corresponding volumeSnapshotClass will
be called within the guest cluster</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolumeType { #hypershift.openshift.io/v1beta1.KubevirtVolumeType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">KubevirtVolume</a>)
</p>
<p>
<p>KubevirtVolumeType is a specific supported KubeVirt volumes</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Persistent&#34;</p></td>
<td><p>KubevirtVolumeTypePersistent represents persistent volume for kubevirt VMs</p>
</td>
</tr></tbody>
</table>
###LoadBalancerPublishingStrategy { #hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>LoadBalancerPublishingStrategy specifies setting used to expose a service as a LoadBalancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>hostname is the name of the DNS record that will be created pointing to the LoadBalancer and passed through to consumers of the service.
If omitted, the value will be inferred from the corev1.Service Load balancer type .status.</p>
</td>
</tr>
</tbody>
</table>
###LogLevel { #hypershift.openshift.io/v1beta1.LogLevel }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec">ClusterVersionOperatorSpec</a>)
</p>
<p>
</p>
###MachineNetworkEntry { #hypershift.openshift.io/v1beta1.MachineNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>MachineNetworkEntry is a single IP address block for node IP blocks.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>CIDR is the IP block address pool for machines within the cluster.</p>
</td>
</tr>
</tbody>
</table>
###ManagedAzureKeyVault { #hypershift.openshift.io/v1beta1.ManagedAzureKeyVault }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities">ControlPlaneManagedIdentities</a>)
</p>
<p>
<p>ManagedAzureKeyVault is an Azure Key Vault on the management cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of the Azure Key Vault on the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>tenantID</code></br>
<em>
string
</em>
</td>
<td>
<p>tenantID is the tenant ID of the Azure Key Vault on the management cluster.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdSpec { #hypershift.openshift.io/v1beta1.ManagedEtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>ManagedEtcdSpec specifies the behavior of an etcd cluster managed by
HyperShift.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storage</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">
ManagedEtcdStorageSpec
</a>
</em>
</td>
<td>
<p>storage specifies how etcd data is persisted.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdStorageSpec { #hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdSpec">ManagedEtcdSpec</a>)
</p>
<p>
<p>ManagedEtcdStorageSpec describes the storage configuration for etcd data.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageType">
ManagedEtcdStorageType
</a>
</em>
</td>
<td>
<p>type is the kind of persistent storage implementation to use for etcd.
Only PersistentVolume is supported at the moment.</p>
</td>
</tr>
<tr>
<td>
<code>persistentVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec">
PersistentVolumeEtcdStorageSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>persistentVolume is the configuration for PersistentVolume etcd storage.
With this implementation, a PersistentVolume will be allocated for every
etcd member (either 1 or 3 depending on the HostedCluster control plane
availability configuration).</p>
</td>
</tr>
<tr>
<td>
<code>restoreSnapshotURL</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>restoreSnapshotURL allows an optional URL to be provided where
an etcd snapshot can be downloaded, for example a pre-signed URL
referencing a storage service.
This snapshot will be restored on initial startup, only when the etcd PV
is empty.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdStorageType { #hypershift.openshift.io/v1beta1.ManagedEtcdStorageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">ManagedEtcdStorageSpec</a>)
</p>
<p>
<p>ManagedEtcdStorageType is a storage type for an etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;PersistentVolume&#34;</p></td>
<td><p>PersistentVolumeEtcdStorage uses PersistentVolumes for etcd storage.</p>
</td>
</tr></tbody>
</table>
###ManagedIdentity { #hypershift.openshift.io/v1beta1.ManagedIdentity }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">AzureKMSSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities">ControlPlaneManagedIdentities</a>)
</p>
<p>
<p>ManagedIdentity contains the client ID, and its certificate name, of a managed identity. This managed identity is
used, by an HCP component, to authenticate with the Azure API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientID</code></br>
<em>
string
</em>
</td>
<td>
<p>clientID is the client ID of a managed identity.
Deprecated: This field was previously required as part of the MIWI phase 2 work; however, this field will be
removed as part of the MIWI phase 3 work, <a href="https://issues.redhat.com/browse/OCPSTRAT-1856">https://issues.redhat.com/browse/OCPSTRAT-1856</a>.</p>
</td>
</tr>
<tr>
<td>
<code>certificateName</code></br>
<em>
string
</em>
</td>
<td>
<p>certificateName is the name of the certificate backing the managed identity. This certificate is expected to
reside in an Azure Key Vault on the management cluster.
Deprecated: This field was previously required as part of the MIWI phase 2 work; however, this field will be
removed as part of the MIWI phase 3 work, <a href="https://issues.redhat.com/browse/OCPSTRAT-1856">https://issues.redhat.com/browse/OCPSTRAT-1856</a>.</p>
</td>
</tr>
<tr>
<td>
<code>objectEncoding</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ObjectEncodingFormat">
ObjectEncodingFormat
</a>
</em>
</td>
<td>
<p>objectEncoding represents the encoding for the Azure Key Vault secret containing the certificate related to
CertificateName. objectEncoding needs to match the encoding format used when the certificate was stored in the
Azure Key Vault. If objectEncoding doesn&rsquo;t match the encoding format of the certificate, the certificate will
unsuccessfully be read by the Secrets CSI driver and an error will occur. This error will only be visible on the
SecretProviderClass custom resource related to the managed identity.</p>
<p>The default value is utf-8.</p>
<p>See this for more info - <a href="https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md">https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md</a></p>
</td>
</tr>
<tr>
<td>
<code>credentialsSecretName</code></br>
<em>
string
</em>
</td>
<td>
<p>credentialsSecretName is the name of an Azure Key Vault secret. This field assumes the secret contains the JSON
format of a UserAssignedIdentityCredentials struct. At a minimum, the secret needs to contain the ClientId,
ClientSecret, AuthenticationEndpoint, NotBefore, and NotAfter, and TenantId.</p>
<p>More info on this struct can be found here - <a href="https://github.com/Azure/msi-dataplane/blob/63fb37d3a1aaac130120624674df795d2e088083/pkg/dataplane/internal/generated_client.go#L156">https://github.com/Azure/msi-dataplane/blob/63fb37d3a1aaac130120624674df795d2e088083/pkg/dataplane/internal/generated_client.go#L156</a>.</p>
<p>credentialsSecretName must be between 1 and 127 characters and use only alphanumeric characters and hyphens.
credentialsSecretName must also be unique within the Azure Key Vault. See more details here - <a href="https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.KeyVault.SecretName/">https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.KeyVault.SecretName/</a>.</p>
<p>TODO set the validation:MinLength=1
TODO set validation:Pattern=<code>^[a-zA-Z0-9-]+$</code></p>
</td>
</tr>
</tbody>
</table>
###MultiQueueSetting { #hypershift.openshift.io/v1beta1.MultiQueueSetting }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disable&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Enable&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###NetworkFilter { #hypershift.openshift.io/v1beta1.NetworkFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">NetworkParam</a>)
</p>
<p>
<p>NetworkFilter specifies a query to select an OpenStack network. At least one property must be set.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is the description of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProjectID is the project ID of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###NetworkParam { #hypershift.openshift.io/v1beta1.NetworkParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.PortSpec">PortSpec</a>)
</p>
<p>
<p>NetworkParam specifies an OpenStack network. It may be specified by either ID or Filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID is the ID of the network to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkFilter">
NetworkFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filter specifies a filter to select an OpenStack network. If provided, cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###NetworkType { #hypershift.openshift.io/v1beta1.NetworkType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>NetworkType specifies the SDN provider used for cluster networking.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Calico&#34;</p></td>
<td><p>Calico specifies Calico as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;OVNKubernetes&#34;</p></td>
<td><p>OVNKubernetes specifies OVN as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;OpenShiftSDN&#34;</p></td>
<td><p>OpenShiftSDN specifies OpenShiftSDN as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;Other&#34;</p></td>
<td><p>Other specifies an undefined SDN provider</p>
</td>
</tr></tbody>
</table>
###NeutronTag { #hypershift.openshift.io/v1beta1.NeutronTag }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">FilterByNeutronTags</a>)
</p>
<p>
<p>NeutronTag represents a tag on a Neutron resource.
It may not be empty and may not contain commas.</p>
</p>
###NodePoolAutoScaling { #hypershift.openshift.io/v1beta1.NodePoolAutoScaling }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolAutoScaling specifies auto-scaling behavior for a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>min</code></br>
<em>
int32
</em>
</td>
<td>
<p>Min is the minimum number of nodes to maintain in the pool. Must be &gt;= 1 and &lt;= .Max.</p>
</td>
</tr>
<tr>
<td>
<code>max</code></br>
<em>
int32
</em>
</td>
<td>
<p>Max is the maximum number of nodes allowed in the pool. Must be &gt;= 1 and &gt;= Min.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolCondition { #hypershift.openshift.io/v1beta1.NodePoolCondition }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">NodePoolStatus</a>)
</p>
<p>
<p>We define our own condition type since metav1.Condition has validation
for Reason that might be broken by what we bubble up from CAPI.
NodePoolCondition defines an observation of NodePool resource operational state.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>Type of condition in CamelCase or in foo.example.com/CamelCase.
Many .condition.type values are consistent across resources like Available, but because arbitrary conditions
can be useful (see .node.status.conditions), the ability to deconflict is important.</p>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#conditionstatus-v1-core">
Kubernetes core/v1.ConditionStatus
</a>
</em>
</td>
<td>
<p>Status of the condition, one of True, False, Unknown.</p>
</td>
</tr>
<tr>
<td>
<code>severity</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Severity provides an explicit classification of Reason code, so the users or machines can immediately
understand the current situation and act accordingly.
The Severity field MUST be set only when Status=False.</p>
</td>
</tr>
<tr>
<td>
<code>lastTransitionTime</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>Last time the condition transitioned from one status to another.
This should be when the underlying condition changed. If that is not known, then using the time when
the API field changed is acceptable.</p>
</td>
</tr>
<tr>
<td>
<code>reason</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The reason for the condition&rsquo;s last transition in CamelCase.
The specific API may choose whether or not this field is considered a guaranteed API.
This field may not be empty.</p>
</td>
</tr>
<tr>
<td>
<code>message</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>A human readable message indicating details about the transition.
This field may be empty.</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code></br>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
###NodePoolManagement { #hypershift.openshift.io/v1beta1.NodePoolManagement }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolManagement specifies behavior for managing nodes in a NodePool, such
as upgrade strategies and auto-repair behaviors.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>upgradeType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UpgradeType">
UpgradeType
</a>
</em>
</td>
<td>
<p>upgradeType specifies the type of strategy for handling upgrades.
This can be either &ldquo;Replace&rdquo; or &ldquo;InPlace&rdquo;.
&ldquo;Replace&rdquo; will update Nodes by recreating the underlying instances.
&ldquo;InPlace&rdquo; will update Nodes by applying changes to the existing instances. This might or might not result in a reboot.</p>
</td>
</tr>
<tr>
<td>
<code>replace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">
ReplaceUpgrade
</a>
</em>
</td>
<td>
<p>replace is the configuration for rolling upgrades.
It defaults to a RollingUpdate strategy with maxSurge of 1 and maxUnavailable of 0.</p>
</td>
</tr>
<tr>
<td>
<code>inPlace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.InPlaceUpgrade">
InPlaceUpgrade
</a>
</em>
</td>
<td>
<p>inPlace is the configuration for in-place upgrades.</p>
</td>
</tr>
<tr>
<td>
<code>autoRepair</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoRepair specifies whether health checks should be enabled for machines in the NodePool. The default is false.
Enabling this feature will cause the controller to automatically delete unhealthy machines.
The unhealthy criteria is reserved for the controller implementation and subject to change.
But generally it&rsquo;s determined by checking the Node ready condition is true and a timeout that might vary depending on the platform provider.
AutoRepair will no-op when more than 2 Nodes are unhealthy at the same time. Giving time for the cluster to stabilize or to the user to manually intervene.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolPlatform { #hypershift.openshift.io/v1beta1.NodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolPlatform specifies the underlying infrastructure provider for the
NodePool and is used to configure platform specific behavior.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>Type specifies the platform name.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">
AWSNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS specifies the configuration used when operating on AWS.</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec">
IBMCloudPlatformSpec
</a>
</em>
</td>
<td>
<p>IBMCloud defines IBMCloud specific settings for components</p>
</td>
</tr>
<tr>
<td>
<code>kubevirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">
KubevirtNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Kubevirt specifies the configuration used when operating on KubeVirt platform.</p>
</td>
</tr>
<tr>
<td>
<code>agent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AgentNodePoolPlatform">
AgentNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Agent specifies the configuration used when using Agent platform.</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">
AzureNodePoolPlatform
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>powervs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">
PowerVSNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PowerVS specifies the configuration used when using IBMCloud PowerVS platform.</p>
</td>
</tr>
<tr>
<td>
<code>openstack</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform">
OpenStackNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OpenStack specifies the configuration used when using OpenStack platform.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolPlatformStatus { #hypershift.openshift.io/v1beta1.NodePoolPlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">NodePoolStatus</a>)
</p>
<p>
<p>NodePoolPlatformStatus contains specific platform statuses</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kubeVirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus">
KubeVirtNodePoolStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeVirt contains the KubeVirt platform statuses</p>
</td>
</tr>
</tbody>
</table>
###NodePoolSpec { #hypershift.openshift.io/v1beta1.NodePoolSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePool">NodePool</a>)
</p>
<p>
<p>NodePoolSpec is the desired behavior of a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterName</code></br>
<em>
string
</em>
</td>
<td>
<p>clusterName is the name of the HostedCluster this NodePool belongs to.
If a HostedCluster with this name doesn&rsquo;t exist, the controller will no-op until it exists.</p>
</td>
</tr>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the OCP release used for the NodePool. This informs the
ignition configuration for machines which includes the kubelet version, as well as other platform specific
machine properties (e.g. an AMI on the AWS platform).
It&rsquo;s not supported to use a release in a NodePool which minor version skew against the Control Plane release is bigger than N-2. Although there&rsquo;s no enforcement that prevents this from happening.
Attempting to use a release with a bigger skew might result in unpredictable behaviour.
Attempting to use a release higher than the HosterCluster one will result in the NodePool being degraded and the ValidReleaseImage condition being false.
Attempting to use a release lower than the current NodePool y-stream will result in the NodePool being degraded and the ValidReleaseImage condition being false.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">
NodePoolPlatform
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the NodePool
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>replicas is the desired number of nodes the pool should maintain. If unset, the controller default value is 0.
replicas is mutually exclusive with autoscaling. If autoscaling is configured, replicas must be omitted and autoscaling will control the NodePool size internally.</p>
</td>
</tr>
<tr>
<td>
<code>management</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">
NodePoolManagement
</a>
</em>
</td>
<td>
<p>management specifies behavior for managing nodes in the pool, such as
upgrade strategies and auto-repair behaviors.</p>
</td>
</tr>
<tr>
<td>
<code>autoScaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolAutoScaling">
NodePoolAutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior for the NodePool.
autoscaling is mutually exclusive with replicas. If replicas is set, this field must be omitted.</p>
</td>
</tr>
<tr>
<td>
<code>config</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>config is a list of references to ConfigMaps containing serialized
MachineConfig resources to be injected into the ignition configurations of
nodes in the NodePool. The MachineConfig API schema is defined here:</p>
<p><a href="https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185">https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</a></p>
<p>Each ConfigMap must have a single key named &ldquo;config&rdquo; whose value is the YML
with one or more serialized machineconfiguration.openshift.io resources:</p>
<ul>
<li>KubeletConfig</li>
<li>ContainerRuntimeConfig</li>
<li>MachineConfig</li>
<li>ClusterImagePolicy</li>
<li>ImageContentSourcePolicy</li>
<li>ImageDigestMirrorSet</li>
</ul>
<p>This is validated in the backend and signaled back via validMachineConfig condition.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeDrainTimeout is the maximum amount of time that the controller will spend on retrying to drain a node until it succeeds.
The default value is 0, meaning that the node can retry drain without any time limitations.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeVolumeDetachTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node.
The default value is 0, meaning that the volumes will be detached from the node without any time limitations.
After the timeout, any remaining attached volumes will be ignored and the removal of the machine will continue.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeLabels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeLabels propagates a list of labels to Nodes, only once on creation.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
</td>
</tr>
<tr>
<td>
<code>taints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Taint">
[]Taint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>taints if specified, propagates a list of taints to Nodes, only once on creation.
These taints are additive to the ones applied by other controllers</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the NodePool controller. Resulting in any change to the NodePool being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>tuningConfig is a list of references to ConfigMaps containing serialized
Tuned or PerformanceProfile resources to define the tuning configuration to be applied to
nodes in the NodePool. The Tuned API is defined here:</p>
<p><a href="https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go">https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</a></p>
<p>The PerformanceProfile API is defined here:
<a href="https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2">https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</a></p>
<p>Each ConfigMap must have a single key named &ldquo;tuning&rdquo; whose value is the
JSON or YAML of a serialized Tuned or PerformanceProfile.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>arch</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>arch is the preferred processor architecture for the NodePool. Different platforms might have different supported architectures.
TODO: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery:
<a href="https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215">https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</a>
TODO Add s390x to enum validation once the architecture is supported</p>
</td>
</tr>
</tbody>
</table>
###NodePoolStatus { #hypershift.openshift.io/v1beta1.NodePoolStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePool">NodePool</a>)
</p>
<p>
<p>NodePoolStatus is the latest observed status of a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the latest observed number of nodes in the pool.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<p>Version is the semantic version of the latest applied release specified by
the NodePool.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus">
NodePoolPlatformStatus
</a>
</em>
</td>
<td>
<p>Platform hols the specific statuses</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolCondition">
[]NodePoolCondition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions represents the latest available observations of the node pool&rsquo;s
current state.</p>
</td>
</tr>
</tbody>
</table>
###NodePortPublishingStrategy { #hypershift.openshift.io/v1beta1.NodePortPublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>NodePortPublishingStrategy specifies a NodePort used to expose a service.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>address</code></br>
<em>
string
</em>
</td>
<td>
<p>address is the host/ip that the NodePort service is exposed over.</p>
</td>
</tr>
<tr>
<td>
<code>port</code></br>
<em>
int32
</em>
</td>
<td>
<p>port is the port of the NodePort service. If &lt;=0, the port is dynamically
assigned when the service is created.</p>
</td>
</tr>
</tbody>
</table>
###OLMCatalogPlacement { #hypershift.openshift.io/v1beta1.OLMCatalogPlacement }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>OLMCatalogPlacement is an enum specifying the placement of OLM catalog components.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;guest&#34;</p></td>
<td><p>GuestOLMCatalogPlacement indicates OLM catalog components will be placed in
the guest cluster.</p>
</td>
</tr><tr><td><p>&#34;management&#34;</p></td>
<td><p>ManagementOLMCatalogPlacement indicates OLM catalog components will be placed in
the management cluster.</p>
</td>
</tr></tbody>
</table>
###ObjectEncodingFormat { #hypershift.openshift.io/v1beta1.ObjectEncodingFormat }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">ManagedIdentity</a>)
</p>
<p>
<p>ObjectEncodingFormat is the type of encoding for an Azure Key Vault secret</p>
</p>
###OpenStackIdentityReference { #hypershift.openshift.io/v1beta1.OpenStackIdentityReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>OpenStackIdentityReference is a reference to an infrastructure
provider identity to be used to provision cluster resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name is the name of a secret in the same namespace as the resource being provisioned.
The secret must contain a key named <code>clouds.yaml</code> which contains an OpenStack clouds.yaml file.
The secret may optionally contain a key named <code>cacert</code> containing a PEM-encoded CA certificate.</p>
</td>
</tr>
<tr>
<td>
<code>cloudName</code></br>
<em>
string
</em>
</td>
<td>
<p>CloudName specifies the name of the entry in the clouds.yaml file to use.</p>
</td>
</tr>
</tbody>
</table>
###OpenStackNodePoolPlatform { #hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>flavor</code></br>
<em>
string
</em>
</td>
<td>
<p>Flavor is the OpenStack flavor to use for the node instances.</p>
</td>
</tr>
<tr>
<td>
<code>imageName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageName is the OpenStack Glance image name to use for node instances. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>availabilityZone is the nova availability zone in which the provider will create the VM.
If not specified, the VM will be created in the default availability zone specified in the nova configuration.
Availability zone names must NOT contain : since it is used by admin users to specify hosts where instances
are launched in server creation. Also, it must not contain spaces otherwise it will lead to node that belongs
to this availability zone register failure, see kubernetes/cloud-provider-openstack#1379 for further information.
The maximum length of availability zone name is 63 as per labels limits.</p>
</td>
</tr>
<tr>
<td>
<code>additionalPorts</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PortSpec">
[]PortSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalPorts is a list of additional ports to create on the node instances.</p>
</td>
</tr>
</tbody>
</table>
###OpenStackPlatformSpec { #hypershift.openshift.io/v1beta1.OpenStackPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>OpenStackPlatformSpec specifies configuration for clusters running on OpenStack.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>identityRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackIdentityReference">
OpenStackIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to a secret holding OpenStack credentials
to be used when reconciling the hosted cluster.</p>
</td>
</tr>
<tr>
<td>
<code>managedSubnets</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetSpec">
[]SubnetSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ManagedSubnets describe the OpenStack Subnet to be created. Cluster actuator will create a network,
and a subnet with the defined DNSNameservers, AllocationPools and the CIDR defined in the HostedCluster
MachineNetwork, and a router connected to the subnet. Currently only one IPv4
subnet is supported.</p>
</td>
</tr>
<tr>
<td>
<code>router</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RouterParam">
RouterParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Router specifies an existing router to be used if ManagedSubnets are
specified. If specified, no new router will be created.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network specifies an existing network to use if no ManagedSubnets
are specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetParam">
[]SubnetParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets specifies existing subnets to use if not ManagedSubnets are
specified. All subnets must be in the network specified by Network.
There can be zero, one, or two subnets. If no subnets are specified,
all subnets in Network will be used. If 2 subnets are specified, one
must be IPv4 and the other IPv6.</p>
</td>
</tr>
<tr>
<td>
<code>networkMTU</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkMTU sets the maximum transmission unit (MTU) value to address fragmentation for the private network ID.
This value will be used only if the Cluster actuator creates the network.
If left empty, the network will have the default MTU defined in Openstack network service.
To use this field, the Openstack installation requires the net-mtu neutron API extension.</p>
</td>
</tr>
<tr>
<td>
<code>externalNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ExternalNetwork is the OpenStack Network to be used to get public internet to the VMs.
This option is ignored if DisableExternalNetwork is set to true.</p>
<p>If ExternalNetwork is defined it must refer to exactly one external network.</p>
<p>If ExternalNetwork is not defined or is empty the controller will use any
existing external network as long as there is only one. It is an
error if ExternalNetwork is not defined and there are multiple
external networks unless DisableExternalNetwork is also set.</p>
<p>If ExternalNetwork is not defined and there are no external networks
the controller will proceed as though DisableExternalNetwork was set.</p>
</td>
</tr>
<tr>
<td>
<code>disableExternalNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>DisableExternalNetwork specifies whether or not to attempt to connect the cluster
to an external network. This allows for the creation of clusters when connecting
to an external network is not possible or desirable, e.g. if using a provider network.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tags to set on all resources in cluster which support tags</p>
</td>
</tr>
<tr>
<td>
<code>ingressFloatingIP</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IngressFloatingIP is an available floating IP in your OpenStack cluster that will
be associated with the OpenShift ingress port.
When not specified, an IP address will be assigned randomly by the OpenStack cloud provider.
When specified, the floating IP has to be pre-created.  If the
specified value is not a floating IP or is already claimed, the
OpenStack cloud provider won&rsquo;t be able to provision the load
balancer.
This value must be a valid IPv4 or IPv6 address.</p>
</td>
</tr>
</tbody>
</table>
###OperatorConfiguration { #hypershift.openshift.io/v1beta1.OperatorConfiguration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>OperatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterVersionOperator</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec">
ClusterVersionOperatorSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterVersionOperator specifies the configuration for the Cluster Version Operator in the hosted cluster.</p>
</td>
</tr>
</tbody>
</table>
###OptionalCapability { #hypershift.openshift.io/v1beta1.OptionalCapability }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">Capabilities</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;ImageRegistry&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###PayloadArchType { #hypershift.openshift.io/v1beta1.PayloadArchType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AMD64&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ARM64&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Multi&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;PPC64LE&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;S390X&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###PersistentVolumeAccessMode { #hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume">KubevirtPersistentVolume</a>)
</p>
<p>
</p>
###PersistentVolumeEtcdStorageSpec { #hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">ManagedEtcdStorageSpec</a>)
</p>
<p>
<p>PersistentVolumeEtcdStorageSpec is the configuration for PersistentVolume
etcd storage.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageClassName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageClassName is the StorageClass of the data volume for each etcd member.
See <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a>.
TODO(alberto): This shouldn&rsquo;t really be a pointer. There&rsquo;s no real different semantic for nil and empty string. Revisit all pointer vs non-pointer choices.</p>
</td>
</tr>
<tr>
<td>
<code>size</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>size is the minimum size of the data volume for each etcd member.
Default is 8Gi.
This field is immutable</p>
</td>
</tr>
</tbody>
</table>
###PlacementOptions { #hypershift.openshift.io/v1beta1.PlacementOptions }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>PlacementOptions specifies the placement options for the EC2 instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>tenancy</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.</p>
<p>Possible values:
default: NodePool instances run on shared hardware.
dedicated: Each NodePool instance runs on single-tenant hardware.
host: NodePool instances run on user&rsquo;s pre-allocated dedicated hosts.</p>
</td>
</tr>
</tbody>
</table>
###PlatformSpec { #hypershift.openshift.io/v1beta1.PlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>PlatformSpec specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>Type is the type of infrastructure provider for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">
AWSPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS specifies configuration for clusters running on Amazon Web Services.</p>
</td>
</tr>
<tr>
<td>
<code>agent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AgentPlatformSpec">
AgentPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Agent specifies configuration for agent-based installations.</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec">
IBMCloudPlatformSpec
</a>
</em>
</td>
<td>
<p>IBMCloud defines IBMCloud specific settings for components</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzurePlatformSpec">
AzurePlatformSpec
</a>
</em>
</td>
<td>
<p>Azure defines azure specific settings</p>
</td>
</tr>
<tr>
<td>
<code>powervs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">
PowerVSPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PowerVS specifies configuration for clusters running on IBMCloud Power VS Service.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>kubevirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">
KubevirtPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeVirt defines KubeVirt specific settings for cluster components.</p>
</td>
</tr>
<tr>
<td>
<code>openstack</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">
OpenStackPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OpenStack specifies configuration for clusters running on OpenStack.</p>
</td>
</tr>
</tbody>
</table>
###PlatformStatus { #hypershift.openshift.io/v1beta1.PlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>PlatformStatus contains platform-specific status</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformStatus">
AWSPlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
</tbody>
</table>
###PlatformType { #hypershift.openshift.io/v1beta1.PlatformType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterConfig">KarpenterConfig</a>, 
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>PlatformType is a specific supported infrastructure provider.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWS&#34;</p></td>
<td><p>AWSPlatform represents Amazon Web Services infrastructure.</p>
</td>
</tr><tr><td><p>&#34;Agent&#34;</p></td>
<td><p>AgentPlatform represents user supplied insfrastructure booted with agents.</p>
</td>
</tr><tr><td><p>&#34;Azure&#34;</p></td>
<td><p>AzurePlatform represents Azure infrastructure.</p>
</td>
</tr><tr><td><p>&#34;IBMCloud&#34;</p></td>
<td><p>IBMCloudPlatform represents IBM Cloud infrastructure.</p>
</td>
</tr><tr><td><p>&#34;KubeVirt&#34;</p></td>
<td><p>KubevirtPlatform represents Kubevirt infrastructure.</p>
</td>
</tr><tr><td><p>&#34;None&#34;</p></td>
<td><p>NonePlatform represents user supplied (e.g. bare metal) infrastructure.</p>
</td>
</tr><tr><td><p>&#34;OpenStack&#34;</p></td>
<td><p>OpenStackPlatform represents OpenStack infrastructure.</p>
</td>
</tr><tr><td><p>&#34;PowerVS&#34;</p></td>
<td><p>PowerVSPlatform represents PowerVS infrastructure.</p>
</td>
</tr></tbody>
</table>
###PortSecurityPolicy { #hypershift.openshift.io/v1beta1.PortSecurityPolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PortSpec">PortSpec</a>)
</p>
<p>
<p>PortSecurityPolicy defines whether or not to enable port security on a port.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;&#34;</p></td>
<td><p>PortSecurityDefault uses the default port security policy.</p>
</td>
</tr><tr><td><p>&#34;Disabled&#34;</p></td>
<td><p>PortSecurityDisabled disables port security on a port.</p>
</td>
</tr><tr><td><p>&#34;Enabled&#34;</p></td>
<td><p>PortSecurityEnabled enables port security on a port.</p>
</td>
</tr></tbody>
</table>
###PortSpec { #hypershift.openshift.io/v1beta1.PortSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform">OpenStackNodePoolPlatform</a>)
</p>
<p>
<p>PortSpec specifies the options for creating a port.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network is a query for an openstack network that the port will be created or discovered on.
This will fail if the query returns more than one network.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is a human-readable description for the port.</p>
</td>
</tr>
<tr>
<td>
<code>allowedAddressPairs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AddressPair">
[]AddressPair
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedAddressPairs is a list of address pairs which Neutron will
allow the port to send traffic from in addition to the port&rsquo;s
addresses. If not specified, the MAC Address will be the MAC Address
of the port. Depending on the configuration of Neutron, it may be
supported to specify a CIDR instead of a specific IP address.</p>
</td>
</tr>
<tr>
<td>
<code>vnicType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>VNICType specifies the type of vNIC which this port should be
attached to. This is used to determine which mechanism driver(s) to
be used to bind the port. The valid values are normal, macvtap,
direct, baremetal, direct-physical, virtio-forwarder, smart-nic and
remote-managed, although these values will not be validated in this
API to ensure compatibility with future neutron changes or custom
implementations. What type of vNIC is actually available depends on
deployments. If not specified, the Neutron default value is used.</p>
</td>
</tr>
<tr>
<td>
<code>portSecurityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PortSecurityPolicy">
PortSecurityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PortSecurityPolicy specifies whether or not to enable port security on the port.
Allowed values are &ldquo;Enabled&rdquo;, &ldquo;Disabled&rdquo; and omitted.
When not set, it takes the value of the corresponding field at the network level.</p>
</td>
</tr>
</tbody>
</table>
###PowerVSNodePoolImageDeletePolicy { #hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolImageDeletePolicy defines image delete policy to be used for PowerVSNodePoolPlatform</p>
</p>
###PowerVSNodePoolPlatform { #hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolPlatform specifies the configuration of a NodePool when operating
on IBMCloud PowerVS platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>systemType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SystemType is the System type used to host the instance.
systemType determines the number of cores and memory that is available.
Few of the supported SystemTypes are s922,e880,e980.
e880 systemType available only in Dallas Datacenters.
e980 systemType available in Datacenters except Dallas and Washington.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The current default is s922 which is generally available.</p>
</td>
</tr>
<tr>
<td>
<code>processorType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType">
PowerVSNodePoolProcType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProcessorType is the VM instance processor type.
It must be set to one of the following values: Dedicated, Capped or Shared.</p>
<p>Dedicated: resources are allocated for a specific client, The hypervisor makes a 1:1 binding of a partitionâ€™s processor to a physical processor core.
Shared: Shared among other clients.
Capped: Shared, but resources do not expand beyond those that are requested, the amount of CPU time is Capped to the value specified for the entitlement.</p>
<p>if the processorType is selected as Dedicated, then Processors value cannot be fractional.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The current default is shared.</p>
</td>
</tr>
<tr>
<td>
<code>processors</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Processors is the number of virtual processors in a virtual machine.
when the processorType is selected as Dedicated the processors value cannot be fractional.
maximum value for the Processors depends on the selected SystemType.
when SystemType is set to e880 or e980 maximum Processors value is 143.
when SystemType is set to s922 maximum Processors value is 15.
minimum value for Processors depends on the selected ProcessorType.
when ProcessorType is set as Shared or Capped, The minimum processors is 0.5.
when ProcessorType is set as Dedicated, The minimum processors is 1.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The default is set based on the selected ProcessorType.
when ProcessorType selected as Dedicated, the default is set to 1.
when ProcessorType selected as Shared or Capped, the default is set to 0.5.</p>
</td>
</tr>
<tr>
<td>
<code>memoryGiB</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>MemoryGiB is the size of a virtual machine&rsquo;s memory, in GiB.
maximum value for the MemoryGiB depends on the selected SystemType.
when SystemType is set to e880 maximum MemoryGiB value is 7463 GiB.
when SystemType is set to e980 maximum MemoryGiB value is 15307 GiB.
when SystemType is set to s922 maximum MemoryGiB value is 942 GiB.
The minimum memory is 32 GiB.</p>
<p>When omitted, this means the user has no opinion and the platform is left to choose a reasonable
default. The current default is 32.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSResourceReference">
PowerVSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Image used for deploying the nodes. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>storageType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType">
PowerVSNodePoolStorageType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageType for the image and nodes, this will be ignored if Image is specified.
The storage tiers in PowerVS are based on I/O operations per second (IOPS).
It means that the performance of your storage volumes is limited to the maximum number of IOPS based on volume size and storage tier.
Although, the exact numbers might change over time, the Tier 3 storage is currently set to 3 IOPS/GB, and the Tier 1 storage is currently set to 10 IOPS/GB.</p>
<p>The default is tier1</p>
</td>
</tr>
<tr>
<td>
<code>imageDeletePolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy">
PowerVSNodePoolImageDeletePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageDeletePolicy is policy for the image deletion.</p>
<p>delete: delete the image from the infrastructure.
retain: delete the image from the openshift but retain in the infrastructure.</p>
<p>The default is delete</p>
</td>
</tr>
</tbody>
</table>
###PowerVSNodePoolProcType { #hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolProcType defines processor type to be used for PowerVSNodePoolPlatform</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;capped&#34;</p></td>
<td><p>PowerVSNodePoolCappedProcType defines capped processor type</p>
</td>
</tr><tr><td><p>&#34;dedicated&#34;</p></td>
<td><p>PowerVSNodePoolDedicatedProcType defines dedicated processor type</p>
</td>
</tr><tr><td><p>&#34;shared&#34;</p></td>
<td><p>PowerVSNodePoolSharedProcType defines shared processor type</p>
</td>
</tr></tbody>
</table>
###PowerVSNodePoolStorageType { #hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolStorageType defines storage type to be used for PowerVSNodePoolPlatform</p>
</p>
###PowerVSPlatformSpec { #hypershift.openshift.io/v1beta1.PowerVSPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>PowerVSPlatformSpec defines IBMCloud PowerVS specific settings for components</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>accountID</code></br>
<em>
string
</em>
</td>
<td>
<p>AccountID is the IBMCloud account id.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>cisInstanceCRN</code></br>
<em>
string
</em>
</td>
<td>
<p>CISInstanceCRN is the IBMCloud CIS Service Instance&rsquo;s Cloud Resource Name
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>resourceGroup</code></br>
<em>
string
</em>
</td>
<td>
<p>ResourceGroup is the IBMCloud Resource Group in which the cluster resides.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the IBMCloud region in which the cluster resides. This configures the
OCP control plane cloud integrations, and is used by NodePool to resolve
the correct boot image for a given release.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<p>Zone is the availability zone where control plane cloud resources are
created.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSResourceReference">
PowerVSResourceReference
</a>
</em>
</td>
<td>
<p>Subnet is the subnet to use for control plane cloud resources.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>serviceInstanceID</code></br>
<em>
string
</em>
</td>
<td>
<p>ServiceInstance is the reference to the Power VS service on which the server instance(VM) will be created.
Power VS service is a container for all Power VS instances at a specific geographic region.
serviceInstance can be created via IBM Cloud catalog or CLI.
ServiceInstanceID is the unique identifier that can be obtained from IBM Cloud UI or IBM Cloud cli.</p>
<p>More detail about Power VS service instance.
<a href="https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server">https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server</a></p>
<p>This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>vpc</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSVPC">
PowerVSVPC
</a>
</em>
</td>
<td>
<p>VPC specifies IBM Cloud PowerVS Load Balancing configuration for the control
plane.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>KubeCloudControllerCreds is a reference to a secret containing cloud
credentials with permissions matching the cloud controller policy.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
<p>TODO(dan): document the &ldquo;cloud controller policy&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>NodePoolManagementCreds is a reference to a secret containing cloud
credentials with permissions matching the node pool management policy.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
<p>TODO(dan): document the &ldquo;node pool management policy&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>ingressOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>IngressOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for ingress operator to get authenticated with ibm cloud.</p>
</td>
</tr>
<tr>
<td>
<code>storageOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>StorageOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for storage operator to get authenticated with ibm cloud.</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ImageRegistryOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for image registry operator to get authenticated with ibm cloud.</p>
</td>
</tr>
</tbody>
</table>
###PowerVSResourceReference { #hypershift.openshift.io/v1beta1.PowerVSResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">PowerVSPlatformSpec</a>)
</p>
<p>
<p>PowerVSResourceReference is a reference to a specific IBMCloud PowerVS resource by ID, or Name.
Only one of ID, or Name may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name of resource</p>
</td>
</tr>
</tbody>
</table>
###PowerVSVPC { #hypershift.openshift.io/v1beta1.PowerVSVPC }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">PowerVSPlatformSpec</a>)
</p>
<p>
<p>PowerVSVPC specifies IBM Cloud PowerVS LoadBalancer configuration for the control
plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name for VPC to used for all the service load balancer.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the IBMCloud region in which VPC gets created, this VPC used for all the ingress traffic
into the OCP cluster.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Zone is the availability zone where load balancer cloud resources are
created.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is the subnet to use for load balancer.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
</tbody>
</table>
###Provisioner { #hypershift.openshift.io/v1beta1.Provisioner }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ProvisionerConfig">ProvisionerConfig</a>)
</p>
<p>
<p>provisioner is a enum specifying the strategy for auto managing Nodes.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Karpenter&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###ProvisionerConfig { #hypershift.openshift.io/v1beta1.ProvisionerConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">AutoNode</a>)
</p>
<p>
<p>ProvisionerConfig is a enum specifying the strategy for auto managing Nodes.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Provisioner">
Provisioner
</a>
</em>
</td>
<td>
<p>name specifies the name of the provisioner to use.</p>
</td>
</tr>
<tr>
<td>
<code>karpenter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterConfig">
KarpenterConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>karpenter specifies the configuration for the Karpenter provisioner.</p>
</td>
</tr>
</tbody>
</table>
###PublishingStrategyType { #hypershift.openshift.io/v1beta1.PublishingStrategyType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>PublishingStrategyType defines publishing strategies for services.</p>
</p>
###QoSClass { #hypershift.openshift.io/v1beta1.QoSClass }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCompute">KubevirtCompute</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Burstable&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Guaranteed&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###Release { #hypershift.openshift.io/v1beta1.Release }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>Release represents the metadata for an OCP release payload image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>image</code></br>
<em>
string
</em>
</td>
<td>
<p>Image is the image pullspec of an OCP release payload image.
See <a href="https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags">https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags</a> for a list of available images.</p>
</td>
</tr>
</tbody>
</table>
###ReplaceUpgrade { #hypershift.openshift.io/v1beta1.ReplaceUpgrade }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>ReplaceUpgrade specifies upgrade behavior that replaces existing nodes
according to a given strategy.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>strategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UpgradeStrategy">
UpgradeStrategy
</a>
</em>
</td>
<td>
<p>strategy is the node replacement strategy for nodes in the pool.
In can be either &ldquo;RollingUpdate&rdquo; or &ldquo;OnDelete&rdquo;. RollingUpdate will rollout Nodes honoring maxSurge and maxUnavailable.
OnDelete provide more granular control and will replace nodes as the old ones are manually deleted.</p>
</td>
</tr>
<tr>
<td>
<code>rollingUpdate</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RollingUpdate">
RollingUpdate
</a>
</em>
</td>
<td>
<p>rollingUpdate specifies a rolling update strategy which upgrades nodes by
creating new nodes and deleting the old ones.</p>
</td>
</tr>
</tbody>
</table>
###RollingUpdate { #hypershift.openshift.io/v1beta1.RollingUpdate }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">ReplaceUpgrade</a>)
</p>
<p>
<p>RollingUpdate specifies a rolling update strategy which upgrades nodes by
creating new nodes and deleting the old ones.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxUnavailable is the maximum number of nodes that can be unavailable
during the update.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding down.</p>
<p>This can not be 0 if MaxSurge is 0.</p>
<p>Defaults to 0.</p>
<p>Example: when this is set to 30%, old nodes can be deleted down to 70% of
desired nodes immediately when the rolling update starts. Once new nodes
are ready, more old nodes be deleted, followed by provisioning new nodes,
ensuring that the total number of nodes available at all times during the
update is at least 70% of desired nodes.</p>
</td>
</tr>
<tr>
<td>
<code>maxSurge</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxSurge is the maximum number of nodes that can be provisioned above the
desired number of nodes.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding up.</p>
<p>This can not be 0 if MaxUnavailable is 0.</p>
<p>Defaults to 1.</p>
<p>Example: when this is set to 30%, new nodes can be provisioned immediately
when the rolling update starts, such that the total number of old and new
nodes do not exceed 130% of desired nodes. Once old nodes have been
deleted, new nodes can be provisioned, ensuring that total number of nodes
running at any time during the update is at most 130% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
###RoutePublishingStrategy { #hypershift.openshift.io/v1beta1.RoutePublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>RoutePublishingStrategy specifies options for exposing a service as a Route.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Hostname is the name of the DNS record that will be created pointing to the Route and passed through to consumers of the service.
If omitted, the value will be inferred from management ingress.Spec.Domain.</p>
</td>
</tr>
</tbody>
</table>
###RouterFilter { #hypershift.openshift.io/v1beta1.RouterFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.RouterParam">RouterParam</a>)
</p>
<p>
<p>RouterFilter specifies a query to select an OpenStack router. At least one property must be set.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is the description of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProjectID is the project ID of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###RouterParam { #hypershift.openshift.io/v1beta1.RouterParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>RouterParam specifies an OpenStack router to use. It may be specified by either ID or filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID is the ID of the router to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RouterFilter">
RouterFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filter specifies a filter to select an OpenStack router. If provided, cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###SecretEncryptionSpec { #hypershift.openshift.io/v1beta1.SecretEncryptionSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>SecretEncryptionSpec contains metadata about the kubernetes secret encryption strategy being used for the
cluster when applicable.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionType">
SecretEncryptionType
</a>
</em>
</td>
<td>
<p>Type defines the type of kube secret encryption being used</p>
</td>
</tr>
<tr>
<td>
<code>kms</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">
KMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KMS defines metadata about the kms secret encryption strategy</p>
</td>
</tr>
<tr>
<td>
<code>aescbc</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AESCBCSpec">
AESCBCSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AESCBC defines metadata about the AESCBC secret encryption strategy</p>
</td>
</tr>
</tbody>
</table>
###SecretEncryptionType { #hypershift.openshift.io/v1beta1.SecretEncryptionType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>SecretEncryptionType defines the type of kube secret encryption being used.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;aescbc&#34;</p></td>
<td><p>AESCBC uses AES-CBC with PKCS#7 padding to do secret encryption</p>
</td>
</tr><tr><td><p>&#34;kms&#34;</p></td>
<td><p>KMS integrates with a cloud provider&rsquo;s key management service to do secret encryption</p>
</td>
</tr></tbody>
</table>
###ServiceNetworkEntry { #hypershift.openshift.io/v1beta1.ServiceNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>ServiceNetworkEntry is a single IP address block for the service network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>cidr is the IP block address pool for services within the cluster in CIDR format (e.g., 192.168.1.0/24 or 2001:0db8::/64)</p>
</td>
</tr>
</tbody>
</table>
###ServicePublishingStrategy { #hypershift.openshift.io/v1beta1.ServicePublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">ServicePublishingStrategyMapping</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PublishingStrategyType">
PublishingStrategyType
</a>
</em>
</td>
<td>
<p>type is the publishing strategy used for the service.
It can be LoadBalancer;NodePort;Route;None;S3</p>
</td>
</tr>
<tr>
<td>
<code>nodePort</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePortPublishingStrategy">
NodePortPublishingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodePort configures exposing a service using a NodePort.</p>
</td>
</tr>
<tr>
<td>
<code>loadBalancer</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy">
LoadBalancerPublishingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>loadBalancer configures exposing a service using a dedicated LoadBalancer.</p>
</td>
</tr>
<tr>
<td>
<code>route</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RoutePublishingStrategy">
RoutePublishingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>route configures exposing a service using a Route through and an ingress controller behind a cloud Load Balancer.
The specifics of the setup are platform dependent.</p>
</td>
</tr>
</tbody>
</table>
###ServicePublishingStrategyMapping { #hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ServicePublishingStrategyMapping specifies how individual control plane services endpoints are published for consumption.
This includes APIServer;OAuthServer;Konnectivity;Ignition.
If a given service is not present in this list, it will be exposed publicly by default.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>service</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServiceType">
ServiceType
</a>
</em>
</td>
<td>
<p>service identifies the type of service being published.
It can be APIServer;OAuthServer;Konnectivity;Ignition
OVNSbDb;OIDC are no-op and kept for backward compatibility.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>servicePublishingStrategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">
ServicePublishingStrategy
</a>
</em>
</td>
<td>
<p>servicePublishingStrategy specifies how to publish a service endpoint.</p>
</td>
</tr>
</tbody>
</table>
###ServiceType { #hypershift.openshift.io/v1beta1.ServiceType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">ServicePublishingStrategyMapping</a>)
</p>
<p>
<p>ServiceType defines what control plane services can be exposed from the
management control plane.</p>
</p>
###SubnetFilter { #hypershift.openshift.io/v1beta1.SubnetFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SubnetParam">SubnetParam</a>)
</p>
<p>
<p>SubnetFilter specifies a filter to select a subnet. At least one parameter must be specified.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is the description of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProjectID is the project ID of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipVersion</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPVersion is the IP version of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>gatewayIP</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>GatewayIP is the gateway IP of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>cidr</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CIDR is the CIDR of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6AddressMode</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6AddressMode is the IPv6 address mode of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6RAMode</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6RAMode is the IPv6 RA mode of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###SubnetParam { #hypershift.openshift.io/v1beta1.SubnetParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>SubnetParam specifies an OpenStack subnet to use. It may be specified by either ID or filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID is the uuid of the subnet. It will not be validated.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetFilter">
SubnetFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filter specifies a filter to select the subnet. It must match exactly one subnet.</p>
</td>
</tr>
</tbody>
</table>
###SubnetSpec { #hypershift.openshift.io/v1beta1.SubnetSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>dnsNameservers</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSNameservers holds a list of DNS server addresses that will be provided when creating
the subnet. These addresses need to have the same IP version as CIDR.</p>
</td>
</tr>
<tr>
<td>
<code>allocationPools</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AllocationPool">
[]AllocationPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllocationPools is an array of AllocationPool objects that will be applied to OpenStack Subnet being created.
If set, OpenStack will only allocate these IPs for Machines. It will still be possible to create ports from
outside of these ranges manually.</p>
</td>
</tr>
</tbody>
</table>
###Taint { #hypershift.openshift.io/v1beta1.Taint }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>taint is as v1 Core but without TimeAdded.
<a href="https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053">https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053</a>
Validation replicates the same validation as the upstream <a href="https://github.com/kubernetes/kubernetes/blob/9a2a7537f035969a68e432b4cc276dbce8ce1735/pkg/util/taints/taints.go#L273">https://github.com/kubernetes/kubernetes/blob/9a2a7537f035969a68e432b4cc276dbce8ce1735/pkg/util/taints/taints.go#L273</a>.
See also <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/">https://kubernetes.io/docs/concepts/overview/working-with-objects/names/</a>.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>key is the taint key to be applied to a node.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>value is the taint value corresponding to the taint key.</p>
</td>
</tr>
<tr>
<td>
<code>effect</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#tainteffect-v1-core">
Kubernetes core/v1.TaintEffect
</a>
</em>
</td>
<td>
<p>effect is the effect of the taint on pods
that do not tolerate the taint.
Valid effects are NoSchedule, PreferNoSchedule and NoExecute.</p>
</td>
</tr>
</tbody>
</table>
###UnmanagedEtcdSpec { #hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>UnmanagedEtcdSpec specifies configuration which enables the control plane to
integrate with an eternally managed etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>endpoint</code></br>
<em>
string
</em>
</td>
<td>
<p>endpoint is the full etcd cluster client endpoint URL. For example:</p>
<pre><code>https://etcd-client:2379
</code></pre>
<p>If the URL uses an HTTPS scheme, the TLS field is required.</p>
</td>
</tr>
<tr>
<td>
<code>tls</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdTLSConfig">
EtcdTLSConfig
</a>
</em>
</td>
<td>
<p>tls specifies TLS configuration for HTTPS etcd client endpoints.</p>
</td>
</tr>
</tbody>
</table>
###UpgradeStrategy { #hypershift.openshift.io/v1beta1.UpgradeStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">ReplaceUpgrade</a>)
</p>
<p>
<p>UpgradeStrategy is a specific strategy for upgrading nodes in a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;OnDelete&#34;</p></td>
<td><p>UpgradeStrategyOnDelete replaces old nodes when the deletion of the
associated node instances are completed.</p>
</td>
</tr><tr><td><p>&#34;RollingUpdate&#34;</p></td>
<td><p>UpgradeStrategyRollingUpdate means use a rolling update for nodes.</p>
</td>
</tr></tbody>
</table>
###UpgradeType { #hypershift.openshift.io/v1beta1.UpgradeType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>UpgradeType is a type of high-level upgrade behavior nodes in a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;InPlace&#34;</p></td>
<td><p>UpgradeTypeInPlace is a strategy which replaces nodes in-place with no
additional node capacity requirements.</p>
</td>
</tr><tr><td><p>&#34;Replace&#34;</p></td>
<td><p>UpgradeTypeReplace is a strategy which replaces nodes using surge node
capacity.</p>
</td>
</tr></tbody>
</table>
###UserManagedDiagnostics { #hypershift.openshift.io/v1beta1.UserManagedDiagnostics }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">Diagnostics</a>)
</p>
<p>
<p>UserManagedDiagnostics specifies the diagnostics settings for a virtual machine when the storage account is managed by the user.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageAccountURI</code></br>
<em>
string
</em>
</td>
<td>
<p>storageAccountURI is the URI of the user-managed storage account.
The URI typically will be <code>https://&lt;mystorageaccountname&gt;.blob.core.windows.net/</code>
but may differ if you are using Azure DNS zone endpoints.
You can find the correct endpoint by looking for the Blob Primary Endpoint in the
endpoints tab in the Azure console or with the CLI by issuing
<code>az storage account list --query='[].{name: name, &quot;resource group&quot;: resourceGroup, &quot;blob endpoint&quot;: primaryEndpoints.blob}'</code>.</p>
</td>
</tr>
</tbody>
</table>
###Volume { #hypershift.openshift.io/v1beta1.Volume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>Volume specifies the configuration options for node instance storage devices.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>size</code></br>
<em>
int64
</em>
</td>
<td>
<p>Size specifies size (in Gi) of the storage device.</p>
<p>Must be greater than the image snapshot size or 8 (whichever is greater).</p>
</td>
</tr>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>Type is the type of the volume.</p>
</td>
</tr>
<tr>
<td>
<code>iops</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>IOPS is the number of IOPS requested for the disk. This is only valid
for type io1.</p>
</td>
</tr>
<tr>
<td>
<code>encrypted</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encrypted is whether the volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionKey</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionKey is the KMS key to use to encrypt the volume. Can be either a KMS key ID or ARN.
If Encrypted is set and this is omitted, the default AWS key will be used.
The key must already exist and be accessible by the controller.</p>
</td>
</tr>
</tbody>
</table>
<hr/>
