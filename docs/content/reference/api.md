---
################# IMPORTANT ########################
# Document generated by `make api-docs`. DO NOT EDIT
################# IMPORTANT ########################
title: API
---
# HyperShift API Reference
<p>Packages:</p>
<ul>
<li>
<a href="#hypershift.openshift.io%2fv1beta1">hypershift.openshift.io/v1beta1</a>
</li>
</ul>
<h2 id="hypershift.openshift.io/v1beta1">hypershift.openshift.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains the HyperShift API.</p>
<p>The HyperShift API enables creating and managing lightweight, flexible, heterogeneous
OpenShift clusters at scale.</p>
<p>HyperShift clusters are deployed in a topology which isolates the &ldquo;control plane&rdquo;
(e.g. etcd, the API server, controller manager, etc.) from the &ldquo;data plane&rdquo; (e.g.
worker nodes and their kubelets, and the infrastructure on which they run). This
enables &ldquo;hosted control plane as a service&rdquo; use cases.</p>
</p>
##CertificateSigningRequestApproval { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval }
<p>
<p>CertificateSigningRequestApproval defines the desired state of CertificateSigningRequestApproval</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>CertificateSigningRequestApproval</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec">
CertificateSigningRequestApprovalSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus">
CertificateSigningRequestApprovalStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
##HostedCluster { #hypershift.openshift.io/v1beta1.HostedCluster }
<p>
<p>HostedCluster is the primary representation of a HyperShift cluster and encapsulates
the control plane and common data plane configuration. Creating a HostedCluster
results in a fully functional OpenShift control plane with no attached nodes.
To support workloads (e.g. pods), a HostedCluster may have one or more associated
NodePool resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>HostedCluster</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">
HostedClusterSpec
</a>
</em>
</td>
<td>
<p>Spec is the desired behavior of the HostedCluster.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>Release specifies the desired OCP release payload for the hosted cluster.</p>
<p>Updating this field will trigger a rollout of the control plane. The
behavior of the rollout will be driven by the ControllerAvailabilityPolicy
and InfrastructureAvailabilityPolicy.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneRelease</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneRelease specifies the desired OCP release payload for
control plane components running on the management cluster.
Updating this field will trigger a rollout of the control plane. The
behavior of the rollout will be driven by the ControllerAvailabilityPolicy
and InfrastructureAvailabilityPolicy.
If not defined, Release is used</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClusterID uniquely identifies this cluster. This is expected to be
an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in
hexadecimal values).
As with a Kubernetes metadata.uid, this ID uniquely identifies this
cluster in space and time.
This value identifies the cluster in metrics pushed to telemetry and
metrics produced by the control plane operators. If a value is not
specified, an ID is generated. After initial creation, the value is
immutable.</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
By default it will use the appropriate update service for the cluster and region.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default
set of updates be applied to this cluster. The default channel will be
contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfraID is a globally unique identifier for the cluster. This identifier
will be used to associate various cloud resources with the HostedCluster
and its associated NodePools.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>Platform specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControllerAvailabilityPolicy specifies the availability policy applied to
critical control plane components. The default value is HighlyAvailable.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfrastructureAvailabilityPolicy specifies the availability policy applied
to infrastructure services which run on cluster nodes. The default value is
SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<p>DNS specifies DNS configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<p>Networking specifies network configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>Etcd specifies configuration for the control plane etcd cluster. The
default ManagementType is Managed. Once set, the ManagementType cannot be
changed.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>Services specifies how individual control plane services are published from
the hosting cluster of the control plane.</p>
<p>If a given service is not present in this list, it will be exposed publicly
by default.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>PullSecret references a pull secret to be injected into the container
runtime of all cluster nodes. The secret must have a key named
&ldquo;.dockerconfigjson&rdquo; whose value is the pull secret JSON.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>SSHKey references an SSH key to be injected into all cluster node sshd
servers. The secret must have a single key &ldquo;id_rsa.pub&rdquo; whose value is the
public part of an SSH key.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IssuerURL is an OIDC issuer URL which is used as the issuer in all
ServiceAccount tokens generated by the control plane API server. The
default value is kubernetes.default.svc, which only works for in-cluster
validation.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountSigningKey is a reference to a secret containing the private key
used by the service account token issuer. The secret is expected to contain
a single key named &ldquo;key&rdquo;. If not specified, a service account signing key will
be generated automatically for the cluster. When specifying a service account
signing key, a IssuerURL must also be specified.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditWebhook contains metadata for configuring an audit webhook endpoint
for a cluster to process cluster audit events. It references a secret that
contains the webhook information for the audit webhook endpoint. It is a
secret because if the endpoint has mTLS the kubeconfig will contain client
keys. The kubeconfig needs to be stored in the secret with a secret key
name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageContentSources specifies image mirrors that can be used by cluster
nodes to pull content.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTrustBundle is a reference to a ConfigMap containing a
PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretEncryption specifies a Kubernetes secret encryption strategy for the
control plane.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>FIPS indicates whether this cluster&rsquo;s nodes will be running in FIPS mode.
If set to true, the control plane&rsquo;s ignition server will be configured to
expect that nodes joining the cluster will be FIPS-enabled.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OLMCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector when specified, must be true for the pods managed by the HostedCluster to be scheduled.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tolerations when specified, define what custome tolerations are added to the hcp pods.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">
HostedClusterStatus
</a>
</em>
</td>
<td>
<p>Status is the latest observed status of the HostedCluster.</p>
</td>
</tr>
</tbody>
</table>
##NodePool { #hypershift.openshift.io/v1beta1.NodePool }
<p>
<p>NodePool is a scalable set of worker nodes attached to a HostedCluster.
NodePool machine architectures are uniform within a given pool, and are
independent of the control planeâ€™s underlying machine architecture.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>NodePool</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">
NodePoolSpec
</a>
</em>
</td>
<td>
<p>Spec is the desired behavior of the NodePool.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>clusterName</code></br>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the name of the HostedCluster this NodePool belongs to.</p>
<p>TODO(dan): Should this be a LocalObjectReference?</p>
</td>
</tr>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>Release specifies the OCP release used for the NodePool. This informs the
ignition configuration for machines, as well as other platform specific
machine properties (e.g. an AMI on the AWS platform).</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">
NodePoolPlatform
</a>
</em>
</td>
<td>
<p>Platform specifies the underlying infrastructure provider for the NodePool
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the desired number of nodes the pool should maintain. If
unset, the default value is 0.</p>
</td>
</tr>
<tr>
<td>
<code>management</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">
NodePoolManagement
</a>
</em>
</td>
<td>
<p>Management specifies behavior for managing nodes in the pool, such as
upgrade strategies and auto-repair behaviors.</p>
</td>
</tr>
<tr>
<td>
<code>autoScaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolAutoScaling">
NodePoolAutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto-scaling behavior for the NodePool.</p>
</td>
</tr>
<tr>
<td>
<code>config</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>Config is a list of references to ConfigMaps containing serialized
MachineConfig resources to be injected into the ignition configurations of
nodes in the NodePool. The MachineConfig API schema is defined here:</p>
<p><a href="https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185">https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</a></p>
<p>Each ConfigMap must have a single key named &ldquo;config&rdquo; whose value is the YML
with one or more serialized machineconfiguration.openshift.io resources:
KubeletConfig
ContainerRuntimeConfig
MachineConfig
ClusterImagePolicy
ImageContentSourcePolicy
or
ImageDigestMirrorSet</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeDrainTimeout is the maximum amount of time that the controller will spend on draining a node.
The default value is 0, meaning that the node can be drained without any time limitations.
NOTE: NodeDrainTimeout is different from <code>kubectl drain --timeout</code>
TODO (alberto): Today changing this field will trigger a recreate rolling update, which kind of defeats
the purpose of the change. In future we plan to propagate this field in-place.
<a href="https://github.com/kubernetes-sigs/cluster-api/issues/5880">https://github.com/kubernetes-sigs/cluster-api/issues/5880</a> / <a href="https://github.com/kubernetes-sigs/cluster-api/pull/10589">https://github.com/kubernetes-sigs/cluster-api/pull/10589</a></p>
</td>
</tr>
<tr>
<td>
<code>nodeVolumeDetachTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node.
The default value is 0, meaning that the volumes will be detached from the node without any time limitations.
After the timeout, the detachment of volumes that haven&rsquo;t been detached yet is skipped.
TODO (cbusse): Same comment as Alberto&rsquo;s for <code>NodeDrainTimeout</code>:
Today changing this field will trigger a recreate rolling update, which kind of defeats
the purpose of the change. In future we plan to propagate this field in-place.
<a href="https://github.com/kubernetes-sigs/cluster-api/issues/5880">https://github.com/kubernetes-sigs/cluster-api/issues/5880</a> / <a href="https://github.com/kubernetes-sigs/cluster-api/pull/10589">https://github.com/kubernetes-sigs/cluster-api/pull/10589</a></p>
</td>
</tr>
<tr>
<td>
<code>nodeLabels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeLabels propagates a list of labels to Nodes, only once on creation.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
</td>
</tr>
<tr>
<td>
<code>taints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Taint">
[]Taint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints if specified, propagates a list of taints to Nodes, only once on creation.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>TuningConfig is a list of references to ConfigMaps containing serialized
Tuned or PerformanceProfile resources to define the tuning configuration to be applied to
nodes in the NodePool. The Tuned API is defined here:</p>
<p><a href="https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go">https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</a></p>
<p>The PerformanceProfile API is defined here:
<a href="https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2">https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</a></p>
<p>Each ConfigMap must have a single key named &ldquo;tuning&rdquo; whose value is the
JSON or YAML of a serialized Tuned or PerformanceProfile.</p>
</td>
</tr>
<tr>
<td>
<code>arch</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Arch is the preferred processor architecture for the NodePool (currently only supported on AWS)
NOTE: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery:
<a href="https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215">https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</a>
TODO Add s390x to enum validation once the architecture is supported</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">
NodePoolStatus
</a>
</em>
</td>
<td>
<p>Status is the latest observed status of the NodePool.</p>
</td>
</tr>
</tbody>
</table>
###AESCBCSpec { #hypershift.openshift.io/v1beta1.AESCBCSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>AESCBCSpec defines metadata about the AESCBC secret encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ActiveKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>BackupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
</tbody>
</table>
###APIServerNetworking { #hypershift.openshift.io/v1beta1.APIServerNetworking }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>APIServerNetworking specifies how the APIServer is exposed inside a cluster
node.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>advertiseAddress</code></br>
<em>
string
</em>
</td>
<td>
<p>AdvertiseAddress is the address that nodes will use to talk to the API
server. This is an address associated with the loopback adapter of each
node. If not specified, the controller will take default values.
The default values will be set as 172.20.0.1 or fd00::1.</p>
</td>
</tr>
<tr>
<td>
<code>port</code></br>
<em>
int32
</em>
</td>
<td>
<p>Port is the port at which the APIServer is exposed inside a node. Other
pods using host networking cannot listen on this port.
If unset 6443 is used.
This is useful to choose a port other than the default one which might interfere with customer environments e.g. <a href="https://github.com/openshift/hypershift/pull/356">https://github.com/openshift/hypershift/pull/356</a>.
Setting this to 443 is possible only for backward compatibility reasons and it&rsquo;s discouraged.
Doing so, it would result in the controller overriding the KAS endpoint in the guest cluster having a discrepancy with the KAS Pod and potentially causing temporarily network failures.</p>
</td>
</tr>
<tr>
<td>
<code>allowedCIDRBlocks</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CIDRBlock">
[]CIDRBlock
</a>
</em>
</td>
<td>
<p>AllowedCIDRBlocks is an allow list of CIDR blocks that can access the APIServer
If not specified, traffic is allowed from all addresses.
This depends on underlying support by the cloud provider for Service LoadBalancerSourceRanges</p>
</td>
</tr>
</tbody>
</table>
###AWSCloudProviderConfig { #hypershift.openshift.io/v1beta1.AWSCloudProviderConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSCloudProviderConfig specifies AWS networking configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is the subnet to use for control plane cloud resources.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Zone is the availability zone where control plane cloud resources are
created.</p>
</td>
</tr>
<tr>
<td>
<code>vpc</code></br>
<em>
string
</em>
</td>
<td>
<p>VPC is the VPC to use for control plane cloud resources.</p>
</td>
</tr>
</tbody>
</table>
###AWSEndpointAccessType { #hypershift.openshift.io/v1beta1.AWSEndpointAccessType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSEndpointAccessType specifies the publishing scope of cluster endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Private&#34;</p></td>
<td><p>Private endpoint access allows only private API server access and private
node communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;Public&#34;</p></td>
<td><p>Public endpoint access allows public API server access and public node
communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;PublicAndPrivate&#34;</p></td>
<td><p>PublicAndPrivate endpoint access allows public API server access and
private node communication with the control plane.</p>
</td>
</tr></tbody>
</table>
###AWSKMSAuthSpec { #hypershift.openshift.io/v1beta1.AWSKMSAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">AWSKMSSpec</a>)
</p>
<p>
<p>AWSKMSAuthSpec defines metadata about the management of credentials used to interact and encrypt data via AWS KMS key.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>awsKms</code></br>
<em>
string
</em>
</td>
<td>
<p>The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]
}</p>
<p>AWSKMSARN is an ARN value referencing a role appropriate for managing the auth via the AWS KMS key.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Encrypt&rdquo;,
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt<em>&rdquo;,
&ldquo;kms:GenerateDataKey</em>&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: %q
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AWSKMSKeyEntry { #hypershift.openshift.io/v1beta1.AWSKMSKeyEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">AWSKMSSpec</a>)
</p>
<p>
<p>AWSKMSKeyEntry defines metadata to locate the encryption key in AWS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code></br>
<em>
string
</em>
</td>
<td>
<p>ARN is the Amazon Resource Name for the encryption key</p>
</td>
</tr>
</tbody>
</table>
###AWSKMSSpec { #hypershift.openshift.io/v1beta1.AWSKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>AWSKMSSpec defines metadata about the configuration of the AWS KMS Secret Encryption provider</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region contains the AWS region</p>
</td>
</tr>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry">
AWSKMSKeyEntry
</a>
</em>
</td>
<td>
<p>ActiveKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry">
AWSKMSKeyEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>BackupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
<tr>
<td>
<code>auth</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSAuthSpec">
AWSKMSAuthSpec
</a>
</em>
</td>
<td>
<p>Auth defines metadata about the management of credentials used to interact with AWS KMS</p>
</td>
</tr>
</tbody>
</table>
###AWSNodePoolPlatform { #hypershift.openshift.io/v1beta1.AWSNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AWSNodePoolPlatform specifies the configuration of a NodePool when operating
on AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceType</code></br>
<em>
string
</em>
</td>
<td>
<p>InstanceType is an ec2 instance type for node instances (e.g. m5.large).</p>
</td>
</tr>
<tr>
<td>
<code>instanceProfile</code></br>
<em>
string
</em>
</td>
<td>
<p>InstanceProfile is the AWS EC2 instance profile, which is a container for an IAM role that the EC2 instance uses.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<p>Subnet is the subnet to use for node instances.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMI is the image id to use for node instances. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroups</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroups is an optional set of security groups to associate with node
instances.</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume specifies configuration for the root volume of node instances.</p>
</td>
</tr>
<tr>
<td>
<code>resourceTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceTag">
[]AWSResourceTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ResourceTags is an optional list of additional tags to apply to AWS node
instances.</p>
<p>These will be merged with HostedCluster scoped tags, and HostedCluster tags
take precedence in case of conflicts.</p>
<p>See <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html</a> for
information on tagging AWS resources. AWS supports a maximum of 50 tags per
resource. OpenShift reserves 25 tags for its use, leaving 25 tags available
for the user.</p>
</td>
</tr>
</tbody>
</table>
###AWSPlatformSpec { #hypershift.openshift.io/v1beta1.AWSPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AWSPlatformSpec specifies configuration for clusters running on Amazon Web Services.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the AWS region in which the cluster resides. This configures the
OCP control plane cloud integrations, and is used by NodePool to resolve
the correct boot AMI for a given release.</p>
</td>
</tr>
<tr>
<td>
<code>cloudProviderConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig">
AWSCloudProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudProviderConfig specifies AWS networking configuration for the control
plane.
This is mainly used for cloud provider controller config:
<a href="https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364">https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364</a>
TODO(dan): should this be named AWSNetworkConfig?</p>
</td>
</tr>
<tr>
<td>
<code>serviceEndpoints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSServiceEndpoint">
[]AWSServiceEndpoint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceEndpoints specifies optional custom endpoints which will override
the default service endpoint of specific AWS Services.</p>
<p>There must be only one ServiceEndpoint for a given service name.</p>
</td>
</tr>
<tr>
<td>
<code>rolesRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSRolesRef">
AWSRolesRef
</a>
</em>
</td>
<td>
<p>RolesRef contains references to various AWS IAM roles required to enable
integrations such as OIDC.</p>
</td>
</tr>
<tr>
<td>
<code>resourceTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceTag">
[]AWSResourceTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ResourceTags is a list of additional tags to apply to AWS resources created
for the cluster. See
<a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html</a> for
information on tagging AWS resources. AWS supports a maximum of 50 tags per
resource. OpenShift reserves 25 tags for its use, leaving 25 tags available
for the user.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSEndpointAccessType">
AWSEndpointAccessType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EndpointAccess specifies the publishing scope of cluster endpoints. The
default is Public.</p>
</td>
</tr>
<tr>
<td>
<code>additionalAllowedPrincipals</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalAllowedPrincipals specifies a list of additional allowed principal ARNs
to be added to the hosted control plane&rsquo;s VPC Endpoint Service to enable additional
VPC Endpoint connection requests to be automatically accepted.
See <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html">https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html</a>
for more details around VPC Endpoint Service allowed principals.</p>
</td>
</tr>
<tr>
<td>
<code>multiArch</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>MultiArch specifies whether the Hosted Cluster will be expected to support NodePools with different
CPU architectures, i.e., supporting arm64 NodePools and supporting amd64 NodePools on the same Hosted Cluster.
Deprecated: This field is no longer used. The HyperShift Operator now performs multi-arch validations
automatically despite the platform type. The HyperShift Operator will set HostedCluster.Status.PayloadArch based
on the HostedCluster release image. This field is used by the NodePool controller to validate the
NodePool.Spec.Arch is supported.</p>
</td>
</tr>
<tr>
<td>
<code>sharedVPC</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPC">
AWSSharedVPC
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SharedVPC contains fields that must be specified if the HostedCluster must use a VPC that is
created in a different AWS account and is shared with the AWS account where the HostedCluster
will be created.</p>
</td>
</tr>
</tbody>
</table>
###AWSPlatformStatus { #hypershift.openshift.io/v1beta1.AWSPlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">PlatformStatus</a>)
</p>
<p>
<p>AWSPlatformStatus contains status specific to the AWS platform</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>defaultWorkerSecurityGroupID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DefaultWorkerSecurityGroupID is the ID of a security group created by
the control plane operator. It is always added to worker machines in
addition to any security groups specified in the NodePool.</p>
</td>
</tr>
</tbody>
</table>
###AWSResourceReference { #hypershift.openshift.io/v1beta1.AWSResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig">AWSCloudProviderConfig</a>, 
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>AWSResourceReference is a reference to a specific AWS resource by ID or filters.
Only one of ID or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>filters</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Filter">
[]Filter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filters is a set of key/value pairs used to identify a resource
They are applied according to the rules defined by the AWS API:
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html</a></p>
</td>
</tr>
</tbody>
</table>
###AWSResourceTag { #hypershift.openshift.io/v1beta1.AWSResourceTag }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSResourceTag is a tag to apply to AWS resources created for the cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>Key is the key of the tag.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<p>Value is the value of the tag.</p>
<p>Some AWS service do not support empty values. Since tags are added to
resources in many services, the length of the tag value must meet the
requirements of all services.</p>
</td>
</tr>
</tbody>
</table>
###AWSRoleCredentials { #hypershift.openshift.io/v1beta1.AWSRoleCredentials }
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>namespace</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
###AWSRolesRef { #hypershift.openshift.io/v1beta1.AWSRolesRef }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSRolesRef contains references to various AWS IAM roles required for operators to make calls against the AWS API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code></br>
<em>
string
</em>
</td>
<td>
<p>The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]
}</p>
<p>IngressARN is an ARN value referencing a role appropriate for the Ingress Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:aws:route53:::PUBLIC_ZONE_ID&rdquo;,
&ldquo;arn:aws:route53:::PRIVATE_ZONE_ID&rdquo;
]
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ImageRegistryARN is an ARN value referencing a role appropriate for the Image Registry Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;s3:CreateBucket&rdquo;,
&ldquo;s3:DeleteBucket&rdquo;,
&ldquo;s3:PutBucketTagging&rdquo;,
&ldquo;s3:GetBucketTagging&rdquo;,
&ldquo;s3:PutBucketPublicAccessBlock&rdquo;,
&ldquo;s3:GetBucketPublicAccessBlock&rdquo;,
&ldquo;s3:PutEncryptionConfiguration&rdquo;,
&ldquo;s3:GetEncryptionConfiguration&rdquo;,
&ldquo;s3:PutLifecycleConfiguration&rdquo;,
&ldquo;s3:GetLifecycleConfiguration&rdquo;,
&ldquo;s3:GetBucketLocation&rdquo;,
&ldquo;s3:ListBucket&rdquo;,
&ldquo;s3:GetObject&rdquo;,
&ldquo;s3:PutObject&rdquo;,
&ldquo;s3:DeleteObject&rdquo;,
&ldquo;s3:ListBucketMultipartUploads&rdquo;,
&ldquo;s3:AbortMultipartUpload&rdquo;,
&ldquo;s3:ListMultipartUploadParts&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>storageARN</code></br>
<em>
string
</em>
</td>
<td>
<p>StorageARN is an ARN value referencing a role appropriate for the Storage Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:CreateSnapshot&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:DeleteSnapshot&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeSnapshots&rdquo;,
&ldquo;ec2:DescribeTags&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DescribeVolumesModifications&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>networkARN</code></br>
<em>
string
</em>
</td>
<td>
<p>NetworkARN is an ARN value referencing a role appropriate for the Network Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInstanceStatus&rdquo;,
&ldquo;ec2:DescribeInstanceTypes&rdquo;,
&ldquo;ec2:UnassignPrivateIpAddresses&rdquo;,
&ldquo;ec2:AssignPrivateIpAddresses&rdquo;,
&ldquo;ec2:UnassignIpv6Addresses&rdquo;,
&ldquo;ec2:AssignIpv6Addresses&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerARN</code></br>
<em>
string
</em>
</td>
<td>
<p>KubeCloudControllerARN is an ARN value referencing a role appropriate for the KCM/KCC.
Source: <a href="https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies">https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies</a></p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;autoscaling:DescribeAutoScalingGroups&rdquo;,
&ldquo;autoscaling:DescribeLaunchConfigurations&rdquo;,
&ldquo;autoscaling:DescribeTags&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeRegions&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;,
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:DeleteRoute&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:AttachLoadBalancerToSubnets&rdquo;,
&ldquo;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerPolicy&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:ConfigureHealthCheck&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:DetachLoadBalancerFromSubnets&rdquo;,
&ldquo;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:ModifyLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:RegisterInstancesWithLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:CreateListener&rdquo;,
&ldquo;elasticloadbalancing:CreateTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeleteListener&rdquo;,
&ldquo;elasticloadbalancing:DeleteTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeregisterTargets&rdquo;,
&ldquo;elasticloadbalancing:DescribeListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerPolicies&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetGroups&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetHealth&rdquo;,
&ldquo;elasticloadbalancing:ModifyListener&rdquo;,
&ldquo;elasticloadbalancing:ModifyTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:RegisterTargets&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&rdquo;,
&ldquo;iam:CreateServiceLinkedRole&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;*&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementARN</code></br>
<em>
string
</em>
</td>
<td>
<p>NodePoolManagementARN is an ARN value referencing a role appropriate for the CAPI Controller.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;ec2:AssociateRouteTable&rdquo;,
&ldquo;ec2:AttachInternetGateway&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateInternetGateway&rdquo;,
&ldquo;ec2:CreateNatGateway&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:CreateRouteTable&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateSubnet&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:DeleteInternetGateway&rdquo;,
&ldquo;ec2:DeleteNatGateway&rdquo;,
&ldquo;ec2:DeleteRouteTable&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteSubnet&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DescribeAccountAttributes&rdquo;,
&ldquo;ec2:DescribeAddresses&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInternetGateways&rdquo;,
&ldquo;ec2:DescribeNatGateways&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;ec2:DescribeVpcAttribute&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DetachInternetGateway&rdquo;,
&ldquo;ec2:DisassociateRouteTable&rdquo;,
&ldquo;ec2:DisassociateAddress&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:ModifySubnetAttribute&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RunInstances&rdquo;,
&ldquo;ec2:TerminateInstances&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;ec2:CreateLaunchTemplate&rdquo;,
&ldquo;ec2:CreateLaunchTemplateVersion&rdquo;,
&ldquo;ec2:DescribeLaunchTemplates&rdquo;,
&ldquo;ec2:DescribeLaunchTemplateVersions&rdquo;,
&ldquo;ec2:DeleteLaunchTemplate&rdquo;,
&ldquo;ec2:DeleteLaunchTemplateVersions&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;<em>&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Condition&rdquo;: {
&ldquo;StringLike&rdquo;: {
&ldquo;iam:AWSServiceName&rdquo;: &ldquo;elasticloadbalancing.amazonaws.com&rdquo;
}
},
&ldquo;Action&rdquo;: [
&ldquo;iam:CreateServiceLinkedRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Action&rdquo;: [
&ldquo;iam:PassRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/</em>-worker-role&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt&rdquo;,
&ldquo;kms:GenerateDataKeyWithoutPlainText&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;<em>&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:CreateGrant&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;</em>&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;Bool&rdquo;: {
&ldquo;kms:GrantIsForAWSResource&rdquo;: true
}
}
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneOperatorARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneOperatorARN  is an ARN value referencing a role appropriate for the Control Plane Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;arn:aws:route53:::%s&rdquo;
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AWSServiceEndpoint { #hypershift.openshift.io/v1beta1.AWSServiceEndpoint }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSServiceEndpoint stores the configuration for services to
override existing defaults of AWS Services.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the AWS service.
This must be provided and cannot be empty.</p>
</td>
</tr>
<tr>
<td>
<code>url</code></br>
<em>
string
</em>
</td>
<td>
<p>URL is fully qualified URI with scheme https, that overrides the default generated
endpoint for a client.
This must be provided and cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###AWSSharedVPC { #hypershift.openshift.io/v1beta1.AWSSharedVPC }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSSharedVPC contains fields needed to create a HostedCluster using a VPC that has been
created and shared from a different AWS account than the AWS account where the cluster
is getting created.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rolesRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef">
AWSSharedVPCRolesRef
</a>
</em>
</td>
<td>
<p>RolesRef contains references to roles in the VPC owner account that enable a
HostedCluster on a shared VPC.</p>
</td>
</tr>
<tr>
<td>
<code>localZoneID</code></br>
<em>
string
</em>
</td>
<td>
<p>LocalZoneID is the ID of the route53 hosted zone for [cluster-name].hypershift.local that is
associated with the HostedCluster&rsquo;s VPC and exists in the VPC owner account.</p>
</td>
</tr>
</tbody>
</table>
###AWSSharedVPCRolesRef { #hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPC">AWSSharedVPC</a>)
</p>
<p>
<p>AWSSharedVPCRolesRef contains references to AWS IAM roles required for a shared VPC hosted cluster.
These roles must exist in the VPC owner&rsquo;s account.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code></br>
<em>
string
</em>
</td>
<td>
<p>IngressARN is an ARN value referencing the role in the VPC owner account that allows the
ingress operator in the cluster account to create and manage records in the private DNS
hosted zone.</p>
<p>The referenced role must have a trust relationship that allows it to be assumed by the
ingress operator role in the VPC creator account.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Sid&rdquo;: &ldquo;Statement1&rdquo;,
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;AWS&rdquo;: &ldquo;arn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-openshift-ingress&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRole&rdquo;
}
]
}</p>
<p>The following is an example of the policy document for this role.
(Based on <a href="https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config">https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config</a>)</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;route53:ListHostedZonesByName&rdquo;,
&ldquo;route53:ChangeTagsForResource&rdquo;,
&ldquo;route53:GetAccountLimit&rdquo;,
&ldquo;route53:GetChange&rdquo;,
&ldquo;route53:GetHostedZone&rdquo;,
&ldquo;route53:ListTagsForResource&rdquo;,
&ldquo;route53:UpdateHostedZoneComment&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;tag:UntagResources&rdquo;
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneARN is an ARN value referencing the role in the VPC owner account that allows
the control plane operator in the cluster account to create and manage a VPC endpoint, its
corresponding Security Group, and DNS records in the hypershift local hosted zone.</p>
<p>The referenced role must have a trust relationship that allows it to be assumed by the
control plane operator role in the VPC creator account.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Sid&rdquo;: &ldquo;Statement1&rdquo;,
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;AWS&rdquo;: &ldquo;arn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-control-plane-operator&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRole&rdquo;
}
]
}</p>
<p>The following is an example of the policy document for this role.</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AgentNodePoolPlatform { #hypershift.openshift.io/v1beta1.AgentNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AgentNodePoolPlatform specifies the configuration of a NodePool when operating
on the Agent platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>agentLabelSelector</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AgentLabelSelector contains labels that must be set on an Agent in order to
be selected for a Machine.</p>
</td>
</tr>
</tbody>
</table>
###AgentPlatformSpec { #hypershift.openshift.io/v1beta1.AgentPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AgentPlatformSpec specifies configuration for agent-based installations.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>agentNamespace</code></br>
<em>
string
</em>
</td>
<td>
<p>AgentNamespace is the namespace where to search for Agents for this cluster</p>
</td>
</tr>
</tbody>
</table>
###AllocationPool { #hypershift.openshift.io/v1beta1.AllocationPool }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SubnetSpec">SubnetSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>start</code></br>
<em>
string
</em>
</td>
<td>
<p>Start represents the start of the AllocationPool, that is the lowest IP of the pool.</p>
</td>
</tr>
<tr>
<td>
<code>end</code></br>
<em>
string
</em>
</td>
<td>
<p>End represents the end of the AlloctionPool, that is the highest IP of the pool.</p>
</td>
</tr>
</tbody>
</table>
###AvailabilityPolicy { #hypershift.openshift.io/v1beta1.AvailabilityPolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>AvailabilityPolicy specifies a high level availability policy for components.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;HighlyAvailable&#34;</p></td>
<td><p>HighlyAvailable means components should be resilient to problems across
fault boundaries as defined by the component to which the policy is
attached. This usually means running critical workloads with 3 replicas and
with little or no toleration of disruption of the component.</p>
</td>
</tr><tr><td><p>&#34;SingleReplica&#34;</p></td>
<td><p>SingleReplica means components are not expected to be resilient to problems
across most fault boundaries associated with high availability. This
usually means running critical workloads with just 1 replica and with
toleration of full disruption of the component.</p>
</td>
</tr></tbody>
</table>
###AzureDiagnosticsStorageAccountType { #hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">Diagnostics</a>)
</p>
<p>
<p>AzureDiagnosticsStorageAccountType specifies the type of storage account for storing Azure VM diagnostics data.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disabled&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Managed&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;UserManaged&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###AzureKMSKey { #hypershift.openshift.io/v1beta1.AzureKMSKey }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">AzureKMSSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>keyVaultName</code></br>
<em>
string
</em>
</td>
<td>
<p>KeyVaultName is the name of the keyvault. Must match criteria specified at <a href="https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name">https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name</a>
Your Microsoft Entra application used to create the cluster must be authorized to access this keyvault, e.g using the AzureCLI:
<code>az keyvault set-policy -n $KEYVAULT_NAME --key-permissions decrypt encrypt --spn &lt;YOUR APPLICATION CLIENT ID&gt;</code></p>
</td>
</tr>
<tr>
<td>
<code>keyName</code></br>
<em>
string
</em>
</td>
<td>
<p>KeyName is the name of the keyvault key used for encrypt/decrypt</p>
</td>
</tr>
<tr>
<td>
<code>keyVersion</code></br>
<em>
string
</em>
</td>
<td>
<p>KeyVersion contains the version of the key to use</p>
</td>
</tr>
</tbody>
</table>
###AzureKMSSpec { #hypershift.openshift.io/v1beta1.AzureKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>AzureKMSSpec defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSKey">
AzureKMSKey
</a>
</em>
</td>
<td>
<p>ActiveKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSKey">
AzureKMSKey
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>BackupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
</tbody>
</table>
###AzureNodePoolPlatform { #hypershift.openshift.io/v1beta1.AzureNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>vmsize</code></br>
<em>
string
</em>
</td>
<td>
<p>VMSize is the Azure VM instance type to use for the nodes being created in the nodepool.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">
AzureVMImage
</a>
</em>
</td>
<td>
<p>ImageID is the id of the image to boot from. If unset, the default image at the location below will be used and
is expected to exist: subscription/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Compute/images/rhcos.x86_64.vhd.
The <subscriptionID> and the <resourceGroupName> are expected to be the same resource group documented in the
Hosted Cluster specification respectively, HostedCluster.Spec.Platform.Azure.SubscriptionID and
HostedCluster.Spec.Platform.Azure.ResourceGroupName.</p>
</td>
</tr>
<tr>
<td>
<code>diskSizeGB</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSizeGB is the size in GB to assign to the OS disk
CAPZ default is 30GB, <a href="https://github.com/kubernetes-sigs/cluster-api-provider-azure/blob/b3708019a67ff19407b87d63c402af94ca4246f6/api/v1beta1/types.go#L599">https://github.com/kubernetes-sigs/cluster-api-provider-azure/blob/b3708019a67ff19407b87d63c402af94ca4246f6/api/v1beta1/types.go#L599</a></p>
</td>
</tr>
<tr>
<td>
<code>diskStorageAccountType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskStorageAccountType is the disk storage account type to use. Valid values are:
* Standard_LRS: HDD
* StandardSSD_LRS: Standard SSD
* Premium_LRS: Premium SDD
* UltraSSD_LRS: Ultra SDD</p>
<p>Defaults to Premium_LRS. For more details, visit the Azure documentation:
<a href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison">https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison</a></p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZone is the failure domain identifier where the VM should be attached to. This must not be specified
for clusters in a location that does not support AvailabilityZone.</p>
</td>
</tr>
<tr>
<td>
<code>diskEncryptionSetID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskEncryptionSetID is the ID of the DiskEncryptionSet resource to use to encrypt the OS disks for the VMs. This
needs to exist in the same subscription id listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.SubscriptionID.
DiskEncryptionSetID should also exist in a resource group under the same subscription id and the same location
listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.Location.</p>
</td>
</tr>
<tr>
<td>
<code>enableEphemeralOSDisk</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EnableEphemeralOSDisk is a flag when set to true, will enable ephemeral OS disk.</p>
</td>
</tr>
<tr>
<td>
<code>subnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>SubnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a
different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must
exist in the same HostedCluster.Spec.Platform.Azure.VnetID and must exist under the same subscription ID,
HostedCluster.Spec.Platform.Azure.SubscriptionID.</p>
</td>
</tr>
<tr>
<td>
<code>diagnostics</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">
Diagnostics
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Diagnostics specifies the diagnostics settings for a virtual machine.
If not specified, then Boot diagnostics will be disabled.</p>
</td>
</tr>
<tr>
<td>
<code>machineIdentityID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>MachineIdentityID is a user-assigned identity assigned to the VMs used to authenticate with Azure services. This
field is expected to exist under the same resource group as HostedCluster.Spec.Platform.Azure.ResourceGroupName. This
user assigned identity is expected to have the Contributor role assigned to it and scoped to the resource group
under HostedCluster.Spec.Platform.Azure.ResourceGroupName.</p>
<p>If this field is not supplied, the Service Principal credentials will be written to a file on the disk of each VM
in order to be accessible by the cloud provider; the aforementioned credentials provided are the same ones as
HostedCluster.Spec.Platform.Azure.Credentials. However, this is less secure than using a managed identity.</p>
</td>
</tr>
</tbody>
</table>
###AzurePlatformSpec { #hypershift.openshift.io/v1beta1.AzurePlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AzurePlatformSpec specifies configuration for clusters running on Azure. Generally, the HyperShift API assumes bring
your own (BYO) cloud infrastructure resources. For example, resources like a resource group, a subnet, or a vnet
would be pre-created and then their names would be used respectively in the ResourceGroupName, SubnetName, VnetName
fields of the Hosted Cluster CR. An existing cloud resource is expected to exist under the same SubscriptionID.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>Credentials is the object containing existing Azure credentials needed for creating and managing cloud
infrastructure resources.</p>
</td>
</tr>
<tr>
<td>
<code>cloud</code></br>
<em>
string
</em>
</td>
<td>
<p>Cloud is the cloud environment identifier, valid values could be found here: <a href="https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33">https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33</a></p>
</td>
</tr>
<tr>
<td>
<code>location</code></br>
<em>
string
</em>
</td>
<td>
<p>Location is the Azure region in where all the cloud infrastructure resources will be created.</p>
<p>Example: eastus</p>
</td>
</tr>
<tr>
<td>
<code>resourceGroup</code></br>
<em>
string
</em>
</td>
<td>
<p>ResourceGroupName is the name of an existing resource group where all cloud resources created by the Hosted
Cluster are to be placed. The resource group is expected to exist under the same subscription as SubscriptionID.</p>
<p>In ARO HCP, this will be the managed resource group where customer cloud resources will be created.</p>
<p>Resource group naming requirements can be found here: <a href="https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/">https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/</a>.</p>
<p>Example: if your resource group ID is /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>, your
ResourceGroupName is <resourceGroupName>.</p>
</td>
</tr>
<tr>
<td>
<code>vnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>VnetID is the ID of an existing VNET to use in creating VMs. The VNET can exist in a different resource group
other than the one specified in ResourceGroupName, but it must exist under the same subscription as
SubscriptionID.</p>
<p>In ARO HCP, this will be the ID of the customer provided VNET.</p>
<p>Example: /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Network/virtualNetworks/<vnetName></p>
</td>
</tr>
<tr>
<td>
<code>subnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>SubnetID is the subnet ID of an existing subnet where the load balancer for node egress will be created. This
subnet is expected to be a subnet within the VNET specified in VnetID. This subnet is expected to exist under the
same subscription as SubscriptionID.</p>
<p>In ARO HCP, managed services will create the aforementioned load balancer in ResourceGroupName.</p>
</td>
</tr>
<tr>
<td>
<code>subscriptionID</code></br>
<em>
string
</em>
</td>
<td>
<p>SubscriptionID is a unique identifier for an Azure subscription used to manage resources.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupID</code></br>
<em>
string
</em>
</td>
<td>
<p>SecurityGroupID is the ID of an existing security group on the SubnetID. This field is provided as part of the
configuration for the Azure cloud provider, aka Azure cloud controller manager (CCM). This security group is
expected to exist under the same subscription as SubscriptionID.</p>
</td>
</tr>
</tbody>
</table>
###AzureVMImage { #hypershift.openshift.io/v1beta1.AzureVMImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
<p>AzureVMImage represents the different types of image data that can be provided for an Azure VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>azureImageType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImageType">
AzureVMImageType
</a>
</em>
</td>
<td>
<p>Type is the type of image data that will be provided to the Azure VM. This can be either &ldquo;ImageID&rdquo; or
&ldquo;AzureMarketplace&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>imageID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageID is the Azure resource ID of a VHD image to use to boot the Azure VMs from.</p>
</td>
</tr>
<tr>
<td>
<code>azureMarketplace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MarketplaceImage">
MarketplaceImage
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AzureMarketplace contains the Azure Marketplace image info to use to boot the Azure VMs from.</p>
</td>
</tr>
</tbody>
</table>
###AzureVMImageType { #hypershift.openshift.io/v1beta1.AzureVMImageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">AzureVMImage</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AzureMarketplace&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ImageID&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###CIDRBlock { #hypershift.openshift.io/v1beta1.CIDRBlock }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.APIServerNetworking">APIServerNetworking</a>)
</p>
<p>
</p>
###CertificateSigningRequestApprovalSpec { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval">CertificateSigningRequestApproval</a>)
</p>
<p>
<p>CertificateSigningRequestApprovalSpec defines the desired state of CertificateSigningRequestApproval</p>
</p>
###CertificateSigningRequestApprovalStatus { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval">CertificateSigningRequestApproval</a>)
</p>
<p>
<p>CertificateSigningRequestApprovalStatus defines the observed state of CertificateSigningRequestApproval</p>
</p>
###ClusterAutoscaling { #hypershift.openshift.io/v1beta1.ClusterAutoscaling }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterAutoscaling specifies auto-scaling behavior that applies to all
NodePools associated with a control plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxNodesTotal</code></br>
<em>
int32
</em>
</td>
<td>
<p>MaxNodesTotal is the maximum allowable number of nodes across all NodePools
for a HostedCluster. The autoscaler will not grow the cluster beyond this
number.</p>
</td>
</tr>
<tr>
<td>
<code>maxPodGracePeriod</code></br>
<em>
int32
</em>
</td>
<td>
<p>MaxPodGracePeriod is the maximum seconds to wait for graceful pod
termination before scaling down a NodePool. The default is 600 seconds.</p>
</td>
</tr>
<tr>
<td>
<code>maxNodeProvisionTime</code></br>
<em>
string
</em>
</td>
<td>
<p>MaxNodeProvisionTime is the maximum time to wait for node provisioning
before considering the provisioning to be unsuccessful, expressed as a Go
duration string. The default is 15 minutes.</p>
</td>
</tr>
<tr>
<td>
<code>podPriorityThreshold</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>PodPriorityThreshold enables users to schedule &ldquo;best-effort&rdquo; pods, which
shouldn&rsquo;t trigger autoscaler actions, but only run when there are spare
resources available. The default is -10.</p>
<p>See the following for more details:
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption</a></p>
</td>
</tr>
</tbody>
</table>
###ClusterConfiguration { #hypershift.openshift.io/v1beta1.ClusterConfiguration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterConfiguration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
<p>The API for individual configuration items is at:
<a href="https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html">https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiServer</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.APIServerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIServer holds configuration (like serving certificates, client CA and CORS domains)
shared by all API servers in the system, among them especially kube-apiserver
and openshift-apiserver.</p>
</td>
</tr>
<tr>
<td>
<code>authentication</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.AuthenticationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Authentication specifies cluster-wide settings for authentication (like OAuth and
webhook token authenticators).</p>
</td>
</tr>
<tr>
<td>
<code>featureGate</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.FeatureGateSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>FeatureGate holds cluster-wide information about feature gates.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.ImageSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Image governs policies related to imagestream imports and runtime configuration
for external registries. It allows cluster admins to configure which registries
OpenShift is allowed to import images from, extra CA trust bundles for external
registries, and policies to block or allow registry hostnames.
When exposing OpenShift&rsquo;s image registry to the public, this also lets cluster
admins specify the external hostname.</p>
</td>
</tr>
<tr>
<td>
<code>ingress</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.IngressSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ingress holds cluster-wide information about ingress, including the default ingress domain
used for routes.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.NetworkSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network holds cluster-wide information about the network. It is used to configure the desired network configuration, such as: IP address pools for services/pod IPs, network plugin, etc.
Please view network.spec for an explanation on what applies when configuring this resource.
TODO (csrwng): Add validation here to exclude changes that conflict with networking settings in the HostedCluster.Spec.Networking field.</p>
</td>
</tr>
<tr>
<td>
<code>oauth</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.OAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OAuth holds cluster-wide information about OAuth.
It is used to configure the integrated OAuth server.
This configuration is only honored when the top level Authentication config has type set to IntegratedOAuth.</p>
</td>
</tr>
<tr>
<td>
<code>operatorhub</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.OperatorHubSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OperatorHub specifies the configuration for the Operator Lifecycle Manager in the HostedCluster. This is only configured at deployment time but the controller are not reconcilling over it.
The OperatorHub configuration will be constantly reconciled if catalog placement is management, but only on cluster creation otherwise.</p>
</td>
</tr>
<tr>
<td>
<code>scheduler</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.SchedulerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scheduler holds cluster-wide config information to run the Kubernetes Scheduler
and influence its placement decisions. The canonical name for this config is <code>cluster</code>.</p>
</td>
</tr>
<tr>
<td>
<code>proxy</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.ProxySpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Proxy holds cluster-wide information on how to configure default proxies for the cluster.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworkEntry { #hypershift.openshift.io/v1beta1.ClusterNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>ClusterNetworkEntry is a single IP address block for pod IP blocks. IP blocks
are allocated with size 2^HostSubnetLength.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>CIDR is the IP block address pool.</p>
</td>
</tr>
<tr>
<td>
<code>hostPrefix</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostPrefix is the prefix size to allocate to each node from the CIDR.
For example, 24 would allocate 2^8=256 adresses to each node. If this
field is not used by the plugin, it can be left unset.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworking { #hypershift.openshift.io/v1beta1.ClusterNetworking }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterNetworking specifies network configuration for a cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>machineNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MachineNetworkEntry">
[]MachineNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MachineNetwork is the list of IP address pools for machines.</p>
</td>
</tr>
<tr>
<td>
<code>clusterNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworkEntry">
[]ClusterNetworkEntry
</a>
</em>
</td>
<td>
<p>ClusterNetwork is the list of IP address pools for pods.</p>
</td>
</tr>
<tr>
<td>
<code>serviceNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServiceNetworkEntry">
[]ServiceNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceNetwork is the list of IP address pools for services.
NOTE: currently only one entry is supported.</p>
</td>
</tr>
<tr>
<td>
<code>networkType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkType">
NetworkType
</a>
</em>
</td>
<td>
<p>NetworkType specifies the SDN provider used for cluster networking.</p>
</td>
</tr>
<tr>
<td>
<code>apiServer</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIServerNetworking">
APIServerNetworking
</a>
</em>
</td>
<td>
<p>APIServer contains advanced network settings for the API server that affect
how the APIServer is exposed inside a cluster node.</p>
</td>
</tr>
</tbody>
</table>
###ClusterVersionStatus { #hypershift.openshift.io/v1beta1.ClusterVersionStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>ClusterVersionStatus reports the status of the cluster versioning,
including any upgrades that are in progress. The current field will
be set to whichever version the cluster is reconciling to, and the
conditions array will report whether the update succeeded, is in
progress, or is failing.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>desired</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.Release
</a>
</em>
</td>
<td>
<p>desired is the version that the cluster is reconciling towards.
If the cluster is not yet fully initialized desired will be set
with the information available, which may be an image or a tag.</p>
</td>
</tr>
<tr>
<td>
<code>history</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.UpdateHistory
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>history contains a list of the most recent versions applied to the cluster.
This value may be empty during cluster startup, and then will be updated
when a new update is being applied. The newest update is first in the
list and it is ordered by recency. Updates in the history have state
Completed if the rollout completed - if an update was failing or halfway
applied the state will be Partial. Only a limited amount of update history
is preserved.</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code></br>
<em>
int64
</em>
</td>
<td>
<p>observedGeneration reports which version of the spec is being synced.
If this value is not equal to metadata.generation, then the desired
and conditions fields may represent a previous version.</p>
</td>
</tr>
<tr>
<td>
<code>availableUpdates</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.Release
</a>
</em>
</td>
<td>
<p>availableUpdates contains updates recommended for this
cluster. Updates which appear in conditionalUpdates but not in
availableUpdates may expose this cluster to known issues. This list
may be empty if no updates are recommended, if the update service
is unavailable, or if an invalid channel has been specified.</p>
</td>
</tr>
<tr>
<td>
<code>conditionalUpdates</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.ConditionalUpdate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditionalUpdates contains the list of updates that may be
recommended for this cluster if it meets specific required
conditions. Consumers interested in the set of updates that are
actually recommended for this cluster should use
availableUpdates. This list may be empty if no updates are
recommended, if the update service is unavailable, or if an empty
or invalid channel has been specified.</p>
</td>
</tr>
</tbody>
</table>
###ConditionType { #hypershift.openshift.io/v1beta1.ConditionType }
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWSDefaultSecurityGroupCreated&#34;</p></td>
<td><p>AWSDefaultSecurityGroupCreated indicates whether the default security group
for AWS workers has been created.
A failure here indicates that NodePools without a security group will be
blocked from creating machines.</p>
</td>
</tr><tr><td><p>&#34;AWSDefaultSecurityGroupDeleted&#34;</p></td>
<td><p>AWSDefaultSecurityGroupDeleted indicates whether the default security group
for AWS workers has been deleted.
A failure here indicates that the Security Group has some dependencies that
there are still pending cloud resources to be deleted that are using that SG.</p>
</td>
</tr><tr><td><p>&#34;AWSEndpointAvailable&#34;</p></td>
<td><p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint has been
created in the guest VPC</p>
</td>
</tr><tr><td><p>&#34;AWSEndpointServiceAvailable&#34;</p></td>
<td><p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint Service
has been created for the specified NLB in the management VPC</p>
</td>
</tr><tr><td><p>&#34;CVOScaledDown&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;CloudResourcesDestroyed&#34;</p></td>
<td><p>CloudResourcesDestroyed bubbles up the same condition from HCP. It signals if the cloud provider infrastructure created by Kubernetes
in the consumer cloud provider account was destroyed.
A failure here may require external user intervention to resolve. E.g. cloud provider perms were corrupted. E.g. the guest cluster was broken
and kube resource deletion that affects cloud infra like service type load balancer can&rsquo;t succeed.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionAvailable&#34;</p></td>
<td><p>ClusterVersionAvailable bubbles up Failing configv1.OperatorAvailable from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionFailing&#34;</p></td>
<td><p>ClusterVersionFailing bubbles up Failing from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionProgressing&#34;</p></td>
<td><p>ClusterVersionProgressing bubbles up configv1.OperatorProgressing from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionReleaseAccepted&#34;</p></td>
<td><p>ClusterVersionReleaseAccepted bubbles up Failing ReleaseAccepted from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionSucceeding&#34;</p></td>
<td><p>ClusterVersionSucceeding indicates the current status of the desired release
version of the HostedCluster as indicated by the Failing condition in the
underlying cluster&rsquo;s ClusterVersion.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionUpgradeable&#34;</p></td>
<td><p>ClusterVersionUpgradeable indicates the Upgradeable condition in the
underlying cluster&rsquo;s ClusterVersion.</p>
</td>
</tr><tr><td><p>&#34;EtcdAvailable&#34;</p></td>
<td><p>EtcdAvailable bubbles up the same condition from HCP. It signals if etcd is available.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;EtcdSnapshotRestored&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ExternalDNSReachable&#34;</p></td>
<td><p>ExternalDNSReachable bubbles up the same condition from HCP. It signals if the configured external DNS is reachable.
A failure here requires external user intervention to resolve. E.g. changing the external DNS domain or making sure the domain is created
and registered correctly.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td><p>HostedClusterAvailable indicates whether the HostedCluster has a healthy
control plane.
When this is false for too long and there&rsquo;s no clear indication in the &ldquo;Reason&rdquo;, please check the remaining more granular conditions.</p>
</td>
</tr><tr><td><p>&#34;Degraded&#34;</p></td>
<td><p>HostedClusterDegraded indicates whether the HostedCluster is encountering
an error that may require user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;HostedClusterDestroyed&#34;</p></td>
<td><p>HostedClusterDestroyed indicates that a hosted has finished destroying and that it is waiting for a destroy grace period to go away.
The grace period is determined by the hypershift.openshift.io/destroy-grace-period annotation in the HostedCluster if present.</p>
</td>
</tr><tr><td><p>&#34;Progressing&#34;</p></td>
<td><p>HostedClusterProgressing indicates whether the HostedCluster is attempting
an initial deployment or upgrade.
When this is false for too long and there&rsquo;s no clear indication in the &ldquo;Reason&rdquo;, please check the remaining more granular conditions.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Degraded&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;IgnitionEndpointAvailable&#34;</p></td>
<td><p>IgnitionEndpointAvailable indicates whether the ignition server for the
HostedCluster is available to handle ignition requests.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;IgnitionServerValidReleaseInfo&#34;</p></td>
<td><p>IgnitionServerValidReleaseInfo indicates if the release contains all the images used by the local ignition provider
and reports missing images if any.</p>
</td>
</tr><tr><td><p>&#34;InfrastructureReady&#34;</p></td>
<td><p>InfrastructureReady bubbles up the same condition from HCP. It signals if the infrastructure for a control plane to be operational,
e.g. load balancers were created successfully.
A failure here may require external user intervention to resolve. E.g. hitting quotas on the cloud provider.</p>
</td>
</tr><tr><td><p>&#34;KubeAPIServerAvailable&#34;</p></td>
<td><p>KubeAPIServerAvailable bubbles up the same condition from HCP. It signals if the kube API server is available.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;KubeVirtNodesLiveMigratable&#34;</p></td>
<td><p>KubeVirtNodesLiveMigratable indicates if all nodes (VirtualMachines) of the kubevirt
hosted cluster can be live migrated without experiencing a node restart</p>
</td>
</tr><tr><td><p>&#34;PlatformCredentialsFound&#34;</p></td>
<td><p>PlatformCredentialsFound indicates that credentials required for the
desired platform are valid.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ReconciliationActive&#34;</p></td>
<td><p>ReconciliationActive indicates if reconciliation of the HostedCluster is
active or paused hostedCluster.spec.pausedUntil.</p>
</td>
</tr><tr><td><p>&#34;ReconciliationSucceeded&#34;</p></td>
<td><p>ReconciliationSucceeded indicates if the HostedCluster reconciliation
succeeded.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;SupportedHostedCluster&#34;</p></td>
<td><p>SupportedHostedCluster indicates whether a HostedCluster is supported by
the current configuration of the hypershift-operator.
e.g. If HostedCluster requests endpointAcess Private but the hypershift-operator
is running on a management cluster outside AWS or is not configured with AWS
credentials, the HostedCluster is not supported.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;UnmanagedEtcdAvailable&#34;</p></td>
<td><p>UnmanagedEtcdAvailable indicates whether a user-managed etcd cluster is
healthy.</p>
</td>
</tr><tr><td><p>&#34;ValidAWSIdentityProvider&#34;</p></td>
<td><p>ValidAWSIdentityProvider indicates if the Identity Provider referenced
in the cloud credentials is healthy. E.g. for AWS the idp ARN is referenced in the iam roles.
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]</p>
<p>A failure here may require external user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;ValidAWSKMSConfig&#34;</p></td>
<td><p>ValidAWSKMSConfig indicates whether the AWS KMS role and encryption key are valid and operational
A failure here indicates that the role or the key are invalid, or the role doesn&rsquo;t have access to use the key.</p>
</td>
</tr><tr><td><p>&#34;ValidAzureKMSConfig&#34;</p></td>
<td><p>ValidAzureKMSConfig indicates whether the given KMS input for the Azure platform is valid and operational
A failure here indicates that the input is invalid, or permissions are missing to use the encryption key.</p>
</td>
</tr><tr><td><p>&#34;ValidConfiguration&#34;</p></td>
<td><p>ValidHostedClusterConfiguration signals if the hostedCluster input is valid and
supported by the underlying management cluster.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidHostedControlPlaneConfiguration&#34;</p></td>
<td><p>ValidHostedControlPlaneConfiguration bubbles up the same condition from HCP. It signals if the hostedControlPlane input is valid and
supported by the underlying management cluster.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidIDPConfiguration&#34;</p></td>
<td><p>ValidIDPConfiguration indicates if the Identity Provider configuration is valid.
A failure here may require external user intervention to resolve
e.g. the user-provided IDP configuration provided is invalid or the IDP is not reachable.</p>
</td>
</tr><tr><td><p>&#34;ValidKubeVirtInfraNetworkMTU&#34;</p></td>
<td><p>ValidKubeVirtInfraNetworkMTU indicates if the MTU configured on an infra cluster
hosting a guest cluster utilizing kubevirt platform is a sufficient value that will avoid
performance degradation due to fragmentation of the double encapsulation in ovn-kubernetes</p>
</td>
</tr><tr><td><p>&#34;ValidOIDCConfiguration&#34;</p></td>
<td><p>ValidOIDCConfiguration indicates if an AWS cluster&rsquo;s OIDC condition is
detected as invalid.
A failure here may require external user intervention to resolve. E.g. oidc was deleted out of band.</p>
</td>
</tr><tr><td><p>&#34;ValidReleaseImage&#34;</p></td>
<td><p>ValidReleaseImage indicates if the release image set in the spec is valid
for the HostedCluster. For example, this can be set false if the
HostedCluster itself attempts an unsupported version before 4.9 or an
unsupported upgrade e.g y-stream upgrade before 4.11.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidReleaseInfo&#34;</p></td>
<td><p>ValidReleaseInfo bubbles up the same condition from HCP. It indicates if the release contains all the images used by hypershift
and reports missing images if any.</p>
</td>
</tr></tbody>
</table>
###DNSSpec { #hypershift.openshift.io/v1beta1.DNSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>DNSSpec specifies the DNS configuration in the cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>baseDomain</code></br>
<em>
string
</em>
</td>
<td>
<p>BaseDomain is the base domain of the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>baseDomainPrefix</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BaseDomainPrefix is the base domain prefix of the cluster.
defaults to clusterName if not set. Set it to &ldquo;&rdquo; if you don&rsquo;t want a prefix to be prepended to BaseDomain.</p>
</td>
</tr>
<tr>
<td>
<code>publicZoneID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicZoneID is the Hosted Zone ID where all the DNS records that are
publicly accessible to the internet exist.</p>
</td>
</tr>
<tr>
<td>
<code>privateZoneID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateZoneID is the Hosted Zone ID where all the DNS records that are only
available internally to the cluster exist.</p>
</td>
</tr>
</tbody>
</table>
###Diagnostics { #hypershift.openshift.io/v1beta1.Diagnostics }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
<p>Diagnostics specifies the diagnostics settings for a virtual machine.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageAccountType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType">
AzureDiagnosticsStorageAccountType
</a>
</em>
</td>
<td>
<p>StorageAccountType determines if the storage account for storing the diagnostics data
should be disabled (Disabled), provisioned by Azure (Managed) or by the user (UserManaged).</p>
</td>
</tr>
<tr>
<td>
<code>storageAccountURI</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageAccountURI is the URI of the user-managed storage account.
The URI typically will be <code>https://&lt;mystorageaccountname&gt;.blob.core.windows.net/</code>
but may differ if you are using Azure DNS zone endpoints.
You can find the correct endpoint by looking for the Blob Primary Endpoint in the
endpoints tab in the Azure console or with the CLI by issuing
<code>az storage account list --query='[].{name: name, &quot;resource group&quot;: resourceGroup, &quot;blob endpoint&quot;: primaryEndpoints.blob}'</code>.</p>
</td>
</tr>
</tbody>
</table>
###EtcdManagementType { #hypershift.openshift.io/v1beta1.EtcdManagementType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>EtcdManagementType is a enum specifying the strategy for managing the cluster&rsquo;s etcd instance</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>Managed means HyperShift should provision and operator the etcd cluster
automatically.</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>Unmanaged means HyperShift will not provision or manage the etcd cluster,
and the user is responsible for doing so.</p>
</td>
</tr></tbody>
</table>
###EtcdSpec { #hypershift.openshift.io/v1beta1.EtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>EtcdSpec specifies configuration for a control plane etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>managementType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdManagementType">
EtcdManagementType
</a>
</em>
</td>
<td>
<p>ManagementType defines how the etcd cluster is managed.</p>
</td>
</tr>
<tr>
<td>
<code>managed</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdSpec">
ManagedEtcdSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Managed specifies the behavior of an etcd cluster managed by HyperShift.</p>
</td>
</tr>
<tr>
<td>
<code>unmanaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec">
UnmanagedEtcdSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Unmanaged specifies configuration which enables the control plane to
integrate with an eternally managed etcd cluster.</p>
</td>
</tr>
</tbody>
</table>
###EtcdTLSConfig { #hypershift.openshift.io/v1beta1.EtcdTLSConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec">UnmanagedEtcdSpec</a>)
</p>
<p>
<p>EtcdTLSConfig specifies TLS configuration for HTTPS etcd client endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ClientSecret refers to a secret for client mTLS authentication with the etcd cluster. It
may have the following key/value pairs:</p>
<pre><code>etcd-client-ca.crt: Certificate Authority value
etcd-client.crt: Client certificate value
etcd-client.key: Client certificate key value
</code></pre>
</td>
</tr>
</tbody>
</table>
###Filter { #hypershift.openshift.io/v1beta1.Filter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">AWSResourceReference</a>)
</p>
<p>
<p>Filter is a filter used to identify an AWS resource</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name of the filter. Filter names are case-sensitive.</p>
</td>
</tr>
<tr>
<td>
<code>values</code></br>
<em>
[]string
</em>
</td>
<td>
<p>Values includes one or more filter values. Filter values are case-sensitive.</p>
</td>
</tr>
</tbody>
</table>
###FilterByNeutronTags { #hypershift.openshift.io/v1beta1.FilterByNeutronTags }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NetworkFilter">NetworkFilter</a>, 
<a href="#hypershift.openshift.io/v1beta1.RouterFilter">RouterFilter</a>, 
<a href="#hypershift.openshift.io/v1beta1.SubnetFilter">SubnetFilter</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>tags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tags is a list of tags to filter by. If specified, the resource must
have all of the tags specified to be included in the result.</p>
</td>
</tr>
<tr>
<td>
<code>tagsAny</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>TagsAny is a list of tags to filter by. If specified, the resource
must have at least one of the tags specified to be included in the
result.</p>
</td>
</tr>
<tr>
<td>
<code>notTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NotTags is a list of tags to filter by. If specified, resources which
contain all of the given tags will be excluded from the result.</p>
</td>
</tr>
<tr>
<td>
<code>notTagsAny</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NotTagsAny is a list of tags to filter by. If specified, resources
which contain any of the given tags will be excluded from the result.</p>
</td>
</tr>
</tbody>
</table>
###HostedClusterSpec { #hypershift.openshift.io/v1beta1.HostedClusterSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedCluster">HostedCluster</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>Release specifies the desired OCP release payload for the hosted cluster.</p>
<p>Updating this field will trigger a rollout of the control plane. The
behavior of the rollout will be driven by the ControllerAvailabilityPolicy
and InfrastructureAvailabilityPolicy.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneRelease</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneRelease specifies the desired OCP release payload for
control plane components running on the management cluster.
Updating this field will trigger a rollout of the control plane. The
behavior of the rollout will be driven by the ControllerAvailabilityPolicy
and InfrastructureAvailabilityPolicy.
If not defined, Release is used</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClusterID uniquely identifies this cluster. This is expected to be
an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in
hexadecimal values).
As with a Kubernetes metadata.uid, this ID uniquely identifies this
cluster in space and time.
This value identifies the cluster in metrics pushed to telemetry and
metrics produced by the control plane operators. If a value is not
specified, an ID is generated. After initial creation, the value is
immutable.</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
By default it will use the appropriate update service for the cluster and region.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default
set of updates be applied to this cluster. The default channel will be
contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfraID is a globally unique identifier for the cluster. This identifier
will be used to associate various cloud resources with the HostedCluster
and its associated NodePools.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>Platform specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControllerAvailabilityPolicy specifies the availability policy applied to
critical control plane components. The default value is HighlyAvailable.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfrastructureAvailabilityPolicy specifies the availability policy applied
to infrastructure services which run on cluster nodes. The default value is
SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<p>DNS specifies DNS configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<p>Networking specifies network configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>Etcd specifies configuration for the control plane etcd cluster. The
default ManagementType is Managed. Once set, the ManagementType cannot be
changed.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>Services specifies how individual control plane services are published from
the hosting cluster of the control plane.</p>
<p>If a given service is not present in this list, it will be exposed publicly
by default.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>PullSecret references a pull secret to be injected into the container
runtime of all cluster nodes. The secret must have a key named
&ldquo;.dockerconfigjson&rdquo; whose value is the pull secret JSON.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>SSHKey references an SSH key to be injected into all cluster node sshd
servers. The secret must have a single key &ldquo;id_rsa.pub&rdquo; whose value is the
public part of an SSH key.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IssuerURL is an OIDC issuer URL which is used as the issuer in all
ServiceAccount tokens generated by the control plane API server. The
default value is kubernetes.default.svc, which only works for in-cluster
validation.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountSigningKey is a reference to a secret containing the private key
used by the service account token issuer. The secret is expected to contain
a single key named &ldquo;key&rdquo;. If not specified, a service account signing key will
be generated automatically for the cluster. When specifying a service account
signing key, a IssuerURL must also be specified.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditWebhook contains metadata for configuring an audit webhook endpoint
for a cluster to process cluster audit events. It references a secret that
contains the webhook information for the audit webhook endpoint. It is a
secret because if the endpoint has mTLS the kubeconfig will contain client
keys. The kubeconfig needs to be stored in the secret with a secret key
name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageContentSources specifies image mirrors that can be used by cluster
nodes to pull content.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTrustBundle is a reference to a ConfigMap containing a
PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretEncryption specifies a Kubernetes secret encryption strategy for the
control plane.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>FIPS indicates whether this cluster&rsquo;s nodes will be running in FIPS mode.
If set to true, the control plane&rsquo;s ignition server will be configured to
expect that nodes joining the cluster will be FIPS-enabled.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OLMCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector when specified, must be true for the pods managed by the HostedCluster to be scheduled.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tolerations when specified, define what custome tolerations are added to the hcp pods.</p>
</td>
</tr>
</tbody>
</table>
###HostedClusterStatus { #hypershift.openshift.io/v1beta1.HostedClusterStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedCluster">HostedCluster</a>)
</p>
<p>
<p>HostedClusterStatus is the latest observed status of a HostedCluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>version</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionStatus">
ClusterVersionStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version is the status of the release version applied to the
HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>kubeconfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeConfig is a reference to the secret containing the default kubeconfig
for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>kubeadminPassword</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeadminPassword is a reference to the secret that contains the initial
kubeadmin user password for the guest cluster.</p>
</td>
</tr>
<tr>
<td>
<code>ignitionEndpoint</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IgnitionEndpoint is the endpoint injected in the ign config userdata.
It exposes the config for instances to become kubernetes nodes.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIEndpoint">
APIEndpoint
</a>
</em>
</td>
<td>
<p>ControlPlaneEndpoint contains the endpoint information by which
external clients can access the control plane. This is populated
after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>oauthCallbackURLTemplate</code></br>
<em>
string
</em>
</td>
<td>
<p>OAuthCallbackURLTemplate contains a template for the URL to use as a callback
for identity providers. The [identity-provider-name] placeholder must be replaced
with the name of an identity provider defined on the HostedCluster.
This is populated after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions represents the latest available observations of a control
plane&rsquo;s current state.</p>
</td>
</tr>
<tr>
<td>
<code>payloadArch</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PayloadArchType">
PayloadArchType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>payloadArch represents the CPU architecture type of the HostedCluster.Spec.Release.Image. The valid values are:
Multi, ARM64, AMD64, S390X, or PPC64LE.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">
PlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Platform contains platform-specific status of the HostedCluster</p>
</td>
</tr>
</tbody>
</table>
###HostedControlPlaneSpec { #hypershift.openshift.io/v1beta1.HostedControlPlaneSpec }
<p>
<p>HostedControlPlaneSpec defines the desired state of HostedControlPlane</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>releaseImage</code></br>
<em>
string
</em>
</td>
<td>
<p>ReleaseImage is the release image applied to the hosted control plane.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneReleaseImage</code></br>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneReleaseImage specifies the desired OCP release payload for
control plane components running on the management cluster.
If not defined, ReleaseImage is used</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
By default it will use the appropriate update service for the cluster and region.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default
set of updates be applied to this cluster. The default channel will be
contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<p>IssuerURL is an OIDC issuer URL which is used as the issuer in all
ServiceAccount tokens generated by the control plane API server. The
default value is kubernetes.default.svc, which only works for in-cluster
validation.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Networking specifies network configuration for the cluster.
Temporarily optional for backward compatibility, required in future releases.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClusterID is the unique id that identifies the cluster externally.
Making it optional here allows us to keep compatibility with previous
versions of the control-plane-operator that have no knowledge of this
field.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountSigningKey is a reference to a secret containing the private key
used by the service account token issuer. The secret is expected to contain
a single key named &ldquo;key&rdquo;. If not specified, a service account signing key will
be generated automatically for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControllerAvailabilityPolicy specifies the availability policy applied to
critical control plane components. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfrastructureAvailabilityPolicy specifies the availability policy applied
to infrastructure services which run on cluster nodes. The default value is
SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>FIPS specifies if the nodes for the cluster will be running in FIPS mode</p>
</td>
</tr>
<tr>
<td>
<code>kubeconfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeConfig specifies the name and key for the kubeconfig secret</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>Services defines metadata about how control plane services are published
in the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditWebhook contains metadata for configuring an audit webhook
endpoint for a cluster to process cluster audit events. It references
a secret that contains the webhook information for the audit webhook endpoint.
It is a secret because if the endpoint has MTLS the kubeconfig will contain client
keys. This is currently only supported in IBM Cloud. The kubeconfig needs to be stored
in the secret with a secret key name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>Etcd contains metadata about the etcd cluster the hypershift managed Openshift control plane components
use to store data.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<p>Configuration embeds resources that correspond to the openshift configuration API:
<a href="https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html">https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</a></p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageContentSources lists sources/repositories for the release-image content.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTrustBundle references a ConfigMap containing a PEM-encoded X.509 certificate bundle</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretEncryption contains metadata about the kubernetes secret encryption strategy being used for the
cluster when applicable.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OLMCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector when specified, must be true for the pods managed by the HostedCluster to be scheduled.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tolerations when specified, define what custome tolerations are added to the hcp pods.</p>
</td>
</tr>
</tbody>
</table>
###HostedControlPlaneStatus { #hypershift.openshift.io/v1beta1.HostedControlPlaneStatus }
<p>
<p>HostedControlPlaneStatus defines the observed state of HostedControlPlane</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code></br>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the HostedControlPlane API Server is ready to
receive requests
This satisfies CAPI contract <a href="https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230">https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230</a></p>
</td>
</tr>
<tr>
<td>
<code>initialized</code></br>
<em>
bool
</em>
</td>
<td>
<p>Initialized denotes whether or not the control plane has
provided a kubeadm-config.
Once this condition is marked true, its value is never changed. See the Ready condition for an indication of
the current readiness of the cluster&rsquo;s control plane.
This satisfies CAPI contract <a href="https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252">https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252</a></p>
</td>
</tr>
<tr>
<td>
<code>externalManagedControlPlane</code></br>
<em>
bool
</em>
</td>
<td>
<p>ExternalManagedControlPlane indicates to cluster-api that the control plane
is managed by an external service.
<a href="https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468">https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468</a></p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIEndpoint">
APIEndpoint
</a>
</em>
</td>
<td>
<p>ControlPlaneEndpoint contains the endpoint information by which
external clients can access the control plane.  This is populated
after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>oauthCallbackURLTemplate</code></br>
<em>
string
</em>
</td>
<td>
<p>OAuthCallbackURLTemplate contains a template for the URL to use as a callback
for identity providers. The [identity-provider-name] placeholder must be replaced
with the name of an identity provider defined on the HostedCluster.
This is populated after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>versionStatus</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionStatus">
ClusterVersionStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>versionStatus is the status of the release version applied by the
hosted control plane operator.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<p>Version is the semantic version of the release applied by
the hosted control plane operator</p>
<p>Deprecated: Use versionStatus.desired.version instead.</p>
</td>
</tr>
<tr>
<td>
<code>releaseImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ReleaseImage is the release image applied to the hosted control plane.</p>
<p>Deprecated: Use versionStatus.desired.image instead.</p>
</td>
</tr>
<tr>
<td>
<code>lastReleaseImageTransitionTime</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>lastReleaseImageTransitionTime is the time of the last update to the current
releaseImage property.</p>
<p>Deprecated: Use versionStatus.history[0].startedTime instead.</p>
</td>
</tr>
<tr>
<td>
<code>kubeConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<p>KubeConfig is a reference to the secret containing the default kubeconfig
for this control plane.</p>
</td>
</tr>
<tr>
<td>
<code>kubeadminPassword</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeadminPassword is a reference to the secret containing the initial kubeadmin password
for the guest cluster.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Condition contains details for one aspect of the current state of the HostedControlPlane.
Current condition types are: &ldquo;Available&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">
PlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Platform contains platform-specific status of the HostedCluster</p>
</td>
</tr>
<tr>
<td>
<code>nodeCount</code></br>
<em>
int
</em>
</td>
<td>
<p>NodeCount tracks the number of nodes in the HostedControlPlane.</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">IBMCloudKMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSAuthSpec defines metadata for how authentication is done with IBM Cloud KMS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType">
IBMCloudKMSAuthType
</a>
</em>
</td>
<td>
<p>Type defines the IBM Cloud KMS authentication strategy</p>
</td>
</tr>
<tr>
<td>
<code>unmanaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec">
IBMCloudKMSUnmanagedAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Unmanaged defines the auth metadata the customer provides to interact with IBM Cloud KMS</p>
</td>
</tr>
<tr>
<td>
<code>managed</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec">
IBMCloudKMSManagedAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Managed defines metadata around the service to service authentication strategy for the IBM Cloud
KMS system (all provider managed).</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSAuthType { #hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSAuthType defines the IBM Cloud KMS authentication strategy</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>IBMCloudKMSManagedAuth defines the KMS authentication strategy where the IKS/ROKS platform uses
service to service auth to call IBM Cloud KMS APIs (no customer credentials requried)</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>IBMCloudKMSUnmanagedAuth defines the KMS authentication strategy where a customer supplies IBM Cloud
authentication to interact with IBM Cloud KMS APIs</p>
</td>
</tr></tbody>
</table>
###IBMCloudKMSKeyEntry { #hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">IBMCloudKMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSKeyEntry defines metadata for an IBM Cloud KMS encryption key</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>crkID</code></br>
<em>
string
</em>
</td>
<td>
<p>CRKID is the customer rook key id</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code></br>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the id for the key protect instance</p>
</td>
</tr>
<tr>
<td>
<code>correlationID</code></br>
<em>
string
</em>
</td>
<td>
<p>CorrelationID is an identifier used to track all api call usage from hypershift</p>
</td>
</tr>
<tr>
<td>
<code>url</code></br>
<em>
string
</em>
</td>
<td>
<p>URL is the url to call key protect apis over</p>
</td>
</tr>
<tr>
<td>
<code>keyVersion</code></br>
<em>
int
</em>
</td>
<td>
<p>KeyVersion is a unique number associated with the key. The number increments whenever a new
key is enabled for data encryption.</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSManagedAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSManagedAuthSpec defines metadata around the service to service authentication strategy for the IBM Cloud
KMS system (all provider managed).</p>
</p>
###IBMCloudKMSSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSSpec defines metadata for the IBM Cloud KMS encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the IBM Cloud region</p>
</td>
</tr>
<tr>
<td>
<code>auth</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">
IBMCloudKMSAuthSpec
</a>
</em>
</td>
<td>
<p>Auth defines metadata for how authentication is done with IBM Cloud KMS</p>
</td>
</tr>
<tr>
<td>
<code>keyList</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry">
[]IBMCloudKMSKeyEntry
</a>
</em>
</td>
<td>
<p>KeyList defines the list of keys used for data encryption</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSUnmanagedAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSUnmanagedAuthSpec defines the auth metadata the customer provides to interact with IBM Cloud KMS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>Credentials should reference a secret with a key field of IBMCloudIAMAPIKeySecretKey that contains a apikey to
call IBM Cloud KMS APIs</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudPlatformSpec { #hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>IBMCloudPlatformSpec defines IBMCloud specific settings for components</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerType</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.IBMCloudProviderType
</a>
</em>
</td>
<td>
<p>ProviderType is a specific supported infrastructure provider within IBM Cloud.</p>
</td>
</tr>
</tbody>
</table>
###ImageContentSource { #hypershift.openshift.io/v1beta1.ImageContentSource }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ImageContentSource specifies image mirrors that can be used by cluster nodes
to pull content. For cluster workloads, if a container image registry host of
the pullspec matches Source then one of the Mirrors are substituted as hosts
in the pullspec and tried in order to fetch the image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>source</code></br>
<em>
string
</em>
</td>
<td>
<p>Source is the repository that users refer to, e.g. in image pull
specifications.</p>
</td>
</tr>
<tr>
<td>
<code>mirrors</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Mirrors are one or more repositories that may also contain the same images.</p>
</td>
</tr>
</tbody>
</table>
###InPlaceUpgrade { #hypershift.openshift.io/v1beta1.InPlaceUpgrade }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>InPlaceUpgrade specifies an upgrade strategy which upgrades nodes in-place
without any new nodes being created or any old nodes being deleted.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailable is the maximum number of nodes that can be unavailable
during the update.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding down.</p>
<p>Defaults to 1.</p>
<p>Example: when this is set to 30%, a max of 30% of the nodes can be made
unschedulable/unavailable immediately when the update starts. Once a set
of nodes is updated, more nodes can be made unschedulable for update,
ensuring that the total number of nodes schedulable at all times during
the update is at least 70% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
###KMSProvider { #hypershift.openshift.io/v1beta1.KMSProvider }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>KMSProvider defines the supported KMS providers</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWS&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Azure&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;IBMCloud&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###KMSSpec { #hypershift.openshift.io/v1beta1.KMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>KMSSpec defines metadata about the kms secret encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provider</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KMSProvider">
KMSProvider
</a>
</em>
</td>
<td>
<p>Provider defines the KMS provider</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">
IBMCloudKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IBMCloud defines metadata for the IBM Cloud KMS encryption strategy</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">
AWSKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS defines metadata about the configuration of the AWS KMS Secret Encryption provider</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">
AzureKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Azure defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>
</td>
</tr>
</tbody>
</table>
###KubeVirtNodePoolStatus { #hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus">NodePoolPlatformStatus</a>)
</p>
<p>
<p>KubeVirtNodePoolStatus contains the KubeVirt platform statuses</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cacheName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CacheName holds the name of the cache DataVolume, if exists</p>
</td>
</tr>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials">
KubevirtPlatformCredentials
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Credentials shows the client credentials used when creating KubeVirt virtual machines.
This filed is only exists when the KubeVirt virtual machines are being placed
on a cluster separate from the one hosting the Hosted Control Plane components.</p>
<p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on
the same cluster and namespace as the Hosted Control Plane.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtCachingStrategy { #hypershift.openshift.io/v1beta1.KubevirtCachingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtCachingStrategy defines the boot image caching strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType">
KubevirtCachingStrategyType
</a>
</em>
</td>
<td>
<p>Type is the type of the caching strategy</p>
</td>
</tr>
</tbody>
</table>
###KubevirtCachingStrategyType { #hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy">KubevirtCachingStrategy</a>)
</p>
<p>
<p>KubevirtCachingStrategyType is the type of the boot image caching mechanism for the KubeVirt provider</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;None&#34;</p></td>
<td><p>KubevirtCachingStrategyNone means that hypershift will not cache the boot image</p>
</td>
</tr><tr><td><p>&#34;PVC&#34;</p></td>
<td><p>KubevirtCachingStrategyPVC means that hypershift will cache the boot image into a PVC; only relevant when using
a QCOW boot image, and is ignored when using a container image</p>
</td>
</tr></tbody>
</table>
###KubevirtCompute { #hypershift.openshift.io/v1beta1.KubevirtCompute }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtCompute contains values associated with the virtual compute hardware requested for the VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>memory</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Memory represents how much guest memory the VM should have</p>
</td>
</tr>
<tr>
<td>
<code>cores</code></br>
<em>
uint32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Cores represents how many cores the guest VM should have</p>
</td>
</tr>
<tr>
<td>
<code>qosClass</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.QoSClass">
QoSClass
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>QosClass If set to &ldquo;Guaranteed&rdquo;, requests the scheduler to place the VirtualMachineInstance on a node with
limit memory and CPU, equal to be the requested values, to set the VMI as a Guaranteed QoS Class;
See here for more details:
<a href="https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances">https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances</a></p>
</td>
</tr>
</tbody>
</table>
###KubevirtDiskImage { #hypershift.openshift.io/v1beta1.KubevirtDiskImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtDiskImage contains values representing where the rhcos image is located</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>containerDiskImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerDiskImage is a string representing the container image that holds the root disk</p>
</td>
</tr>
</tbody>
</table>
###KubevirtHostDevice { #hypershift.openshift.io/v1beta1.KubevirtHostDevice }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code></br>
<em>
string
</em>
</td>
<td>
<p>DeviceName is the name of the host device that is desired to be utilized in the HostedCluster&rsquo;s NodePool
The device can be any supported PCI device, including GPU, either as a passthrough or a vGPU slice.</p>
</td>
</tr>
<tr>
<td>
<code>count</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>Count is the number of instances the specified host device will be attached to each of the
NodePool&rsquo;s nodes. Default is 1.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtManualStorageDriverConfig { #hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">KubevirtStorageDriverSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageClassMapping</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping">
[]KubevirtStorageClassMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageClassMapping maps StorageClasses on the infra cluster hosting
the KubeVirt VMs to StorageClasses that are made available within the
Guest Cluster.</p>
<p>NOTE: It is possible that not all capablities of an infra cluster&rsquo;s
storageclass will be present for the corresponding guest clusters storageclass.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSnapshotClassMapping</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping">
[]KubevirtVolumeSnapshotClassMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
</tbody>
</table>
###KubevirtNetwork { #hypershift.openshift.io/v1beta1.KubevirtNetwork }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtNetwork specifies the configuration for a virtual machine
network interface</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name specify the network attached to the nodes
it is a value with the format &ldquo;[namespace]/[name]&rdquo; to reference the
multus network attachment definition</p>
</td>
</tr>
</tbody>
</table>
###KubevirtNodePoolPlatform { #hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>KubevirtNodePoolPlatform specifies the configuration of a NodePool when operating
on KubeVirt platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rootVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">
KubevirtRootVolume
</a>
</em>
</td>
<td>
<p>RootVolume represents values associated with the VM volume that will host rhcos</p>
</td>
</tr>
<tr>
<td>
<code>compute</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCompute">
KubevirtCompute
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Compute contains values representing the virtual hardware requested for the VM</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceMultiqueue</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MultiQueueSetting">
MultiQueueSetting
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaceMultiQueue If set to &ldquo;Enable&rdquo;, virtual network interfaces configured with a virtio bus will also
enable the vhost multiqueue feature for network devices. The number of queues created depends on additional
factors of the VirtualMachineInstance, like the number of guest CPUs.</p>
</td>
</tr>
<tr>
<td>
<code>additionalNetworks</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNetwork">
[]KubevirtNetwork
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalNetworks specify the extra networks attached to the nodes</p>
</td>
</tr>
<tr>
<td>
<code>attachDefaultNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>AttachDefaultNetwork specify if the default pod network should be attached to the nodes
this can only be set to false if AdditionalNetworks are configured</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector is a selector which must be true for the kubevirt VirtualMachine to fit on a node.
Selector which must match a node&rsquo;s labels for the VM to be scheduled on that node. More info:
<a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></p>
</td>
</tr>
<tr>
<td>
<code>hostDevices</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtHostDevice">
[]KubevirtHostDevice
</a>
</em>
</td>
<td>
<p>KubevirtHostDevices specifies the host devices (e.g. GPU devices) to be passed
from the management cluster, to the nodepool nodes</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPersistentVolume { #hypershift.openshift.io/v1beta1.KubevirtPersistentVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">KubevirtVolume</a>)
</p>
<p>
<p>KubevirtPersistentVolume contains the values involved with provisioning persistent storage for a KubeVirt VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>size</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Size is the size of the persistent storage volume</p>
</td>
</tr>
<tr>
<td>
<code>storageClass</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageClass is the storageClass used for the underlying PVC that hosts the volume</p>
</td>
</tr>
<tr>
<td>
<code>accessModes</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode">
[]PersistentVolumeAccessMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessModes is an array that contains the desired Access Modes the root volume should have.
More info: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a></p>
</td>
</tr>
<tr>
<td>
<code>volumeMode</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#persistentvolumemode-v1-core">
Kubernetes core/v1.PersistentVolumeMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VolumeMode defines what type of volume is required by the claim.
Value of Filesystem is implied when not included in claim spec.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPlatformCredentials { #hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus">KubeVirtNodePoolStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">KubevirtPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>infraKubeConfigSecret</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<p>InfraKubeConfigSecret is a reference to a secret that contains the kubeconfig for the external infra cluster
that will be used to host the KubeVirt virtual machines for this cluster.</p>
</td>
</tr>
<tr>
<td>
<code>infraNamespace</code></br>
<em>
string
</em>
</td>
<td>
<p>InfraNamespace defines the namespace on the external infra cluster that is used to host the KubeVirt
virtual machines. This namespace must already exist before creating the HostedCluster and the kubeconfig
referenced in the InfraKubeConfigSecret must have access to manage the required resources within this
namespace.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPlatformSpec { #hypershift.openshift.io/v1beta1.KubevirtPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>KubevirtPlatformSpec specifies configuration for kubevirt guest cluster installations</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>baseDomainPassthrough</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>BaseDomainPassthrough toggles whether or not an automatically
generated base domain for the guest cluster should be used that
is a subdomain of the management cluster&rsquo;s *.apps DNS.</p>
<p>For the KubeVirt platform, the basedomain can be autogenerated using
the *.apps domain of the management/infra hosting cluster
This makes the guest cluster&rsquo;s base domain a subdomain of the
hypershift infra/mgmt cluster&rsquo;s base domain.</p>
<p>Example:
Infra/Mgmt cluster&rsquo;s DNS
Base: example.com
Cluster: mgmt-cluster.example.com
Apps:    *.apps.mgmt-cluster.example.com
KubeVirt Guest cluster&rsquo;s DNS
Base: apps.mgmt-cluster.example.com
Cluster: guest.apps.mgmt-cluster.example.com
Apps: *.apps.guest.apps.mgmt-cluster.example.com</p>
<p>This is possible using OCP wildcard routes</p>
</td>
</tr>
<tr>
<td>
<code>generateID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>GenerateID is used to uniquely apply a name suffix to resources associated with
kubevirt infrastructure resources</p>
</td>
</tr>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials">
KubevirtPlatformCredentials
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Credentials defines the client credentials used when creating KubeVirt virtual machines.
Defining credentials is only necessary when the KubeVirt virtual machines are being placed
on a cluster separate from the one hosting the Hosted Control Plane components.</p>
<p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on
the same cluster and namespace as the Hosted Control Plane.</p>
</td>
</tr>
<tr>
<td>
<code>storageDriver</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">
KubevirtStorageDriverSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageDriver defines how the KubeVirt CSI driver exposes StorageClasses on
the infra cluster (hosting the VMs) to the guest cluster.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtRootVolume { #hypershift.openshift.io/v1beta1.KubevirtRootVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtRootVolume represents the volume that the rhcos disk will be stored and run from.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>diskImage</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtDiskImage">
KubevirtDiskImage
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Image represents what rhcos image to use for the node pool</p>
</td>
</tr>
<tr>
<td>
<code>KubevirtVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">
KubevirtVolume
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubevirtVolume</code> are embedded into this type.)
</p>
<p>KubevirtVolume represents of type of storage to run the image on</p>
</td>
</tr>
<tr>
<td>
<code>cacheStrategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy">
KubevirtCachingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CacheStrategy defines the boot image caching strategy. Default - no caching</p>
</td>
</tr>
</tbody>
</table>
###KubevirtStorageClassMapping { #hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">KubevirtManualStorageDriverConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<p>Group contains which group this mapping belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>infraStorageClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>InfraStorageClassName is the name of the infra cluster storage class that
will be exposed to the guest.</p>
</td>
</tr>
<tr>
<td>
<code>guestStorageClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>GuestStorageClassName is the name that the corresponding storageclass will
be called within the guest cluster</p>
</td>
</tr>
</tbody>
</table>
###KubevirtStorageDriverConfigType { #hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">KubevirtStorageDriverSpec</a>)
</p>
<p>
<p>KubevirtStorageDriverConfigType defines how the kubevirt storage driver is configured.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Default&#34;</p></td>
<td><p>DefaultKubevirtStorageDriverConfigType means the kubevirt storage driver maps to the
underlying infra cluster&rsquo;s default storageclass</p>
</td>
</tr><tr><td><p>&#34;Manual&#34;</p></td>
<td><p>ManualKubevirtStorageDriverConfigType means the kubevirt storage driver mapping is
explicitly defined.</p>
</td>
</tr><tr><td><p>&#34;None&#34;</p></td>
<td><p>NoneKubevirtStorageDriverConfigType means no kubevirt storage driver is used</p>
</td>
</tr></tbody>
</table>
###KubevirtStorageDriverSpec { #hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">KubevirtPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType">
KubevirtStorageDriverConfigType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type represents the type of kubevirt csi driver configuration to use</p>
</td>
</tr>
<tr>
<td>
<code>manual</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">
KubevirtManualStorageDriverConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Manual is used to explicilty define how the infra storageclasses are
mapped to guest storageclasses</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolume { #hypershift.openshift.io/v1beta1.KubevirtVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtVolume represents what kind of storage to use for a KubeVirt VM volume</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolumeType">
KubevirtVolumeType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type represents the type of storage to associate with the kubevirt VMs.</p>
</td>
</tr>
<tr>
<td>
<code>persistent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume">
KubevirtPersistentVolume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Persistent volume type means the VM&rsquo;s storage is backed by a PVC
VMs that use persistent volumes can survive disruption events like restart and eviction
This is the default type used when no storage type is defined.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolumeSnapshotClassMapping { #hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">KubevirtManualStorageDriverConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<p>Group contains which group this mapping belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>infraVolumeSnapshotClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>InfraStorageClassName is the name of the infra cluster volume snapshot class that
will be exposed to the guest.</p>
</td>
</tr>
<tr>
<td>
<code>guestVolumeSnapshotClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>GuestVolumeSnapshotClassName is the name that the corresponding volumeSnapshotClass will
be called within the guest cluster</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolumeType { #hypershift.openshift.io/v1beta1.KubevirtVolumeType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">KubevirtVolume</a>)
</p>
<p>
<p>KubevirtVolumeType is a specific supported KubeVirt volumes</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Persistent&#34;</p></td>
<td><p>KubevirtVolumeTypePersistent represents persistent volume for kubevirt VMs</p>
</td>
</tr></tbody>
</table>
###LoadBalancerPublishingStrategy { #hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>LoadBalancerPublishingStrategy specifies setting used to expose a service as a LoadBalancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Hostname is the name of the DNS record that will be created pointing to the LoadBalancer.</p>
</td>
</tr>
</tbody>
</table>
###MachineNetworkEntry { #hypershift.openshift.io/v1beta1.MachineNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>MachineNetworkEntry is a single IP address block for node IP blocks.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>CIDR is the IP block address pool for machines within the cluster.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdSpec { #hypershift.openshift.io/v1beta1.ManagedEtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>ManagedEtcdSpec specifies the behavior of an etcd cluster managed by
HyperShift.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storage</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">
ManagedEtcdStorageSpec
</a>
</em>
</td>
<td>
<p>Storage specifies how etcd data is persisted.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdStorageSpec { #hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdSpec">ManagedEtcdSpec</a>)
</p>
<p>
<p>ManagedEtcdStorageSpec describes the storage configuration for etcd data.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageType">
ManagedEtcdStorageType
</a>
</em>
</td>
<td>
<p>Type is the kind of persistent storage implementation to use for etcd.</p>
</td>
</tr>
<tr>
<td>
<code>persistentVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec">
PersistentVolumeEtcdStorageSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PersistentVolume is the configuration for PersistentVolume etcd storage.
With this implementation, a PersistentVolume will be allocated for every
etcd member (either 1 or 3 depending on the HostedCluster control plane
availability configuration).</p>
</td>
</tr>
<tr>
<td>
<code>restoreSnapshotURL</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RestoreSnapshotURL allows an optional URL to be provided where
an etcd snapshot can be downloaded, for example a pre-signed URL
referencing a storage service.
This snapshot will be restored on initial startup, only when the etcd PV
is empty.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdStorageType { #hypershift.openshift.io/v1beta1.ManagedEtcdStorageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">ManagedEtcdStorageSpec</a>)
</p>
<p>
<p>ManagedEtcdStorageType is a storage type for an etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;PersistentVolume&#34;</p></td>
<td><p>PersistentVolumeEtcdStorage uses PersistentVolumes for etcd storage.</p>
</td>
</tr></tbody>
</table>
###MarketplaceImage { #hypershift.openshift.io/v1beta1.MarketplaceImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">AzureVMImage</a>)
</p>
<p>
<p>MarketplaceImage specifies the information needed to create an Azure VM from an Azure Marketplace image. This struct
replicates the same fields found in CAPZ - <a href="https://github.com/kubernetes-sigs/cluster-api-provider-azure/blob/main/api/v1beta1/types.go">https://github.com/kubernetes-sigs/cluster-api-provider-azure/blob/main/api/v1beta1/types.go</a>.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>publisher</code></br>
<em>
string
</em>
</td>
<td>
<p>Publisher is the name of the organization that created the image</p>
</td>
</tr>
<tr>
<td>
<code>offer</code></br>
<em>
string
</em>
</td>
<td>
<p>Offer specifies the name of a group of related images created by the publisher.</p>
</td>
</tr>
<tr>
<td>
<code>sku</code></br>
<em>
string
</em>
</td>
<td>
<p>SKU specifies an instance of an offer, such as a major release of a distribution.
For example, 18.04-LTS, 2019-Datacenter</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<p>Version specifies the version of an image sku. The allowed formats are Major.Minor.Build or &lsquo;latest&rsquo;. Major,
Minor, and Build are decimal numbers. Specify &lsquo;latest&rsquo; to use the latest version of an image available at
deployment time. Even if you use &lsquo;latest&rsquo;, the VM image will not automatically update after deploy time even if a
new version becomes available.</p>
</td>
</tr>
</tbody>
</table>
###MultiQueueSetting { #hypershift.openshift.io/v1beta1.MultiQueueSetting }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disable&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Enable&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###NetworkFilter { #hypershift.openshift.io/v1beta1.NetworkFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">NetworkParam</a>)
</p>
<p>
<p>NetworkFilter specifies a query to select an OpenStack network. At least one property must be set.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is the description of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProjectID is the project ID of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###NetworkParam { #hypershift.openshift.io/v1beta1.NetworkParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>NetworkParam specifies an OpenStack network. It may be specified by either ID or Filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID is the ID of the network to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkFilter">
NetworkFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filter specifies a filter to select an OpenStack network. If provided, cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###NetworkType { #hypershift.openshift.io/v1beta1.NetworkType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>NetworkType specifies the SDN provider used for cluster networking.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Calico&#34;</p></td>
<td><p>Calico specifies Calico as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;OVNKubernetes&#34;</p></td>
<td><p>OVNKubernetes specifies OVN as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;OpenShiftSDN&#34;</p></td>
<td><p>OpenShiftSDN specifies OpenShiftSDN as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;Other&#34;</p></td>
<td><p>Other specifies an undefined SDN provider</p>
</td>
</tr></tbody>
</table>
###NeutronTag { #hypershift.openshift.io/v1beta1.NeutronTag }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">FilterByNeutronTags</a>)
</p>
<p>
<p>NeutronTag represents a tag on a Neutron resource.
It may not be empty and may not contain commas.</p>
</p>
###NodePoolAutoScaling { #hypershift.openshift.io/v1beta1.NodePoolAutoScaling }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolAutoScaling specifies auto-scaling behavior for a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>min</code></br>
<em>
int32
</em>
</td>
<td>
<p>Min is the minimum number of nodes to maintain in the pool. Must be &gt;= 1.</p>
</td>
</tr>
<tr>
<td>
<code>max</code></br>
<em>
int32
</em>
</td>
<td>
<p>Max is the maximum number of nodes allowed in the pool. Must be &gt;= 1.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolCondition { #hypershift.openshift.io/v1beta1.NodePoolCondition }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">NodePoolStatus</a>)
</p>
<p>
<p>We define our own condition type since metav1.Condition has validation
for Reason that might be broken by what we bubble up from CAPI.
NodePoolCondition defines an observation of NodePool resource operational state.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>Type of condition in CamelCase or in foo.example.com/CamelCase.
Many .condition.type values are consistent across resources like Available, but because arbitrary conditions
can be useful (see .node.status.conditions), the ability to deconflict is important.</p>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#conditionstatus-v1-core">
Kubernetes core/v1.ConditionStatus
</a>
</em>
</td>
<td>
<p>Status of the condition, one of True, False, Unknown.</p>
</td>
</tr>
<tr>
<td>
<code>severity</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Severity provides an explicit classification of Reason code, so the users or machines can immediately
understand the current situation and act accordingly.
The Severity field MUST be set only when Status=False.</p>
</td>
</tr>
<tr>
<td>
<code>lastTransitionTime</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>Last time the condition transitioned from one status to another.
This should be when the underlying condition changed. If that is not known, then using the time when
the API field changed is acceptable.</p>
</td>
</tr>
<tr>
<td>
<code>reason</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The reason for the condition&rsquo;s last transition in CamelCase.
The specific API may choose whether or not this field is considered a guaranteed API.
This field may not be empty.</p>
</td>
</tr>
<tr>
<td>
<code>message</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>A human readable message indicating details about the transition.
This field may be empty.</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code></br>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
###NodePoolManagement { #hypershift.openshift.io/v1beta1.NodePoolManagement }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolManagement specifies behavior for managing nodes in a NodePool, such
as upgrade strategies and auto-repair behaviors.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>upgradeType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UpgradeType">
UpgradeType
</a>
</em>
</td>
<td>
<p>UpgradeType specifies the type of strategy for handling upgrades.</p>
</td>
</tr>
<tr>
<td>
<code>replace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">
ReplaceUpgrade
</a>
</em>
</td>
<td>
<p>Replace is the configuration for rolling upgrades.</p>
</td>
</tr>
<tr>
<td>
<code>inPlace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.InPlaceUpgrade">
InPlaceUpgrade
</a>
</em>
</td>
<td>
<p>InPlace is the configuration for in-place upgrades.</p>
</td>
</tr>
<tr>
<td>
<code>autoRepair</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>AutoRepair specifies whether health checks should be enabled for machines
in the NodePool. The default is false.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolPlatform { #hypershift.openshift.io/v1beta1.NodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolPlatform specifies the underlying infrastructure provider for the
NodePool and is used to configure platform specific behavior.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>Type specifies the platform name.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">
AWSNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS specifies the configuration used when operating on AWS.</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec">
IBMCloudPlatformSpec
</a>
</em>
</td>
<td>
<p>IBMCloud defines IBMCloud specific settings for components</p>
</td>
</tr>
<tr>
<td>
<code>kubevirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">
KubevirtNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Kubevirt specifies the configuration used when operating on KubeVirt platform.</p>
</td>
</tr>
<tr>
<td>
<code>agent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AgentNodePoolPlatform">
AgentNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Agent specifies the configuration used when using Agent platform.</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">
AzureNodePoolPlatform
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>powervs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">
PowerVSNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PowerVS specifies the configuration used when using IBMCloud PowerVS platform.</p>
</td>
</tr>
<tr>
<td>
<code>openstack</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform">
OpenStackNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OpenStack specifies the configuration used when using OpenStack platform.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolPlatformStatus { #hypershift.openshift.io/v1beta1.NodePoolPlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">NodePoolStatus</a>)
</p>
<p>
<p>NodePoolPlatformStatus contains specific platform statuses</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kubeVirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus">
KubeVirtNodePoolStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeVirt contains the KubeVirt platform statuses</p>
</td>
</tr>
</tbody>
</table>
###NodePoolSpec { #hypershift.openshift.io/v1beta1.NodePoolSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePool">NodePool</a>)
</p>
<p>
<p>NodePoolSpec is the desired behavior of a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterName</code></br>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the name of the HostedCluster this NodePool belongs to.</p>
<p>TODO(dan): Should this be a LocalObjectReference?</p>
</td>
</tr>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>Release specifies the OCP release used for the NodePool. This informs the
ignition configuration for machines, as well as other platform specific
machine properties (e.g. an AMI on the AWS platform).</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">
NodePoolPlatform
</a>
</em>
</td>
<td>
<p>Platform specifies the underlying infrastructure provider for the NodePool
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the desired number of nodes the pool should maintain. If
unset, the default value is 0.</p>
</td>
</tr>
<tr>
<td>
<code>management</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">
NodePoolManagement
</a>
</em>
</td>
<td>
<p>Management specifies behavior for managing nodes in the pool, such as
upgrade strategies and auto-repair behaviors.</p>
</td>
</tr>
<tr>
<td>
<code>autoScaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolAutoScaling">
NodePoolAutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto-scaling behavior for the NodePool.</p>
</td>
</tr>
<tr>
<td>
<code>config</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>Config is a list of references to ConfigMaps containing serialized
MachineConfig resources to be injected into the ignition configurations of
nodes in the NodePool. The MachineConfig API schema is defined here:</p>
<p><a href="https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185">https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</a></p>
<p>Each ConfigMap must have a single key named &ldquo;config&rdquo; whose value is the YML
with one or more serialized machineconfiguration.openshift.io resources:
KubeletConfig
ContainerRuntimeConfig
MachineConfig
ClusterImagePolicy
ImageContentSourcePolicy
or
ImageDigestMirrorSet</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeDrainTimeout is the maximum amount of time that the controller will spend on draining a node.
The default value is 0, meaning that the node can be drained without any time limitations.
NOTE: NodeDrainTimeout is different from <code>kubectl drain --timeout</code>
TODO (alberto): Today changing this field will trigger a recreate rolling update, which kind of defeats
the purpose of the change. In future we plan to propagate this field in-place.
<a href="https://github.com/kubernetes-sigs/cluster-api/issues/5880">https://github.com/kubernetes-sigs/cluster-api/issues/5880</a> / <a href="https://github.com/kubernetes-sigs/cluster-api/pull/10589">https://github.com/kubernetes-sigs/cluster-api/pull/10589</a></p>
</td>
</tr>
<tr>
<td>
<code>nodeVolumeDetachTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node.
The default value is 0, meaning that the volumes will be detached from the node without any time limitations.
After the timeout, the detachment of volumes that haven&rsquo;t been detached yet is skipped.
TODO (cbusse): Same comment as Alberto&rsquo;s for <code>NodeDrainTimeout</code>:
Today changing this field will trigger a recreate rolling update, which kind of defeats
the purpose of the change. In future we plan to propagate this field in-place.
<a href="https://github.com/kubernetes-sigs/cluster-api/issues/5880">https://github.com/kubernetes-sigs/cluster-api/issues/5880</a> / <a href="https://github.com/kubernetes-sigs/cluster-api/pull/10589">https://github.com/kubernetes-sigs/cluster-api/pull/10589</a></p>
</td>
</tr>
<tr>
<td>
<code>nodeLabels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeLabels propagates a list of labels to Nodes, only once on creation.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
</td>
</tr>
<tr>
<td>
<code>taints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Taint">
[]Taint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints if specified, propagates a list of taints to Nodes, only once on creation.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>TuningConfig is a list of references to ConfigMaps containing serialized
Tuned or PerformanceProfile resources to define the tuning configuration to be applied to
nodes in the NodePool. The Tuned API is defined here:</p>
<p><a href="https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go">https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</a></p>
<p>The PerformanceProfile API is defined here:
<a href="https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2">https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</a></p>
<p>Each ConfigMap must have a single key named &ldquo;tuning&rdquo; whose value is the
JSON or YAML of a serialized Tuned or PerformanceProfile.</p>
</td>
</tr>
<tr>
<td>
<code>arch</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Arch is the preferred processor architecture for the NodePool (currently only supported on AWS)
NOTE: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery:
<a href="https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215">https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</a>
TODO Add s390x to enum validation once the architecture is supported</p>
</td>
</tr>
</tbody>
</table>
###NodePoolStatus { #hypershift.openshift.io/v1beta1.NodePoolStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePool">NodePool</a>)
</p>
<p>
<p>NodePoolStatus is the latest observed status of a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the latest observed number of nodes in the pool.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<p>Version is the semantic version of the latest applied release specified by
the NodePool.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus">
NodePoolPlatformStatus
</a>
</em>
</td>
<td>
<p>Platform hols the specific statuses</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolCondition">
[]NodePoolCondition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions represents the latest available observations of the node pool&rsquo;s
current state.</p>
</td>
</tr>
</tbody>
</table>
###NodePortPublishingStrategy { #hypershift.openshift.io/v1beta1.NodePortPublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>NodePortPublishingStrategy specifies a NodePort used to expose a service.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>address</code></br>
<em>
string
</em>
</td>
<td>
<p>Address is the host/ip that the NodePort service is exposed over.</p>
</td>
</tr>
<tr>
<td>
<code>port</code></br>
<em>
int32
</em>
</td>
<td>
<p>Port is the port of the NodePort service. If &lt;=0, the port is dynamically
assigned when the service is created.</p>
</td>
</tr>
</tbody>
</table>
###OLMCatalogPlacement { #hypershift.openshift.io/v1beta1.OLMCatalogPlacement }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>OLMCatalogPlacement is an enum specifying the placement of OLM catalog components.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;guest&#34;</p></td>
<td><p>GuestOLMCatalogPlacement indicates OLM catalog components will be placed in
the guest cluster.</p>
</td>
</tr><tr><td><p>&#34;management&#34;</p></td>
<td><p>ManagementOLMCatalogPlacement indicates OLM catalog components will be placed in
the management cluster.</p>
</td>
</tr></tbody>
</table>
###OpenStackIdentityReference { #hypershift.openshift.io/v1beta1.OpenStackIdentityReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>OpenStackIdentityReference is a reference to an infrastructure
provider identity to be used to provision cluster resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name is the name of a secret in the same namespace as the resource being provisioned.
The secret must contain a key named <code>clouds.yaml</code> which contains an OpenStack clouds.yaml file.
The secret may optionally contain a key named <code>cacert</code> containing a PEM-encoded CA certificate.</p>
</td>
</tr>
<tr>
<td>
<code>cloudName</code></br>
<em>
string
</em>
</td>
<td>
<p>CloudName specifies the name of the entry in the clouds.yaml file to use.</p>
</td>
</tr>
</tbody>
</table>
###OpenStackNodePoolPlatform { #hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>flavor</code></br>
<em>
string
</em>
</td>
<td>
<p>Flavor is the OpenStack flavor to use for the node instances.</p>
</td>
</tr>
<tr>
<td>
<code>imageName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageName is the OpenStack Glance image name to use for node instances. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
</tbody>
</table>
###OpenStackPlatformSpec { #hypershift.openshift.io/v1beta1.OpenStackPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>OpenStackPlatformSpec specifies configuration for clusters running on OpenStack.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>identityRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackIdentityReference">
OpenStackIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to a secret holding OpenStack credentials
to be used when reconciling the hosted cluster.</p>
</td>
</tr>
<tr>
<td>
<code>managedSubnets</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetSpec">
[]SubnetSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ManagedSubnets describe the OpenStack Subnet to be created. Cluster actuator will create a network,
and a subnet with the defined DNSNameservers, AllocationPools and the CIDR defined in the HostedCluster
MachineNetwork, and a router connected to the subnet. Currently only one IPv4
subnet is supported.</p>
</td>
</tr>
<tr>
<td>
<code>router</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RouterParam">
RouterParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Router specifies an existing router to be used if ManagedSubnets are
specified. If specified, no new router will be created.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network specifies an existing network to use if no ManagedSubnets
are specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetParam">
[]SubnetParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets specifies existing subnets to use if not ManagedSubnets are
specified. All subnets must be in the network specified by Network.
There can be zero, one, or two subnets. If no subnets are specified,
all subnets in Network will be used. If 2 subnets are specified, one
must be IPv4 and the other IPv6.</p>
</td>
</tr>
<tr>
<td>
<code>networkMTU</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkMTU sets the maximum transmission unit (MTU) value to address fragmentation for the private network ID.
This value will be used only if the Cluster actuator creates the network.
If left empty, the network will have the default MTU defined in Openstack network service.
To use this field, the Openstack installation requires the net-mtu neutron API extension.</p>
</td>
</tr>
<tr>
<td>
<code>externalNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ExternalNetwork is the OpenStack Network to be used to get public internet to the VMs.
This option is ignored if DisableExternalNetwork is set to true.</p>
<p>If ExternalNetwork is defined it must refer to exactly one external network.</p>
<p>If ExternalNetwork is not defined or is empty the controller will use any
existing external network as long as there is only one. It is an
error if ExternalNetwork is not defined and there are multiple
external networks unless DisableExternalNetwork is also set.</p>
<p>If ExternalNetwork is not defined and there are no external networks
the controller will proceed as though DisableExternalNetwork was set.</p>
</td>
</tr>
<tr>
<td>
<code>disableExternalNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>DisableExternalNetwork specifies whether or not to attempt to connect the cluster
to an external network. This allows for the creation of clusters when connecting
to an external network is not possible or desirable, e.g. if using a provider network.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tags to set on all resources in cluster which support tags</p>
</td>
</tr>
</tbody>
</table>
###PayloadArchType { #hypershift.openshift.io/v1beta1.PayloadArchType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AMD64&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ARM64&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Multi&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;PPC64LE&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;S390X&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###PersistentVolumeAccessMode { #hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume">KubevirtPersistentVolume</a>)
</p>
<p>
</p>
###PersistentVolumeEtcdStorageSpec { #hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">ManagedEtcdStorageSpec</a>)
</p>
<p>
<p>PersistentVolumeEtcdStorageSpec is the configuration for PersistentVolume
etcd storage.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageClassName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageClassName is the StorageClass of the data volume for each etcd member.</p>
<p>See <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a>.</p>
</td>
</tr>
<tr>
<td>
<code>size</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Size is the minimum size of the data volume for each etcd member.</p>
</td>
</tr>
</tbody>
</table>
###PlatformSpec { #hypershift.openshift.io/v1beta1.PlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>PlatformSpec specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>Type is the type of infrastructure provider for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">
AWSPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS specifies configuration for clusters running on Amazon Web Services.</p>
</td>
</tr>
<tr>
<td>
<code>agent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AgentPlatformSpec">
AgentPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Agent specifies configuration for agent-based installations.</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec">
IBMCloudPlatformSpec
</a>
</em>
</td>
<td>
<p>IBMCloud defines IBMCloud specific settings for components</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzurePlatformSpec">
AzurePlatformSpec
</a>
</em>
</td>
<td>
<p>Azure defines azure specific settings</p>
</td>
</tr>
<tr>
<td>
<code>powervs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">
PowerVSPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PowerVS specifies configuration for clusters running on IBMCloud Power VS Service.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>kubevirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">
KubevirtPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeVirt defines KubeVirt specific settings for cluster components.</p>
</td>
</tr>
<tr>
<td>
<code>openstack</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">
OpenStackPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OpenStack specifies configuration for clusters running on OpenStack.</p>
</td>
</tr>
</tbody>
</table>
###PlatformStatus { #hypershift.openshift.io/v1beta1.PlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>PlatformStatus contains platform-specific status</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformStatus">
AWSPlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
</tbody>
</table>
###PlatformType { #hypershift.openshift.io/v1beta1.PlatformType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>PlatformType is a specific supported infrastructure provider.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWS&#34;</p></td>
<td><p>AWSPlatform represents Amazon Web Services infrastructure.</p>
</td>
</tr><tr><td><p>&#34;Agent&#34;</p></td>
<td><p>AgentPlatform represents user supplied insfrastructure booted with agents.</p>
</td>
</tr><tr><td><p>&#34;Azure&#34;</p></td>
<td><p>AzurePlatform represents Azure infrastructure.</p>
</td>
</tr><tr><td><p>&#34;IBMCloud&#34;</p></td>
<td><p>IBMCloudPlatform represents IBM Cloud infrastructure.</p>
</td>
</tr><tr><td><p>&#34;KubeVirt&#34;</p></td>
<td><p>KubevirtPlatform represents Kubevirt infrastructure.</p>
</td>
</tr><tr><td><p>&#34;None&#34;</p></td>
<td><p>NonePlatform represents user supplied (e.g. bare metal) infrastructure.</p>
</td>
</tr><tr><td><p>&#34;OpenStack&#34;</p></td>
<td><p>OpenStackPlatform represents OpenStack infrastructure.</p>
</td>
</tr><tr><td><p>&#34;PowerVS&#34;</p></td>
<td><p>PowerVSPlatform represents PowerVS infrastructure.</p>
</td>
</tr></tbody>
</table>
###PowerVSNodePoolImageDeletePolicy { #hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolImageDeletePolicy defines image delete policy to be used for PowerVSNodePoolPlatform</p>
</p>
###PowerVSNodePoolPlatform { #hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolPlatform specifies the configuration of a NodePool when operating
on IBMCloud PowerVS platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>systemType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SystemType is the System type used to host the instance.
systemType determines the number of cores and memory that is available.
Few of the supported SystemTypes are s922,e880,e980.
e880 systemType available only in Dallas Datacenters.
e980 systemType available in Datacenters except Dallas and Washington.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The current default is s922 which is generally available.</p>
</td>
</tr>
<tr>
<td>
<code>processorType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType">
PowerVSNodePoolProcType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProcessorType is the VM instance processor type.
It must be set to one of the following values: Dedicated, Capped or Shared.</p>
<p>Dedicated: resources are allocated for a specific client, The hypervisor makes a 1:1 binding of a partitionâ€™s processor to a physical processor core.
Shared: Shared among other clients.
Capped: Shared, but resources do not expand beyond those that are requested, the amount of CPU time is Capped to the value specified for the entitlement.</p>
<p>if the processorType is selected as Dedicated, then Processors value cannot be fractional.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The current default is shared.</p>
</td>
</tr>
<tr>
<td>
<code>processors</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Processors is the number of virtual processors in a virtual machine.
when the processorType is selected as Dedicated the processors value cannot be fractional.
maximum value for the Processors depends on the selected SystemType.
when SystemType is set to e880 or e980 maximum Processors value is 143.
when SystemType is set to s922 maximum Processors value is 15.
minimum value for Processors depends on the selected ProcessorType.
when ProcessorType is set as Shared or Capped, The minimum processors is 0.5.
when ProcessorType is set as Dedicated, The minimum processors is 1.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The default is set based on the selected ProcessorType.
when ProcessorType selected as Dedicated, the default is set to 1.
when ProcessorType selected as Shared or Capped, the default is set to 0.5.</p>
</td>
</tr>
<tr>
<td>
<code>memoryGiB</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>MemoryGiB is the size of a virtual machine&rsquo;s memory, in GiB.
maximum value for the MemoryGiB depends on the selected SystemType.
when SystemType is set to e880 maximum MemoryGiB value is 7463 GiB.
when SystemType is set to e980 maximum MemoryGiB value is 15307 GiB.
when SystemType is set to s922 maximum MemoryGiB value is 942 GiB.
The minimum memory is 32 GiB.</p>
<p>When omitted, this means the user has no opinion and the platform is left to choose a reasonable
default. The current default is 32.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSResourceReference">
PowerVSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Image used for deploying the nodes. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>storageType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType">
PowerVSNodePoolStorageType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageType for the image and nodes, this will be ignored if Image is specified.
The storage tiers in PowerVS are based on I/O operations per second (IOPS).
It means that the performance of your storage volumes is limited to the maximum number of IOPS based on volume size and storage tier.
Although, the exact numbers might change over time, the Tier 3 storage is currently set to 3 IOPS/GB, and the Tier 1 storage is currently set to 10 IOPS/GB.</p>
<p>The default is tier1</p>
</td>
</tr>
<tr>
<td>
<code>imageDeletePolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy">
PowerVSNodePoolImageDeletePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageDeletePolicy is policy for the image deletion.</p>
<p>delete: delete the image from the infrastructure.
retain: delete the image from the openshift but retain in the infrastructure.</p>
<p>The default is delete</p>
</td>
</tr>
</tbody>
</table>
###PowerVSNodePoolProcType { #hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolProcType defines processor type to be used for PowerVSNodePoolPlatform</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;capped&#34;</p></td>
<td><p>PowerVSNodePoolCappedProcType defines capped processor type</p>
</td>
</tr><tr><td><p>&#34;dedicated&#34;</p></td>
<td><p>PowerVSNodePoolDedicatedProcType defines dedicated processor type</p>
</td>
</tr><tr><td><p>&#34;shared&#34;</p></td>
<td><p>PowerVSNodePoolSharedProcType defines shared processor type</p>
</td>
</tr></tbody>
</table>
###PowerVSNodePoolStorageType { #hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolStorageType defines storage type to be used for PowerVSNodePoolPlatform</p>
</p>
###PowerVSPlatformSpec { #hypershift.openshift.io/v1beta1.PowerVSPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>PowerVSPlatformSpec defines IBMCloud PowerVS specific settings for components</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>accountID</code></br>
<em>
string
</em>
</td>
<td>
<p>AccountID is the IBMCloud account id.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>cisInstanceCRN</code></br>
<em>
string
</em>
</td>
<td>
<p>CISInstanceCRN is the IBMCloud CIS Service Instance&rsquo;s Cloud Resource Name
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>resourceGroup</code></br>
<em>
string
</em>
</td>
<td>
<p>ResourceGroup is the IBMCloud Resource Group in which the cluster resides.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the IBMCloud region in which the cluster resides. This configures the
OCP control plane cloud integrations, and is used by NodePool to resolve
the correct boot image for a given release.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<p>Zone is the availability zone where control plane cloud resources are
created.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSResourceReference">
PowerVSResourceReference
</a>
</em>
</td>
<td>
<p>Subnet is the subnet to use for control plane cloud resources.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>serviceInstanceID</code></br>
<em>
string
</em>
</td>
<td>
<p>ServiceInstance is the reference to the Power VS service on which the server instance(VM) will be created.
Power VS service is a container for all Power VS instances at a specific geographic region.
serviceInstance can be created via IBM Cloud catalog or CLI.
ServiceInstanceID is the unique identifier that can be obtained from IBM Cloud UI or IBM Cloud cli.</p>
<p>More detail about Power VS service instance.
<a href="https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server">https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server</a></p>
<p>This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>vpc</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSVPC">
PowerVSVPC
</a>
</em>
</td>
<td>
<p>VPC specifies IBM Cloud PowerVS Load Balancing configuration for the control
plane.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>KubeCloudControllerCreds is a reference to a secret containing cloud
credentials with permissions matching the cloud controller policy.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
<p>TODO(dan): document the &ldquo;cloud controller policy&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>NodePoolManagementCreds is a reference to a secret containing cloud
credentials with permissions matching the node pool management policy.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
<p>TODO(dan): document the &ldquo;node pool management policy&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>ingressOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>IngressOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for ingress operator to get authenticated with ibm cloud.</p>
</td>
</tr>
<tr>
<td>
<code>storageOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>StorageOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for storage operator to get authenticated with ibm cloud.</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ImageRegistryOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for image registry operator to get authenticated with ibm cloud.</p>
</td>
</tr>
</tbody>
</table>
###PowerVSResourceReference { #hypershift.openshift.io/v1beta1.PowerVSResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">PowerVSPlatformSpec</a>)
</p>
<p>
<p>PowerVSResourceReference is a reference to a specific IBMCloud PowerVS resource by ID, or Name.
Only one of ID, or Name may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name of resource</p>
</td>
</tr>
</tbody>
</table>
###PowerVSVPC { #hypershift.openshift.io/v1beta1.PowerVSVPC }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">PowerVSPlatformSpec</a>)
</p>
<p>
<p>PowerVSVPC specifies IBM Cloud PowerVS LoadBalancer configuration for the control
plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>Name for VPC to used for all the service load balancer.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>Region is the IBMCloud region in which VPC gets created, this VPC used for all the ingress traffic
into the OCP cluster.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Zone is the availability zone where load balancer cloud resources are
created.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is the subnet to use for load balancer.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
</tbody>
</table>
###PublishingStrategyType { #hypershift.openshift.io/v1beta1.PublishingStrategyType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>PublishingStrategyType defines publishing strategies for services.</p>
</p>
###QoSClass { #hypershift.openshift.io/v1beta1.QoSClass }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCompute">KubevirtCompute</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Burstable&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Guaranteed&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###Release { #hypershift.openshift.io/v1beta1.Release }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>Release represents the metadata for an OCP release payload image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>image</code></br>
<em>
string
</em>
</td>
<td>
<p>Image is the image pullspec of an OCP release payload image.</p>
</td>
</tr>
</tbody>
</table>
###ReplaceUpgrade { #hypershift.openshift.io/v1beta1.ReplaceUpgrade }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>ReplaceUpgrade specifies upgrade behavior that replaces existing nodes
according to a given strategy.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>strategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UpgradeStrategy">
UpgradeStrategy
</a>
</em>
</td>
<td>
<p>Strategy is the node replacement strategy for nodes in the pool.</p>
</td>
</tr>
<tr>
<td>
<code>rollingUpdate</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RollingUpdate">
RollingUpdate
</a>
</em>
</td>
<td>
<p>RollingUpdate specifies a rolling update strategy which upgrades nodes by
creating new nodes and deleting the old ones.</p>
</td>
</tr>
</tbody>
</table>
###RollingUpdate { #hypershift.openshift.io/v1beta1.RollingUpdate }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">ReplaceUpgrade</a>)
</p>
<p>
<p>RollingUpdate specifies a rolling update strategy which upgrades nodes by
creating new nodes and deleting the old ones.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailable is the maximum number of nodes that can be unavailable
during the update.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding down.</p>
<p>This can not be 0 if MaxSurge is 0.</p>
<p>Defaults to 0.</p>
<p>Example: when this is set to 30%, old nodes can be deleted down to 70% of
desired nodes immediately when the rolling update starts. Once new nodes
are ready, more old nodes be deleted, followed by provisioning new nodes,
ensuring that the total number of nodes available at all times during the
update is at least 70% of desired nodes.</p>
</td>
</tr>
<tr>
<td>
<code>maxSurge</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxSurge is the maximum number of nodes that can be provisioned above the
desired number of nodes.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding up.</p>
<p>This can not be 0 if MaxUnavailable is 0.</p>
<p>Defaults to 1.</p>
<p>Example: when this is set to 30%, new nodes can be provisioned immediately
when the rolling update starts, such that the total number of old and new
nodes do not exceed 130% of desired nodes. Once old nodes have been
deleted, new nodes can be provisioned, ensuring that total number of nodes
running at any time during the update is at most 130% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
###RoutePublishingStrategy { #hypershift.openshift.io/v1beta1.RoutePublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>RoutePublishingStrategy specifies options for exposing a service as a Route.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Hostname is the name of the DNS record that will be created pointing to the Route.</p>
</td>
</tr>
</tbody>
</table>
###RouterFilter { #hypershift.openshift.io/v1beta1.RouterFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.RouterParam">RouterParam</a>)
</p>
<p>
<p>RouterFilter specifies a query to select an OpenStack router. At least one property must be set.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is the description of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProjectID is the project ID of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###RouterParam { #hypershift.openshift.io/v1beta1.RouterParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>RouterParam specifies an OpenStack router to use. It may be specified by either ID or filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID is the ID of the router to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RouterFilter">
RouterFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filter specifies a filter to select an OpenStack router. If provided, cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###SecretEncryptionSpec { #hypershift.openshift.io/v1beta1.SecretEncryptionSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>SecretEncryptionSpec contains metadata about the kubernetes secret encryption strategy being used for the
cluster when applicable.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionType">
SecretEncryptionType
</a>
</em>
</td>
<td>
<p>Type defines the type of kube secret encryption being used</p>
</td>
</tr>
<tr>
<td>
<code>kms</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">
KMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>KMS defines metadata about the kms secret encryption strategy</p>
</td>
</tr>
<tr>
<td>
<code>aescbc</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AESCBCSpec">
AESCBCSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AESCBC defines metadata about the AESCBC secret encryption strategy</p>
</td>
</tr>
</tbody>
</table>
###SecretEncryptionType { #hypershift.openshift.io/v1beta1.SecretEncryptionType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>SecretEncryptionType defines the type of kube secret encryption being used.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;aescbc&#34;</p></td>
<td><p>AESCBC uses AES-CBC with PKCS#7 padding to do secret encryption</p>
</td>
</tr><tr><td><p>&#34;kms&#34;</p></td>
<td><p>KMS integrates with a cloud provider&rsquo;s key management service to do secret encryption</p>
</td>
</tr></tbody>
</table>
###ServiceNetworkEntry { #hypershift.openshift.io/v1beta1.ServiceNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>ServiceNetworkEntry is a single IP address block for the service network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>CIDR is the IP block address pool for services within the cluster.</p>
</td>
</tr>
</tbody>
</table>
###ServicePublishingStrategy { #hypershift.openshift.io/v1beta1.ServicePublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">ServicePublishingStrategyMapping</a>)
</p>
<p>
<p>ServicePublishingStrategy specfies how to publish a ServiceType.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PublishingStrategyType">
PublishingStrategyType
</a>
</em>
</td>
<td>
<p>Type is the publishing strategy used for the service.</p>
</td>
</tr>
<tr>
<td>
<code>nodePort</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePortPublishingStrategy">
NodePortPublishingStrategy
</a>
</em>
</td>
<td>
<p>NodePort configures exposing a service using a NodePort.</p>
</td>
</tr>
<tr>
<td>
<code>loadBalancer</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy">
LoadBalancerPublishingStrategy
</a>
</em>
</td>
<td>
<p>LoadBalancer configures exposing a service using a LoadBalancer.</p>
</td>
</tr>
<tr>
<td>
<code>route</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RoutePublishingStrategy">
RoutePublishingStrategy
</a>
</em>
</td>
<td>
<p>Route configures exposing a service using a Route.</p>
</td>
</tr>
</tbody>
</table>
###ServicePublishingStrategyMapping { #hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ServicePublishingStrategyMapping specifies how individual control plane
services are published from the hosting cluster of a control plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>service</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServiceType">
ServiceType
</a>
</em>
</td>
<td>
<p>Service identifies the type of service being published.</p>
</td>
</tr>
<tr>
<td>
<code>servicePublishingStrategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">
ServicePublishingStrategy
</a>
</em>
</td>
<td>
<p>ServicePublishingStrategy specifies how to publish Service.</p>
</td>
</tr>
</tbody>
</table>
###ServiceType { #hypershift.openshift.io/v1beta1.ServiceType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">ServicePublishingStrategyMapping</a>)
</p>
<p>
<p>ServiceType defines what control plane services can be exposed from the
management control plane.</p>
</p>
###SubnetFilter { #hypershift.openshift.io/v1beta1.SubnetFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SubnetParam">SubnetParam</a>)
</p>
<p>
<p>SubnetFilter specifies a filter to select a subnet. At least one parameter must be specified.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Description is the description of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProjectID is the project ID of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipVersion</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPVersion is the IP version of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>gatewayIP</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>GatewayIP is the gateway IP of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>cidr</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CIDR is the CIDR of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6AddressMode</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6AddressMode is the IPv6 address mode of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6RAMode</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6RAMode is the IPv6 RA mode of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###SubnetParam { #hypershift.openshift.io/v1beta1.SubnetParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>SubnetParam specifies an OpenStack subnet to use. It may be specified by either ID or filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID is the uuid of the subnet. It will not be validated.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetFilter">
SubnetFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filter specifies a filter to select the subnet. It must match exactly one subnet.</p>
</td>
</tr>
</tbody>
</table>
###SubnetSpec { #hypershift.openshift.io/v1beta1.SubnetSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>dnsNameservers</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSNameservers holds a list of DNS server addresses that will be provided when creating
the subnet. These addresses need to have the same IP version as CIDR.</p>
</td>
</tr>
<tr>
<td>
<code>allocationPools</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AllocationPool">
[]AllocationPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllocationPools is an array of AllocationPool objects that will be applied to OpenStack Subnet being created.
If set, OpenStack will only allocate these IPs for Machines. It will still be possible to create ports from
outside of these ranges manually.</p>
</td>
</tr>
</tbody>
</table>
###Taint { #hypershift.openshift.io/v1beta1.Taint }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>Taint is as v1 Core but without TimeAdded.
<a href="https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053">https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>Required. The taint key to be applied to a node.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The taint value corresponding to the taint key.</p>
</td>
</tr>
<tr>
<td>
<code>effect</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#tainteffect-v1-core">
Kubernetes core/v1.TaintEffect
</a>
</em>
</td>
<td>
<p>Required. The effect of the taint on pods
that do not tolerate the taint.
Valid effects are NoSchedule, PreferNoSchedule and NoExecute.</p>
</td>
</tr>
</tbody>
</table>
###UnmanagedEtcdSpec { #hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>UnmanagedEtcdSpec specifies configuration which enables the control plane to
integrate with an eternally managed etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>endpoint</code></br>
<em>
string
</em>
</td>
<td>
<p>Endpoint is the full etcd cluster client endpoint URL. For example:</p>
<pre><code>https://etcd-client:2379
</code></pre>
<p>If the URL uses an HTTPS scheme, the TLS field is required.</p>
</td>
</tr>
<tr>
<td>
<code>tls</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdTLSConfig">
EtcdTLSConfig
</a>
</em>
</td>
<td>
<p>TLS specifies TLS configuration for HTTPS etcd client endpoints.</p>
</td>
</tr>
</tbody>
</table>
###UpgradeStrategy { #hypershift.openshift.io/v1beta1.UpgradeStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">ReplaceUpgrade</a>)
</p>
<p>
<p>UpgradeStrategy is a specific strategy for upgrading nodes in a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;OnDelete&#34;</p></td>
<td><p>UpgradeStrategyOnDelete replaces old nodes when the deletion of the
associated node instances are completed.</p>
</td>
</tr><tr><td><p>&#34;RollingUpdate&#34;</p></td>
<td><p>UpgradeStrategyRollingUpdate means use a rolling update for nodes.</p>
</td>
</tr></tbody>
</table>
###UpgradeType { #hypershift.openshift.io/v1beta1.UpgradeType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>UpgradeType is a type of high-level upgrade behavior nodes in a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;InPlace&#34;</p></td>
<td><p>UpgradeTypeInPlace is a strategy which replaces nodes in-place with no
additional node capacity requirements.</p>
</td>
</tr><tr><td><p>&#34;Replace&#34;</p></td>
<td><p>UpgradeTypeReplace is a strategy which replaces nodes using surge node
capacity.</p>
</td>
</tr></tbody>
</table>
###Volume { #hypershift.openshift.io/v1beta1.Volume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>Volume specifies the configuration options for node instance storage devices.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>size</code></br>
<em>
int64
</em>
</td>
<td>
<p>Size specifies size (in Gi) of the storage device.</p>
<p>Must be greater than the image snapshot size or 8 (whichever is greater).</p>
</td>
</tr>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>Type is the type of the volume.</p>
</td>
</tr>
<tr>
<td>
<code>iops</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>IOPS is the number of IOPS requested for the disk. This is only valid
for type io1.</p>
</td>
</tr>
<tr>
<td>
<code>encrypted</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encrypted is whether the volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionKey</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionKey is the KMS key to use to encrypt the volume. Can be either a KMS key ID or ARN.
If Encrypted is set and this is omitted, the default AWS key will be used.
The key must already exist and be accessible by the controller.</p>
</td>
</tr>
</tbody>
</table>
<hr/>
