# HyperShift Documentation (Aggregated)

This file contains all HyperShift documentation aggregated into a single file
for use with AI tools like NotebookLM.

Total documents: 260

---

## Source: docs/content/contribute/branch-process.md

## OCP Branching Tasks for the HyperShift Team
These are a set of tasks we need to perform on every OCP branching. We need to:

1. Update the HyperShift Repository to add the latest supported OCP version - Update Supported Version
1. Update the base images in our Dockerfiles (if they are available at branching) - Update Dockerfiles
1. Update the Renovate configuration to include the new release branch - Update Renovate
1. Update the OpenShift Release repository to fix the step registry configuration files - OpenShift/Release
1. Update TestGrid to include the new OCP version tests - TestGrid

!!! danger
    If test platform are testing new OCP releases before the release is cut the hypershift test will fail and block payloads until:

    - There are at least two accepted nightly payloads for the new release.
    - The supported versions in the HyperShift repository are updated.

---
### HyperShift Repository

#### Update Supported Version
We need to add the latest supported version in the `hypershift` repository. We need to modify two files:

- `support/supportedversion/version.go` which contains the variable called `LatestSupportedVersion`. This one contains, as you can imagine, the Latest supported version. We need to put the new version here.
- `support/supportedversion/version_test.go` contains the tests to validate the Latest version. It should comply with the established contract to support 2 versions prior to the Latest.

Example Supported Version Bump PR

#### Update Dockerfiles
We also need to bump the base images in our Dockerfiles.

Example Base Image Bump PR

#### Update Renovate Configuration
We need to add the new release branch to the Renovate configuration so that security updates are automatically applied to the release branch.

Update `renovate.json` to include the new release branch in two places:

1. Add the new branch to the `baseBranches` array at the top of the file
2. Add the new branch to the `matchBaseBranches` array in the security-only Go updates package rule

!!! note
    If any release branch has reached End of Life and is no longer supported, remove it from both locations in `renovate.json` to stop automated updates on that branch.

Example change for release-4.21:
```json
{
  "baseBranches": [
    "main",
    "release-4.16",
    "release-4.17",
    "release-4.18",
    "release-4.19",
    "release-4.20",
    "release-4.21"
  ],
  "packageRules": [
    {
      "description": "Enable security-only Go updates on release branches",
      "matchManagers": ["gomod"],
      "matchBaseBranches": [
        "release-4.16",
        "release-4.17",
        "release-4.18",
        "release-4.19",
        "release-4.20",
        "release-4.21"
      ],
      "matchUpdateTypes": ["patch"]
    }
  ]
}
```

---

### Openshift/Release Repository
The Step registry config should be updated by Test Platform. However, the Test Platform is not aware of custom configurations of the different version for specific hypershift tests.
So, we need to check over the Step registry config and make sure that the hypershift tests are correctly configured. Below is an example of the necessary changes to the Step registry config after test platform bumps:

Example Release Repo PR

We should also ensure that the latest release branch is using the Hypershift Operator and e2e from main.

Example Release Branch PR

---

### Update TestGrid
We need to update TestGrid to include the new OCP version tests. 

Here is an Example PR to do that.


---

## Source: docs/content/contribute/contribute-docs.md

---
title: Contribute documentation
---

# Contributing documentation

HyperShift's documentation is based on MkDocs with the
Material theme and roughly follows the
Di√°taxis Framework for content organization and stylistic
approach.

The documentation site is built and published automatically to https://hypershift.pages.dev/.

## Overview

All documentation lives in the `docs` directory of the Git repository.

All content should be Markdown files placed in the `docs/content` directory.
The MkDocs configuration file
contains all the MkDocs and Material theme configuration, including the navigation
structure for the site.

The `quay.io/hypershift/mkdocs-material:latest` image (Dockerfile)
is published to provide an easy and portable way to run `mkdocs` fully configured
to preview the site equivalent to the published site.

!!! note

    The `README.md`
    file in the repository root is a minimal overview which quickly links users to
    the latest published documentation. Most content should go in the docs.

!!! important

    The API reference is generated automatically. Do not
    edit it manually. See the API generation section
    section for details.

## Preview the site locally

To start a live preview of the site which automatically rebuilds and refreshes in
response to local content and configuration changes, run the following from the
`docs` directory:

```shell
make serve-containerized
```

Visit the site at http://0.0.0.0:8000.

!!! note

    The `serve-containerized` Make target runs the `quay.io/hypershift/mkdocs-material:latest`
    image with the local container runtime. Running `mkdocs` natively is possible
    but not supported.

    If you need more control over the local preview server, consult the Makefile
    as a guide to constructing your own local server command.

## Generate the API reference

The API reference is automatically generated by the
`gen-crd-api-reference-docs` tool.

The `gen-crd-api-reference-docs` tool processes the HyperShift API Go type
definitions and reads the
Kubernetes Custom Resource Definition
metadata associated with the API types. Then `gen-crd-api-reference-docs` executes
a Go template
which is provided with context about the processed Go packages. The output of
template execution is the `docs/content/reference/api.md` file, which contains the
API reference documentation content.

- To change documentation of specific API types, edit the API Go type definitions.
- To change the structure of the API reference page itself, edit the `gen-crd-api-reference-docs`
  Go templates.

To run the API reference docs generator, run the following from the HyperShift
Git repository root:

```shell
make api-docs
```


---

## Source: docs/content/contribute/cpo-overrides.md

# CPO Overrides Configuration Guide

## Overview

The CPO (Control Plane Operator) overrides mechanism allows you to specify custom Control Plane Operator images for specific OpenShift versions and platforms. This is particularly useful for applying hotfixes and patches before they're officially released.

## Configuration File Structure

The CPO overrides are configured using a YAML file with the following structure:

```yaml
platforms:
  aws:
    overrides:
      - version: "4.17.9"
        cpoImage: "quay.io/hypershift/control-plane-operator:4.17.9"
      - version: "4.17.8"
        cpoImage: "quay.io/hypershift/control-plane-operator:4.17.8"
    testing:
      latest: "quay.io/openshift-release-dev/ocp-release:4.17.9-x86_64"
      previous: "quay.io/openshift-release-dev/ocp-release:4.17.8-x86_64"
  azure:
    overrides:
      - version: "4.16.17"
        cpoImage: "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1a50894aafa6b750bf890ef147a20699ff5b807e586d15506426a8a615580797"
      - version: "4.16.18"
        cpoImage: "quay.io/hypershift/hypershift-cpo:patch"
```

## Configuration Elements

### Platforms Section

The top-level `platforms` section contains platform-specific configurations. Currently supported platforms:

- **`aws`**: Amazon Web Services
- **`azure`**: Microsoft Azure

Each platform section is optional. If a platform is not configured, the default CPO image from the OpenShift release will be used.

### Platform Configuration

Each platform can contain:

#### `overrides` (Array)
A list of version-specific CPO image overrides:

- **`version`** (string): The exact OpenShift version (e.g., "4.17.9", "4.16.17")
- **`cpoImage`** (string): The full container image reference to use for this version

#### `testing` (Object, Optional)
Configuration for automated testing:

- **`latest`** (string): The latest OpenShift release image for testing
- **`previous`** (string): The previous OpenShift release image for testing
- **`runTests`** (boolean): Whether to run override tests for this platform when TEST_CPO_OVERRIDE=1 is set. Only one platform should have this set to `true` at a time

## Configuration Examples

### Basic Platform-Specific Override

```yaml
platforms:
  aws:
    overrides:
      - version: "4.17.9"
        cpoImage: "quay.io/myorg/custom-cpo-aws:4.17.9"
```

### Multiple Versions and Platforms

```yaml
platforms:
  aws:
    overrides:
      - version: "4.17.9"
        cpoImage: "quay.io/hypershift/control-plane-operator-aws:4.17.9"
      - version: "4.17.8"
        cpoImage: "quay.io/hypershift/control-plane-operator-aws:4.17.8"
      - version: "4.16.15"
        cpoImage: "quay.io/hotfix/cpo-aws:4.16.15-hotfix"
    testing:
      latest: "quay.io/openshift-release-dev/ocp-release:4.17.9-x86_64"
      previous: "quay.io/openshift-release-dev/ocp-release:4.17.8-x86_64"
  azure:
    overrides:
      - version: "4.17.9"
        cpoImage: "quay.io/hypershift/control-plane-operator-azure:4.17.9"
      - version: "4.16.20"
        cpoImage: "quay.io/security-fix/cpo-azure:4.16.20-security"
```

## How It Works

### Image Resolution Process

1. **Override Check**: When a HostedCluster is created or updated, the system checks if CPO overrides are enabled
2. **Platform Lookup**: The system looks up overrides for the specific platform (case-insensitive)
3. **Version Match**: If the platform exists, it searches for an exact version match
4. **Fallback**: If no override is found, the default CPO image from the OpenShift release is used

### Platform Handling

- Platform names are **case-insensitive** (`AWS`, `aws`, `Aws` all work)
- Unknown platforms return empty string (graceful degradation)
- Missing platforms in configuration are handled safely (no panics)

## Enabling CPO Overrides

CPO overrides are controlled by an environment variable on the HyperShift operator:

```bash
export ENABLE_CPO_OVERRIDES=1
```

When this environment variable is set to `1`, the override system is activated. Without this variable, all overrides are ignored and default images are used.

## File Locations

The override configuration files are embedded in the binary at build time:

`hypershift-operator/controlplaneoperator-overrides/assets/overrides.yaml`


## Best Practices

### Version Management
- Use exact version strings (e.g., "4.17.9", not "4.17.x")
- Keep override lists sorted by version for maintainability

### Image References
- Use full image references including registry and tag/digest
- Prefer digest references for production overrides for immutability

### Platform Configuration
- Only configure platforms that need overrides
- Keep platform-specific testing configurations separate
- Document the purpose of each override in comments

### Testing
- The test section allows specifying a pair of release images to use for a single hypershift e2e test run and controlling which platform's override tests should run
- The `runTests` field controls whether override tests run for each platform when `TEST_CPO_OVERRIDE=1` is set
- **CI Resource Optimization**: Typically, only one platform should have `runTests: true` at a time to avoid wasting CI resources
  - If a PR only updates AWS overrides, set `runTests: true` for AWS and `runTests: false` for Azure
  - If a PR only updates Azure overrides, set `runTests: true` for Azure and `runTests: false` for AWS
- **Testing Multiple Platforms**: Both platforms can have `runTests: true` simultaneously if your changes affect both platforms (e.g., structural changes to the override system itself)

#### Running Override Tests

The `TEST_CPO_OVERRIDE` environment variable enables override testing across the e2e test suite.

**What TEST_CPO_OVERRIDE does:**
1. Configures all e2e tests to use the release images specified in the `testing` section of `overrides.yaml` instead of the default latest/previous releases
2. In CI environments, automatically enables CPO overrides in the HyperShift operator (`--enable-cpo-overrides` flag)
3. Triggers verification in `TestCreateCluster` that validates the CPO pod is running with the expected override image

**How override verification works:**
When `TEST_CPO_OVERRIDE=1` is set, the `TestCreateCluster` test automatically verifies that the control-plane-operator pod is running with the correct override image from `overrides.yaml`. This verification happens for all platforms during the standard cluster creation test, so no dedicated override test is needed.

To run override tests:

1. Set the `TEST_CPO_OVERRIDE` environment variable:
   ```bash
   export TEST_CPO_OVERRIDE=1
   ```

2. Configure the testing release images in `overrides.yaml` and set which platform(s) to test:
   ```yaml
   platforms:
     aws:
       testing:
         latest: "quay.io/openshift-release-dev/ocp-release:4.19.10-x86_64"
         previous: "quay.io/openshift-release-dev/ocp-release:4.19.9-x86_64"
         runTests: true  # Set to true to test this platform's overrides
     azure:
       testing:
         latest: "quay.io/openshift-release-dev/ocp-release:4.19.10-x86_64"
         previous: "quay.io/openshift-release-dev/ocp-release:4.19.9-x86_64"
         runTests: false  # Set to false to skip this platform's tests (saves CI resources)
   ```

   Note: Both platforms can have `runTests: true` if your changes affect both platforms.

3. When running locally, ensure CPO overrides are enabled when installing the operator:
   ```bash
   bin/hypershift install --enable-cpo-overrides ...
   ```

4. Run the e2e tests on the desired platform:
   - All tests will use the configured override test releases
   - `TestCreateCluster` will automatically verify the CPO override image is correctly applied

## Troubleshooting

### Common Issues

1. **Override Not Applied**
   - Check if `ENABLE_CPO_OVERRIDES=1` is set
   - Verify platform name matches exactly (case-insensitive)
   - Confirm version string is exact match

2. **Image Pull Failures**
   - Verify image exists and is accessible
   - Check image registry authentication
   - Validate image digest/tag is correct

3. **Platform Not Found**
   - Returns empty string (safe fallback)
   - Check platform configuration in YAML
   - Verify platform name spelling


## Security/Production Considerations

- Only use trusted image registries for CPO overrides
- Validate override images are signed and verified
- If multiple architectures are supported in the data plane, ensure that override image references point to multi-arch repositories that have all architectures supported in the data plane.

---

## Source: docs/content/contribute/custom-images.md

---
title: Use custom operator images
---

# How to install HyperShift with a custom image

1. Build and push a custom image build to your own repository.

    ```shell linenums="1"
      export QUAY_ACCOUNT=example

      make build
      make RUNTIME=podman IMG=quay.io/${QUAY_ACCOUNT}/hypershift:latest docker-build docker-push
    ```

1. Install HyperShift using the custom image:

    ```shell linenums="1"
      hypershift install \
        --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \
        --oidc-storage-provider-s3-credentials $AWS_CREDS \
        --oidc-storage-provider-s3-region $REGION \
        --hypershift-image quay.io/${QUAY_ACCOUNT}/hypershift:latest \
    ```

1. (Optional) If your repository is private, create a secret:

    ```shell
      oc create secret --namespace hypershift generic hypershift-operator-pull-secret \
        --from-file=.dockerconfig=/my/pull-secret --type=kubernetes.io/dockerconfig
    ```

    Then update the operator ServiceAccount in the hypershift namespace:

    ```shell
      oc patch serviceaccount --namespace hypershift operator \
        -p '{"imagePullSecrets": [{"name": "hypershift-operator-pull-secret"}]}'
    ```


---

## Source: docs/content/contribute/develop_in_cluster.md

---
title: Develop HyperShift components in-cluster
---

# How to develop HyperShift components in-cluster

Sometimes when developing HyperShift components it's useful to iterate on new
binary builds inside the cluster itself, especially when working on functionality
that depends on the Kubernetes or cloud environment for one reason or another.

Because such in-cluster build/image/publish/redeploy development workflows can be
very tedious and slow, the HyperShift project includes a few tools and techniques
to help make the feedback loop as fast as possible.

This guide makes use of the ko tool to rapidly
build lightweight images which are then published directly into an OCP cluster's
internal registry. This approach has the following properties which can speed up
development:

- No local container runtime required to build images, and image builds are
  extremely fast.
- Resulting images are almost as small as the Go binary being published.
- Images are published directly into OCP's internal image registry, so images
  are immediately available on or near the machines that will be pulling them.

## Prerequisites

- An OCP 4.9+ cluster
- The `oc` CLI tool
- The ko CLI tool

For this workflow, the OCP cluster must be configured to expose its internal
image registry externally so the `ko` tool can publish to it.

First, expose the cluster's image registry:

```shell
oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{"spec":{"defaultRoute":true}}' --type=merge
```

Next, generate an authentication token for the registry and install it into the
local Docker config file so that `ko` can push images into the registry. Be sure
to replace `<password>` with your actual `kubeadmin` password.

```shell linenums="1"
oc login -u kubeadmin -p <password>
oc registry login --to=$HOME/.docker/config.json --skip-check --registry $(oc get routes --namespace openshift-image-registry default-route -o jsonpath='{.spec.host}')
```

Finally, configure OCP to allow any authenticated user to pull images from the
internal registry. This will enable HyperShift component pods to pull the custom
images you publish.

```shell
oc create clusterrolebinding authenticated-registry-viewer --clusterrole registry-viewer --group system:authenticated
```

## Build and publish a component

To build and publish a given component into the OCP cluster from local source,
use the `publish-ocp.sh` script. This tool uses `ko` to build and publish
the image, and will output to stdout a single line containing the _internal_ pullspec
suitable for use by any HyperShift component deployment.

For example, to build and publishing the `hypershift-operator`, run:

```shell
hack/publish-ocp.sh ./hypershift-operator
```

Here's what the output will look like:

```linenums="1" hl_lines="9 10"
2021/12/01 16:49:54 Using base gcr.io/distroless/static:nonroot for github.com/openshift/hypershift/hypershift-operator
2021/12/01 16:49:55 Building github.com/openshift/hypershift/hypershift-operator for linux/amd64
2021/12/01 16:50:02 Publishing default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02:latest
2021/12/01 16:50:02 existing blob: sha256:250c06f7c38e52dc77e5c7586c3e40280dc7ff9bb9007c396e06d96736cf8542
2021/12/01 16:50:02 existing blob: sha256:e8614d09b7bebabd9d8a450f44e88a8807c98a438a2ddd63146865286b132d1b
2021/12/01 16:50:02 existing blob: sha256:cde5c5024aed9d3daaa3cd7b87fa21a66b10d2f2e8a1b9d339e2fb505cbde8c0
2021/12/01 16:50:02 existing blob: sha256:a589e39bc5bc084dd0ec79f5492ff9dc2ac6dbbb5fb95eb200b319246a7b8207
2021/12/01 16:50:03 default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02:latest: digest: sha256:20b0baf90c58a92a5e384eaa8d40cd47cc1c8cabce27bedccd7bbc2f54ca4c5b size: 953
2021/12/01 16:50:03 Published default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02@sha256:20b0baf90c58a92a5e384eaa8d40cd47cc1c8cabce27bedccd7bbc2f54ca4c5b
image-registry.openshift-image-registry.svc:5000/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02@sha256:20b0baf90c58a92a5e384eaa8d40cd47cc1c8cabce27bedccd7bbc2f54ca4c5b
```

The `publish-ocp.sh` script prints only the internal repo pullspec to stdout to
make it easy to incorporate the script into pipelines.

!!! note

    Notice on line 9 that public pullspec of the image is
    `default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22...`.
    Pods in the cluster cannot pull the image using the public repo name because the
    host's certificate is likely self-signed, which would require additional
    configuration in the cluster to enable pods to pull it.

    Pods must reference the _internal repo pullspec_ as printed to stdout on line
    10: `image-registry.openshift-image-registry.svc:5000/hypershift/hypershift-operator-cd22...`.

## Launch a custom `hypershift-operator` image interactively

To iterate on the `hypershift-operator` binary in-cluster interactively, first
scale down the operator's deployment:

```shell
oc scale --replicas 0 --namespace hypershift deployments/operator
```

Alternatively, run the HyperShift CLI `install` command with the `--development`
flag which sets up the deployment with zero replicas:

```shell
go run . install \
  --oidc-storage-provider-s3-bucket-name=$BUCKET_NAME \
  --oidc-storage-provider-s3-region=$BUCKET_REGION \
  --oidc-storage-provider-s3-credentials=$AWS_CREDS \
  --development
```

Now, you can build and publish the `hypershift-operator` image and run it interactively
in a single shot using `publish-ocp.sh` together with the `oc debug` command:

```shell hl_lines="3 4"
oc debug --namespace hypershift deployments/operator --image $(hack/publish-ocp.sh ./hypershift-operator) -- \
  /ko-app/hypershift-operator run \
  --oidc-storage-provider-s3-region $BUCKET_REGION \
  --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \
  --oidc-storage-provider-s3-credentials /etc/oidc-storage-provider-s3-creds/credentials \
  --namespace hypershift \
  --pod-name operator-debug
```

!!! note

    Make sure to replace `$BUCKET_NAME` and `$BUCKET_REGION` with the same values used to
    install HyperShift.

Your latest code should be deployed and logs should soon begin streaming. Just
press `ctrl-c` to terminate and delete the pod.

!!! note

    See Use custom operator images to use your own registry.

## Configure a HostedCluster for iterative control plane development

To iterate on control plane components which are deployed and managed in a
`HostedCluster` control plane namespace (e.g. the `control-plane-operator`
or `ignition-server`), it's possible to configure the `HostedCluster` resource to
scale down individual control plane components and facilitate various development
workflows.

The `hypershift.openshift.io/debug-deployments` annotation on a `HostedCluster`
is used to configure individual control plane components as targets for development
and debugging. The value of the annotation is a comma-delimited list of control
plane deployment names. Any control plane component in the list will always be
scaled to 0, enabling developers to replace the components with their own processes
(inside or outside the cluster) while preserving the `Deployment` resources to
use as templates for the replacement process environments.

For example, to scale the `control-plane-operator` and `ignition-server` deployments
to 0:

```shell
oc annotate -n clusters HostedCluster test-cluster hypershift.openshift.io/debug-deployments=control-plane-operator,ignition-server
```

!!! note

    Update the name of the HostedCluster to match your cluster.

This will result in a `HostedCluster` like so:

```yaml linenums="1" hl_lines="5"
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  annotations:
    hypershift.openshift.io/debug-deployments: control-plane-operator,ignition-server
  namespace: clusters
  name: test
spec:
  release:
    image: quay.io/openshift-release-dev/ocp-release:4.9.0-x86_64
# <remainder of resource omitted>
```

To scale back up a given component's original deployment simply remove the component's
deployment name from the list.

The `hypershift.openshift.io/pod-security-admission-label-override` annotation
may also need to be set in order to run debug pods locally.

```shell
oc annotate -n clusters HostedCluster test-cluster hypershift.openshift.io/pod-security-admission-label-override=baseline
```

## Launch a custom `control-plane-operator` image interactively

To iterate on the `control-plane-operator` binary in-cluster interactively, first
configure the HostedCluster
to scale down the `control-plane-operator` deployment.

Now, you can build and publish the `control-plane-operator` image and run it interactively
in a single shot using `publish-ocp.sh` together with the `oc debug` command. Be
sure to replace `$NAMESPACE` with the namespace of the control plane that was deployed
for the `HostedCluster`.

```shell
oc debug --namespace $NAMESPACE deployments/control-plane-operator --image $(hack/publish-ocp.sh ./control-plane-operator) -- /ko-app/control-plane-operator run
```

Your latest code should be deployed and logs should soon begin streaming. Just
press `ctrl-c` to terminate and delete the pod.

!!! note

    The default arguments to `control-plane-operator run` should be sufficient to
    get started.

## Launch a custom `ignition-server` interactively

To iterate on the ignition server in-cluster interactively, first
configure the HostedCluster
to scale down the `ignition-server` deployment.

Now, you can build and publish the `control-plane-operator` image and run the
`ignition-server` command interactively in a single shot using `publish-ocp.sh`
together with the `oc debug` command. Be sure to replace `$NAMESPACE` with the
namespace of the control plane that was deployed for the `HostedCluster`.

```shell
oc debug --namespace $NAMESPACE deployments/ignition-server --image $(hack/publish-ocp.sh ./control-plane-operator) -- /ko-app/control-plane-operator ignition-server
```

Your latest code should be deployed and logs should soon begin streaming. Just
press `ctrl-c` to terminate and delete the pod.

!!! note

    The default arguments to `ignition-server` should be sufficient to
    get started.


---

## Source: docs/content/contribute/index.md

---
title: Contribute
---

# Contribute

Use these resources to contribute to HyperShift.

- Contributing guidelines (GitHub)
- Release Process
- Custom Images
- Onboard a Platform
- Run Tests
- Develop in Cluster
- Run hypershift-operator locally
- CPO Overrides
- Contribute to docs
- Pre-commit hook help




---

## Source: docs/content/contribute/onboard-a-platform.md

---
title: Onboard a platform
---

# How to extend HyperShift to support a new platform

A Platform represents a series of assumptions and choices that HyperShift makes about the environment where it's running, e.g AWS, IBMCloud, Kubevirt.
The implementation of a new platform crosses multiple controllers.

The HostedCluster controller requires an implementation of the Platform interface to shim a particular CAPI implementation and manage required cloud credentials.

The NodePool controller requires an implementation of the machine template reconciliation.

The ControlPlane Operator requires the following:

- Implement cloud credentials

- Reconcile Kubernetes cloud provider config

- Reconcile the OCP Infrastructure CR

- Reconcile secret encryption (if your provider supports KMS)

## End to end testing

The end-to-end tests require an implementation of the Cluster interface.
As a starting point, check out
the None-Platform implementation
and
its basic test

## Supported platforms

- AWS.

---

## Source: docs/content/contribute/precommit-hook-help.md

# General Help on Using precommit Hooks in the HyperShift Repo
precommit hooks are helpful in catching issues prior to any new code or pull request appearing in the HyperShift repo. 
In the long run, the precommit hooks will help you save time by catching issues that would normally cause the `verify` 
and `unit` tests fail on your pull request. The following sections will walk you through how to quickly install the 
hooks, quickly uninstall the hooks, and how to bypass the hooks.

## Installing precommit hooks
Once you have precommit installed on your machine(see this for more info), it's quite simple to install the precommit hooks.

```shell
% pre-commit install
pre-commit installed at .git/hooks/pre-commit
pre-commit installed at .git/hooks/pre-push
```

The jobs ran at the pre-commit and pre-push stages are defined in the .golangci.yml file at the base of the HyperShift 
repo.

## Uninstalling precommit hooks
Sometimes it might be useful to turn off the precommit hooks briefly.

```shell
% pre-commit uninstall
pre-commit uninstalled
pre-push uninstalled
```

## Bypassing precommit hooks
Sometimes you may want to bypass the precommit hooks on a `git push` command, for example, if you just updated something really minor, updating your local `main` branch, or just needed to rerun a `go mod tidy` command, etc. To ignore the `pre-push` hooks, just add the `--no-verify` flag to your command.

```shell
% git push --set-upstream origin remove-autorest --no-verify 
% git push -f --no-verify
```

---

## Source: docs/content/contribute/release-process.md

---
title: Release process
---

# HO/CPO Release process

!!! important

    This is a complex process that involves some changes in multiple repositories and will affect multiple teams daily basis work.
    Make sure you have multiple reviewers from Core dev team which could guide you in the full process.

## Preparing a release in Openshift/Hypershift repository

### Bumping release version and generating Release Notes

The hypershift repo produces two different artifacts: Hypershift Operator (HO) and Control Plane Operator (CPO).

The CPO release lifecycle is dictated by the OCP release payload.

The HO has an independent release cadence. For consumer products:

- Our internal image build system builds from our latest commit in main several times a day.
- To roll out a new build we apply the following process:
  - Create a git tag for the commit belonging to the image to be rolled out:
    - `git co $commit-sha`
    - `git tag v0.1.1`
    - Push against remote.
  - Generate release notes:
    - `FROM=v0.1.0 TO=v0.1.1 make release`
    - Use the output to create the PR for bump the new image in the product gitOps repo. E.g.

This is a sample of how the release notes looks like added to the PR:

  ```
  ## area/control-plane-operator

  - cpo: cno: follow image name change in release payload

  ## area/hypershift-operator

  - Added documentation around supported-versions configmap
  - Add comment for BaseDomainPrefix
  - Add condition to NodePool indicating whether a security group for it is available
  - HOSTEDCP-827: Add root volume encryption e2e test
  - fix(hypershift): reduce CAPI rbac access
  - Validate Network Input for HostedCluster
  ```


---

## Source: docs/content/contribute/run-hypershift-operator-locally.md

---
title: Run the hypershift-operator locally
---

## Run the HyperShift Operator locally

To run the HyperShift Operator locally, follow these steps:

1. Ensure that the `KUBECONFIG` environment variable is set to a management cluster where HyperShift has not been installed yet.

  ```shell linenums="1"
   export KUBECONFIG="/path/to/your/kubeconfig"
  ```

2. Build HyperShift.

!!! note     
 
    `requires go v1.22+

```shell linenums="1"
  make build
```

3. Set the necessary environment variables

  ```shell linenums="1"
    export HYPERSHIFT_REGION="your-region"
    export HYPERSHIFT_BUCKET_NAME="your-bucket"
  ```

!!! note 

    `Consider setting HYPERSHIFT_REGION and HYPERSHIFT_BUCKET_NAME in your shell init script (e.g., $HOME/.bashrc).

!!! note 

    `Default values are provided for HYPERSHIFT_REGION and HYPERSHIFT_BUCKET_NAME so Step #4 will function without requiring you to export any values.

4. Install HyperShift in development mode which causes the operator deployment to be deployment scaled to zero so that it doesn't conflict with your local operator process (see Prerequisites):
  
```shell linenums="1"
  make hypershift-install-aws-dev
```

5. Run the HyperShift operator locally.

```shell linenums="1"
  make run-operator-locally-aws-dev
```



---

## Source: docs/content/contribute/run-tests.md

---
title: Run tests
---

# How to run all e2e tests

1. Install HyperShift.
2. Run all tests:

        make e2e
        bin/test-e2e -test.v -test.timeout 0 \
        --e2e.aws-credentials-file /my/aws-credentials \
        --e2e.pull-secret-file /my/pull-secret \
        --e2e.base-domain my-basedomain \
        --e2e.aws-oidc-s3-bucket-name my-bucket-name

*Note* if the s3 bucket is not in the `us-east-1` region then you need to add the `--e2e.aws-region` flag.

# How to run e2e tests for the "None" platform

        make e2e
        bin/test-e2e -test.v -test.timeout 0 \
        --e2e.pull-secret-file /my/pull-secret \
        --e2e.base-domain my-basedomain -test.run='^TestNone.*'


---

## Source: docs/content/getting-started.md

---
title: Getting started
---

# Getting started
HyperShift is middleware for hosting OpenShift control planes at scale that
solves for cost and time to provision, as well as portability across cloud service providers with
strong separation of concerns between management and workloads. Clusters are
fully compliant OpenShift Container Platform (OCP) clusters and are compatible
with standard OCP and Kubernetes toolchains.

This guide will lead you through the process of creating a new hosted cluster.
Throughout the instructions, shell variables are used to indicate values that
you should adjust to your own environment.

## Prerequisites
1. Install the HyperShift CLI (`hypershift`) using Go 1.19:
        ```shell linenums="1"
        git clone https://github.com/openshift/hypershift.git
        cd hypershift
        make build
        sudo install -m 0755 bin/hypershift /usr/local/bin/hypershift
        ```
2. Admin access to an OpenShift cluster (version 4.12+) specified by the `KUBECONFIG` environment variable.
3. The OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`). 
4. A valid pull secret file for the `quay.io/openshift-release-dev` repository. 
5. AWS credentials with permissions to create infrastructure for the cluster. You will need:
     - An IAM role ARN with the required permissions
     - STS credentials (session token) that can be generated using `aws sts get-session-token` 
6. A Route53 public zone for cluster DNS records. To create a public zone:
        ```shell linenums="1"
        BASE_DOMAIN=www.example.com
        aws route53 create-hosted-zone --name $BASE_DOMAIN --caller-reference $(whoami)-$(date --rfc-3339=date)
        ```

    !!! important

        To access applications in your guest clusters, the public zone must be routable. If the public zone exists, skip 
        this step. Otherwise, the public zone will affect the existing functions.

7. An S3 bucket with public access to host OIDC discovery documents for your clusters. To create the bucket in *us-east-1*:
        ```shell linenums="1"
        export BUCKET_NAME=your-bucket-name
        aws s3api create-bucket --bucket $BUCKET_NAME
        aws s3api delete-public-access-block --bucket $BUCKET_NAME
        echo '{
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": "*",
              "Action": "s3:GetObject",
              "Resource": "arn:aws:s3:::${BUCKET_NAME}/*"
            }
          ]
        }' | envsubst > policy.json
        aws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json
        ```

    To create the bucket in a region other than us-east-1:
        ```shell linenums="1"
        export BUCKET_NAME=your-bucket-name
        REGION=us-east-2
        aws s3api create-bucket --bucket $BUCKET_NAME \
          --create-bucket-configuration LocationConstraint=$REGION \
          --region $REGION
        aws s3api delete-public-access-block --bucket $BUCKET_NAME
        echo '{
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": "*",
              "Action": "s3:GetObject",
              "Resource": "arn:aws:s3:::${BUCKET_NAME}/*"
            }
          ]
        }' | envsubst > policy.json
        aws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json
        ```

## Install HyperShift Operator
Install the HyperShift Operator into the management cluster, specifying the OIDC bucket, its region and credentials to access it (see Prerequisites):

```shell linenums="1"
REGION=us-east-1
BUCKET_NAME=your-bucket-name
AWS_CREDS="$HOME/.aws/credentials"

hypershift install \
  --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \
  --oidc-storage-provider-s3-credentials $AWS_CREDS \
  --oidc-storage-provider-s3-region $REGION \
  --enable-defaulting-webhook true
```

!!! note 

    `enable-defaulting-webhook` is only for OCP version 4.14 and higher.

## Create a Hosted Cluster
Create a new hosted cluster, specifying the domain of the public zone provided in the
Prerequisites.

### Set Up AWS Credentials

The HyperShift CLI uses AWS STS (Security Token Service) for authentication. You need:

1. An IAM role with the required permissions to create cluster infrastructure
2. Temporary STS credentials to authenticate

Create an IAM role using the HyperShift CLI. The `--aws-creds` flag points to the credentials file for your
personal IAM user (the one used by the AWS CLI), which HyperShift uses to create the new role:

```shell linenums="1"
# Path to your existing AWS credentials file (for your personal IAM user)
AWS_CREDS="$HOME/.aws/credentials"

# Create a unique role name for your user
hypershift create iam cli-role --aws-creds $AWS_CREDS --name my-hypershift-cli-role
```

The command will output the role ARN. Save this value for use in subsequent commands:

```shell
ROLE_ARN="arn:aws:iam::123456789012:role/my-hypershift-cli-role"
```

!!! note

    The `hypershift create iam cli-role` command creates an IAM role with all required permissions.
    The role's trust policy is configured to allow only your IAM identity to assume it. If the role
    already exists, the command will reuse it and update its permissions policy.

!!! important

    Each user should create their own role with a unique `--name` to avoid conflicts.
    The role's trust policy is tied to the IAM identity that created it, so sharing a role
    between users with different IAM identities will not work.

Generate temporary STS credentials:

```shell
aws sts get-session-token --output json > sts-creds.json
```

### Create the Cluster

Create the hosted cluster using the STS credentials:

```shell linenums="1"
REGION=us-east-1
CLUSTER_NAME=example
BASE_DOMAIN=example.com
ROLE_ARN="arn:aws:iam::123456789012:role/your-hypershift-role"
STS_CREDS="/tmp/sts-creds.json"
PULL_SECRET="$HOME/pull-secret"

hypershift create cluster aws \
  --name $CLUSTER_NAME \
  --node-pool-replicas=3 \
  --base-domain $BASE_DOMAIN \
  --pull-secret $PULL_SECRET \
  --role-arn $ROLE_ARN \
  --sts-creds $STS_CREDS \
  --region $REGION \
  --generate-ssh
```

!!! note

    The `--role-arn` flag specifies the ARN of an IAM role with permissions to create cluster infrastructure.
    The `--sts-creds` flag points to a JSON file containing temporary STS credentials generated by
    `aws sts get-session-token`. These credentials are used to assume the role specified by `--role-arn`.

!!! important

    The cluster name (`--name`) _must be unique within the base domain_ to
    avoid unexpected and conflicting cluster management behavior.

    The cluster name must also adhere to the RFC1123 standard.

!!! important

    You must include either flag, `release-image` or `release-stream`, when the `enable-defaulting-webhook` is not enabled on the installation of the HyperShift operator.

!!! note

    A default NodePool will be created for the cluster with 3 replicas per the
    `--node-pool-replicas` flag. 

!!! note 

    The default NodePool name will be a combination of your cluster name and zone name for 
    AWS (example, `example-us-east-1a`). For other providers, the default NodePool 
    name will be the same as the cluster name.

!!! note

    The `--generate-ssh` flag is not strictly necessary but it will help in
    debugging why a node has not joined your cluster.

After a few minutes, check the `hostedclusters` resources in the `clusters`
namespace and when ready it will look similar to the following:

```
oc get --namespace clusters hostedclusters
NAME      VERSION   KUBECONFIG                 PROGRESS    AVAILABLE   PROGRESSING   MESSAGE
example   4.12.0    example-admin-kubeconfig   Completed   True        False         The hosted control plane is available

oc get nodepools --namespace clusters
NAME                 CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
example-us-east-1a   example   2               2               False         False        4.12.0
```

Eventually the cluster's kubeconfig will become available and can be printed to
standard out using the `hypershift` CLI:

```shell
hypershift create kubeconfig
```

## Create Additional NodePools
Create additional NodePools for a cluster by specifying a name, number of replicas
and additional information such as instance type.

```shell linenums="1"
NODEPOOL_NAME=${CLUSTER_NAME}-work
INSTANCE_TYPE=m5.2xlarge
NODEPOOL_REPLICAS=2

hypershift create nodepool aws \
  --cluster-name $CLUSTER_NAME \
  --namespace clusters \
  --name $NODEPOOL_NAME \
  --replicas $NODEPOOL_REPLICAS \
  --instance-type $INSTANCE_TYPE
```

!!! important

    The default infrastructure created for the cluster during Create a HostedCluster
    lives in a single availability zone. Any additional NodePool created for that
    cluster must be in the same availability zone and subnet.

Check the status of the NodePool by listing `nodepool` resources in the `clusters`
namespace:

```shell
oc get nodepools --namespace clusters
```

## Scale a NodePool
Manually scale a NodePool using the `oc scale` command:

```shell linenums="1"
NODEPOOL_NAME=${CLUSTER_NAME}-work
NODEPOOL_REPLICAS=5

oc scale nodepool/$NODEPOOL_NAME \
  --namespace clusters \
  --replicas=$NODEPOOL_REPLICAS
```

!!! note

    See the Scale Down section of the NodePool lifecycle page for more details when scaling down NodePools.

## Delete a Hosted Cluster
To delete a Hosted Cluster:

```shell
hypershift destroy cluster aws \
  --name $CLUSTER_NAME \
  --role-arn $ROLE_ARN \
  --sts-creds $STS_CREDS
```

!!! note

    The same `--role-arn` and `--sts-creds` flags used during cluster creation should be provided
    when destroying the cluster. If your STS credentials have expired, generate new ones using
    `aws sts get-session-token --output json > /tmp/sts-creds.json`.


---

## Source: docs/content/how-to/agent/create-agent-cluster.md

# Create an Agent cluster

This document explains how to create HostedClusters and NodePools using the Agent platform.

The Agent platform uses the Infrastructure Operator (AKA Assisted Installer) to add
worker nodes to a hosted cluster. For a primer on the Infrastructure Operator, see
here.

## Overview

When you create a HostedCluster with the Agent platform, HyperShift will install the Agent CAPI
provider in the Hosted Control Plane (HCP) namespace.

Upon scaling up a NodePool, a Machine will be created, and the CAPI provider will find a suitable Agent to match this Machine.
Suitable means that the Agent is approved, is passing validations, is not currently bound (in use), and has the requirements
specified on the NodePool Spec (e.g., minimum CPU/RAM, labels matching the label selector). You may monitor the installation of an
Agent by checking its `Status` and `Conditions`.

Upon scaling down a NodePool, Agents will be unbound from the corresponding cluster. However, you must boot them with the Discovery
Image once again before reusing them.

## Install HyperShift Operator

Before installing the HyperShift operator we need to get the HyperShift CLI. We have two methods for getting the CLI installed in our system.

### Method 1 - Build the HyperShift CLI

Follow instructions for building the HyperShift CLI in Getting
Started

### Method 2 - Extract HyperShift CLI from the Operator Image

> **INFO:** We are using Podman in the example, same applies to Docker.

~~~sh
export HYPERSHIFT_RELEASE=4.11

podman cp $(podman create --name hypershift --rm --pull always quay.io/hypershift/hypershift-operator:${HYPERSHIFT_RELEASE}):/usr/bin/hypershift /tmp/hypershift && podman rm -f hypershift

sudo install -m 0755 -o root -g root /tmp/hypershift /usr/local/bin/hypershift
~~~

### Deploy the HyperShift Operator

With the CLI deployed, we can go ahead and deploy the operator:

> **WARN:** If we don't define the HyperShift image we want to use, by default the CLI will deploy `latest`. Usually you want to deploy the image matching the release of the OpenShift cluster where HyperShift will run.

~~~sh
# This install latest
hypershift install
# You may want to run this instead
hypershift install --hypershift-image quay.io/hypershift/hypershift-operator:4.11
~~~

You will see the operator running in the `hypershift` namespace:

~~~sh
oc -n hypershift get pods

NAME                      READY   STATUS    RESTARTS   AGE
operator-55fffbd6-whkxs   1/1     Running   0          61s
~~~

## Install Assisted Service and Hive Operators

> **NOTE**: If Red Hat Advanced Cluster Management (RHACM) is already installed, this can be skipped as the Infrastructure Operator
> and Hive Operator are dependencies of RHACM.

We will leverage `tasty` to deploy the required operators easily.

Install tasty:

~~~sh
curl -s -L https://github.com/karmab/tasty/releases/download/v0.4.0/tasty-linux-amd64 > ./tasty
sudo install -m 0755 -o root -g root ./tasty /usr/local/bin/tasty
~~~

Install the operators

~~~sh
tasty install assisted-service-operator hive-operator
~~~

## Configure Agent Service

Create the `AgentServiceConfig` resource

~~~sh
export DB_VOLUME_SIZE="10Gi"
export FS_VOLUME_SIZE="10Gi"
export OCP_VERSION="4.11.5"
export OCP_MAJMIN=${OCP_VERSION%.*}
export ARCH="x86_64"
export OCP_RELEASE_VERSION=$(curl -s https://mirror.openshift.com/pub/openshift-v4/${ARCH}/clients/ocp/${OCP_VERSION}/release.txt | awk '/machine-os / { print $2 }')
export ISO_URL="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH}-live.${ARCH}.iso"
export ROOT_FS_URL="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH}-live-rootfs.${ARCH}.img"

envsubst <<"EOF" | oc apply -f -
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
 name: agent
spec:
  databaseStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: ${DB_VOLUME_SIZE}
  filesystemStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: ${FS_VOLUME_SIZE}
  osImages:
    - openshiftVersion: "${OCP_VERSION}"
      version: "${OCP_RELEASE_VERSION}"
      url: "${ISO_URL}"
      rootFSUrl: "${ROOT_FS_URL}"
      cpuArchitecture: "${ARCH}"
EOF
~~~

## Configure DNS

The API Server for the Hosted Cluster is exposed a Service of type NodePort.

A DNS entry must exist for `api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN}` pointing to destination where the API Server can be reached.

This can be as simple as an A record pointing to one of the nodes in the management cluster (i.e. the cluster running the HCP).  It can also point to a load balancer deployed to redirect incoming traffic to the ingress pods.

### Example DNS Config

~~~conf
api.example.krnl.es.    IN A 192.168.122.20
api.example.krnl.es.    IN A 192.168.122.21
api.example.krnl.es.    IN A 192.168.122.22
api-int.example.krnl.es.    IN A 192.168.122.20
api-int.example.krnl.es.    IN A 192.168.122.21
api-int.example.krnl.es.    IN A 192.168.122.22
*.apps.example.krnl.es. IN A 192.168.122.23
~~~

## Create a Hosted Cluster

> **WARN:** Make sure you have a default storage class configured for your cluster, otherwise you may end up with pending PVCs.

~~~sh
export CLUSTERS_NAMESPACE="clusters"
export HOSTED_CLUSTER_NAME="example"
export HOSTED_CONTROL_PLANE_NAMESPACE="${CLUSTERS_NAMESPACE}-${HOSTED_CLUSTER_NAME}"
export BASEDOMAIN="krnl.es"
export PULL_SECRET_FILE=$PWD/pull-secret
export OCP_RELEASE=4.11.5-x86_64
export MACHINE_CIDR=192.168.122.0/24
# Typically the namespace is created by the hypershift-operator
# but agent cluster creation generates a capi-provider role that
# needs the namespace to already exist
oc create ns ${HOSTED_CONTROL_PLANE_NAMESPACE}

hypershift create cluster agent \
    --name=${HOSTED_CLUSTER_NAME} \
    --pull-secret=${PULL_SECRET_FILE} \
    --agent-namespace=${HOSTED_CONTROL_PLANE_NAMESPACE} \
    --base-domain=${BASEDOMAIN} \
    --api-server-address=api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN} \
    --release-image=quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE}
~~~

After a few moments we should see our hosted control plane pods up and running:

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get pods

NAME                                             READY   STATUS    RESTARTS   AGE
capi-provider-7dcf5fc4c4-nr9sq                   1/1     Running   0          4m32s
catalog-operator-6cd867cc7-phb2q                 2/2     Running   0          2m50s
certified-operators-catalog-884c756c4-zdt64      1/1     Running   0          2m51s
cluster-api-f75d86f8c-56wfz                      1/1     Running   0          4m32s
cluster-autoscaler-7977864686-2rz4c              1/1     Running   0          4m13s
cluster-network-operator-754cf4ffd6-lwfm2        1/1     Running   0          2m51s
cluster-policy-controller-784f995d5-7cbrz        1/1     Running   0          2m51s
cluster-version-operator-5c68f7f4f8-lqzcm        1/1     Running   0          2m51s
community-operators-catalog-58599d96cd-vpj2v     1/1     Running   0          2m51s
control-plane-operator-f6b4c8465-4k5dh           1/1     Running   0          4m32s
etcd-0                                           1/1     Running   0          4m13s
hosted-cluster-config-operator-c4776f89f-dt46j   1/1     Running   0          2m51s
ignition-server-7cd8676fc5-hjx29                 1/1     Running   0          4m22s
ingress-operator-75484cdc8c-zhdz5                1/2     Running   0          2m51s
konnectivity-agent-c5485c9df-jsm9s               1/1     Running   0          4m13s
konnectivity-server-85dc754888-7z8vm             1/1     Running   0          4m13s
kube-apiserver-db5fb5549-zlvpq                   3/3     Running   0          4m13s
kube-controller-manager-5fbf7b7b7b-mrtjj         1/1     Running   0          90s
kube-scheduler-776c59d757-kfhv6                  1/1     Running   0          3m12s
machine-approver-c6b947895-lkdbk                 1/1     Running   0          4m13s
oauth-openshift-787b87cff6-trvd6                 2/2     Running   0          87s
olm-operator-69c4657864-hxwzk                    2/2     Running   0          2m50s
openshift-apiserver-67f9d9c5c7-c9bmv             2/2     Running   0          89s
openshift-controller-manager-5899fc8778-q89xh    1/1     Running   0          2m51s
openshift-oauth-apiserver-569c78c4d-568v8        1/1     Running   0          2m52s
packageserver-ddfffb8d7-wlz6l                    2/2     Running   0          2m50s
redhat-marketplace-catalog-7dd77d896-jtxkd       1/1     Running   0          2m51s
redhat-operators-catalog-d66b5c965-qwhn7         1/1     Running   0          2m51s
~~~

## Create an InfraEnv

An InfraEnv is a environment to which hosts booting the live ISO can join as Agents.  In this case, the Agents will be created in the
same namespace as our HostedControlPlane.

~~~sh
export SSH_PUB_KEY=$(cat $HOME/.ssh/id_rsa.pub)

envsubst <<"EOF" | oc apply -f -
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: ${HOSTED_CLUSTER_NAME}
  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}
spec:
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ${SSH_PUB_KEY}
EOF
~~~

This will generate a live ISO that allows machines (VMs or bare-metal) to join as Agents.

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get InfraEnv ${HOSTED_CLUSTER_NAME} -ojsonpath="{.status.isoDownloadURL}"
~~~

## Adding Agents

You can add Agents by manually configuring the machine to boot with the live ISO or by using Metal3.

### Manual

The live ISO may be downloaded and used to boot a node (bare-metal or VM).

On boot, the node will communicate with the assisted-service and register as an Agent in the same namespace as the InfraEnv.

Once each Agent is created, optionally set its installation_disk_id and hostname in the Spec. Then approve it to indicate that the Agent is ready for use.

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agents

NAME                                   CLUSTER   APPROVED   ROLE          STAGE
86f7ac75-4fc4-4b36-8130-40fa12602218                        auto-assign
e57a637f-745b-496e-971d-1abbf03341ba                        auto-assign
~~~

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} patch agent 86f7ac75-4fc4-4b36-8130-40fa12602218 -p '{"spec":{"installation_disk_id":"/dev/sda","approved":true,"hostname":"worker-0.example.krnl.es"}}' --type merge

oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} patch agent 23d0c614-2caa-43f5-b7d3-0b3564688baa -p '{"spec":{"installation_disk_id":"/dev/sda","approved":true,"hostname":"worker-1.example.krnl.es"}}' --type merge
~~~

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agents

NAME                                   CLUSTER   APPROVED   ROLE          STAGE
86f7ac75-4fc4-4b36-8130-40fa12602218             true       auto-assign
e57a637f-745b-496e-971d-1abbf03341ba             true       auto-assign
~~~

### Metal3

We will leverage the Assisted Service and Hive to create the custom ISO as well as the Baremetal Operator to perform the installation.

> **WARN:** Since the `BaremetalHost` objects will be created outside the baremetal-operator namespace we need to configure the operator to watch all namespaces.

~~~sh
oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true }}'
~~~

> **INFO:** This will trigger a restart of the `metal3` pod in the `openshift-machine-api` namespace.

* Wait until the `metal3` pod is ready again:

~~~sh
until oc wait -n openshift-machine-api $(oc get pods -n openshift-machine-api -l baremetal.openshift.io/cluster-baremetal-operator=metal3-state -o name) --for condition=containersready --timeout 10s >/dev/null 2>&1 ; do sleep 1 ; done
~~~

Now we can go ahead and create our BaremetalHost objects. We will need to configure some variables required to be able to boot our bare-metal nodes.

* `BMC_USERNAME`: Username to be used for connecting to the BMC.
* `BMC_PASSWORD`: Password to be used for connecting to the BMC.
* `BMC_IP`: IP used by Metal3 to connect to the BMC.
* `WORKER_NAME`: Name of the BaremetalHost object (this will be used as hostname as well)
* `BOOT_MAC_ADDRESS`: MAC address of the NIC connected to the MachineNetwork.
* `UUID`: Redfish UUID, this is usually `1`. If using sushy-tools this will be a long UUID. If using iDrac this will be `System.Embedded.1`. You may need to check with the vendor.
* `REDFISH_SCHEME`: The Redfish provider to use. If using hardware that uses a standard Redfish implementation you can set this to `redfish-virtualmedia`. iDRAC will use `idrac-virtualmedia`. iLO5 will use `ilo5-virtualmedia`. You may need to check with the vendor.
* `REDFISH`: Redfish connection endpoint.

~~~sh
export BMC_USERNAME=$(echo -n "root" | base64 -w0)
export BMC_PASSWORD=$(echo -n "calvin" | base64 -w0)
export BMC_IP="192.168.124.228"
export WORKER_NAME="ocp-worker-0"
export BOOT_MAC_ADDRESS="aa:bb:cc:dd:ee:ff"
export UUID="1"
export REDFISH_SCHEME="redfish-virtualmedia"
export REDFISH="${REDFISH_SCHEME}://${BMC_IP}/redfish/v1/Systems/${UUID}"
~~~

With the required information ready, let's create the BaremetalHost. First we will create the BMC Secret:

~~~sh
envsubst <<"EOF" | oc apply -f -
apiVersion: v1
data:
  password: ${BMC_PASSWORD}
  username: ${BMC_USERNAME}
kind: Secret
metadata:
  name: ${WORKER_NAME}-bmc-secret
  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}
type: Opaque
EOF
~~~

Second, we will create the BMH:

> **INFO:** `infraenvs.agent-install.openshift.io` label is used to specify which InfraEnv is used to boot the BMH. `bmac.agent-install.openshift.io/hostname` is used to manually set a hostname.

In case you want to manually specify the installation disk you can make use of the rootDeviceHints in the BMH Spec. If rootDeviceHints are not provided, the agent will pick the installation disk that better suits the installation requirements.

~~~sh
envsubst <<"EOF" | oc apply -f -
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: ${WORKER_NAME}
  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}
  labels:
    infraenvs.agent-install.openshift.io: ${HOSTED_CLUSTER_NAME}
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: ${WORKER_NAME}
spec:
  automatedCleaningMode: disabled
  bmc:
    disableCertificateVerification: True
    address: ${REDFISH}
    credentialsName: ${WORKER_NAME}-bmc-secret
  bootMACAddress: ${BOOT_MAC_ADDRESS}
  online: true
EOF
~~~

The Agent should be automatically approved, if not, make sure the `bootMACAddress` is correct.

The BMH will be provisioned:

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get bmh

NAME           STATE          CONSUMER   ONLINE   ERROR   AGE
ocp-worker-0   provisioning              true             2m50s
~~~

BMH will reach `provisioned` state eventually.

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get bmh
NAME           STATE          CONSUMER   ONLINE   ERROR   AGE
ocp-worker-0   provisioned               true             72s
~~~

Provisioned means that the node was configured to boot from the virtualCD properly. It will take a few moments for the Agent to show up:

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent

NAME                                   CLUSTER   APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6             true       auto-assign  
~~~

As you can see it was auto-approved. We will repeat this with another two nodes.

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent

NAME                                   CLUSTER   APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6             true       auto-assign  
d9198891-39f4-4930-a679-65fb142b108b             true       auto-assign
da503cf1-a347-44f2-875c-4960ddb04091             true       auto-assign
~~~

## Accessing the HostedCluster

We have the HostedControlPlane running and the Agents ready to join the HostedCluster. Before we join the Agents let's access the HostedCluster.

First, we need to generate the kubeconfig:

~~~sh
hypershift create kubeconfig --namespace ${CLUSTERS_NAMESPACE} --name ${HOSTED_CLUSTER_NAME} > ${HOSTED_CLUSTER_NAME}.kubeconfig
~~~

If we access the cluster we will see that we don't have any nodes and that the ClusterVersion is trying to reconcile the OCP release:

~~~sh
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,nodes

NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version             False       True          8m6s    Unable to apply 4.11.5: some cluster operators have not yet rolled out
~~~

In order to get the cluster in a running state we need to add some nodes to it. Let's do it.

## Scale the NodePool

We add nodes to our HostedCluster by scaling the NodePool object. In this case we will start by scaling the NodePool object to two nodes:

~~~sh
oc -n ${CLUSTERS_NAMESPACE} scale nodepool ${NODEPOOL_NAME} --replicas 2
~~~

The ClusterAPI Agent provider will pick two agents randomly that will get assigned to the HostedCluster. These agents will go over different states and will finally join the HostedCluster as OpenShift nodes.

> **INFO:** States will be `binding` -> `discoverying` -> `insufficient` -> `installing` -> `installing-in-progress` -> `added-to-existing-cluster`

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent

NAME                                   CLUSTER         APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6   hypercluster1   true       auto-assign  
d9198891-39f4-4930-a679-65fb142b108b                   true       auto-assign  
da503cf1-a347-44f2-875c-4960ddb04091   hypercluster1   true       auto-assign

oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'

BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: binding
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: insufficient
~~~

Once the agents have reached the `added-to-existing-cluster` state, we should see the OpenShift nodes after a few moments:

~~~sh
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes

NAME           STATUS   ROLES    AGE     VERSION
ocp-worker-1   Ready    worker   5m41s   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   6m3s    v1.24.0+3882f8f
~~~

At this point some ClusterOperators will start to reconcile by adding workloads to the nodes.

We can also see that two Machines were created when we scaled up the NodePool:

~~~sh
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get machines

NAME                            CLUSTER               NODENAME       PROVIDERID                                     PHASE     AGE   VERSION
hypercluster1-c96b6f675-m5vch   hypercluster1-b2qhl   ocp-worker-1   agent://da503cf1-a347-44f2-875c-4960ddb04091   Running   15m   4.11.5
hypercluster1-c96b6f675-tl42p   hypercluster1-b2qhl   ocp-worker-2   agent://4dac1ab2-7dd5-4894-a220-6a3473b67ee6   Running   15m   4.11.5
~~~

At some point the clusterversion reconcile will reach a point where only Ingress and Console cluster operators will be missing:

~~~sh
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co

NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version             False       True          40m     Unable to apply 4.11.5: the cluster operator console has not yet successfully rolled out

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.11.5    False       False         False      11m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.hypercluster1.domain.com): Get "https://console-openshift-console.apps.hypercluster1.domain.com": dial tcp 10.19.3.29:443: connect: connection refused
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.11.5    True        False         False      10m  
clusteroperator.config.openshift.io/dns                                        4.11.5    True        False         False      9m16s  
clusteroperator.config.openshift.io/image-registry                             4.11.5    True        False         False      9m5s  
clusteroperator.config.openshift.io/ingress                                    4.11.5    True        False         True       39m     The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)
clusteroperator.config.openshift.io/insights                                   4.11.5    True        False         False      11m  
clusteroperator.config.openshift.io/kube-apiserver                             4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/kube-controller-manager                    4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/kube-scheduler                             4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.11.5    True        False         False      10m  
clusteroperator.config.openshift.io/monitoring                                 4.11.5    True        False         False      7m38s  
clusteroperator.config.openshift.io/network                                    4.11.5    True        False         False      11m  
clusteroperator.config.openshift.io/openshift-apiserver                        4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/openshift-controller-manager               4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/openshift-samples                          4.11.5    True        False         False      8m54s  
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.11.5    True        False         False      40m  
clusteroperator.config.openshift.io/service-ca                                 4.11.5    True        False         False      11m  
clusteroperator.config.openshift.io/storage                                    4.11.5    True        False         False      11m
~~~

Let's fix the Ingress.

## Handling Ingress

Every OpenShift cluster comes set up with a default application ingress
controller, which is expected have an external DNS record associated with it.

For example, if a HyperShift cluster named `example` with the base domain
`krnl.es` is created, then the wildcard domain
`*.apps.example.krnl.es` is expected to be routable.

### Set up a LoadBalancer and wildcard DNS record for the `*.apps`.

This option requires deploying MetalLB, configuring a new LoadBalancer service that routes to the ingress deployment, as well as assigning a wildcard DNS entry to the LoadBalancer's IP address.

#### Step 1 - Get the MetalLB Operator Deployed

Set up MetalLB so that when you create a service of type LoadBalancer, MetalLB will add an external IP address for the service.

~~~sh
cat <<"EOF" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: metallb
  labels:
    openshift.io/cluster-monitoring: "true"
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: metallb-operator-operatorgroup
  namespace: metallb
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator
  namespace: metallb
spec:
  channel: "stable"
  name: metallb-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
~~~

Once the operator is up and running, create the MetalLB instance:

~~~sh
cat <<"EOF" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb
EOF
~~~

#### Step 2 - Get the MetalLB Operator Configured

We will create an `IPAddressPool` with a single IP address and L2Advertisement to advertise the LoadBalancer IPs provided by the `IPAddressPool` via L2.
Since layer 2 mode relies on ARP and NDP, the IP address must be on the same subnet as the network used by the cluster nodes in order for the MetalLB to work.
more information about metalLB configuration options is available here.
> **WARN:** Change `INGRESS_IP` env var to match your environments addressing.

~~~sh
export INGRESS_IP=192.168.122.23

envsubst <<"EOF" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-public-ip
  namespace: metallb
spec:
  protocol: layer2
  autoAssign: false
  addresses:
    - ${INGRESS_IP}-${INGRESS_IP}
---

apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-public-ip
  namespace: metallb
EOF
~~~

#### Step 3 - Get the OpenShift Router exposed via MetalLB

Set up the LoadBalancer Service that routes ingress traffic to the ingress deployment.

~~~sh
cat <<"EOF" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -
kind: Service
apiVersion: v1
metadata:
  annotations:
    metallb.universe.tf/address-pool: ingress-public-ip
  name: metallb-ingress
  namespace: openshift-ingress
spec:
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
    - name: https
      protocol: TCP
      port: 443
      targetPort: 443
  selector:
    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  type: LoadBalancer
EOF
~~~

We already configured the wildcard record in our example DNS config:

~~~config
*.apps.example.krnl.es. IN A 192.168.122.23
~~~

So we should be able to reach the OCP Console now:

~~~sh
curl -kI https://console-openshift-console.apps.example.krnl.es

HTTP/1.1 200 OK
~~~

And if we check the clusterversion and clusteroperator we should have everything up and running now:

~~~sh
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co

NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   4.11.5    True        False         3m32s   Cluster version is 4.11.5

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.11.5    True        False         False      3m50s  
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.11.5    True        False         False      25m  
clusteroperator.config.openshift.io/dns                                        4.11.5    True        False         False      23m  
clusteroperator.config.openshift.io/image-registry                             4.11.5    True        False         False      23m  
clusteroperator.config.openshift.io/ingress                                    4.11.5    True        False         False      53m  
clusteroperator.config.openshift.io/insights                                   4.11.5    True        False         False      25m  
clusteroperator.config.openshift.io/kube-apiserver                             4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/kube-controller-manager                    4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/kube-scheduler                             4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.11.5    True        False         False      25m  
clusteroperator.config.openshift.io/monitoring                                 4.11.5    True        False         False      21m  
clusteroperator.config.openshift.io/network                                    4.11.5    True        False         False      25m  
clusteroperator.config.openshift.io/openshift-apiserver                        4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/openshift-controller-manager               4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/openshift-samples                          4.11.5    True        False         False      23m  
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.11.5    True        False         False      54m  
clusteroperator.config.openshift.io/service-ca                                 4.11.5    True        False         False      25m  
clusteroperator.config.openshift.io/storage                                    4.11.5    True        False         False      25m  
~~~

## Enabling Node Auto-Scaling for the Hosted Cluster

Auto-scaling can be enabled, if we choose to enable auto-scaling, when more capacity is require in our Hosted Cluster a new Agent will be installed (providing that we have spare agents). In order to enable auto-scaling we can run the following command:

> **INFO:** In this case the minimum nodes will be 2 and the maximum 5.

~~~sh
oc -n ${CLUSTERS_NAMESPACE} patch nodepool ${HOSTED_CLUSTER_NAME} --type=json -p '[{"op": "remove", "path": "/spec/replicas"},{"op":"add", "path": "/spec/autoScaling", "value": { "max": 5, "min": 2 }}]'
~~~

If 10 minutes passes without requiring the additional capacity the agent will be decommissioned and placed in the spare queue again.

1. Let's create a workload that requires a new node.

    ~~~sh
    cat <<EOF | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      creationTimestamp: null
      labels:
        app: reversewords
      name: reversewords
      namespace: default
    spec:
      replicas: 40
      selector:
        matchLabels:
          app: reversewords
      strategy: {}
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: reversewords
        spec:
          containers:
          - image: quay.io/mavazque/reversewords:latest
            name: reversewords
            resources:
              requests:
                memory: 2Gi
    status: {}
    EOF
    ~~~

2. We will see the remaining agent starts getting deployed.

    > **INFO:** The spare agent `d9198891-39f4-4930-a679-65fb142b108b` started getting provisioned.

    ~~~sh
    oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'

    BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: added-to-existing-cluster
    BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: installing-in-progress
    BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: added-to-existing-cluster
    ~~~

3. If we check the nodes we will see a new one joined the cluster.

    > **INFO:** We got ocp-worker-0 added to the cluster

    ~~~sh
    oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes

    NAME           STATUS   ROLES    AGE   VERSION
    ocp-worker-0   Ready    worker   35s   v1.24.0+3882f8f
    ocp-worker-1   Ready    worker   40m   v1.24.0+3882f8f
    ocp-worker-2   Ready    worker   41m   v1.24.0+3882f8f
    ~~~

4. If we delete the workload and wait 10 minutes the node will be removed.

    ~~~sh
    oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig -n default delete deployment reversewords
    ~~~

5. After 10 minutes.

    ~~~sh
    oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes

    NAME           STATUS   ROLES    AGE   VERSION
    ocp-worker-1   Ready    worker   51m   v1.24.0+3882f8f
    ocp-worker-2   Ready    worker   52m   v1.24.0+3882f8f
    ~~~

    ~~~sh
    oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'

    BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: added-to-existing-cluster
    BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
    BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: added-to-existing-cluster
    ~~~


---

## Source: docs/content/how-to/agent/create-heterogeneous-nodepools.md

# Create Heterogeneous NodePools on Agent HostedClusters

This document explains how to create heterogeneous nodepools on agent platform. 
Please refer to set up the env for agent cluster, this document only covers the things you need to configure to have heterogeneous nodepools.

## Configure AgentServiceConfig with two heterogeneous architecture OS images

~~~sh
export DB_VOLUME_SIZE="10Gi"
export FS_VOLUME_SIZE="10Gi"
export OCP_VERSION="4.15.0"
export OCP_MAJMIN=${OCP_VERSION%.*}

export ARCH_X86="x86_64"
export OCP_RELEASE_VERSION_X86=$(curl -s https://mirror.openshift.com/pub/openshift-v4/${ARCH_X86}/clients/ocp/${OCP_VERSION}/release.txt | awk '/machine-os / { print $2 }')
export ISO_URL_X86="https://mirror.openshift.com/pub/openshift-v4/${ARCH_X86}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_X86}-live.${ARCH_X86}.iso"
export ROOT_FS_URL_X8="https://mirror.openshift.com/pub/openshift-v4/${ARCH_X86}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_X86}-live-rootfs.${ARCH_X86}.img"

export ARCH_PPC64LE="ppc64le"
export OCP_RELEASE_VERSION_PPC64LE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/${ARCH_PPC64LE}/clients/ocp/${OCP_VERSION}/release.txt | awk '/machine-os / { print $2 }')
export ISO_URL_PPC64LE="https://mirror.openshift.com/pub/openshift-v4/${ARCH_PPC64LE}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_PPC64LE}-live.${ARCH_PPC64LE}.iso"
export ROOT_FS_URL_PPC64LE="https://mirror.openshift.com/pub/openshift-v4/${ARCH_PPC64LE}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_PPC64LE}-live-rootfs.${ARCH_PPC64LE}.img"

envsubst <<"EOF" | oc apply -f -
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
 name: agent
spec:
  databaseStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: ${DB_VOLUME_SIZE}
  filesystemStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: ${FS_VOLUME_SIZE}
  osImages:
    - openshiftVersion: "${OCP_VERSION}"
      version: "${OCP_RELEASE_VERSION_X86}"
      url: "${ISO_URL_X86}"
      rootFSUrl: "${ROOT_FS_URL_X8}"
      cpuArchitecture: "${ARCH_X86}"
    - openshiftVersion: "${OCP_VERSION}"
      version: "${OCP_RELEASE_VERSION_PPC64LE}"
      url: "${ISO_URL_PPC64LE}"
      rootFSUrl: "${ROOT_FS_URL_PPC64LE}"
      cpuArchitecture: "${ARCH_PPC64LE}"
EOF
~~~

The above configuration allows you to create ISO for both x86_64 and ppc64le architectures.

## Configure DNS

`*.apps.<cluster_name>` record can be pointed to either one of the worker node where ingress application is hosted, or if you are able to set up a load balancer on top of the worker nodes, point this record to this load balancer.
When you are creating heterogeneous nodepool, please make sure the workers are reachable from each other or keep them in the same network.

## Create a Hosted Cluster

Need to use multi arch release image while creating the cluster to use heterogeneous nodepools. Find the latest multi arch images from here 
~~~sh
export CLUSTERS_NAMESPACE="clusters"
export HOSTED_CLUSTER_NAME="example"
export HOSTED_CONTROL_PLANE_NAMESPACE="${CLUSTERS_NAMESPACE}-${HOSTED_CLUSTER_NAME}"
export BASEDOMAIN="krnl.es"
export PULL_SECRET_FILE=$PWD/pull-secret
export OCP_RELEASE=4.15.0-multi
export MACHINE_CIDR=192.168.122.0/24
# Typically the namespace is created by the hypershift-operator 
# but agent cluster creation generates a capi-provider role that
# needs the namespace to already exist
oc create ns ${HOSTED_CONTROL_PLANE_NAMESPACE}

hypershift create cluster agent \
    --name=${HOSTED_CLUSTER_NAME} \
    --pull-secret=${PULL_SECRET_FILE} \
    --agent-namespace=${HOSTED_CONTROL_PLANE_NAMESPACE} \
    --base-domain=${BASEDOMAIN} \
    --api-server-address=api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN} \
    --release-image=quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE}
~~~

## Create Heterogeneous NodePool

By default, the previous section creates a nodepool on `x86_64` architecture, to create heterogeneous nodepools, you can apply the below manifest with the architecture you want.

~~~shell
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: ${HOSTED_CLUSTER_NAME}
  namespace: ${CLUSTERS_NAMESPACE}
spec:
  arch: ${ARCH_PPC64LE}
  clusterName: ${HOSTED_CLUSTER_NAME}
  management:
    autoRepair: false
    upgradeType: InPlace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    agent:
      agentLabelSelector:
        matchLabels:
          inventory.agent-install.openshift.io/cpu-architecture: ${ARCH_PPC64LE}
    type: Agent
  release:
    image: quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE}
  replicas: 0
~~~
This will create nodepool of architecture `ppc64le` with 0 replicas.

~~~shell
      agentLabelSelector:
        matchLabels:
          inventory.agent-install.openshift.io/cpu-architecture: ppc64le
~~~
This selector block is used to select the agents which are matching given label. Which will ensure it will select only the agents from `ppc64le` arch when it scaled.

## Create Infraenv

For heterogeneous nodepools, you need to create infraenv for each architecture you are going to have in your HCP.
i.e. for heterogeneous nodepools with x86_64 and ppc64le architecture, you will need to create two infraenvs with both architectures.

~~~sh
export SSH_PUB_KEY=$(cat $HOME/.ssh/id_rsa.pub)

envsubst <<"EOF" | oc apply -f -
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: ${HOSTED_CLUSTER_NAME}-${ARCH_X86}
  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}
spec:
  cpuArchitecture: ${ARCH_X86}
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ${SSH_PUB_KEY}
EOF

envsubst <<"EOF" | oc apply -f -
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: ${HOSTED_CLUSTER_NAME}-${ARCH_PPC64LE}
  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}
spec:
  cpuArchitecture: ${ARCH_PPC64LE}
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ${SSH_PUB_KEY}
EOF
~~~
This will create two infraenvs with x86_64 & ppc64le architectures. Before creating this, need to ensure respective architecture's OS image is added in `AgentServiceConfig`

After this you can use the above infraenvs to get the minimal iso and follow the create-agent-cluster.md to add them to your cluster as agents.

## Scale the NodePool

Once your agents are approved, you can scale the nodepools. `agentLabelSelector` configured in nodepool ensures that only matching agents gets added to the cluster.
This also aids in descaling the nodepool. To remove specific arch nodes from the cluster, you can descale the corresponding nodepool.

---

## Source: docs/content/how-to/agent/exposing-services-from-hcp.md

# Exposing the Hosted Control Plane Services

To publish the services from the Hosted Control Plane, we need to understand the available strategies and their implications. Let's explore them.

## Service Publishing Strategies

Let's delve into the motivations for each of the strategies.

The **NodePort** strategy allows you to expose services without requiring a logical LoadBalancer like MetalLB or similar infrastructure. It is one of the simplest methods to implement. This strategy is supported by all services; however, the limitation arises in high availability (HA) environments where you will be pointing to one of the NodePorts instead of all three.

The **LoadBalancer** strategy enables you to expose certain services through a load balancer. While not all services support this strategy, it is the preferred method for exposing the KubeApiServer, as it allows for a single entry point in an HA configuration without relying in the Ingress Controller of the Manamgement cluster.

The **Route** strategy allows you to expose the HostedControlPlane services using the ingress of the Management OpenShift cluster. This strategy is supported by all services but kubeapi-server.

### NodePort

Exposing a service via NodePort is a method used in OpenShift to make a service accessible from outside the cluster. When you expose a service using NodePort, OpenShift allocates a port on each node in the cluster (if the cluster availability policy is set to HighlyAvailable). This port on each node is mapped to the port of the service, allowing external traffic to reach the service by accessing any node's IP address and the allocated NodePort.

This is the default configuration when you use `Agent` and `None` provider platforms. The services relevant for on-premise platforms are:

- APIServer
- OAuthServer
- Konnectivity
- Ignition

!!! Note
    If any of the services are not relevant for your deployment, it is not necessary to specify them.

Here is how it looks in the HostedCluster CR:

```yaml
spec:
...
...
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: <IP_ADDRESS>
        port: <PORT>
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
...
...
```

### Route

OpenShift routes provide a way to expose services within the cluster to external clients. A route in OpenShift maps an external request (typically HTTP/HTTPS) to an internal service. The route specifies a hostname that external clients will use to access the service. OpenShift‚Äôs router (based on HAProxy) will handle the traffic coming to this hostname.

HostedControlPlanes operate in two domains: the Control Plane and the Data Plane. The Control Plane uses routes through the MGMT Cluster ingress to expose services for each of the HostedControlPlanes, and the routes are created in the HostedControlPlane namespace. For the Data Plane, the Ingress handles `*.apps.subdomain.tld` URLs, and all routes under this wildcard are directed to the Namespace by the OpenShift Router on the worker nodes.

The usual configuration for the Hosted Cluster is similar to the LoadBalancer setup we will discuss next.

### LoadBalancer

The LoadBalancer strategy in OpenShift is used to expose services to external clients using an external load balancer. When you create a service of type LoadBalancer, Kubernetes interacts with the underlying cloud platform or appropriate LoadBalancer controllers to provision an external load balancer, which then routes traffic to the service's endpoints (pods).

This is how looks like the most common configuration to expose the services from HCP side:

```yaml
spec:
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
      Route:
        hostname: <URL>
  - service: Konnectivity
    servicePublishingStrate
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
```

If you wanna know more about how to expose the ingress service in the Data Plane side, please access the Recipes section to see how to do it with MetalLB.


---

## Source: docs/content/how-to/agent/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/agent/other-sdn-providers.md

This document explains how to create a HostedCluster that runs an SDN provider different from OVNKubernetes. The document assumes that you already have the required infrastructure in place to create HostedClusters.

!!! important

    The work described here is **not supported**. SDN providers **must** certify their software on HyperShift before it becomes a supported solution. The steps described here are just a technical reference for people who wants to try different SDN providers in HyperShift.

Versions used while writing this doc:

- Management cluster running OpenShift `v4.14.5` and HyperShift Operator version `e87182ca75da37c74b371aa0f17aeaa41437561a`.
- HostedCluster release set to OpenShift `v4.14.10`.

!!! important

    To configure a different CNI provider for the Hosted Cluster, you must adjust the `hostedcluster.spec.networking.networkType` to `Other`. By doing so, the Control Plane Operator will skip the deployment of the default CNI provider.

## Calico
### Deployment

In this scenario we are using the Calico version v3.27.0 which is the last one at the time of this writing. The steps followed rely on the docs by Tigera to deploy Calico on OpenShift.

1. Create a `HostedCluster` and set its `HostedCluster.spec.networking.networkType` to `Other`.

2. Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.

3. Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in `NotReady` state.

    ~~~sh
    export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig
    oc get nodes
    ~~~

    ~~~output
    NAME             STATUS     ROLES    AGE     VERSION
    hosted-worker1   NotReady   worker   2m51s   v1.27.8+4fab27b
    hosted-worker2   NotReady   worker   2m52s   v1.27.8+4fab27b
    ~~~

4. Apply the yaml manifests provided by `Tigera` in the HostedCluster:

    ~~~sh
    mkdir calico
    wget -qO- https://github.com/projectcalico/calico/releases/download/v3.27.0/ocp.tgz | tar xvz --strip-components=1 -C calico
    cd calico/
    ls *crd*.yaml | xargs -n1 oc apply -f
    ls 00* | xargs -n1 oc apply -f
    ls 01* | xargs -n1 oc apply -f
    ls 02* | xargs -n1 oc apply -f
    ~~~

    ~~~output
    customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
    namespace/calico-apiserver created
    namespace/calico-system created
    namespace/tigera-operator created
    apiserver.operator.tigera.io/default created
    installation.operator.tigera.io/default created
    configmap/calico-resources created
    clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
    clusterrole.rbac.authorization.k8s.io/tigera-operator created
    serviceaccount/tigera-operator created
    deployment.apps/tigera-operator created
    ~~~

### Checks

We should see the following pods running in the `tigera-operator` namespace:

  ~~~sh
  oc -n tigera-operator get pods
  ~~~

  ~~~output
  NAME                              READY   STATUS    RESTARTS   AGE
  tigera-operator-dc7c9647f-fvcvd   1/1     Running   0          2m1s
  ~~~

We should see the following pods running in the `calico-system` namespace:

  ~~~sh
  oc -n calico-system get pods
  ~~~

  ~~~output
  NAME                                       READY   STATUS    RESTARTS   AGE
  calico-kube-controllers-69d6d5ff89-5ftcn   1/1     Running   0          2m1s
  calico-node-6bzth                          1/1     Running   0          2m2s
  calico-node-bl4b6                          1/1     Running   0          2m2s
  calico-typha-6558c4c89d-mq2hw              1/1     Running   0          2m2s
  csi-node-driver-l948w                      2/2     Running   0          2m1s
  csi-node-driver-r6rgw                      2/2     Running   0          2m2s
  ~~~

We should see the following pods running in the `calico-apiserver` namespace:

  ~~~sh
  oc -n calico-apiserver get pods
  ~~~

  ~~~output
  NAME                                READY   STATUS    RESTARTS   AGE
  calico-apiserver-7bfbf8fd7c-d75fw   1/1     Running   0          84s
  calico-apiserver-7bfbf8fd7c-fqxjm   1/1     Running   0          84s
  ~~~

The nodes should've moved to `Ready` state:

  ~~~sh
  oc get nodes
  ~~~

  ~~~output
  NAME             STATUS   ROLES    AGE   VERSION
  hosted-worker1   Ready    worker   10m   v1.27.8+4fab27b
  hosted-worker2   Ready    worker   10m   v1.27.8+4fab27b
  ~~~

The HostedCluster deployment will continue, at this point the SDN is running.

## Cilium
### Deployment

In this scenario we are using the Cilium version v1.14.5 which is the last one at the time of this writing. The steps followed rely on the docs by Cilium project to deploy Cilium on OpenShift.

1. Create a `HostedCluster` and set its `HostedCluster.spec.networking.networkType` to `Other`.

2. Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.

3. Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in `NotReady` state.

    ~~~sh
    export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig
    oc get nodes
    ~~~

    ~~~output
    NAME             STATUS     ROLES    AGE     VERSION
    hosted-worker1   NotReady   worker   2m30s   v1.27.8+4fab27b
    hosted-worker2   NotReady   worker   2m33s   v1.27.8+4fab27b
    ~~~

4. Apply the yaml manifests provided by `Isovalent` in the HostedCluster:

    ~~~sh
    #!/bin/bash

    version="1.14.5"
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-03-cilium-ciliumconfigs-crd.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00000-cilium-namespace.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00001-cilium-olm-serviceaccount.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00002-cilium-olm-deployment.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00003-cilium-olm-service.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00004-cilium-olm-leader-election-role.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00005-cilium-olm-role.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00006-leader-election-rolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00007-cilium-olm-rolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00008-cilium-cilium-olm-clusterrole.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00009-cilium-cilium-clusterrole.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00010-cilium-cilium-olm-clusterrolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00011-cilium-cilium-clusterrolebinding.yaml
    ~~~


5. Use the right configuration for each network stack

=== "IPv4"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv4PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: true
      ipv6:
        enabled: false
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv4PodCIDRList:
            - "10.128.0.0/14"
          clusterPoolIPv4MaskSize: "23"
      nativeRoutingCIDR: "10.128.0.0/14"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~

=== "IPv6"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv6PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: false
      ipv6:
        enabled: true
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv6PodCIDRList:
            - "fd01::/48"
          clusterPoolIPv6MaskSize: "48"
      nativeRoutingCIDR: "fd01::/48"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~


=== "Dual stack"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv4PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: true
      ipv6:
        enabled: true
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv4PodCIDRList:
            - "10.128.0.0/14"
          clusterPoolIPv4MaskSize: "23"
      nativeRoutingCIDR: "10.128.0.0/14"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~

!!! important

    Make sure you've changed the networking values according to your platform details `spec.ipam.operator.clusterPoolIPv4PodCIDRList`, `spec.ipam.operator.clusterPoolIPv4MaskSize` and `nativeRoutingCIDR` in IPv4 and `spec.ipam.operator.clusterPoolIPv6PodCIDRList`, `spec.ipam.operator.clusterPoolIPv6MaskSize` and `nativeRoutingCIDR` in IPv6 case.

### Checks

This will be the output:

  ~~~output
  customresourcedefinition.apiextensions.k8s.io/ciliumconfigs.cilium.io created
  namespace/cilium created
  serviceaccount/cilium-olm created
  Warning: would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), hostPort (container "operator" uses hostPort 9443), allowPrivilegeEscalation != false (container "operator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "operator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "operator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "operator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
  deployment.apps/cilium-olm created
  service/cilium-olm created
  role.rbac.authorization.k8s.io/cilium-olm-leader-election created
  role.rbac.authorization.k8s.io/cilium-olm created
  rolebinding.rbac.authorization.k8s.io/leader-election created
  rolebinding.rbac.authorization.k8s.io/cilium-olm created
  clusterrole.rbac.authorization.k8s.io/cilium-cilium-olm created
  clusterrole.rbac.authorization.k8s.io/cilium-cilium created
  clusterrolebinding.rbac.authorization.k8s.io/cilium-cilium-olm created
  clusterrolebinding.rbac.authorization.k8s.io/cilium-cilium created
  ciliumconfig.cilium.io/cilium created
  ~~~

We should see the following pods running in the `cilium` namespace:

  ~~~sh
  oc -n cilium get pods
  ~~~

  ~~~output
  NAME                               READY   STATUS    RESTARTS   AGE
  cilium-ds5tr                       1/1     Running   0          106s
  cilium-olm-7c9cf7c948-txkvt        1/1     Running   0          2m36s
  cilium-operator-595594bf7d-gbnns   1/1     Running   0          106s
  cilium-operator-595594bf7d-mn5wc   1/1     Running   0          106s
  cilium-wzhdk                       1/1     Running   0          106s
  ~~~

The nodes should've moved to `Ready` state:

  ~~~sh
  oc get nodes
  ~~~

  ~~~output
  NAME             STATUS   ROLES    AGE   VERSION
  hosted-worker1   Ready    worker   8m    v1.27.8+4fab27b
  hosted-worker2   Ready    worker   8m    v1.27.8+4fab27b
  ~~~

The HostedCluster deployment will continue, at this point the SDN is running.

### Validation

Additionally you have some conformance tests that could be deployed into the HostedCluster in order to validate if the Cilium SDN was deployed and working properly.

In order for Cilium connectivity test pods to run on OpenShift, a simple custom SecurityContextConstraints object is required to allow hostPort/hostNetwork which the connectivity test pods relies on. it should only set allowHostPorts and allowHostNetwork without any other privileges.

  ~~~sh
  oc apply -f - <<EOF
  apiVersion: security.openshift.io/v1
  kind: SecurityContextConstraints
  metadata:
    name: cilium-test
  allowHostPorts: true
  allowHostNetwork: true
  users:
    - system:serviceaccount:cilium-test:default
  priority: null
  readOnlyRootFilesystem: false
  runAsUser:
    type: MustRunAsRange
  seLinuxContext:
    type: MustRunAs
  volumes: null
  allowHostDirVolumePlugin: false
  allowHostIPC: false
  allowHostPID: false
  allowPrivilegeEscalation: false
  allowPrivilegedContainer: false
  allowedCapabilities: null
  defaultAddCapabilities: null
  requiredDropCapabilities: null
  groups: null
  EOF
  ~~~

  ~~~sh
  version="1.14.5"
  oc apply -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/${version}/examples/kubernetes/connectivity-check/connectivity-check.yaml
  ~~~

  ~~~sh
  oc get pod -n cilium-test
  ~~~

  ~~~output
  NAME                                                     READY   STATUS    RESTARTS   AGE
  echo-a-846dcb4-kq7zh                                     1/1     Running   0          23h
  echo-b-58f67d5b86-5mrtx                                  1/1     Running   0          23h
  echo-b-host-84d7468c8d-nf4vk                             1/1     Running   0          23h
  host-to-b-multi-node-clusterip-b98ff785c-b9vgf           1/1     Running   0          23h
  host-to-b-multi-node-headless-5c55d85dfc-5xjbc           1/1     Running   0          23h
  pod-to-a-6b996b7675-46kkf                                1/1     Running   0          23h
  pod-to-a-allowed-cnp-c958b55bf-6vskb                     1/1     Running   0          23h
  pod-to-a-denied-cnp-6d9b8cbff5-lbrgp                     1/1     Running   0          23h
  pod-to-b-intra-node-nodeport-5f9c4c866f-mhfs4            1/1     Running   0          23h
  pod-to-b-multi-node-clusterip-7cb4bf5495-hmmtg           1/1     Running   0          23h
  pod-to-b-multi-node-headless-68975fc557-sqbgq            1/1     Running   0          23h
  pod-to-b-multi-node-nodeport-559c54c6fc-2rhvv            1/1     Running   0          23h
  pod-to-external-1111-5c4cfd9497-6slss                    1/1     Running   0          23h
  pod-to-external-fqdn-allow-google-cnp-7d65d9b747-w4cx5   1/1     Running   0          23h
  ~~~


---

## Source: docs/content/how-to/audit-log-persistence.md

# Audit Log Persistence

The Audit Log Persistence feature provides persistent storage for kube-apiserver audit logs in hosted clusters, along with automatic snapshot creation for crash recovery. This feature enables management cluster administrators to ensure audit logs survive pod restarts and crashes, making them available for compliance, security auditing, and troubleshooting purposes.

## Overview

When enabled, the audit log persistence feature:

- Creates PersistentVolumeClaims (PVCs) for each kube-apiserver pod to store audit logs
- Configures kube-apiserver to write audit logs to persistent storage
- Automatically creates VolumeSnapshots when kube-apiserver pods crash or restart
- Manages snapshot retention based on configurable policies

## Enabling the Feature

The audit log persistence feature must be enabled at two levels:

### 1. Operator-Level Enablement

Enable the feature during HyperShift operator installation using the `--enable-audit-log-persistence` flag:

```bash
hypershift install --enable-audit-log-persistence
```

This flag:

- Installs the `AuditLogPersistenceConfig` CRD
- Sets the `ENABLE_AUDIT_LOG_PERSISTENCE=true` environment variable on the HyperShift operator deployment
- Registers mutating webhooks for Pods and ConfigMaps (requires webhook certificates)
- Enables the snapshot controller that watches for pod crashes

### 2. Configuration-Level Enablement

After the operator is installed, create an `AuditLogPersistenceConfig` resource to enable and configure the feature:

```yaml
apiVersion: auditlogpersistence.hypershift.openshift.io/v1alpha1
kind: AuditLogPersistenceConfig
metadata:
  name: cluster
spec:
  enabled: true
```

**Important**: The `AuditLogPersistenceConfig` resource must be named `cluster` and is cluster-scoped. Only one instance can exist in the management cluster.

## Configuration Options

The `AuditLogPersistenceConfig` resource provides comprehensive configuration options organized into three main sections:

### Global Enablement

- **`spec.enabled`** (boolean, default: `false`)
  - Enables or disables the audit log persistence feature globally
  - When disabled, no PVCs will be created and no snapshots will be taken
  - Must be set to `true` to activate the feature

### Storage Configuration (`spec.storage`)

Controls how PersistentVolumeClaims are created for audit log storage:

- **`spec.storage.storageClassName`** (string, optional)
  - The name of the StorageClass to use for PVCs
  - If not specified, the default storage class will be used
  - Example: `"gp3-csi"`

- **`spec.storage.size`** (string, default: `"5Gi"`)
  - The size of each PVC created for kube-apiserver pods
  - Must be a valid Kubernetes quantity (e.g., `"5Gi"`, `"10Gi"`, `"100G"`)
  - Each kube-apiserver pod gets its own PVC

### Audit Log Configuration (`spec.auditLog`)

Controls kube-apiserver audit log settings:

- **`spec.auditLog.maxSize`** (integer, default: `200`)
  - Maximum size in megabytes of the audit log file before rotation
  - Corresponds to the `--audit-log-maxsize` kube-apiserver argument
  - Minimum value: `1`
  - Example: `200` (200 MB)

- **`spec.auditLog.maxBackup`** (integer, default: `10`)
  - Maximum number of old audit log files to retain
  - Corresponds to the `--audit-log-maxbackup` kube-apiserver argument
  - Minimum value: `1`
  - Example: `10` (keeps 10 rotated log files)

### Snapshot Configuration (`spec.snapshots`)

Controls automatic snapshot creation and retention:

- **`spec.snapshots.enabled`** (boolean, default: `true`)
  - Enables or disables automatic snapshot creation on pod crashes
  - When enabled, snapshots are created when kube-apiserver pod restart count increases
  - Can be disabled independently of the main feature

- **`spec.snapshots.minInterval`** (string, default: `"1h"`)
  - Minimum time interval between snapshots for the same pod
  - Prevents creating too many snapshots in rapid succession
  - Must be a valid duration string (e.g., `"1h"`, `"30m"`, `"2h30m"`)
  - Format: `^([0-9]+(ns|us|¬µs|ms|s|m|h))+$`
  - Example: `"1h"` (one hour minimum between snapshots)

- **`spec.snapshots.perPodRetentionCount`** (integer, default: `10`)
  - Maximum number of snapshots to retain per PVC
  - When this limit is reached, the oldest snapshot for that PVC is deleted
  - Minimum value: `1`
  - Example: `10` (keeps 10 snapshots per pod)

- **`spec.snapshots.namespaceRetentionCount`** (integer, default: `50`)
  - Maximum total number of snapshots to retain per namespace (hosted control plane)
  - When this limit is reached, the oldest snapshot in the namespace is deleted
  - Minimum value: `1`
  - Example: `50` (keeps 50 snapshots total per hosted cluster)

- **`spec.snapshots.volumeSnapshotClassName`** (string, optional)
  - The name of the VolumeSnapshotClass to use for creating snapshots
  - If not specified, the system attempts to match the PVC's StorageClass provisioner to an appropriate VolumeSnapshotClass
  - Example: `"csi-aws-vsc"`

## Complete Configuration Example

Here's a complete example configuration with all options specified:

```yaml
apiVersion: auditlogpersistence.hypershift.openshift.io/v1alpha1
kind: AuditLogPersistenceConfig
metadata:
  name: cluster
spec:
  enabled: true
  storage:
    storageClassName: "gp3-csi"
    size: "10Gi"
  auditLog:
    maxSize: 500      # 500 MB per log file
    maxBackup: 20     # Keep 20 rotated log files
  snapshots:
    enabled: true
    minInterval: "2h"                    # Minimum 2 hours between snapshots
    perPodRetentionCount: 15             # Keep 15 snapshots per pod
    namespaceRetentionCount: 100         # Keep 100 snapshots per namespace
    volumeSnapshotClassName: "csi-aws-vsc"
```

## How It Works

### Pod Mutation

When a kube-apiserver pod is created in a hosted control plane namespace:

1. The pod mutating webhook intercepts the pod creation
2. Checks if the feature is enabled via `AuditLogPersistenceConfig`
3. Creates a PVC named `kas-audit-logs-<pod-name>` if it doesn't exist
4. Replaces the ephemeral `logs` volume with a PVC-backed volume
5. Sets the PVC owner reference to the pod's ReplicaSet for proper cleanup

### ConfigMap Mutation

When the `kas-config` ConfigMap is created or updated:

1. The ConfigMap mutating webhook intercepts the change
2. Checks if the feature is enabled
3. Updates the kube-apiserver configuration JSON to include:
   - `--audit-log-maxsize` based on `spec.auditLog.maxSize`
   - `--audit-log-maxbackup` based on `spec.auditLog.maxBackup`

### Snapshot Creation

The snapshot controller watches kube-apiserver pods and:

1. Detects when a pod's restart count increases (indicating a crash)
2. Checks if snapshots are enabled and minimum interval has passed
3. Creates a VolumeSnapshot with a timestamped name: `<pvc-name>-snapshot-<timestamp>`
4. Labels the snapshot with:
   - `hypershift.openshift.io/audit-logs-pvc`: PVC name
   - `hypershift.openshift.io/audit-logs-pod`: Pod name
   - `hypershift.openshift.io/hosted-control-plane-namespace`: Namespace
5. Updates pod annotations with the last snapshotted restart count and timestamp
6. Manages retention by deleting oldest snapshots when limits are exceeded

## Default Values Summary

| Configuration | Default Value |
|---------------|---------------|
| `spec.enabled` | `false` |
| `spec.storage.size` | `"5Gi"` |
| `spec.storage.storageClassName` | (uses default StorageClass) |
| `spec.auditLog.maxSize` | `200` (MB) |
| `spec.auditLog.maxBackup` | `10` |
| `spec.snapshots.enabled` | `true` |
| `spec.snapshots.minInterval` | `"1h"` |
| `spec.snapshots.perPodRetentionCount` | `10` |
| `spec.snapshots.namespaceRetentionCount` | `50` |
| `spec.snapshots.volumeSnapshotClassName` | (auto-detected) |

## Requirements

- HyperShift operator with `--enable-audit-log-persistence` flag
- Webhook certificates configured (for mutating webhooks)
- StorageClass available in hosted control plane namespaces
- VolumeSnapshotClass available (for snapshot functionality)
- CSI snapshotter installed (for VolumeSnapshot support)

## Troubleshooting

### Feature Not Working

1. Verify the operator has `ENABLE_AUDIT_LOG_PERSISTENCE=true` environment variable:
   ```bash
   kubectl get deployment -n hypershift hypershift-operator -o jsonpath='{.spec.template.spec.containers[0].env}' | grep ENABLE_AUDIT_LOG_PERSISTENCE
   ```

2. Check that `AuditLogPersistenceConfig` exists and is enabled:
   ```bash
   kubectl get auditlogpersistenceconfig cluster -o yaml
   ```

3. Verify webhooks are registered (check operator logs):
   ```bash
   kubectl logs -n hypershift deployment/hypershift-operator | grep "Audit log persistence"
   ```

### PVCs Not Created

- Ensure the feature is enabled at both operator and configuration levels
- Check webhook logs for errors
- Verify StorageClass exists and is accessible in hosted control plane namespaces

### Snapshots Not Created

- Verify `spec.snapshots.enabled` is `true`
- Check that VolumeSnapshotClass is available
- Review snapshot controller logs for errors
- Ensure minimum interval has passed since last snapshot

### Storage Issues

- Verify StorageClass has sufficient capacity
- Check PVC status: `kubectl get pvc -n <hosted-control-plane-namespace>`
- Review storage provider logs for provisioning errors

## Disabling the Feature

To disable the feature:

1. Set `spec.enabled: false` in the `AuditLogPersistenceConfig`:
   ```bash
   kubectl patch auditlogpersistenceconfig cluster --type=merge -p '{"spec":{"enabled":false}}'
   ```

2. Existing PVCs will remain but new ones won't be created
3. To fully remove, delete the `AuditLogPersistenceConfig` resource (after disabling)

**Note**: Disabling at the operator level (removing `--enable-audit-log-persistence`) requires operator redeployment and will prevent the CRD and webhooks from functioning.



---

## Source: docs/content/how-to/automated-machine-management/configure-machines.md

# Configuring Machines in HyperShift

In standalone OpenShift, machine configuration is managed via resources like MachineConfig, KubeletConfig, and ContainerRuntimeConfig inside the cluster, and then applied to a set of machines via a MachineConfigPool. In HyperShift clusters, machine configuration is not managed from within a hosted cluster, but rather via a NodePool resource in the hosting cluster.

The NodePool field `.spec.config` can be populated with a list of references to configmaps that contain MachineConfig, KubeletConfig, or ContainerRuntimeConfig manifests. This configuration is applied to all machines belonging to that NodePool.

## Creating a NodePool with custom configuration

### 1. Create MachineConfig and ContainerRuntimeConfig manifests

```shell
cat <<EOF > machine-config.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: example-machineconfig
spec:
  config:
    storage:
      files:
        - path: /etc/custom.cfg
          mode: 420
          contents:
            source: data:text/plain;charset=utf-8;base64,ZXhhbXBsZSBjb25maWd1cmF0aW9u
  systemd:
    units:
      - name: example-service.service
        enabled: true
        contents: |
          [Unit]
          Description=Example Service
          
          [Service]
          ExecStart=/usr/local/bin/example-service
          Restart=always
EOF

cat <<EOF > cr-config.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
  name: example-container-runtime-config
spec:
  containerRuntimeConfig:
    pidsLimit: 30
EOF
```

### 2. Create ConfigMaps for every configuration resource

```shell
oc create configmap example-machineconfig -n clusters --from-file config=machine-config.yaml 
oc create configmap example-crc -n clusters --from-file config=cr-config.yaml
```

NOTE: the key in each ConfigMap must be `config`

### 3. Create a NodePool that references these configurations

```shell
cat <<EOF > custom-nodepool.yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: custom-nodepool
  namespace: clusters
spec:
  arch: amd64
  clusterName: example-cluster
  config:
  - name: example-machineconfig
  - name: example-crc
  management:
    autoRepair: true
    replace:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      strategy: RollingUpdate
    upgradeType: Replace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    aws:
      instanceProfile: example-profile
      instanceType: m5.large
      rootVolume:
        size: 120
        type: gp3
    type: AWS
  release:
    image: quay.io/openshift-release-dev/ocp-release:4.14.6-x86_64
  replicas: 2
```

```shell
oc apply -f custom-nodepool.yaml
```

---

## Source: docs/content/how-to/automated-machine-management/haproxy-image-override.md

# Customize Worker Node HAProxy Image

This guide explains how to customize the HAProxy image used for worker node API server proxy on a per-NodePool basis.

## Overview

Worker nodes in HyperShift use HAProxy to proxy connections to the hosted control plane API server. By default, the HAProxy image comes from the OpenShift release payload. However, you can override this image using either:

1. **NodePool annotation** (recommended for per-NodePool customization)
2. **Environment variable** (for global override when shared ingress is enabled)

## Image Resolution Priority

The HAProxy image is resolved in the following priority order (highest to lowest):

1. **NodePool annotation** `hypershift.openshift.io/haproxy-image` (highest priority)
2. **Environment variable** `IMAGE_SHARED_INGRESS_HAPROXY` (when shared ingress is enabled)
3. **Hardcoded default** (when shared ingress is enabled)
4. **Release payload** (default when shared ingress is disabled)

## Per-NodePool Customization

### Use Case

Use the NodePool annotation when you want to:
- Test a newer HAProxy version on a specific NodePool
- Use different HAProxy images for different workload types
- Gradually roll out HAProxy updates across NodePools
- Use a custom HAProxy image with specific patches or configurations

### Configuration

To override the HAProxy image for a specific NodePool, add the `hypershift.openshift.io/haproxy-image` annotation:

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: my-nodepool
  namespace: clusters
  annotations:
    hypershift.openshift.io/haproxy-image: "quay.io/my-org/haproxy:custom-v2.9.1"
spec:
  clusterName: my-cluster
  replicas: 3
  # ... rest of spec
```

### Applying the Annotation

You can add the annotation to an existing NodePool using `kubectl annotate`:

```bash
kubectl annotate nodepool my-nodepool \
  -n clusters \
  hypershift.openshift.io/haproxy-image="quay.io/my-org/haproxy:custom-v2.9.1"
```

### Removing the Override

To remove the override and revert to the default behavior:

```bash
kubectl annotate nodepool my-nodepool \
  -n clusters \
  hypershift.openshift.io/haproxy-image-
```

## Global Override (Shared Ingress Only)

When shared ingress is enabled, you can set a global HAProxy image override using the `IMAGE_SHARED_INGRESS_HAPROXY` environment variable on the HyperShift operator. This affects all NodePools that don't have the annotation set.

**Note**: The NodePool annotation always takes precedence over the environment variable.

## Verification

After applying the annotation, new worker nodes will use the specified HAProxy image. To verify:

1. Check the NodePool's token secret generation to ensure the new configuration is picked up
2. Verify the ignition configuration contains the correct image
3. On a worker node, check the static pod manifest:

```bash
# On a worker node
cat /etc/kubernetes/manifests/kube-apiserver-proxy.yaml | grep image:
```

## Rollout Behavior

The HAProxy image change triggers a NodePool rollout:
- New ignition configuration is generated with the updated image
- Worker nodes are replaced according to the NodePool's upgrade strategy
- The rollout respects `maxUnavailable` settings

## Important Notes

1. **Image Availability**: Ensure the custom HAProxy image is accessible from worker nodes
2. **Pull Secrets**: The worker nodes must have credentials to pull the custom image
3. **Compatibility**: The custom HAProxy image must be compatible with HyperShift's configuration expectations
4. **Shared Ingress**: When shared ingress is enabled, ensure the custom image supports proxy protocol v2 with TLV (requires HAProxy v2.9+)
5. **Multiple NodePools**: Each NodePool can have a different HAProxy image override


## Troubleshooting

### Image Pull Errors

If worker nodes fail to pull the custom image:
1. Verify the image exists and is accessible
2. Check that the global pull secret includes credentials for the image registry
3. Verify network connectivity from worker nodes to the image registry

### Wrong Image in Use

If the expected image is not being used:
1. Check the annotation is correctly set on the NodePool
2. Verify the NodePool has reconciled (check status conditions)
3. Inspect the ignition configuration in the NodePool's token secret

### Rollout Issues

If the rollout doesn't complete:
1. Check NodePool conditions for errors
2. Verify the custom image is compatible
3. Check worker node logs for HAProxy startup failures
4. Ensure the image supports the required features (e.g., proxy protocol for shared ingress)

---

## Source: docs/content/how-to/automated-machine-management/index.md

---
title: Automated Machine Management
---

This section of the HyperShift documentation takes care of the Automated Machine management actions which could be triggered in order to help the customer with the daily basis duties. It includes the implications, caveats, limitations, etc...


---

## Source: docs/content/how-to/automated-machine-management/node-tuning.md

---
title: Node tuning
---

# Node tuning

Manage node-level tuning with the Node Tuning Operator.

## Creating a simple TuneD profile for setting sysctl settings
If you would like to set some node-level tuning on the nodes in your hosted cluster, you can use the Node Tuning Operator. In HyperShift, node tuning can be configured by creating ConfigMaps which contain Tuned objects, and referencing these ConfigMaps in your NodePools.

1. Create a ConfigMap which contains a valid Tuned manifest and reference it in a NodePool. The example Tuned manifest below defines a profile which sets `vm.dirty_ratio` to 55, on Nodes which contain the Node label `tuned-1-node-label` with any value.

    Save the ConfigMap manifest in a file called `tuned-1.yaml`:
    ```
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: tuned-1
      namespace: clusters
    data:
      tuning: |
        apiVersion: tuned.openshift.io/v1
        kind: Tuned
        metadata:
          name: tuned-1
          namespace: openshift-cluster-node-tuning-operator
        spec:
          profile:
          - data: |
              [main]
              summary=Custom OpenShift profile
              include=openshift-node

              [sysctl]
              vm.dirty_ratio="55"
            name: tuned-1-profile
          recommend:
          - priority: 20
            profile: tuned-1-profile
    ```
    > **_NOTE:_**  In the case where no labels are added to an entry in the `spec.recommend` section of the Tuned spec, NodePool based matching is assumed, so the highest priority profile in the `spec.recommend` section will be applied to Nodes in the pool. While more fine-grained Node label based matching is still possible by setting a label value in the Tuned `.spec.recommend.match`, users should be aware that Node labels will not persist during an upgrade, unless the NodePool `.spec.management.upgradeType` is set to `InPlace`.

    Create the ConfigMap in the management cluster:
    ```
    oc --kubeconfig="$MGMT_KUBECONFIG" create -f tuned-1.yaml
    ```

    Reference the ConfigMap in the NodePools `spec.tuningConfig` field, either by editing an existing NodePool or creating a new NodePool. In this example we assume we only have one NodePool called `nodepool-1`, containing 2 Nodes.
    ```
    apiVersion: hypershift.openshift.io/v1alpha1
    kind: NodePool
    metadata:
      ...
      name: nodepool-1
      namespace: clusters
    ...
    spec:
      ...
      tuningConfig:
      - name: tuned-1
    status:
    ...
    ```
    > **_NOTE:_**  You may reference the same ConfigMap in multiple NodePools. In HyperShift, NTO will append a hash of the NodePool name and namespace to the name of the Tuneds to distinguish them. Outside of this case, users should be careful not to create multiple Tuned profiles of the same name in different Tuneds for the same hosted cluster.

2. Now that the ConfigMap containing a Tuned manifest has been created and referenced in a NodePool, the Node Tuning Operator will sync the Tuned objects into the hosted cluster. You can check which Tuneds are defined and which profiles are set for each Node.

    List the Tuned objects in the hosted cluster:
    ```
    oc --kubeconfig="$HC_KUBECONFIG" get Tuneds -n openshift-cluster-node-tuning-operator
    ```

    Example output:
    ```
    NAME       AGE
    default    7m36s
    rendered   7m36s
    tuned-1    65s
    ```
   
    List the Profiles in the hosted cluster:
    ```
    oc --kubeconfig="$HC_KUBECONFIG" get Profiles -n openshift-cluster-node-tuning-operator
    ```

    Example output:
    ```
    NAME                           TUNED            APPLIED   DEGRADED   AGE
    nodepool-1-worker-1            tuned-1-profile  True      False      7m43s
    nodepool-1-worker-2            tuned-1-profile  True      False      7m14s
    ```

    As we can see, both worker nodes in the NodePool have the tuned-1-profile applied. Note that if no custom profiles are created, the `openshift-node` profile will be applied by default.


3. To confirm the tuning was applied correctly, we can start a debug shell on a Node and check the sysctl values:
    ```
    oc --kubeconfig="$HC_KUBECONFIG" debug node/nodepool-1-worker-1 -- chroot /host sysctl vm.dirty_ratio
    ```

    Example output:
    ```
    vm.dirty_ratio = 55
    ```

## Applying tuning which requires kernel boot parameters 
You can also use the Node Tuning Operator for more complex tuning which requires setting kernel boot parameters. 
As an example, the following steps can be followed to create a NodePool with huge pages reserved.

1. Create the following ConfigMap which contains a Tuned object manifest for creating 10 hugepages of size 2M.

    Save this ConfigMap manifest in a file called `tuned-hugepages.yaml`:
    ```
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: tuned-hugepages
      namespace: clusters
    data:
      tuning: |
        apiVersion: tuned.openshift.io/v1
        kind: Tuned
        metadata:
          name: hugepages
          namespace: openshift-cluster-node-tuning-operator
        spec:
          profile:
          - data: |
              [main]
              summary=Boot time configuration for hugepages
              include=openshift-node
              [bootloader]
              cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50
            name: openshift-node-hugepages
          recommend:
          - priority: 20
            profile: openshift-node-hugepages
    ```
    > **_NOTE:_**  The `.spec.recommend.match` field is intentionally left blank. In this case this Tuned will be applied to all Nodes in the NodePool where this ConfigMap is referenced. It is advised to group Nodes with the same hardware configuration into the same NodePool. Not following this practice might result in TuneD operands calculating conflicting kernel parameters for two or more nodes sharing the same NodePool.

    Create the ConfigMap in the management cluster:
    ```
    oc --kubeconfig="$MGMT_KUBECONFIG" create -f tuned-hugepages.yaml
    ```

2. Create a new NodePool manifest YAML file, customize the NodePools upgrade type, and reference the previously created ConfigMap in the `spec.tuningConfig` section before creating it in the management cluster.

    Create the NodePool manifest and save it in a file called `hugepages-nodepool.yaml`:
    ```
    NODEPOOL_NAME=hugepages-example
    INSTANCE_TYPE=m5.2xlarge
    NODEPOOL_REPLICAS=2

    hypershift create nodepool aws \
      --cluster-name $CLUSTER_NAME \
      --name $NODEPOOL_NAME \
      --replicas $NODEPOOL_REPLICAS \
      --instance-type $INSTANCE_TYPE \
      --render > hugepages-nodepool.yaml
    ```

    Edit `hugepages-nodepool.yaml`. Set `.spec.management.upgradeType` to `InPlace`, and set `.spec.tuningConfig` to reference the `tuned-hugepages` ConfigMap you created.
    ```
    apiVersion: hypershift.openshift.io/v1alpha1
    kind: NodePool
    metadata:
      name: hugepages-nodepool
      namespace: clusters
      ...
    spec:
      management:
        ...
        upgradeType: InPlace
      ...
      tuningConfig:
      - name: tuned-hugepages
    ```
    > **_NOTE:_**  Setting `.spec.management.upgradeType` to `InPlace` is recommended to avoid unnecessary Node recreations when applying the new MachineConfigs. With the `Replace` upgrade type, Nodes will be fully deleted and new nodes will replace them when applying the new kernel boot parameters that are calculated by the TuneD operand.

    Create the NodePool in the management cluster:
    ```
    oc --kubeconfig="$MGMT_KUBECONFIG" create -f hugepages-nodepool.yaml
    ```


3. After the Nodes become available, the containerized TuneD daemon will calculate the required kernel boot parameters based on the applied TuneD profile. After the Nodes become `Ready` and reboot once to apply the generated MachineConfig, you can verify that the Tuned profile is applied and that the kernel boot parameters have been set.

    List the Tuned objects in the hosted cluster:
    ```
    oc --kubeconfig="$HC_KUBECONFIG" get Tuneds -n openshift-cluster-node-tuning-operator
    ```

    Example output:
    ```
    NAME                 AGE
    default              123m
    hugepages-8dfb1fed   1m23s
    rendered             123m
    ```
   
    List the Profiles in the hosted cluster:
    ```
    oc --kubeconfig="$HC_KUBECONFIG" get Profiles -n openshift-cluster-node-tuning-operator
    ```

    Example output:
    ```
    NAME                           TUNED                      APPLIED   DEGRADED   AGE
    nodepool-1-worker-1            openshift-node             True      False      132m
    nodepool-1-worker-2            openshift-node             True      False      131m
    hugepages-nodepool-worker-1    openshift-node-hugepages   True      False      4m8s
    hugepages-nodepool-worker-2    openshift-node-hugepages   True      False      3m57s
    ```

    Both worker nodes in the new NodePool have the `openshift-node-hugepages` profile applied.


4. To confirm the tuning was applied correctly, we can start a debug shell on a Node and check `/proc/cmdline`
    ```
    oc --kubeconfig="$HC_KUBECONFIG" debug node/nodepool-1-worker-1 -- chroot /host cat /proc/cmdline
    ```

    Example output:
    ```
    BOOT_IMAGE=(hd0,gpt3)/ostree/rhcos-... hugepagesz=2M hugepages=50
    ```

## How to debug Node Tuning issues
If you face issues with Node Tuning, first check the Condition `ValidTuningConfig` in the NodePool that references your Tuned config. This reports any issue that may prevent the configuration load.
```
- lastTransitionTime: "2023-03-06T14:30:35Z"
  message: ConfigMap "tuned" not found
  observedGeneration: 2
  reason: ValidationFailed
  status: "False"
  type: ValidTuningConfig
```

If the NodePool condition shows no issues, it means that the configuration has been loaded and propagated to the NodePool. You can then check the status of the relevant `Profile` Custom Resource in your HostedCluster. In the conditions you should see if the configuration has been applied successfully and whether there are any outstanding Warning or Errors. An example can be seen below.
```
status:
  bootcmdline: ""
  conditions:
  - lastTransitionTime: "2023-03-06T14:22:14Z"
    message: The TuneD daemon profile not yet applied, or application failed.
    reason: Failed
    status: "False"
    type: Applied
  - lastTransitionTime: "2023-03-06T14:22:14Z"
    message: 'TuneD daemon issued one or more error message(s) during profile application.
      TuneD stderr:  ERROR    tuned.daemon.controller: Failed to reload TuneD: Cannot
      load profile(s) ''tuned-1-profile'': Cannot find profile ''openshift-node-notexistin''
      in ''[''/etc/tuned'', ''/usr/lib/tuned'']''.'
    reason: TunedError
    status: "True"
    type: Degraded
  tunedProfile: tuned-1-profile
```


---

## Source: docs/content/how-to/automated-machine-management/nodepool-lifecycle.md

---
title: NodePool lifecycle
---

# NodePool lifecycle

NodePools represent homogeneous groups of Nodes with a common lifecycle management and upgrade cadence.

## Upgrades and data propagation

There are three main areas that will trigger rolling upgrades across the Nodes when they are changed:

- OCP Version dictated by `spec.release`.
- Machine configuration via `spec.config`, a knob for `machineconfiguration.openshift.io`.
- Platform specific changes via `.spec.platform`. Some fields might be immutable whereas other might allow changes e.g. aws instance type.

Some cluster config changes (e.g. proxy, certs) may also trigger a rolling upgrade if the change needs to be propagated to the node.

NodePools support two types of rolling upgrades: Replace and InPlace, specified via UpgradeType.

!!! important

    You cannot switch the UpgradeType once the NodePool is created. You must specify UpgradeType during NodePool 
    creation. Modifying the field after the fact may cause nodes to become unmanaged.

### Replace Upgrades

This will create new instances in the new version while removing old nodes in a rolling fashion. This is usually a good choice in cloud environments where this level of immutability is cost effective.

### InPlace Upgrades

This will directly perform updates to the Operating System of the existing instances. This is usually a good choice for environments where the infrastructure constraints are higher e.g. bare metal.

When you are using in place upgrades, Platform specific changes will only affect upcoming new Nodes.

### Data propagation

There some fields which will only propagate in place regardless of the upgrade strategy that is set.
`.spec.nodeLabels` and `.spec.taints` will be propagated only to new upcoming machines.


## Triggering Upgrades examples

### Upgrading to a new OCP version

These upgrades can be triggered via changing the `spec.release.image` of the NodePool. Note that you should only upgrade NodePools to the current version of the Hosted Control Plane.

### Adding a new MachineConfig

You can create a MachineConfig inside a ConfigMap in the management cluster as follows:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${CONFIGMAP_NAME}
  namespace: clusters
data:
  config: |
    apiVersion: machineconfiguration.openshift.io/v1
    kind: MachineConfig
    metadata:
      labels:
        machineconfiguration.openshift.io/role: worker
      name: ${MACHINECONFIG_NAME}
    spec:
      config:
        ignition:
          version: 3.2.0
        storage:
          files:
          - contents:
              source: data:...
            mode: 420
            overwrite: true
            path: ${PATH}
```

Once that is applied to the cluster, you can specify that to the NodePool via:

```
spec:
  config:
    - name: ${CONFIGMAP_NAME}
```

## Scale Down

Scaling a NodePool down will remove Nodes from the hosted cluster.

### Scaling To Zero

Node(s) can become stuck when removing all Nodes from a cluster (scaling NodePool(s) down to zero) because the Node(s) cannot be drained successfully from the cluster.

Several conditions can prevent Node(s) from being drained successfully:

- The hosted cluster contains `PodDisruptionBudgets` that require at least 
- The hosted cluster contains pods that use `PersistentVolumes``

#### Prevention

To prevent Nodes from becoming stuck when scaling down, set the `.spec.nodeDrainTimeout` and `.spec.nodeVolumeDetachTimeout` in the NodePool CR to a value greater than `0s`. 

This forces Nodes to be removed once the timeout specified in the field has been reached, regardless of whether the node can be drained or the volumes can be detached successfully.

!!! note
    See the Hypershift API reference page for more details.


---

## Source: docs/content/how-to/automated-machine-management/performance-profiling.md

# Manage node-level performance profiling with Performance Profile Controller

## Intro

The `Performance Profile controller`, formerly known as `Performance-Addon Operator`, is now part of `Node Tuning Operator`.
Performance Profile controller allows you to optimize nodes in your hosted cluster for applications sensitive to CPU and network latency.

In HyperShift, node profiling can be configured by creating ConfigMaps which contain `PerformanceProfile` objects, and referencing these ConfigMaps in your NodePools.

## Steps

1. Create a ConfigMap which contains a valid `PerformanceProfile` manifest and reference it in a NodePool.
   The example `PerformanceProfile` manifest defines several parameters like how to partition your CPUs into housekeeping and workload partitions, what topology policy to use, and more.
   Save this ConfigMap as `perfprof-1.yaml`.

    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: perfprof-1
      namespace: clusters
    data:
      tuning: |
        apiVersion: performance.openshift.io/v2
        kind: PerformanceProfile
        metadata:
          name: performance
        spec:
            additionalKernelArgs:
            - nmi_watchdog=0
            - audit=0
            - mce=off
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - idle=poll
            cpu:
                isolated: "1,3,5,7,9,11,13,15,17,19-39,41,43,45,47,49,51,53,55,57"
                reserved: "0,2,4,6,8,10,12,14,16,18,40,42,44,46,48,50,52,54,56,58"
                offlined: "59-79"
            nodeSelector:
                node-role.kubernetes.io/worker-cnf: ""
            numa:
                topologyPolicy: single-numa-node
            realTimeKernel:
                enabled: true
            workloadHints:
                highPowerConsumption: true
                realTime: true
    ```

    > **_NOTE:_** See PerformanceProfile Creator for help in creating this `PerformanceProfile` manifests.

2. Create the ConfigMap in the management cluster:

    ```bash
    oc --kubeconfig="$MGMT_KUBECONFIG" create -f perfprof-1.yaml
    ```

3. Reference the ConfigMap in the NodePools `spec.tuningConfig` field, either by editing an existing NodePool or creating a new NodePool. In this example we assume we only have one NodePool called `nodepool-1`, containing 2 Nodes.

    ```yaml
    apiVersion: hypershift.openshift.io/v1alpha1
    kind: NodePool
    metadata:
      ...
      name: nodepool-1
      namespace: clusters
    ...
    spec:
      ...
      tuningConfig:
      - name: perfprof-1
    status:
    ...
    ```

    > **_NOTE:_**  Remember one NodePool can only reference at most one `PerformanceProfile`

4. Now that the ConfigMap containing a `PerformanceProfile` manifest has been created and referenced in a NodePool, the Performance Profile Controller will handle the `PerformanceProfile` object and will create the following elements:
    - `MachineConfig`: This will be embedded into a ConfigMap that will be handled by NTO as any other ConfigMap with MachineConfig embedded (see this PR for further info)
    - `KubeletConfig`: This will be embedded into a ConfigMap that will be handled by NTO in a similar way as a ConfigMap with a MachineConfig embedded
    - `Tuned`: This will be embedded into a ConfigMap and so handled directly by the NTO.
    - `RuntimeClass`: This will be created directly inside the guest cluster.


---

## Source: docs/content/how-to/automated-machine-management/scale-to-zero-dataplane.md

---
title: Scaling down data plane to Zero
---

# Scaling down data plane to Zero

## Context and Considerations

The main reason to go through this scenario it's mainly to save resources and money, once you are not using an already created Hosted Control Plane.

In order to continue with the next steps, we need to have in mind some considerations:

- **This is a destructive action**, all the workloads in the worker nodes will disappear.
- The Hosted Control Plane will stay up and running, and you can scale up the *NodePool* whenever you want.
- Some pods in the control plane will stay in "Pending" state.  
- Once you rescale the *NodePool/s* it will take time until they reach the fully **Ready** state.
- We will add an annotate to the nodes which will ensure the pod drainning does not happen. This we will save time and money and also we will avoid stuck pods.

Now let's explain the workflow.

## Workflow

### Limitations and Caveats

To temporarily remove your data plane (Compute Nodes) in the *HostedCluster* all you need to do is scaling all *NodePools* down to zero. The default draining policy will protect any workloads, trying to move the pods to other available nodes. You can get around that policy deliberately by setting drainTimeout to a lower value, but this option has a caveat:

- **Caveat**: Changing *drainTimeout* in an existing *NodePool* will trigger a rolling update first and so the new value will only affect to the new created *Machines*. If you are in a situation where the intent is skip draining and don't want to assume the rolling upgrade triggered by changing the field, you can skip draining forcefully in a given *Machine* by setting `"machine.cluster.x-k8s.io/exclude-node-draining="` annotation in the *HostedCluster* machines as we will follow in this document.

This is a known limitation and we plane to prevent changing *drainTimeout* from triggering a rolling update in the future.

**NOTE**: `machines.machine.cluster.x-k8s.io` are considered a lower level resource and any consumer interaction is recommended to happen via *NodePool*.

### Procedure

This is a not difficult procedure but you need to be sure that you have set the right `KUBECONFIG` and you point to the correct context because maybe you already have some Kubeconfigs or contexts in the same file and we wanna work over the Management cluster.

- Set the right *Kubeconfig/Context* to work over the Management Cluster (The one where you have Hypershift installed)
- Now you need to annotate the *Machine* object from the Hosted Control Plane namespace with `"machine.cluster.x-k8s.io/exclude-node-draining="`. This annotation will allow the Cluster API to avoid the Pod drainning for every *Machine* you wanna shutdown, that means, the AWS Instance deletion will be done instantly and Openshift will not try to drain the Pods and move them over other node.

<details>
<summary> How to annotate the *Machine* objects </summary>

You can do it manually using this command:
```bash
oc annotate -n <HostedClusterNamespace>-<HostedClusterName> machines --all "machine.cluster.x-k8s.io/exclude-node-draining="
```

or execute this script:

```bash
#!/bin/bash

function annotate_nodes() {
    MACHINES="$(oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name | wc -l)"
    if [[ ${MACHINES} -le 0 ]];then
        echo "There is not machines or machineSets in the Hosted ControlPlane namespace, exiting..."
        echo "HC Namespace: ${HC_CLUSTER_NS}"
        echo "HC Clusted Name: ${HC_CLUSTER_NAME}"
        exit 1
    fi

    echo "Annotating Nodes to avoid Draining"
    oc annotate -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} machines --all "machine.cluster.x-k8s.io/exclude-node-draining="
    echo "Nodes annotated!"
}


## Fill these variables first
export KUBECONFIG=<KubeconfigPath>
export HC_CLUSTER_NS=<HostedClusterNamespace>
export HC_CLUSTER_NAME=<HostedClusterName>

CHECK_NS="$(oc get ns -o name ${2})"
if [[ -z "${CHECK_NS}" ]];then
    echo "Namespace does not exists in the Management Cluster"
    exit 1
fi

CHECK_HC="$(oc get hc -n ${HC_CLUSTER_NS} -o name ${3})"
if [[ -z "${CHECK_HC}" ]];then
    echo "HC ${3} does not exists in the namespace ${2} of the Management Cluster"
    exit 1
fi

annotate_nodes
```

</details>

- The next step it's basically scale down the *NodePool* associated to our *HostedCluster*. In order to identify it we need to execute an `oc get nodepool -n <HostedCluster Namespace>`, the one where you created the *HostedCluster* and *NodePool* on deployment time.
- To perform the scale down, you just need to grab the *NodePool* name from the last command and execute this other one `oc scale nodepool/<NodePool Name> --namespace <HostedCluster Namespace> --replicas=0`


<details>
<summary> How to scale down the nodes programmatically </summary>

```bash
#!/bin/bash

function scale_down_pool() {

    # Validated that the nodes in AWS Scale down instantly, they take sometime to disappear inside of Openshift
    # but the draining is avoided for sure
    echo "Scalling down the nodes for ${HC_CLUSTER_NAME} cluster"
    NODEPOOLS=$(oc get nodepools -n ${HC_CLUSTER_NS} -o=jsonpath='{.items[?(@.spec.clusterName=="'${HC_CLUSTER_NAME}'")].metadata.name}')
    oc scale nodepool/${NODEPOOLS} --namespace ${HC_CLUSTER_NS} --replicas=0
    echo "NodePool ${NODEPOOLS} scaled down!"
}

## Fill these variables first
export KUBECONFIG=<KubeconfigPath>
export HC_CLUSTER_NS=<HostedClusterNamespace>
export HC_CLUSTER_NAME=<HostedClusterName>

CHECK_NS="$(oc get ns -o name ${2})"
if [[ -z "${CHECK_NS}" ]];then
    echo "Namespace does not exists in the Management Cluster"
    exit 1
fi

CHECK_HC="$(oc get hc -n ${HC_CLUSTER_NS} -o name ${3})"
if [[ -z "${CHECK_HC}" ]];then
    echo "HC ${3} does not exists in the namespace ${2} of the Management Cluster"
    exit 1
fi

scale_down_nodepool
```

</details>

After these steps, you will see how the (in the AWS case) instances will be terminated instantly, but Openshift will take some time until the nodes get deleted because of the default timeouts set on the platforms.


---

## Source: docs/content/how-to/aws/create-aws-hosted-cluster-arm-workers.md

---
title: Create Arm NodePools on AWS HostedClusters
---

# Create Arm NodePools on AWS HostedClusters

The `arch` field was added to the NodePool Spec in OCP 4.14. The `arch` field sets the required processor architecture for the NodePool (currently only supported on AWS).

!!! note 

    Currently, the only valid values for '--arch' are 'arm64' and 'amd64'. The HyperShift CLI will default to 'amd64' when the 'arch' field is not specified by the user.

## Creating Arm NodePools Through the API
The HostedCluster custom resource (CR) must utilize a multi-arch manifested image. Multi-arch nightly images can be found at https://multi.ocp.releases.ci.openshift.org/.

Here is an example of an OCP 4.14 multi-arch nightly image:
```
% oc image info quay.io/openshift-release-dev/ocp-release-nightly@sha256:9b992c71f77501678c091e3dc77c7be066816562efe3d352be18128b8e8fce94 -a ~/pull-secrets.json

error: the image is a manifest list and contains multiple images - use --filter-by-os to select from:

  OS            DIGEST
  linux/amd64   sha256:c9dc4d07788ebc384a3d399a0e17f80b62a282b2513062a13ea702a811794a60
  linux/ppc64le sha256:c59c99d6ff1fe7b85790e24166cfc448a3c2ac3ef3422fce3c7259e48d2c9aab
  linux/s390x   sha256:07fcd16d5bee95196479b1e6b5b3b8064dd5359dac75e3d81f0bd4be6b8fe208
  linux/arm64   sha256:1d93a6beccc83e2a4c56ecfc37e921fe73d8964247c1a3ec34c4d66f175d9b3d
```

The CPU architecture for a NodePool is specified through its CR spec: `spec.arch`. Here is an example:
```
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  creationTimestamp: null
  name: hypershift-arm-us-east-1a
  namespace: clusters
spec:
  arch: arm64
  clusterName: hypershift-arm
  management:
    autoRepair: false
    upgradeType: Replace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    aws:
      instanceProfile: hypershift-arm-2m289-worker
      instanceType: m6g.large
      rootVolume:
        size: 120
        type: gp3
      securityGroups:
      - id: sg-064ea63968d258493
      subnet:
        id: subnet-02c74cf1cf1e7413f
    type: AWS
  release:
    image: quay.io/openshift-release-dev/ocp-release-nightly@sha256:390a33cebc940912a201a35ca03927ae5b058fbdae9626f7f4679786cab4fb1c
  replicas: 3
status:
  replicas: 0
```

## Creating a HostedCluster with Arm NodePool Through HyperShift CLI

Create a new cluster, specifying the `RELEASE_IMAGE` and `ARCH`:

```shell linenums="1"
REGION=us-east-1
CLUSTER_NAME=example
BASE_DOMAIN=example.com
AWS_CREDS="$HOME/.aws/credentials"
PULL_SECRET="$HOME/pull-secret"
RELEASE_IMAGE="quay.io/openshift-release-dev/ocp-release-nightly@sha256:390a33cebc940912a201a35ca03927ae5b058fbdae9626f7f4679786cab4fb1c"
ARCH="arm64"

hypershift create cluster aws \
--name $CLUSTER_NAME \
--node-pool-replicas=3 \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--release-image $RELEASE_IMAGE \
--arch $ARCH \
--generate-ssh
```

The HostedCluster will spin up with an Arm NodePool. The default AWS Arm instance type is `m6g.large`.

## Creating Arm NodePools on Existing HostedClusters Through HyperShift CLI

As long as a HostedCluster was created with a manifest listed image in the `--release-image`, Arm NodePools can be added to the HostedCluster:

```shell linenums="1"
CLUSTER_NAME=example
NODE_POOLNAME=example-worker
ARCH="arm64"

hypershift create nodepool aws \
--cluster-name $CLUSTER_NAME \
--name $NODE_POOLNAME \
--replicas=3 \
--arch $ARCH \
```

---

## Source: docs/content/how-to/aws/create-aws-hosted-cluster-autonode.md

---
title: Create AutoNode hosted clusters
---

HyperShift AutoNode (Powered by Karpenter) is a feature that runs
Karpenter management side as a control plane component while it
watches `openshiftNodeClasses`, `nodePools.karpenter.sh`,
and `nodeclaims.karpenter.sh` resources in the guest cluster.

To create a hosted cluster with autoNode enabled the Hypershift Operator
needs to be installed with the [feature gate `--tech-preview-no-upgrade=true`][featuregate].

The Hypershift controller creates and manages the default `openshiftNodeClasses`
in the hosted cluster allowing you to deploy workloads based in
your environment with [`NodePools`(`nodepools.karpenter.sh`)][nodepools].

The following steps describes how to install OpenShift workload cluster
on AWS with AutoNode feature by Karpenter.

> Note: `openshiftNodeClasses` is exposed for [API consumers][OpenshiftEC2NodeClassSpec].

## Prerequisites

- Install the latest and greatest HyperShift CLI.
    - Make sure all prerequisites have been satisfied (Pull Secret, Hosted Zone, OIDC Bucket, etc)
- Ensure that the AWS service-linked role for Spot is enabled in the account where the hosted cluster will be installed. This is a one-time setup per account.
    - You can verify if the role already exists using the following command:
```sh
aws iam get-role --role-name AWSServiceRoleForEC2Spot
```
    - If the role does not exist, create it with:
```sh
aws iam create-service-linked-role --aws-service-name spot.amazonaws.com
```
- Export environment variables, adjusting according to your setup:
```sh
# AWS config
export AWS_CREDS="$HOME/.aws/credentials"
export AWS_REGION=us-east-1

# OpenShift credentials and configuration
export CLUSTER_PREFIX=hcp-aws
export CLUSTER_BASE_DOMAIN=devcluster.openshift.com
export PULL_SECRET_FILE="${HOME}/.openshift/pull-secret-latest.json"
export SSH_PUB_KEY_FILE=$HOME/.ssh/id_rsa.pub

## S3 Bucket name hosting the OIDC discovery documents
# You must have set it up, see Getting Started for more information:
# https://hypershift.pages.dev/getting-started/
export OIDC_BUCKET_NAME="${CLUSTER_PREFIX}-oidc"
```

### Install the Hypershift Operator

This section describes hands on steps to install the Hypershift Operator with AutoNode feature by enabling the feature gate `tech-preview-no-upgrade`. See the following documents for more information:

- Install HyperShift Operator
- [Feature Gates][featuregate]

Steps:

- Install the operator:
```sh
./hypershift install \
  --oidc-storage-provider-s3-bucket-name="${OIDC_BUCKET_NAME}" \
  --oidc-storage-provider-s3-credentials="${AWS_CREDS}" \
  --oidc-storage-provider-s3-region="${AWS_REGION}" \
  --tech-preview-no-upgrade=true
```

- Check if controller is running as expected:

```sh
oc get all -n hypershift
```

## Create Workload Cluster with AutoNode

Create the workload cluster with HyperShift AutoNode.

Choose the desired target release image name (release controller).

Create a hosted cluster, enabling the flag `--auto-node`:

```sh
HOSTED_CLUSTER_NAME=${CLUSTER_PREFIX}-wl
OCP_RELEASE_IMAGE=<CHANGE_ME_TO_LATEST_RELEASE_IMAGE>
# Example of image: quay.io/openshift-release-dev/ocp-release:4.19.0-rc.5-x86_64

./hypershift create cluster aws \
  --name="${HOSTED_CLUSTER_NAME}" \
  --region="${AWS_REGION}" \
  --zones="${AWS_REGION}a" \
  --node-pool-replicas=1 \
  --base-domain="${CLUSTER_BASE_DOMAIN}" \
  --pull-secret="${PULL_SECRET_FILE}" \
  --aws-creds="${AWS_CREDS}" \
  --ssh-key="${SSH_PUB_KEY_FILE}" \
  --release-image="${OCP_RELEASE_IMAGE}" \
  --auto-node=true
```

Check the cluster information:

```sh
oc get --namespace clusters hostedclusters
oc get --namespace clusters nodepools
```

When completed, extract the credentials for workload cluster:

```sh
./hypershift create kubeconfig --name ${HOSTED_CLUSTER_NAME} > kubeconfig-${HOSTED_CLUSTER_NAME}

# kubeconfig for workload cluster
export KUBECONFIG=$PWD/kubeconfig-${HOSTED_CLUSTER_NAME}
```

Check the managed `openshiftNodeClasses` object created in the workload cluster:

```sh
oc get openshiftNodeClasses
```

Now you are ready to use AutoNode feature by setting the Karpenter configuration to fit your workloads.

## Deploy Sample Workloads

This section provides examples to getting started exploring HyperShift AutoNode.

### Using AutoNode with a Simple Web App

This example demonstrates how to use the AutoNode feature by creating a NodePool
to fit the sample application. The sample application selects the
instance type `t3.large`, matching the `node.kubernetes.io/instance-type`
selector in the `NodePool`.

Create a Karpenter NodePool with the configuration for the workload:
 
```sh
cat << EOF | oc apply -f -
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
 name: spot-and-gpu
spec:
  disruption:
    budgets:
      - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  weight: 10
  template:
    spec:
      expireAfter: 336h
      terminationGracePeriod: 24h0m0s
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot"]
      - key: karpenter.sh/instance-family
        operator: In
        values: ["g4dn", "m5", "m6i", "c5", "c6i", "t3"]
      - key: karpenter.sh/instance-size
        operator: In
        values: ["large", "xlarge", "2xlarge"]
EOF
```

**Create a Sample App deployment:**

This section demonstrates how to deploy sample applications to test and scale Karpenter's AutoNode feature. By creating workloads with specific resource requirements, you can observe how Karpenter provisions nodes dynamically to meet the demands of your applications.

```sh
cat << EOF | oc apply -f -
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: web-app
spec:
 replicas: 0
 selector:
   matchLabels:
     app: web-app
 template:
   metadata:
     labels:
       app: web-app
   spec:
     affinity:
       podAntiAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
           - labelSelector:
               matchLabels:
                 app: web-app
             topologyKey: "kubernetes.io/hostname"   
     securityContext:
       runAsUser: 1000
       runAsGroup: 3000
       fsGroup: 2000
     containers:
     - image: public.ecr.aws/eks-distro/kubernetes/pause:3.2
       name: web-app
       resources:
         requests:
           cpu: "1"
           memory: 256M
       securityContext:
         allowPrivilegeEscalation: false
     nodeSelector:
       node.kubernetes.io/instance-type: "t3.large"
EOF
```

Scale the application:

```sh
oc scale --replicas=1 deployment.apps/web-app
```

**Monitor and Debug Node Provisioning with AutoNode:**

Monitor the `nodeClaims` objects to track the provisioning of nodes by AutoNode. These objects provide detailed insights into the lifecycle of nodes, including their current state and any associated events. Use the following command to continuously watch the `nodeClaims`:

!!! warning "Provisioning Time"
  The provisioning process for an instance to become a node may take approximately 10 minutes. While waiting, monitor the progress using the provided commands to ensure the process completes successfully.


```sh
oc get nodeclaims --watch
```

To investigate a specific `nodeClaim` in detail, use the following command:

```sh
oc describe nodeclaim <nodeClaimName>
```

This will provide comprehensive information about the selected `nodeClaim`, helping you debug and confirm that nodes are being provisioned and functioning as expected.


**Verify Node Join Status:**

Ensure that the node has successfully joined the cluster. Use the following command to check:

```sh
oc get nodes -l karpenter.sh/nodepool=spot-and-gpu
```

This command filters the nodes associated with the `spot-and-gpu` NodePool, allowing you to confirm that the AutoNode feature is functioning as expected.

**Verify Application Scheduling:**

Ensure that the application has been successfully scheduled onto the newly provisioned node. Use the following command to check the status of the pods associated with the application:

```sh
oc get pods -l app=web-app -o wide
```

This command filters the pods by the label `app=web-app`, allowing you to confirm that the application is running on the expected node provisioned by AutoNode.


[featuregate]: https://hypershift.pages.dev/how-to/feature-gates
[ec2nodeclass]: https://karpenter.sh/docs/concepts/nodeclasses/
[nodepools]: https://karpenter.sh/docs/concepts/nodepools/
[OpenshiftEC2NodeClassSpec]: https://github.com/openshift/hypershift/blob/892474ca2fd481eeab741da377d017051256914a/api/karpenter/v1beta1/karpenter_types.go#L53-L55


---

## Source: docs/content/how-to/aws/create-aws-hosted-cluster-multiple-zones.md

---
title: Create AWS Hosted Cluster in Multiple Zones
---

# Create AWS Hosted Cluster in Multiple Zones

## Prerequisites

Complete the Prerequisites and Before you begin.

## Creating the Hosted Cluster

Create a new cluster, specifying the `BASE_DOMAIN` of the public zone provided in the
Prerequisites:

```shell linenums="1"  hl_lines="15"
REGION=us-east-1
ZONES=us-east-1a,us-east-1b
CLUSTER_NAME=example
BASE_DOMAIN=example.com
AWS_CREDS="$HOME/.aws/credentials"
PULL_SECRET="$HOME/pull-secret"

hypershift create cluster aws \
--name $CLUSTER_NAME \
--node-pool-replicas=3 \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--zones $ZONES
```

!!! important

    The `--zones` flag must specify Availability Zones (AZs) within the region specified by the `--region` flag

The `--zones` flag is also available on the  `hypershift create infra aws` command used to create infrastructure separately.

The following per-zone infrastructure is created for all specified zones:

* Public subnet
* Private subnet
* NAT gateway
* Private route table (Public route table is shared across public subnets)

Two `NodePool` resources are created, one for each zone.  The `NodePool` name is suffixed by the zone name.  The private subnet for zone is set in `spec.platform.aws.subnet.id`.


---

## Source: docs/content/how-to/aws/create-aws-hosted-cluster-secret-credentials.md

---
title: Create an AWS Hosted Cluster using a credential secret
---

# Create an AWS Hosted Cluster using a credential secret

## Creating the credential secret

If using the Multi-cluster Engine, the console provides a credential page that allows you to create the AWS credential secret.

To manually create the secret, apply the following secret yaml:

```yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: my-aws-credentials
  namespace: clusters
  labels:
    cluster.open-cluster-management.io/credentials: ""
    cluster.open-cluster-management.io/type: aws
stringData:
  aws_access_key_id: <value>
  aws_secret_access_key: <value>
  baseDomain: <value>
  pullSecret: <value>
  ssh-privatekey: <value>
  ssh-publickey: <value>
```

!!! important
    
    The required parameters when using a secret to create a cluster are `--secret-creds <SECRET_NAME> --namespace <NAMESPACE_NAME>`. 
    
    If `--namespace` is not included, then the "clusters" namespace will used.

!!! note
    
    The labels on this secret allow it to be displayed by the multi-cluster engine console.

## Create a HostedCluster using a credential secret

Prerequisites:

```shell linenums="1"
REGION=us-east-1
CLUSTER_NAME=example
SECRET_CREDS="example-multi-cluster-engine-credential-secret"
NAMESPACE="example-namespace"

hypershift create cluster aws \
  --name $CLUSTER_NAME \
  --namespace $NAMESPACE \
  --node-pool-replicas=3 \
  --secret-creds $SECRET_CREDS \
  --region $REGION \
```

## Credential secret defaults
The following values will be extracted from the credential and used when creating the hosted cluster:

  * AWS key id and AWS secret key
  * Base domain
  * Pull secret
  * SSH key

## Using credential secret overrides
The following command line parameters can be used with `--secret-creds`. Each is optional, but when provided they will override the
values found in the credential secret.

  * `--aws-creds`
  * `--base-domain`
  * `--pull-secret`
  * `--ssh-key`


---

## Source: docs/content/how-to/aws/create-infra-iam-separately.md

---
title: Create AWS infra and IAM resources separately
---

# Create AWS infra and IAM resources separately

The default behavior of the `hypershift create cluster aws` command is to create cloud infrastructure
along with the HostedCluster and apply it. It is possible to create the cloud infrastructure portion
separately so that the `hypershift create cluster aws` command can just be used to create the cluster,
or simply render it so it can be modified before applying.

In order to do this, you need to:

1. Create the AWS infrastructure
2. Create AWS IAM resources
3. Create the cluster

## Creating the AWS infra

Use the `hypershift create infra aws` command:

    hypershift create infra aws --name CLUSTER_NAME \
        --aws-creds AWS_CREDENTIALS_FILE \
        --base-domain BASEDOMAIN \
        --infra-id INFRA_ID \
        --region REGION \
        --output-file OUTPUT_INFRA_FILE

where

* `CLUSTER_NAME` is the name of the hosted cluster you intend to create. This is used for creating
    the Route53 private hosted zones that belong to the cluster.
* `AWS_CREDENTIALS_FILE` points to an AWS credentials file that has permission to create
    infrastructure resources for your cluster such as VPCs, subnets, NAT gateways, etc.
    It should correspond to the AWS account for your guest cluster, where workers will live.
* `BASEDOMAIN` is the base domain that will be used for your hosted cluster's ingress. It must
    correspond to an existing Route53 public zone that you have access to create records in.
* `INFRA_ID` is a unique name that will be used to identify your infrastructure via tags. It is used
    by the cloud controller manager in Kubernetes and the CAPI manager to identify infrastructure
    for your cluster. Typically this is the name of your cluster (CLUSTER_NAME) with a random
    suffix appended to it.
* `REGION` is the region where you want to create the infrastructure for your cluster.
* `OUTPUT_INFRA_FILE` is the file where IDs of the infrastructure that has been created will be stored in JSON format.
    This file can then be used as input to the `hypershift create cluster aws` command to populate
    the appropriate fields in the HostedCluster and NodePool resources.


Running this command should result in the following resources getting created:

* 1 VPC
* 1 DHCP Options
* 1 Private Subnet
* 1 Public Subnet
* 1 Internet Gateway
* 1 NAT Gateway
* 1 Security Group for Worker Nodes
* 2 Route Tables (1 Private, 1 Public)
* 2 Private Hosted Zones (1 for Cluster Ingress, 1 for PrivateLink, in case you will be creating a private cluster)

All of these resources will contain the following tag:
`kubernetes.io/cluster/INFRA_ID=owned`
where `INFRA_ID` is what you specified on the command invocation.

## Creating the AWS IAM resources

Use the `hypershift create iam aws` command:

    hypershift create iam aws --infra-id INFRA_ID \
        --aws-creds AWS_CREDENTIALS_FILE \
        --oidc-storage-provider-s3-bucket-name OIDC_BUCKET_NAME \
        --oidc-storage-provider-s3-region OIDC_BUCKET_REGION \
        --region REGION \
        --public-zone-id PUBLIC_ZONE_ID \
        --private-zone-id PRIVATE_ZONE_ID \
        --local-zone-id LOCAL_ZONE_ID \
        --output-file OUTPUT_IAM_FILE

where

* `INFRA_ID` should be the same id that was specified in the `create infra aws` command. It is
    used to identify the IAM resources associated with the hosted cluster.
* `AWS_CREDENTIALS_FILE` points to an AWS credentials file that has permission to create
    IAM resources such as roles. It does not have to be the same credentials specified to create
    the infrastructure but it does have to correspond to the same AWS account.
* `OIDC_BUCKET_NAME` is the name of the bucket used to store OIDC documents. This bucket should have been
    created as a prerequisite for installing Hypershift (See Prerequisites)
    The name of the bucket is used to construct URLs for the OIDC provider created by this command.
* `OIDC_BUCKET_REGION` is the region where the OIDC bucket lives.
* `REGION` is the region where the infrastructure of the cluster will live. This is used to create a worker
    instance profile for machines that belong to the hosted cluster.
* `PUBLIC_ZONE_ID` is the ID of the public zone for the guest cluster. It is used in creating the policy for the ingress operator.
    It can be found in the `OUTPUT_INFRA_FILE` generated by the `create infra aws` command.
* `PRIVATE_ZONE_ID` is the ID of the private zone for the guest cluster. It is used in creating the policy for the ingress operator.
    It can be found in the `OUTPUT_INFRA_FILE` generated by the `create infra aws` command.
* `LOCAL_ZONE_ID` is the ID of the local zone for the guest cluster (when creating a private cluster). It is used in creating the policy
    for the control plane operator so it can manage records for the PrivateLink endpoint.
    It can be found in the `OUTPUT_INFRA_FILE` generated by the `create infra aws` command.
* `OUTPUT_IAM_FILE` is the file where IDs of the IAM resources that have been created will be stored in JSON format.
    This file can then be used as input to the `hypershift create cluster aws` command to populate
    the appropriate fields in the HostedCluster and NodePool resource.

Running this command should result in the following resources getting created:

* 1 OIDC Provider (required for enabling STS authentication)
* 7 Roles (separate roles for every component that interacts with the provider: kube controller manager, capi provider, registry, etc)
* 1 Instance Profile (the profile that is assigned to all worker instances of the cluster)

## Creating the Cluster

Use the `hypershift create cluster aws` command:

    hypershift create cluster aws \
        --infra-id INFRA_ID \
        --name CLUSTER_NAME \
        --aws-creds AWS_CREDENTIALS \
        --infra-json OUTPUT_INFRA_FILE \
        --iam-json OUTPUT_IAM_FILE \
        --pull-secret PULL_SECRET_FILE \
        --generate-ssh \
        --node-pool-replicas 3

where

* `INFRA_ID` should be the same id that was specified in the `create infra aws` command. It is
    used to identify the IAM resources associated with the hosted cluster.
* `CLUSTER_NAME` should be the same name that was specified in the `create infra aws` command.
* `AWS_CREDENTIALS` should be the same that was specified in the `create infra aws` command.
* `OUTPUT_INFRA_FILE` is the file where the output of the `create infra aws` command was saved.
* `OUTPUT_IAM_FILE` is the file where the output of the `create iam aws` command was saved.
* `PULL_SECRET_FILE` is a file that contains a valid OpenShift pull secret.

!!! note

    The --generate-ssh flag is optional but is a good idea to have in case you need to ssh 
    to your workers. An ssh key will have been generated for you and stored as a secret in the 
    same namespace as the hosted cluster.

Running this command should result in the following resources getting applied to your cluster:

* Namespace
* Secret with your pull secret
* HostedCluster
* NodePool
* 3 AWS STS secrets for control plane components
* 1 SSH key secret (if --generate-ssh was specified)

You can also add the `--render` flag to the command and redirect output to a file where you 
can do further editing of the resources before applying them to the cluster.


---

## Source: docs/content/how-to/aws/define-custom-kube-api-name.md

---
title: Define Custom KubeAPI Name
---

## What is this for?

`KubeAPIServerDNSName` is a spec field used to declare a custom Kubernetes API URI. To make this work, you simply need to define the URI (e.g., `api.example.com`) in the `HostedCluster` object.

## How does this work?

- This can be defined both during day-1 (initial setup) and day-2 (post-deployment updates).
- The CPO (ControlPlaneOperator) controllers will create a new kubeconfig stored in the HCP namespace. This kubeconfig will be based on certificates and named `custom-admin-kubeconfig`.
- The certificates are generated from the root CA, with their expiration and renewal managed by the `HostedControlPlane`.
- The CPO will report a new kubeconfig, called `CustomKubeconfig`, in the `HostedControlPlane`. This kubeconfig will use the new server defined in the `KubeAPIServerDNSName` field.
- This custom kubeconfig will also be referenced in the `HostedCluster` object under the status field as `CustomKubeconfig`.
- A new secret, named `{HOSTEDCLUSTER_NAME}-custom-admin-kubeconfig`, will be created in the `HostedCluster` namespace. This secret can be used to easily access the HostedCluster API server.

!!! NOTE
    This does not directly affect the dataplane, so no rollouts are expected to occur.

- If you remove this field from the spec, all newly generated secrets and the `CustomKubeconfig` reference will be removed from the cluster and from the status field.

## Additional Notes

This other field called `CustomKubeConfig` is optional and can only be used if `KubeAPIServerDNSName` is not empty. When set, it triggers the generation of a secret with the specified name containing a kubeconfig within the `HostedCluster` namespace. This kubeconfig will also be referenced in the `HostedCluster.status` as `customkubeconfig`. If removed during day-2 operations, all related secrets and status references will also be deleted:

- This action will not cause a NodePool rollout, ensuring zero impact on customers.
- The `HostedControlPlane` object will receive the changes progressed by the Hypershift Operator and delete the corresponding field.
- The `.status.customkubeconfig` will be removed from both `HostedCluster` and `HostedControlPlane` objects.
- The secret in the `HostedControlPlane` namespace, named `custom-admin-kubeconfig`, will be deleted.
- The secret in the `HostedCluster` namespace, named `{HOSTEDCLUSTER_NAME}-custom-admin-kubeconfig`, will also be deleted.






---

## Source: docs/content/how-to/aws/deploy-aws-private-clusters.md

---
title: Deploy AWS private clusters
---

# Deploying AWS private clusters

By default, HyperShift guest clusters are publicly accessible through public DNS
and the management cluster's default router.

For private clusters on AWS, all communication with the guest cluster occur over
AWS PrivateLink. This guide will lead you
through the process of configuring HyperShift for private cluster support on AWS.

## Before you begin

To enable private hosted clusters, HyperShift must be installed with private
cluster support. This guide assumes you have performed all the
Getting started guide prerequisites. The
following steps will reference elements of the steps you already performed.

1. Create the private cluster IAM policy document.
    
    === "Shell"

        ```shell
        cat << EOF >> policy.json
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": [
                "ec2:CreateVpcEndpointServiceConfiguration",
                "ec2:DescribeVpcEndpointServiceConfigurations",
                "ec2:DeleteVpcEndpointServiceConfigurations",
                "ec2:DescribeVpcEndpointServicePermissions",
                "ec2:ModifyVpcEndpointServicePermissions",
                "ec2:RejectVpcEndpointConnections",
                "ec2:DescribeVpcEndpointConnections",
                "ec2:DescribeInstanceTypes",
                "ec2:CreateTags",
                "elasticloadbalancing:DescribeLoadBalancers"
              ],
              "Resource": "*"
            }
          ]
        }
        EOF
        ```

    === "JSON"

        ```json
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": [
                "ec2:CreateVpcEndpointServiceConfiguration",
                "ec2:DescribeVpcEndpointServiceConfigurations",
                "ec2:DeleteVpcEndpointServiceConfigurations",
                "ec2:DescribeVpcEndpointServicePermissions",
                "ec2:ModifyVpcEndpointServicePermissions",
                "ec2:RejectVpcEndpointConnections",
                "ec2:DescribeVpcEndpointConnections",
                "ec2:DescribeInstanceTypes",
                "ec2:CreateTags",
                "elasticloadbalancing:DescribeLoadBalancers"
              ],
              "Resource": "*"
            }
          ]
        }
        ```

3. Create the IAM policy in AWS.

    ```shell
    aws iam create-policy --policy-name=hypershift-operator-policy --policy-document=file://policy.json
    ```

4. Create a `hypershift-operator` IAM user.

    ```shell
    aws iam create-user --user-name=hypershift-operator
    ```

5. Attach the policy to the `hypershift-operator` user, replacing `$POLICY_ARN` with the ARN of the policy
   created in step 2.

    ```shell
    aws iam attach-user-policy --user-name=hypershift-operator --policy-arn=$POLICY_ARN
    ```

6. Create an IAM access key for the user.

    ```shell
    aws iam create-access-key --user-name=hypershift-operator
    ```

7. Create a credentials file (`$AWS_PRIVATE_CREDS`) with the access ID and key for the user
   created in step 5.

    ```shell
    cat << EOF >> $AWS_PRIVATE_CREDS
    [default]
    aws_access_key_id = <secret>
    aws_secret_access_key = <secret>
    EOF
    ```

8. Now you can install HyperShift with private cluster support.

    ```shell linenums="1"
    REGION=us-east-1
    BUCKET_NAME=your-bucket-name
    AWS_CREDS="$HOME/.aws/credentials"

    hypershift install \
    --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \
    --oidc-storage-provider-s3-credentials $AWS_CREDS \
    --oidc-storage-provider-s3-region $REGION \
    --private-platform=AWS \
    --aws-private-creds=$AWS_PRIVATE_CREDS \
    --aws-private-region=$REGION
    ```

    !!! note

        Even if you already installed HyperShift using the Getting started guide, you
        can safely run `hypershift install` again with private cluster support to update the existing installation.

    !!! important

        Although public clusters can be created in any region, **private clusters can only be created in the same
        region specified by `--aws-private-region`**.

## Create a private HostedCluster

Create a new private cluster, specifying values used in the Before you
begin section.

```shell linenums="1" hl_lines="13"
CLUSTER_NAME=example
BASE_DOMAIN=example.com
AWS_CREDS="$HOME/.aws/credentials"
PULL_SECRET="$HOME/pull-secret"

hypershift create cluster aws \
--name $CLUSTER_NAME \
--node-pool-replicas=3 \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--endpoint-access Private
```

!!! note

    The `--endpoint-access` flag is used to designate whether a cluster is public or private.

The cluster's API endpoints will be accessible through a private DNS zone:

- `api.$CLUSTER_NAME.hypershift.local`
- `*.apps.$CLUSTER_NAME.hypershift.local`

## Access a private HostedCluster

Use a bastion host to access a private cluster.

Start a bastion instance, replacing `$SSH_KEY` with credentials to use for connecting to the bastion.

```shell
hypershift create bastion aws --aws-creds=$AWS_CREDS --infra-id=$INFRA_ID --region=$REGION --ssh-key-file=$SSH_KEY
```

Find the private IPs of nodes in the cluster's NodePool.

```shell
aws ec2 describe-instances --filter="Name=tag:kubernetes.io/cluster/$INFRA_ID,Values=owned" | jq '.Reservations[] | .Instances[] | select(.PublicDnsName=="") | .PrivateIpAddress'
```

Create a kubeconfig for the cluster which can be copied to a node.

```shell
hypershift create kubeconfig > $CLUSTER_KUBECONFIG
```

SSH into one of the nodes via the bastion using the IP printed from the `create bastion` command.

```shell
ssh -o ProxyCommand="ssh ec2-user@$BASTION_IP -W %h:%p" core@$NODE_IP
```

From the SSH shell, copy the kubeconfig contents to a file on the node.

```shell
cat << EOF >> kubeconfig
<paste kubeconfig contents>
EOF
export KUBECONFIG=$PWD/kubeconfig
```

From the SSH shell, observe the guest cluster status or run other `oc` commands.

```shell
oc get clusteroperators
oc get clusterversion
# ...
```


---

## Source: docs/content/how-to/aws/disaster-recovery.md

---
title: Disaster Recovery
---

# Disaster Recovery

## Migrating Hosted Cluster within the same AWS Region

### Use cases for Disaster Recovery

This procedure **is helpful** for:

1. *The control-plane is down for your hosted cluster (api-server, etcd,...)*
2. *Hypershift operator is not working, it‚Äôs down and it cannot be recovered*

This procedure **is not helpful** for:

1. *My compute nodes are frozen or not working fine*
    - For this situation you will need to access the serial console of the node in order to see what is happening.

2. *The management cluster API-server/etcd is down?*
    - For this second situation it does not make sense to use this procedure, so you can use a backup/recovery tool like Velero in order to recover the Etcd.

!!! note Important
    Other situations should be carefully examined, to ensure the stability of the other deployed HostedClusters

!!! warning
    These are some examples where this procedure could be useful. We don't recommend following this procedure unless it is strictly necessary.

### Preface and Considerations

The behaviour of this implementation is focused on the transparency for the user. Hypershift will not disrupt any customer workloads at anytime, also have in mind that the service workloads will be up and running during the migration process. Maybe at some point the Cluster API will be down but this will not affect the services running on the worker nodes.

In the storage side, It's mandatory to have under consideration that when we move a HostedControlPlane to another Management cluster we use some external services, which allows the migration to make it happen. We reusethe storage provisioned in AWS by the initial ControlPlane (PVs/PVCs) in the destination Management cluster.

Regarding the Workers nodes assigned to the cluster, during the migration they will still point to the same DNS entry, and we, under the hood, change the DNS Records to point to the new Management Cluster API, that way the node migration is transparent for the user.

!!! important
    Keep in mind that this **"migration"** capability is only for **disaster recovery purposes**, please **DO NOT** use this to perform clusters migrations as a common task in your platform.

These next arguments depend on how the Hypershift Operator has been deployed and how a Hosted Cluster has been created. E.G If we want to go ahead with the procedure and our cluster is **private** we need to make sure that our **Hypershift Operator** has been deployed with the arguments set in the **Private** tab for **Hypershift Operator Deployment access endpoints arguments** and our **Hosted Cluster** has been created using the arguments following the **Private** tab in the **Arguments of the CLI when creating a HostedCluster** section down below.

!!! warning
    Since this is a disaster recovery procedure, unexpected things could happen because of all the moving components involved. To assist, see this troubleshooting section for the most common issues identified.

- Hypershift Operator Deployment endpoint access arguments

=== "**Public** and **PublicAndPrivate**"

    ```bash
    --external-dns-provider=aws \
    --external-dns-credentials=<AWS Credentials location> \
    --external-dns-domain-filter=<External DNS for HostedCluster>
    ```

=== "**Private**"

    ```bash
    --private-platform aws \
    --aws-private-creds <Path to AWS Credentials> \
    --aws-private-region <AWS Region>
    ```

- Arguments of the CLI when creating a HostedCluster

=== "**Public**"

    ```bash
    --external-dns-domain=<External DNS Domain> \
    --endpoint-access=Public
    ```

=== "**PublicAndPrivate**"

    ```bash
    --external-dns-domain=<External DNS Domain> \
    --endpoint-access=PublicAndPrivate
    ```


=== "**Private**"

    ```bash
    --endpoint-access=Private
    ```

This way, the server URL will end in something like this: "https://api-sample-hosted.sample-hosted.aws.openshift.com"

The way that a Hosted Cluster migration follows, it's basically done in 3 phases:

1. **Backup**
2. **Restoration**
3. **Teardown**

Let's setup the environment to start the migration with our first cluster.

### Environment and Context

Our scenario involves 3 Clusters, 2 Management ones and 1 HostedCluster, which will be migrated. Depending on the situation we would like to migrate just the ControlPlane or the Controlplane + nodes.

These are the relevant data we need to know in order to migrate a cluster:

- **Source MGMT Namespace**: Source Management Namespace
- **Source MGMT ClusterName**: Source Management Cluster Name
- **Source MGMT Kubeconfig**: Source Management Kubeconfig
- **Destination MGMT Kubeconfig**: Destination Management Kubeconfig
- **HC Kubeconfig**: Hosted Cluster Kubeconfig
- **SSH Key File**: SSH Public Key
- **Pull Secret**: Pull Secret file to access to the Release Images
- **AWS Credentials**: AWS Credentials file
- **AWS Region**: AWS Region
- **Base Domain**: DNS Base Domain to use it as external DNS.
- **S3 Bucket Name**: This is the bucket in the same **AWS Region** where the ETCD backup will be uploaded

These are the Variables we will use in the scripts:

<details>
<summary>Sample Environment Variables</summary>

- Ensure all the file it's correct regarding you folder tree and put this env file in your filesystem, then execute `source env_file` from a terminal

```bash
SSH_KEY_FILE=${HOME}/.ssh/id_rsa.pub
BASE_PATH=${HOME}/hypershift
BASE_DOMAIN="aws.sample.com"
PULL_SECRET_FILE="${HOME}/pull_secret.json"
AWS_CREDS="${HOME}/.aws/credentials"
AWS_ZONE_ID="Z02718293M33QHDEQBROL"

CONTROL_PLANE_AVAILABILITY_POLICY=SingleReplica
HYPERSHIFT_PATH=${BASE_PATH}/src/hypershift
HYPERSHIFT_CLI=${HYPERSHIFT_PATH}/bin/hypershift
HYPERSHIFT_IMAGE=${HYPERSHIFT_IMAGE:-"quay.io/${USER}/hypershift:latest"}
NODE_POOL_REPLICAS=${NODE_POOL_REPLICAS:-2}

# MGMT Context
MGMT_REGION=us-west-1
MGMT_CLUSTER_NAME="${USER}-dev"
MGMT_CLUSTER_NS=${USER}
MGMT_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT_CLUSTER_NS}-${MGMT_CLUSTER_NAME}"
MGMT_KUBECONFIG="${MGMT_CLUSTER_DIR}/kubeconfig"

# MGMT2 Context
MGMT2_CLUSTER_NAME="${USER}-dest"
MGMT2_CLUSTER_NS=${USER}
MGMT2_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT2_CLUSTER_NS}-${MGMT2_CLUSTER_NAME}"
MGMT2_KUBECONFIG="${MGMT2_CLUSTER_DIR}/kubeconfig"

# Hosted Cluster Context
HC_CLUSTER_NS=clusters
HC_REGION=us-west-1
HC_CLUSTER_NAME="${USER}-hosted"
HC_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}"
HC_KUBECONFIG="${HC_CLUSTER_DIR}/kubeconfig"
BACKUP_DIR=${HC_CLUSTER_DIR}/backup

BUCKET_NAME="${USER}-hosted-${MGMT_REGION}"

# DNS
AWS_ZONE_ID="Z07342811SH9AA102K1AC"
EXTERNAL_DNS_DOMAIN="hc.jpdv.aws.kerbeross.com"
```

</details>

And this is how the Migration workflow will happen




### Backup

This section complains interaction among multiple components. We will need to backup all the relevant things to raise up this same cluster in our target management cluster.

To do that we will:

1. Mark the Hosted Cluster with a ConfigMap which will declare the source Management Cluster it comes from (This is not mandatory but useful).

<details>
<summary>Config Map creation to set the Source Management Cluster</summary>


```bash
oc create configmap mgmt-parent-cluster -n default --from-literal=from=${MGMT_CLUSTER_NAME}
```

</details>

2. Shutdown the reconciliation in the HostedCluster we want to migrate and also in the Nodepools.


<details>
<summary>ControlPlane Migration</summary>

```bash
PAUSED_UNTIL="true"
oc patch -n ${HC_CLUSTER_NS} hostedclusters/${HC_CLUSTER_NAME} -p '{"spec":{"pausedUntil":"'${PAUSED_UNTIL}'"}}' --type=merge
oc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 kube-apiserver openshift-apiserver openshift-oauth-apiserver control-plane-operator
```

</details>

<details>
<summary>ControlPlane + NodePool Migration</summary>

```bash
PAUSED_UNTIL="true"
oc patch -n ${HC_CLUSTER_NS} hostedclusters/${HC_CLUSTER_NAME} -p '{"spec":{"pausedUntil":"'${PAUSED_UNTIL}'"}}' --type=merge
oc patch -n ${HC_CLUSTER_NS} nodepools/${NODEPOOLS} -p '{"spec":{"pausedUntil":"'${PAUSED_UNTIL}'"}}' --type=merge
oc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 kube-apiserver openshift-apiserver openshift-oauth-apiserver control-plane-operator
```

</details>

3. Backup ETCD and Upload to S3 Bucket

The whole process of this step is documented here, even with that we will go through the process in a more programmatically way.

To do this programmatically it's a bit more complicated, but we will try to put all the necessary steps in a bash script

<details>
<summary>ETCD Backup and Upload to S3 procedure</summary>

- As an advice, we recommend to wrap it up in a function and call it from the main function.

```bash
# ETCD Backup
ETCD_PODS="etcd-0"
if [ "${CONTROL_PLANE_AVAILABILITY_POLICY}" = "HighlyAvailable" ]; then
  ETCD_PODS="etcd-0 etcd-1 etcd-2"
fi

## If you are in 4.12 or above, use this one
ETCD_CA_LOCATION=/etc/etcd/tls/etcd-ca/ca.crt

## If you are in 4.11 or below, use this other one
#ETCD_CA_LOCATION=/etc/etcd/tls/client/etcd-client-ca.crt

for POD in ${ETCD_PODS}; do
  # Create an etcd snapshot
  oc exec -it ${POD} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -- env ETCDCTL_API=3 /usr/bin/etcdctl --cacert ${ETCD_CA_LOCATION} --cert /etc/etcd/tls/client/etcd-client.crt --key /etc/etcd/tls/client/etcd-client.key --endpoints=localhost:2379 snapshot save /var/lib/data/snapshot.db

  oc exec -it ${POD} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/data/snapshot.db

  FILEPATH="/${BUCKET_NAME}/${HC_CLUSTER_NAME}-${POD}-snapshot.db"
  CONTENT_TYPE="application/x-compressed-tar"
  DATE_VALUE=`date -R`
  SIGNATURE_STRING="PUT\n\n${CONTENT_TYPE}\n${DATE_VALUE}\n${FILEPATH}"

  set +x
  ACCESS_KEY=$(grep aws_access_key_id ${AWS_CREDS} | head -n1 | cut -d= -f2 | sed "s/ //g")
  SECRET_KEY=$(grep aws_secret_access_key ${AWS_CREDS} | head -n1 | cut -d= -f2 | sed "s/ //g")
  SIGNATURE_HASH=$(echo -en ${SIGNATURE_STRING} | openssl sha1 -hmac "${SECRET_KEY}" -binary | base64)
  set -x

  # FIXME: this is pushing to the OIDC bucket
  oc exec -it etcd-0 -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -- curl -X PUT -T "/var/lib/data/snapshot.db" \
    -H "Host: ${BUCKET_NAME}.s3.amazonaws.com" \
    -H "Date: ${DATE_VALUE}" \
    -H "Content-Type: ${CONTENT_TYPE}" \
    -H "Authorization: AWS ${ACCESS_KEY}:${SIGNATURE_HASH}" \
    https://${BUCKET_NAME}.s3.amazonaws.com/${HC_CLUSTER_NAME}-${POD}-snapshot.db
done
```

</details>

!!! warning Warning
    The CA Certificate of ETCD has changed the location in 4.12, so take care about the command execution because it will fail. It's safe to reexecute this piece of code, it just will backup the ETCD in S3. In order to know which version you have installed, just execute this command `oc version -o json | jq -e .openshiftVersion`


4. Backup Kubernetes/Openshift objects

    - From HostedCluster Namespace:
        - HostedCluster and NodePool Objects
        - HostedCluster Secrets
    - From Hosted Control Plane Namespace:
        - HostedControlPlane
        - Cluster
        - AWSCluster, AWSMachineTemplate, AWSMachine
        - MachineDeployments, MachineSets and Machines
        - ControlPlane Secrets

<details>
<summary>Openshift Objects backup</summary>

```bash
mkdir -p ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS} ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}
chmod 700 ${BACKUP_DIR}/namespaces/

# HostedCluster
echo "Backing Up HostedCluster Objects:"
oc get hc ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS} -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}.yaml
echo "--> HostedCluster"
sed -i '' -e '/^status:$/,$d' ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}.yaml

# NodePool
oc get np ${NODEPOOLS} -n ${HC_CLUSTER_NS} -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/np-${NODEPOOLS}.yaml
echo "--> NodePool"
sed -i '' -e '/^status:$/,$ d' ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/np-${NODEPOOLS}.yaml

# Secrets in the HC Namespace
echo "--> HostedCluster Secrets:"
for s in $(oc get secret -n ${HC_CLUSTER_NS} | grep "^${HC_CLUSTER_NAME}" | awk '{print $1}'); do
    oc get secret -n ${HC_CLUSTER_NS} $s -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/secret-${s}.yaml
done

# Secrets in the HC Control Plane Namespace
echo "--> HostedCluster ControlPlane Secrets:"
for s in $(oc get secret -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} | egrep -v "docker|service-account-token|oauth-openshift|NAME|token-${HC_CLUSTER_NAME}" | awk '{print $1}'); do
    oc get secret -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/secret-${s}.yaml
done

# Hosted Control Plane
echo "--> HostedControlPlane:"
oc get hcp ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/hcp-${HC_CLUSTER_NAME}.yaml

# Cluster
echo "--> Cluster:"
CL_NAME=$(oc get hcp ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o jsonpath={.metadata.labels.\*} | grep ${HC_CLUSTER_NAME})
oc get cluster ${CL_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/cl-${HC_CLUSTER_NAME}.yaml

# AWS Cluster
echo "--> AWS Cluster:"
oc get awscluster ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awscl-${HC_CLUSTER_NAME}.yaml

# AWS MachineTemplate
echo "--> AWS Machine Template:"
oc get awsmachinetemplate ${NODEPOOLS} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsmt-${HC_CLUSTER_NAME}.yaml

# AWS Machines
echo "--> AWS Machine:"
CL_NAME=$(oc get hcp ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o jsonpath={.metadata.labels.\*} | grep ${HC_CLUSTER_NAME})
for s in $(oc get awsmachines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --no-headers | grep ${CL_NAME} | cut -f1 -d\ ); do
    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} awsmachines $s -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsm-${s}.yaml
done

# MachineDeployments
echo "--> HostedCluster MachineDeployments:"
for s in $(oc get machinedeployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do
    mdp_name=$(echo ${s} | cut -f 2 -d /)
    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machinedeployment-${mdp_name}.yaml
done

# MachineSets
echo "--> HostedCluster MachineSets:"
for s in $(oc get machineset -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do
    ms_name=$(echo ${s} | cut -f 2 -d /)
    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machineset-${ms_name}.yaml
done

# Machines
echo "--> HostedCluster Machine:"
for s in $(oc get machine -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do
    m_name=$(echo ${s} | cut -f 2 -d /)
    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml > ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machine-${m_name}.yaml
done
```

</details>

5. Cleanup the ControlPlane Routes (only in `PublicAndPrivate` and `Public` clusters)

    - This will allow the **ExternalDNS Operator** to delete the Route53 entries in AWS and they will not be recreated because of this HostedCluster it's paused.

<details>
<summary>HostedCluster ControlPlane Routes Cleanup</summary>

- Just to clean the routes you could execute this command, but you will need to wait until the Route53 are clean (this is why I will add an alternative to validate this step).

```bash
oc delete routes -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all
```

- (Alternative bash script) Cleanup OCP HC ControlPlane Routes and wait until Route53 it's clean (only in `PublicAndPrivate` and `Public` clusters)
```bash
function clean_routes() {

    if [[ -z "${1}" ]];then
        echo "Give me the NS where to clean the routes"
        exit 1
    fi

    # Constants
    if [[ -z "${2}" ]];then
        echo "Give me the Route53 zone ID"
        exit 1
    fi

    ZONE_ID=${2}
    ROUTES=10
    timeout=40
    count=0

    # This allows us to remove the ownership in the AWS for the API route
    oc delete route -n ${1} --all

    while [ ${ROUTES} -gt 2 ]
    do
        echo "Waiting for ExternalDNS Operator to clean the DNS Records in AWS Route53 where the zone id is: ${ZONE_ID}..."
        echo "Try: (${count}/${timeout})"
        sleep 10
        if [[ $count -eq timeout ]];then
            echo "Timeout waiting for cleaning the Route53 DNS records"
            exit 1
        fi
        count=$((count+1))
        ROUTES=$(aws route53 list-resource-record-sets --hosted-zone-id ${ZONE_ID} --max-items 10000 --output json | grep -c ${EXTERNAL_DNS_DOMAIN})
    done
}

# SAMPLE: clean_routes "<HC ControlPlane Namespace>" "<AWS_ZONE_ID>"
clean_routes "${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}" "${AWS_ZONE_ID}"
```

</details>

!!! warning Warning
    This step is only relevant if you have a HostedCluster with a `--endpoint-access` argument as `PublicAndPrivate` or `Public`. If that's not the case, you will not have the need to execute this part.

This was the last step on Backup stage, now we encourage you to validate all the OCP Objects and the S3 Bucket in order to ensure all is fine.

### Restoration

This step it's basically catch all the objects which has been backuped up and restore them in the Destination Management Cluster.

!!! note
    Ensure you have the destination's cluster's Kubeconfig placed as is set in `MGMT2_KUBECONFIG` (if you follow the final script) or `KUBECONFIG` variable if you are going step by step. `export KUBECONFIG=${MGMT2_KUBECONFIG}` or `export KUBECONFIG=<Kubeconfig FilePath>`

1. Ensure you don't have an old Namespace in the new MGMT Cluster with the same name as the cluster that are you migrating.

<details>
<summary>Delete the Namespace that will be used by the Migrated Cluster and the Control plane</summary>

```bash
# Just in case
export KUBECONFIG=${MGMT2_KUBECONFIG}
BACKUP_DIR=${HC_CLUSTER_DIR}/backup

# Namespace deletion in the destination Management cluster
oc delete ns ${HC_CLUSTER_NS} || true
oc delete ns ${HC_CLUSTER_NS}-{HC_CLUSTER_NAME} || true
```

</details>

2. ReCreate the deleted namespaces from fresh start

<details>
<summary>Namespace creation</summary>

```bash
# Namespace creation
oc new-project ${HC_CLUSTER_NS}
oc new-project ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}
```

</details>

3. Restore Secrets in the HC Namespace

<details>
<summary>Secrets Restoration in HostedCluster Namespace</summary>

```bash
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/secret-*
```

</details>

4. Restore Objects in the HC ControlPlane Namespace

<details>
<summary>Restore OCP Objects related with the HC ControlPlane</summary>

```bash
# Secrets
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/secret-*

# Cluster
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/hcp-*
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/cl-*
```

</details>

5. (Optional) Restore objects in the HC ControlPlane Namespace

    !!! note
        This step it's only relevant if you are migrating the Nodes and the NodePool to reuse the AWS Instances.

<details>
<summary>Restore OCP Nodes related objects within the HC ControlPlane Namespace</summary>

```bash
# AWS
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awscl-*
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsmt-*
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsm-*

# Machines
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machinedeployment-*
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machineset-*
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machine-*
```

</details>

6. Restore the ETCD Backup and HostedCluster

<details>
<summary>Bash script to restore ETCD and HostedCluster object</summary>

```bash
ETCD_PODS="etcd-0"
if [ "${CONTROL_PLANE_AVAILABILITY_POLICY}" = "HighlyAvailable" ]; then
  ETCD_PODS="etcd-0 etcd-1 etcd-2"
fi

HC_RESTORE_FILE=${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}-restore.yaml
HC_BACKUP_FILE=${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}.yaml
HC_NEW_FILE=${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}-new.yaml
cat ${HC_BACKUP_FILE} > ${HC_NEW_FILE}
cat > ${HC_RESTORE_FILE} <<EOF
    restoreSnapshotURL:
EOF

for POD in ${ETCD_PODS}; do
  # Create a pre-signed URL for the etcd snapshot
  ETCD_SNAPSHOT="s3://${BUCKET_NAME}/${HC_CLUSTER_NAME}-${POD}-snapshot.db"
  ETCD_SNAPSHOT_URL=$(AWS_DEFAULT_REGION=${MGMT2_REGION} aws s3 presign ${ETCD_SNAPSHOT})

  # FIXME no CLI support for restoreSnapshotURL yet
  cat >> ${HC_RESTORE_FILE} <<EOF
    - "${ETCD_SNAPSHOT_URL}"
EOF
done

cat ${HC_RESTORE_FILE}

if ! grep ${HC_CLUSTER_NAME}-snapshot.db ${HC_NEW_FILE}; then
  sed -i '' -e "/type: PersistentVolume/r ${HC_RESTORE_FILE}" ${HC_NEW_FILE}
  sed -i '' -e '/pausedUntil:/d' ${HC_NEW_FILE}
fi

HC=$(oc get hc -n ${HC_CLUSTER_NS} ${HC_CLUSTER_NAME} -o name || true)
if [[ ${HC} == "" ]];then
    echo "Deploying HC Cluster: ${HC_CLUSTER_NAME} in ${HC_CLUSTER_NS} namespace"
    oc apply -f ${HC_NEW_FILE}
else
    echo "HC Cluster ${HC_CLUSTER_NAME} already exists, avoiding step"
fi
```

</details>

7. (Optional) Restore the NodePool

    !!! note
        This step it's only relevant if you are migrating the Nodes and the NodePool to reuse the AWS Instances.

<details>
<summary>Restore the NodePool object</summary>

```bash
oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/np-*
```

</details>

This was our last step in the **Restoration** phase. If you are not migrating nodes, congratulations, you can pass to the next section **Teardown**.


(Optional) Now we will need to wait for some time until the Nodes gets fully migrated. We recommend to use this function

<details>
<summary>Ensure Nodes Migrated</summary>

```bash
timeout=40
count=0
NODE_STATUS=$(oc get nodes --kubeconfig=${HC_KUBECONFIG} | grep -v NotReady | grep -c "worker") || NODE_STATUS=0

while [ ${NODE_POOL_REPLICAS} != ${NODE_STATUS} ]
do
    echo "Waiting for Nodes to be Ready in the destination MGMT Cluster: ${MGMT2_CLUSTER_NAME}"
    echo "Try: (${count}/${timeout})"
    sleep 30
    if [[ $count -eq timeout ]];then
        echo "Timeout waiting for Nodes in the destination MGMT Cluster"
        exit 1
    fi
    count=$((count+1))
    NODE_STATUS=$(oc get nodes --kubeconfig=${HC_KUBECONFIG} | grep -v NotReady | grep -c "worker") || NODE_STATUS=0
done
```

</details>

### Teardown

In this section we will shutdown and delete the HostedCluster in the source Management Cluster.
!!! note
    Ensure you have the source's cluster's Kubeconfig placed as is set in `MGMT_KUBECONFIG` (if you follow the final script) or `KUBECONFIG` variable if you are going step by step. `export KUBECONFIG=${MGMT_KUBECONFIG}` or `export KUBECONFIG=<Kubeconfig FilePath>`

1. Scale The Deployments and StatefulSets

<details>
<summary>ScaleDown Pod relevant objects in the HC ControlPlane Namespace</summary>

```bash
# Just in case
export KUBECONFIG=${MGMT_KUBECONFIG}

# Scale down deployments
oc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 --all
oc scale statefulset.apps -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 --all
sleep 15
```

</details>

2. Delete the NodePool objects

<details>
<summary>Delete NodePools</summary>

```bash
NODEPOOLS=$(oc get nodepools -n ${HC_CLUSTER_NS} -o=jsonpath='{.items[?(@.spec.clusterName=="'${HC_CLUSTER_NAME}'")].metadata.name}')
if [[ ! -z "${NODEPOOLS}" ]];then
    oc patch -n "${HC_CLUSTER_NS}" nodepool ${NODEPOOLS} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]'
    oc delete np -n ${HC_CLUSTER_NS} ${NODEPOOLS}
fi
```

</details>

3. Delete the Machines and MachineSets

<details>
<summary>Delete Machines and MachineSets</summary>

```bash
# Machines
for m in $(oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do
    oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]' || true
    oc delete -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} || true
done

oc delete machineset -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all || true
```

</details>

4. Delete Cluster object

<details>
<summary>Delete the Cluster</summary>

```bash
# Cluster
C_NAME=$(oc get cluster -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name)
oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${C_NAME} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]'
oc delete cluster.cluster.x-k8s.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all
```

</details>

5. Delete the AWS Machines (Kubernetes Objects)

!!! note
    Don't worry about the real AWS Machines, even if you delete this object, the CAPI controllers are down and will not affect the cloud instances


<details>
<summary>Delete AWS Machines OCP Objects</summary>

```bash
# AWS Machines
for m in $(oc get awsmachine.infrastructure.cluster.x-k8s.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name)
do
    oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]' || true
    oc delete -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} || true
done
```

</details>

6. Delete HostedControlPlane and Controlplane HC Namespace

<details>
<summary>Delete HostedControlPlane and ControlPlane HC Namespace objects</summary>

```bash
# Delete HCP and ControlPlane HC NS
oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} hostedcontrolplane.hypershift.openshift.io ${HC_CLUSTER_NAME} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]'
oc delete hostedcontrolplane.hypershift.openshift.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all
oc delete ns ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} || true
```

</details>

7. Delete the HostedCluster and HC Namespace

<details>
<summary>Delete the HostedCluster object</summary>

```bash
# Delete HC and HC Namespace
oc -n ${HC_CLUSTER_NS} patch hostedclusters ${HC_CLUSTER_NAME} -p '{"metadata":{"finalizers":null}}' --type merge || true
oc delete hc -n ${HC_CLUSTER_NS} ${HC_CLUSTER_NAME}  || true
oc delete ns ${HC_CLUSTER_NS} || true
```

</details>

And that was it, following this whole process you could migrate an HostedCluster from one Management Cluster to other one in the same AWS Region.

To ensure all is working fine, you just need to validate that all the objects are in the right place:

```bash
# Validations
export KUBECONFIG=${MGMT2_KUBECONFIG}

oc get hc -n ${HC_CLUSTER_NS}
oc get np -n ${HC_CLUSTER_NS}
oc get pod -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}
oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}

# Inside the HostedCluster
export KUBECONFIG=${HC_KUBECONFIG}
oc get clusterversion
oc get nodes
```

8. (Optional) Restart OVN Pods in compute nodes (only in `PublicAndPrivate` and `Public` clusters)
After the Teardown of the HostedCluster in the source Management Cluster **you will need to delete the OVN pods in the HostedCluster** in order to perform the connection with the new OVN Master running in the new Management Cluster.

To do that you just need to load the proper KUBECONFIG env var with the Hosted Cluster Kubeconfig path and execute this command:

```bash
oc delete pod -n openshift-ovn-kubernetes --all
```

with that, all the ClusterOperators that were failing and all the new pods generated, will get executed without issues.

!!! warning Warning
    This step is only relevant if you have a HostedCluster with a `--endpoint-access` argument as `PublicAndPrivate` or `Public`. If that's not the case, you will not have the need to execute this part.

### Migration Helper script

In order to ensure the that whole migration works fine, you could use this helper script that should work out of the box.

<details>
<summary>HC Migration Script</summary>

In order to execute the script, just:
- Fill the common variables and save the file as `../common/common.sh`
- Execute the migration script without params.

Now let's take a look to that script

- Common Variables

```bash
# Fill the Common variables to fit your environment, this is just a sample
SSH_KEY_FILE=${HOME}/.ssh/id_rsa.pub
BASE_PATH=${HOME}/hypershift
BASE_DOMAIN="aws.sample.com"
PULL_SECRET_FILE="${HOME}/pull_secret.json"
AWS_CREDS="${HOME}/.aws/credentials"
CONTROL_PLANE_AVAILABILITY_POLICY=SingleReplica
HYPERSHIFT_PATH=${BASE_PATH}/src/hypershift
HYPERSHIFT_CLI=${HYPERSHIFT_PATH}/bin/hypershift
HYPERSHIFT_IMAGE=${HYPERSHIFT_IMAGE:-"quay.io/${USER}/hypershift:latest"}
NODE_POOL_REPLICAS=${NODE_POOL_REPLICAS:-2}

# MGMT Context
MGMT_REGION=us-west-1
MGMT_CLUSTER_NAME="${USER}-dev"
MGMT_CLUSTER_NS=${USER}
MGMT_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT_CLUSTER_NS}-${MGMT_CLUSTER_NAME}"
MGMT_KUBECONFIG="${MGMT_CLUSTER_DIR}/kubeconfig"

# MGMT2 Context
MGMT2_REGION=us-west-1
MGMT2_CLUSTER_NAME="${USER}-dest"
MGMT2_CLUSTER_NS=${USER}
MGMT2_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT2_CLUSTER_NS}-${MGMT2_CLUSTER_NAME}"
MGMT2_KUBECONFIG="${MGMT2_CLUSTER_DIR}/kubeconfig"

# Hosted Cluster Context
HC_CLUSTER_NS=clusters
HC_REGION=us-west-1
HC_CLUSTER_NAME="${USER}-hosted"
HC_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}"
HC_KUBECONFIG="${HC_CLUSTER_DIR}/kubeconfig"
BACKUP_DIR=${HC_CLUSTER_DIR}/backup

BUCKET_NAME="${USER}-hosted-${MGMT_REGION}"

# DNS
AWS_ZONE_ID="Z026552815SS3YPH9H6MG"
EXTERNAL_DNS_DOMAIN="guest.jpdv.aws.kerbeross.com"
```

- Migration Script
The migration script is maintained at https://github.com/openshift/hypershift/blob/main/contrib/migration/migrate-hcp.sh
</details>

## HostedCluster Configuration Requirements for AWS Self-Managed Platforms

!!! important "AWS Self-Managed Platform Configuration"

    When using AWS platform with self-managed infrastructure, to ensure workloads from existing nodes propagate correctly to new NodePool nodes during disaster recovery, the APIServer service configuration must use a **Route** service publishing strategy with a **fixed hostname** specified.

    This configuration is critical for:
    - Proper workload migration to new nodes in restored NodePools
    - Service continuity during disaster recovery processes
    - Consistent DNS resolution for applications
    - Maintaining cluster connectivity after node reprovisioning

### Required Configuration

For AWS self-managed platforms, ensure your HostedCluster includes the APIServer service publishing strategy with a fixed hostname using Route type:

```yaml
spec:
  platform:
    aws:
      endpointAccess: Public
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: Route
      route:
        hostname: api.example.com
```

### Why This Configuration is Required

During disaster recovery scenarios:

1. **Node Reprovisioning**: New NodePool nodes are created to replace the original nodes
2. **Workload Migration**: Applications and workloads need to be transferred from old nodes to new nodes
3. **Service Continuity**: The fixed hostname ensures that services remain accessible throughout the migration process
4. **DNS Consistency**: A stable hostname prevents DNS resolution issues that could disrupt application connectivity

Without this configuration, workloads may fail to propagate correctly to new nodes, potentially causing service disruptions during the disaster recovery process.

## Fixing OIDC Identity Provider After OADP Restore

After restoring a HostedCluster via OADP, the AWS IAM OIDC identity provider and its S3 discovery documents may be missing or inconsistent. This causes the control-plane-operator to fail with `WebIdentityErr` and prevents the default security group from being reconciled, leaving NodePool nodes in a not-ready state.

The `hypershift fix dr-oidc-iam` command resolves this by re-uploading the OIDC discovery documents using the **existing** service account signing key from the cluster and recreating the IAM OIDC provider if needed. It also schedules a rolling restart of the HostedCluster control plane to recover any pods stuck in CrashLoopBackOff.

### Usage

```bash
# Recommended: auto-detect configuration from the HostedCluster
hypershift fix dr-oidc-iam \
  --hc-name <cluster-name> \
  --hc-namespace <namespace> \
  --aws-creds ~/.aws/credentials

# Preview changes without applying them
hypershift fix dr-oidc-iam \
  --hc-name <cluster-name> \
  --hc-namespace <namespace> \
  --aws-creds ~/.aws/credentials \
  --dry-run

# Force complete regeneration of OIDC documents and provider
hypershift fix dr-oidc-iam \
  --hc-name <cluster-name> \
  --hc-namespace <namespace> \
  --aws-creds ~/.aws/credentials \
  --force-recreate

# Adjust the delay before the rolling restart (default: 5m)
hypershift fix dr-oidc-iam \
  --hc-name <cluster-name> \
  --hc-namespace <namespace> \
  --aws-creds ~/.aws/credentials \
  --restart-delay 10m
```

### What the Command Does

1. Checks if OIDC discovery documents exist in S3
2. Checks if the IAM OIDC identity provider exists
3. Ensures the S3 bucket is properly configured with public read access
4. Retrieves the existing service account signing public key from the `sa-signing-key` secret in the hosted control plane namespace
5. Generates and uploads OIDC discovery and JWKS documents using the existing key
6. Creates or recreates the IAM OIDC identity provider
7. Verifies the configuration and schedules a rolling restart of the HostedCluster


---

## Source: docs/content/how-to/aws/etc-backup-restore.md

---
title: Etcd backup/restore (manual process)
---

# Etcd backup/restore (manual process)

It is possible to manually snapshot the etcd state for a HostedCluster, currently this
is a fully manual process and requires API downtime.

## Creating the Etcd snapshot

First we must Pause reconciliation of the HostedCluster:

    oc patch -n clusters hostedclusters/${CLUSTER_NAME} -p '{"spec":{"pausedUntil":"'${PAUSED_UNTIL}'"}}' --type=merge

Now stop all etcd-writer deployments:

    oc scale deployment -n ${HOSTED_CLUSTER_NAMESPACE} --replicas=0 kube-apiserver openshift-apiserver openshift-oauth-apiserver

You can now perform an etcd snapshot via `exec` in any etcd container:

    oc exec -it etcd-0 -n ${HOSTED_CLUSTER_NAMESPACE} -- env ETCDCTL_API=3 /usr/bin/etcdctl --cacert /etc/etcd/tls/client/etcd-client-ca.crt --cert /etc/etcd/tls/client/etcd-client.crt --key /etc/etcd/tls/client/etcd-client.key --endpoints=localhost:2379 snapshot save /var/lib/data/snapshot.db
    oc exec -it etcd-0 -n ${HOSTED_CLUSTER_NAMESPACE} -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/data/snapshot.db

The snapshot data can then be copied to some location where it can be later retrieved, for example S3:

    BUCKET_NAME=somebucket
    FILEPATH="/${BUCKET_NAME}/${CLUSTER_NAME}-snapshot.db"
    CONTENT_TYPE="application/x-compressed-tar"
    DATE_VALUE=`date -R`
    SIGNATURE_STRING="PUT\n\n${CONTENT_TYPE}\n${DATE_VALUE}\n${FILEPATH}"
    ACCESS_KEY=accesskey
    SECRET_KEY=secret
    SIGNATURE_HASH=`echo -en ${SIGNATURE_STRING} | openssl sha1 -hmac ${SECRET_KEY} -binary | base64`

    oc exec -it etcd-0 -n ${HOSTED_CLUSTER_NAMESPACE} -- curl -X PUT -T "/var/lib/data/snapshot.db" \
      -H "Host: ${BUCKET_NAME}.s3.amazonaws.com" \
      -H "Date: ${DATE_VALUE}" \
      -H "Content-Type: ${CONTENT_TYPE}" \
      -H "Authorization: AWS ${ACCESS_KEY}:${SIGNATURE_HASH}" \
      https://${BUCKET_NAME}.s3.amazonaws.com/${CLUSTER_NAME}-snapshot.db

If you wish to restore the snapshot on some new cluster later, ensure you have saved the encryption secret referenced from the HostedCluster resource, e.g:

    oc get hostedcluster $CLUSTER_NAME -o=jsonpath='{.spec.secretEncryption.aescbc}'
    {"activeKey":{"name":"CLUSTER_NAME-etcd-encryption-key"}}

    # Save this secret, or the key it contains so the etcd data can later be decrypted
    oc get secret ${CLUSTER_NAME}-etcd-encryption-key -o=jsonpath='{.data.key}'

## Restoring an etcd snapshot

An etcd snapshot can currently only be restored on cluster creation, this can be
achieved by modifying the output from `create cluster --render`, and defining a
`restoreSnapshotURL` in the etcd section of the HostedCluster spec.

First we must create a pre-signed URL so the previously saved etcd snapshot
can be downloaded from S3 without passing credentials to the etcd deployment,
with the `aws` CLI this can be achieved like:

    ETCD_SNAPSHOT=${ETCD_SNAPSHOT:-"s3://${BUCKET_NAME}/${CLUSTER_NAME}-snapshot.db"}
    ETCD_SNAPSHOT_URL=$(aws s3 presign ${ETCD_SNAPSHOT})

Now we modify the HostedCluster spec to refer to this URL, e.g:

    spec:
      etcd:
        managed:
          storage:
            persistentVolume:
              size: 4Gi
            type: PersistentVolume
            restoreSnapshotURL:
            - "${ETCD_SNAPSHOT_URL}"
        managementType: Managed

Finally you must ensure the secret referenced from the `spec.secretEncryption.aescbc` contains
the same AES key saved in the previous steps.


---

## Source: docs/content/how-to/aws/external-dns.md

---
title: External DNS
---

# External DNS

Hypershift separation between Control Plane and Data Plane enables two independent areas for DNS configuration: 

* Ingress for workloads within the hosted cluster (traditionally *.apps.service-consumer-domain.com).
* Ingress for service endpoints within the management cluster (e.g. api / oauth endpoints via *.service-provider-domain.com).


The input for the `hostedCluster.spec.dns` dictates the first one.
The input for `hostedCluster.spec.services.routePublishingStrategy.hostname` dictates the second one, which we'll elaborate in this doc.

Note: External DNS will only make a difference for setups with Public endpoints i.e. "Public" or "PublicAndPrivate". For a "Private" setup all endpoints will be accessible via `.hypershift.local`, which will contain CNAME records to the appropriate Private Link Endpoint Services.

# Use Service-level DNS for Control Plane Services
There are four service that are exposed by a Hosted Control Plane (HCP)

* `APIServer`
* `OAuthService`
* `Konnectivity`
* `Ignition`

Each of these services is exposed using a `servicePublishingStrategy` in the HostedCluster spec.

By default, for `servicePublishingStrategy` types `LoadBalancer` and `Route`, the service will be published using the hostname of the LoadBalancer found in the status of the `Service` with type `LoadBalancer`, or in the `status.host` field of the `Route`.

This is acceptable for Hypershift development environments.  However, when deploying Hypershift in a managed service context, this method leaks the ingress subdomain of the underlying management cluster and can limit options for management cluster lifecycling and disaster recovery.  For example, if the AWS load balancer for a service is lost for whatever reason, the DNS name of that load balancer is in the kubelet kubeconfig of each node in the guest cluster.  Restoring the cluster would involve an out-of-band update of all kubelet kubeconfigs on existing nodes.

Having a DNS indirection layer on top of the `LoadBalancer` and `Route` publishing types allows a managed service operator to publish all _public_ HostedCluster `services` using a service-level domain.  This allows remampping on the DNS name to a new `LoadBalancer` or `Route` and does not expose the ingress domain of the management cluster.

## External-dns setup

Hypershift uses external-dns to achieve this indirection.

`external-dns` is optionally deployed alongside the `hypershift-operator` in the `hypershift` namespace of the management cluster. It watches the cluster for `Services` or `Routes` with the `external-dns.alpha.kubernetes.io/hostname` annotation.  This value of this annotation is used to create a DNS record pointing to the `Service` (A record) or `Route` (CNAME record).

`hypershift install` will create the `external-dns` deployment if the proper flags are set:

```
hypershift install --external-dns-provider=aws --external-dns-credentials=route53-aws-creds --external-dns-domain-filter=service-provider-domain.com ...
```

where `external-dns-provider` is the DNS provider that manages the service-level DNS zone, `external-dns-credentials` is the credentials file appropriate for the specified provider, and `external-dns-domain-filter` is the service-level domain.

### Creating the public DNS Hosted zone

**external-dns-domain**: Should point to an external public domain (pre-created).

To create this Hosted Zone, you can do it in the AWS Route53 Management console:

- Create a Hosted zone in your Management console



- Grab the values from the zone created to configure them (in our case) in the subzone



- In the main domain, create the NS record to redirect the DNS requests to that delegated zone (use the values from the previous step)



- In order to verify that all works fine, create a test entry in the new subzone and test it with a dig command





## HostedCluster with Service Hostnames

Create a HostedCluster that sets `hostname` for `LoadBalancer` and `Route` services:

```
hypershift create cluster aws --name=example --endpoint-access=PublicAndPrivate --external-dns-domain=service-provider-domain.com ...
```

> **NOTE:** The **external-dns-domain** should match the Public Hosted Zone created in the previous step

The resulting HostedCluster `services` block looks like this:

```
  platform:
    aws:
      endpointAccess: PublicAndPrivate
...
  services:
  - service: APIServer
    servicePublishingStrategy:
      route:
        hostname: api-example.service-provider-domain.com
      type: Route
  - service: OAuthServer
    servicePublishingStrategy:
      route:
        hostname: oauth-example.service-provider-domain.com
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
```

When the `Services` and `Routes` are created by the Control Plane Operator (CPO), it will annotate them with the `external-dns.alpha.kubernetes.io/hostname` annotation. The value will be the `hostname` field in the `servicePublishingStrategy` for that type.  The CPO uses this name blindly for the service endpoints and assumes that if `hostname` is set, there is some mechanism external-dns or otherwise, that will create the DNS records.

There is an interaction between the `spec.platform.aws.endpointAccess` and which services are permitted to set `hostname` when using AWS Private clustering.  Only *public* services can have service-level DNS indirection.  Private services use the `hypershift.local` private zone and it is not valid to set `hostname` for `services` that are private for a given `endpointAccess` type.

The following table notes when it is valid to set hostname for a particular `service` and `endpointAccess` combination:

|              | Public | PublicAndPrivate | Private |
|--------------|--------|------------------|---------|
| APIServer    | Y      | Y                | N       |
| OAuthServer  | Y      | Y                | N       |
| Konnectivity | Y      | N                | N       |
| Ingition     | Y      | N                | N       |

## Examples of how to deploy a cluster using the CLI and externalDNS

Having a vanilla Openshift cluster, follow this steps

<details>
<summary>Deploy Hypershift and ExternalDNS operators with the external Public HostedZone already created</summary>

- Ensure the public hosted zone already exists, in our case is `service-provider-domain.com` 
- Hypershift Deployment command
```bash
export KUBECONFIG=<PATH TO MANAGEMENT's CLUSTER's KUBECONFIG's>
export REGION=us-west-1
export BUCKET_NAME=jparrill-hosted-us-west-1
export AWS_CREDS=~/.aws/credentials

hypershift install \
    --oidc-storage-provider-s3-bucket-name ${BUCKET_NAME} \
    --oidc-storage-provider-s3-credentials ${AWS_CREDS} \
    --oidc-storage-provider-s3-region ${REGION} \
    --external-dns-provider=aws \
    --external-dns-credentials=${AWS_CREDS} \
    --external-dns-domain-filter=service-provider-domain.com \
    --private-platform AWS \
    --aws-private-creds ${AWS_CREDS} \
    --aws-private-region ${REGION}
```

</details>

<details>
<summary>Deploy HostedCluster using ExternalDNS feature</summary>

- Ensure the `externaldns` operator is up and the internal flags points to the desired public hosted zone
- HostedCluster Deployment command 
```bash
export KUBECONFIG=<MGMT Cluster Kubeconfig>
export AWS_CREDS=~/.aws/credentials
export REGION=us-west-1

hypershift create cluster aws \
    --aws-creds ${AWS_CREDS} \
    --instance-type m6i.xlarge \
    --region ${REGION} \
    --auto-repair \
    --generate-ssh \
    --name jparrill-hosted \
    --namespace clusters \
    --base-domain service-consumer-domain.com \
    --node-pool-replicas 2 \
    --pull-secret ${HOME}/pull_secret.json \
    --release-image quay.io/openshift-release-dev/ocp-release:4.12.0-ec.3-x86_64 \
    --external-dns-domain=service-provider-domain.com \
    --endpoint-access=PublicAndPrivate
```

Let's remark some things from this command:<br>

```bash
- external-dns-domain: Points to our public externalDNS hosted zone service-provider-domain.com, typically in an AWS account owned by the service provider.
- base-domain: Points to the public hosted zone service-consumer-domain.com, typically in an AWS account owned by the service consumer. 
- endpoint-access: Is set as PublicAndPrivate. ExternalDNS feature only could be used with Public and PublicAndPrivate configurations.
```

</details>


---

## Source: docs/content/how-to/aws/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/aws/other-sdn-providers.md

This document explains how to create a HostedCluster that runs an SDN provider different from OVNKubernetes. The document assumes that you already have the required infrastructure in place to create HostedClusters.

!!! important

    The work described here is **not supported**. SDN providers **must** certify their software on HyperShift before it becomes a supported solution. The steps described here are just a technical reference for people who wants to try different SDN providers in HyperShift.

Versions used while writing this doc:

- Management cluster running OpenShift `v4.14.5` and HyperShift Operator version `e87182ca75da37c74b371aa0f17aeaa41437561a`.
- HostedCluster release set to OpenShift `v4.14.10`.

!!! important

    To configure a different CNI provider for the Hosted Cluster, you must adjust the `hostedcluster.spec.networking.networkType` to `Other`. By doing so, the Control Plane Operator will skip the deployment of the default CNI provider.

## Calico
### Deployment

In this scenario we are using the Calico version v3.27.0 which is the last one at the time of this writing. The steps followed rely on the docs by Tigera to deploy Calico on OpenShift.

1. Create a `HostedCluster` and set its `HostedCluster.spec.networking.networkType` to `Other`.

2. Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.

3. Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in `NotReady` state.

    ~~~sh
    export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig
    oc get nodes
    ~~~

    ~~~output
    NAME             STATUS     ROLES    AGE     VERSION
    hosted-worker1   NotReady   worker   2m51s   v1.27.8+4fab27b
    hosted-worker2   NotReady   worker   2m52s   v1.27.8+4fab27b
    ~~~

4. Apply the yaml manifests provided by `Tigera` in the HostedCluster:

    ~~~sh
    mkdir calico
    wget -qO- https://github.com/projectcalico/calico/releases/download/v3.27.0/ocp.tgz | tar xvz --strip-components=1 -C calico
    cd calico/
    ls *crd*.yaml | xargs -n1 oc apply -f
    ls 00* | xargs -n1 oc apply -f
    ls 01* | xargs -n1 oc apply -f
    ls 02* | xargs -n1 oc apply -f
    ~~~

    ~~~output
    customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
    namespace/calico-apiserver created
    namespace/calico-system created
    namespace/tigera-operator created
    apiserver.operator.tigera.io/default created
    installation.operator.tigera.io/default created
    configmap/calico-resources created
    clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
    clusterrole.rbac.authorization.k8s.io/tigera-operator created
    serviceaccount/tigera-operator created
    deployment.apps/tigera-operator created
    ~~~

### Checks

We should see the following pods running in the `tigera-operator` namespace:

  ~~~sh
  oc -n tigera-operator get pods
  ~~~

  ~~~output
  NAME                              READY   STATUS    RESTARTS   AGE
  tigera-operator-dc7c9647f-fvcvd   1/1     Running   0          2m1s
  ~~~

We should see the following pods running in the `calico-system` namespace:

  ~~~sh
  oc -n calico-system get pods
  ~~~

  ~~~output
  NAME                                       READY   STATUS    RESTARTS   AGE
  calico-kube-controllers-69d6d5ff89-5ftcn   1/1     Running   0          2m1s
  calico-node-6bzth                          1/1     Running   0          2m2s
  calico-node-bl4b6                          1/1     Running   0          2m2s
  calico-typha-6558c4c89d-mq2hw              1/1     Running   0          2m2s
  csi-node-driver-l948w                      2/2     Running   0          2m1s
  csi-node-driver-r6rgw                      2/2     Running   0          2m2s
  ~~~

We should see the following pods running in the `calico-apiserver` namespace:

  ~~~sh
  oc -n calico-apiserver get pods
  ~~~

  ~~~output
  NAME                                READY   STATUS    RESTARTS   AGE
  calico-apiserver-7bfbf8fd7c-d75fw   1/1     Running   0          84s
  calico-apiserver-7bfbf8fd7c-fqxjm   1/1     Running   0          84s
  ~~~

The nodes should've moved to `Ready` state:

  ~~~sh
  oc get nodes
  ~~~

  ~~~output
  NAME             STATUS   ROLES    AGE   VERSION
  hosted-worker1   Ready    worker   10m   v1.27.8+4fab27b
  hosted-worker2   Ready    worker   10m   v1.27.8+4fab27b
  ~~~

The HostedCluster deployment will continue, at this point the SDN is running.

## Cilium
### Deployment

In this scenario we are using the Cilium version v1.14.5 which is the last one at the time of this writing. The steps followed rely on the docs by Cilium project to deploy Cilium on OpenShift.

1. Create a `HostedCluster` and set its `HostedCluster.spec.networking.networkType` to `Other`.

2. Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.

3. Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in `NotReady` state.

    ~~~sh
    export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig
    oc get nodes
    ~~~

    ~~~output
    NAME             STATUS     ROLES    AGE     VERSION
    hosted-worker1   NotReady   worker   2m30s   v1.27.8+4fab27b
    hosted-worker2   NotReady   worker   2m33s   v1.27.8+4fab27b
    ~~~

4. Apply the yaml manifests provided by `Isovalent` in the HostedCluster:

    ~~~sh
    #!/bin/bash

    version="1.14.5"
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-03-cilium-ciliumconfigs-crd.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00000-cilium-namespace.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00001-cilium-olm-serviceaccount.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00002-cilium-olm-deployment.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00003-cilium-olm-service.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00004-cilium-olm-leader-election-role.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00005-cilium-olm-role.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00006-leader-election-rolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00007-cilium-olm-rolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00008-cilium-cilium-olm-clusterrole.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00009-cilium-cilium-clusterrole.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00010-cilium-cilium-olm-clusterrolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00011-cilium-cilium-clusterrolebinding.yaml
    ~~~


5. Use the right configuration for each network stack

=== "IPv4"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv4PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: true
      ipv6:
        enabled: false
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv4PodCIDRList:
            - "10.128.0.0/14"
          clusterPoolIPv4MaskSize: "23"
      nativeRoutingCIDR: "10.128.0.0/14"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~

=== "IPv6"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv6PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: false
      ipv6:
        enabled: true
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv6PodCIDRList:
            - "fd01::/48"
          clusterPoolIPv6MaskSize: "48"
      nativeRoutingCIDR: "fd01::/48"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~


=== "Dual stack"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv4PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: true
      ipv6:
        enabled: true
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv4PodCIDRList:
            - "10.128.0.0/14"
          clusterPoolIPv4MaskSize: "23"
      nativeRoutingCIDR: "10.128.0.0/14"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~

!!! important

    Make sure you've changed the networking values according to your platform details `spec.ipam.operator.clusterPoolIPv4PodCIDRList`, `spec.ipam.operator.clusterPoolIPv4MaskSize` and `nativeRoutingCIDR` in IPv4 and `spec.ipam.operator.clusterPoolIPv6PodCIDRList`, `spec.ipam.operator.clusterPoolIPv6MaskSize` and `nativeRoutingCIDR` in IPv6 case.

### Checks

This will be the output:

  ~~~output
  customresourcedefinition.apiextensions.k8s.io/ciliumconfigs.cilium.io created
  namespace/cilium created
  serviceaccount/cilium-olm created
  Warning: would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), hostPort (container "operator" uses hostPort 9443), allowPrivilegeEscalation != false (container "operator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "operator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "operator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "operator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
  deployment.apps/cilium-olm created
  service/cilium-olm created
  role.rbac.authorization.k8s.io/cilium-olm-leader-election created
  role.rbac.authorization.k8s.io/cilium-olm created
  rolebinding.rbac.authorization.k8s.io/leader-election created
  rolebinding.rbac.authorization.k8s.io/cilium-olm created
  clusterrole.rbac.authorization.k8s.io/cilium-cilium-olm created
  clusterrole.rbac.authorization.k8s.io/cilium-cilium created
  clusterrolebinding.rbac.authorization.k8s.io/cilium-cilium-olm created
  clusterrolebinding.rbac.authorization.k8s.io/cilium-cilium created
  ciliumconfig.cilium.io/cilium created
  ~~~

We should see the following pods running in the `cilium` namespace:

  ~~~sh
  oc -n cilium get pods
  ~~~

  ~~~output
  NAME                               READY   STATUS    RESTARTS   AGE
  cilium-ds5tr                       1/1     Running   0          106s
  cilium-olm-7c9cf7c948-txkvt        1/1     Running   0          2m36s
  cilium-operator-595594bf7d-gbnns   1/1     Running   0          106s
  cilium-operator-595594bf7d-mn5wc   1/1     Running   0          106s
  cilium-wzhdk                       1/1     Running   0          106s
  ~~~

The nodes should've moved to `Ready` state:

  ~~~sh
  oc get nodes
  ~~~

  ~~~output
  NAME             STATUS   ROLES    AGE   VERSION
  hosted-worker1   Ready    worker   8m    v1.27.8+4fab27b
  hosted-worker2   Ready    worker   8m    v1.27.8+4fab27b
  ~~~

The HostedCluster deployment will continue, at this point the SDN is running.

### Validation

Additionally you have some conformance tests that could be deployed into the HostedCluster in order to validate if the Cilium SDN was deployed and working properly.

In order for Cilium connectivity test pods to run on OpenShift, a simple custom SecurityContextConstraints object is required to allow hostPort/hostNetwork which the connectivity test pods relies on. it should only set allowHostPorts and allowHostNetwork without any other privileges.

  ~~~sh
  oc apply -f - <<EOF
  apiVersion: security.openshift.io/v1
  kind: SecurityContextConstraints
  metadata:
    name: cilium-test
  allowHostPorts: true
  allowHostNetwork: true
  users:
    - system:serviceaccount:cilium-test:default
  priority: null
  readOnlyRootFilesystem: false
  runAsUser:
    type: MustRunAsRange
  seLinuxContext:
    type: MustRunAs
  volumes: null
  allowHostDirVolumePlugin: false
  allowHostIPC: false
  allowHostPID: false
  allowPrivilegeEscalation: false
  allowPrivilegedContainer: false
  allowedCapabilities: null
  defaultAddCapabilities: null
  requiredDropCapabilities: null
  groups: null
  EOF
  ~~~

  ~~~sh
  version="1.14.5"
  oc apply -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/${version}/examples/kubernetes/connectivity-check/connectivity-check.yaml
  ~~~

  ~~~sh
  oc get pod -n cilium-test
  ~~~

  ~~~output
  NAME                                                     READY   STATUS    RESTARTS   AGE
  echo-a-846dcb4-kq7zh                                     1/1     Running   0          23h
  echo-b-58f67d5b86-5mrtx                                  1/1     Running   0          23h
  echo-b-host-84d7468c8d-nf4vk                             1/1     Running   0          23h
  host-to-b-multi-node-clusterip-b98ff785c-b9vgf           1/1     Running   0          23h
  host-to-b-multi-node-headless-5c55d85dfc-5xjbc           1/1     Running   0          23h
  pod-to-a-6b996b7675-46kkf                                1/1     Running   0          23h
  pod-to-a-allowed-cnp-c958b55bf-6vskb                     1/1     Running   0          23h
  pod-to-a-denied-cnp-6d9b8cbff5-lbrgp                     1/1     Running   0          23h
  pod-to-b-intra-node-nodeport-5f9c4c866f-mhfs4            1/1     Running   0          23h
  pod-to-b-multi-node-clusterip-7cb4bf5495-hmmtg           1/1     Running   0          23h
  pod-to-b-multi-node-headless-68975fc557-sqbgq            1/1     Running   0          23h
  pod-to-b-multi-node-nodeport-559c54c6fc-2rhvv            1/1     Running   0          23h
  pod-to-external-1111-5c4cfd9497-6slss                    1/1     Running   0          23h
  pod-to-external-fqdn-allow-google-cnp-7d65d9b747-w4cx5   1/1     Running   0          23h
  ~~~


---

## Source: docs/content/how-to/aws/shared-vpc.md

---
title: Shared VPC
---

# Creating a HostedCluster in a Shared VPC

## Background

AWS allows sharing a VPC's subnets from one account to another.
This enables a use case where network management can be done in a single AWS account, while workloads such as OpenShift clusters exist
in a separate, satellite account.

Creating a standalone OCP cluster is already supported in ROSA classic.

In hosted clusters, the shared VPC architecture is very similar to standalone OCP. However, hosted clusters require some new resources to be created.

The following resources must be created in the VPC owner account:

* The VPC (along with related resources such as subnets, nat gateway, internet gateway, route tables, etc.)
* Private Hosted Zone - this will be the same domain as the public hosted zone of the HostedCluster where the cluster ingress operator will place router records.
* Local Hosted Zone - the `[cluster-name].hypershift.local` hosted zone where the control plane operator places records that point to the VPC endpoint that sends traffic to the control plane.
* Ingress Shared VPC Role - a role that allows managing records in the private hosted zone. The ingress operator of the hosted cluster will assume this role in order to create records in the private hosted zone.
* Control Plane Shared VPC Role - a role that allows:
   - Managing a VPC endpoint so that workers can communicate with the control plane
   - Managing a Security Group for the VPC endpoint
   - Managing records in the local hosted zone so that workers can use known names to communicate with the VPC endpoint.


Once created, both public and private subnets of the VPC must be shared with the account where the cluster will be created.
Once shared, the subnets in the cluster creator account must be tagged with either `kubernetes.io/role/elb=1` (for public subnets)
or `kubernetes.io/role/internal-elb=1` (for private subnets). It is important to note that any tags that were set on the subnets in the VPC owner account do not transfer to the cluster creator account when shared.

```mermaid
---
config:
  layout: dagre
  theme: mc
  look: handDrawn
---
flowchart LR
 subgraph vpc1["VPC"]
        vpc1_subnets["vpc1_subnets"]
        igw["Internet Gateway"]
        nat["NAT Gateway"]
        ep["VPC Endpoint"]
  end
 subgraph vpc1_subnets["Subnets"]
        subnet1_1["Subnet A"]
        subnet1_2["Subnet B"]
        subnet1_3["Subnet C"]
  end
 subgraph account1["VPC Owner AWS Account"]
        vpc1
        account1_hz["account1_hz"]
        account1_iam["account1_iam"]
  end
 subgraph account1_hz["Route53"]
        hz1["Private Hosted Zone"]
        hz2["HyperShift Local Hosted Zone"]
  end
 subgraph account1_iam["IAM"]
        role1["Ingress Shared VPC Role"]
        role2["Control Plane Shared VPC Role"]
  end
 subgraph vpc2["VPC"]
        vpc2_subnets["vpc2_subnets"]
  end
 subgraph vpc2_subnets["Subnets"]
        subnet2_1["subnet2_1"]
        subnet2_2["subnet2_2"]
        subnet2_3["subnet2_3"]
  end
 subgraph subnet2_1["Subnet A"]
        instance1["Instance 1"]
        instance2["Instance 2"]
  end
 subgraph subnet2_2["Subnet B"]
        instance3["Instance 3"]
        instance4["Instance 4"]
  end
 subgraph subnet2_3["Subnet C"]
        instance5["Instance 5"]
  end
 subgraph account2["Cluster Creator AWS Account"]
        vpc2
  end
    hz1 --- vpc1
    hz2 --- vpc1
    role1 -.-> hz1
    role2 -.-> hz2 & ep
    vpc1_subnets -.-> vpc2_subnets
    style vpc2_subnets stroke:#777
    style subnet2_1 stroke:#777,fill:#eee,stroke-dasharray: 5 5
    style subnet2_2 stroke:#777,fill:#eee,stroke-dasharray: 5 5
    style subnet2_3 stroke:#777,fill:#eee,stroke-dasharray: 5 5
    style vpc2 stroke:#777
```

## API fields to support a Shared VPC cluster

The HostedCluster API exposes a `sharedVPC` field under `.spec.platform.aws` that allows specifying the following fields:
- IngressRoleARN - the ARN of a role in the VPC owner account that allows managing DNS records in the private hosted zone of the HostedCluster. The ingress operator will assume this role to manage DNS records.
- ControlPlaneRoleARN - the ARN of a role in the VPC owner account that allows creating a VPC endpoint, a corresponding security group and DNS records in the `[cluster-name].hypershift.local` hosted zone for this HostedCluster.
- LocalZoneID - the ID of the route53 hosted zone for `[cluster-name].hypershift.local` that is associated with the HostedCluster's VPC and exists in the VPC owner account.

The `sharedVPC` field is optional. However, if it is set, all of the above fields are required and the HostedCluster is assumed to be using a shared VPC for its infrastructure.


## Using the CLI to create a Hosted Cluster in a Shared VPC

The `hypershift create cluster aws` command exposes a flag `--vpc-owner-aws-creds` that allows specifying a file that contains credentials to the VPC owner account. With this credential file, the `hypershift create cluster aws` command will create the VPC and all required artifacts in the VPC owner account, share the subnets, and create a hosted cluster in the cluster creator account (the one for which AWS creds are normally specified).

Example command:

```
hypershift create cluster aws \
  --aws-creds ${AWS_CREDS_FILE} \
  --vpc-owner-aws-creds ${VPC_OWNER_AWS_CREDS_FILE} \
  --region us-east-2 \
  --auto-repair \
  --generate-ssh \
  --name ${NAME} \
  --endpoint-access PublicAndPrivate \
  --node-pool-replicas 2 \
  --pull-secret ${PULL_SECRET_FILE} \
  --base-domain ${BASE_DOMAIN} \
  --external-dns-domain ${CUSTOM_DOMAIN} \
  --release-image ${RELEASE_IMAGE}
```

To destroy the HostedCluster, the `--vpc-owner-aws-creds` must also be specified so that resources in the VPC owner account can also be destroyed.


The `--vpc-owner-aws-creds` flag has been added to the following commands:

* `hypershift create cluster aws`
* `hypershift create infra aws`
* `hypershift create iam aws`
* `hypershift destroy cluster aws`
* `hypershift destroy infra aws`
* `hypershift destroy iam aws`


---

## Source: docs/content/how-to/aws/troubleshooting/debug-nodes.md

# Debug why nodes have not joined the cluster

If your control plane API endpoint has become available, but nodes are not joining the hosted cluster,
you can check the following:

1. Check that your machines have been created in the control plane namespace:
   ```
   HC_NAMESPACE="clusters"
   HC_NAME="cluster-name"
   CONTROL_PLANE_NAMESPACE="${HC_NAMESPACE}-${HC_NAME}"
   oc get machines.cluster.x-k8s.io -n $CONTROL_PLANE_NAMESPACE
   oc get awsmachines -n $CONTROL_PLANE_NAMESPACE
   ```

    If machines don't exist, check that a machinedeployment and machineset have been created:
    ```
    oc get machinedeployment -n $CONTROL_PLANE_NAMESPACE
    oc get machineset -n $CONTROL_PLANE_NAMESPACE
    ```
    In the case that no machinedeployment has been created look at the logs of the hypershift 
    operator:
    ```
    oc logs deployment/operator -n hypershift
    ```

    If the machines exist but have not been provisioned, check the log of the cluster API provider:
    ```
    oc logs deployment/capi-provider -c manager -n $CONTROL_PLANE_NAMESPACE
    ```

2. If machines exist and have been provisioned, check that they have been initialized via ignition.
   If you are using AWS, you can look at the system console logs of the machines by using the hypershift
   console-logs utility:

    ```
    ./bin/hypershift console-logs aws --name $HC_NAME --aws-creds ~/.aws/credentials --output-dir /tmp/console-logs
    ```
   
    The console logs will be placed in the destination directory.
    When looking at the console logs look for any errors accessing the ignition endpoint via https. If there are,
    then issue is somehow the ignition endpoint exposed by the control plane is not accessible from the worker
    instances.

3. If the machines look like they have been provisioned correctly, you can access the systemd journal of
   each machine. You can do this by first creating a bastion machine that will allow you to ssh to worker nodes
   and running a utility script that will download logs from the machines.

    Extract the public/private key for the cluster. If you created the cluster with the --generate-ssh flag, a
    ssh key for your cluster was placed in the same namespace as the hosted cluster (default `clusters`). If you 
    specified your own key and know how to access it, you can skip this step.
    ```
    mkdir /tmp/ssh
    oc get secret -n clusters ${HC_NAME}-ssh-key -o jsonpath='{ .data.id_rsa }' | base64 -d > /tmp/ssh/id_rsa
    oc get secret -n clusters ${HC_NAME}-ssh-key -o jsonpath='{ .data.id_rsa\.pub }' | base64 -d > /tmp/ssh/id_rsa.pub
    ```
 
    Create a bastion machine
    ```
    ./bin/hypershift create bastion aws --aws-creds ~/.aws/credentials --name $CLUSTER_NAME --ssh-key-file /tmp/ssh/id_rsa.pub
    ```
 
    Run the following script to extract journals from each of your workers:
    ```
    mkdir /tmp/journals
    INFRAID="$(oc get hc -n clusters $CLUSTER_NAME -o jsonpath='{ .spec.infraID }')"
    SSH_PRIVATE_KEY=/tmp/ssh/id_rsa
    ./test/e2e/util/dump/copy-machine-journals.sh /tmp/journals
    ```
 
    Machine journals should be placed in the `/tmp/journals` directory in compressed format. Extract them and look for a repeating
    error near the bottom that should indicate why the kubelet has not been able to join the cluster.

---

## Source: docs/content/how-to/aws/troubleshooting/index.md

# Troubleshooting HyperShift on AWS
This section of the HyperShift documentation contains pages related to troubleshooting specific issues when using the AWS cloud provider.

- Debug Missing Nodes
- Debug Disaster Recovery Issues

---

## Source: docs/content/how-to/aws/troubleshooting/troubleshooting-disaster-recovery.md

# Debug Disaster Recovery - Hosted Cluster Migration
These are issues related to disaster recovery that we've identified, and you could face during a Hosted Cluster migration.

## New workloads do not get scheduled in the new migrated cluster
Everything looks normal, in the destination Management or Hosted Cluster and in the old Management and Hosted Cluster, but your new workloads do not schedule in your migrated Hosted Cluster (your old ones should work properly).

Eventually your pods begin to fall down and the cluster status becomes degraded.

1. First thing you need to check is the cluster operators and validate all of them work properly:
```
oc get co
```

2. If there are some of them degraded and with errors, please check the logs and validate things point to an OVN issue.
```
oc get co <Operator's Name> -o yaml
```

3. To solve the issue, we need to ensure the old Hosted Cluster is in pause (we also need this) or deleted.

4. Now we need to delete the OVN pods
```
oc --kubeconfig=${HC_KUBECONFIG} delete pod -n openshift-ovn-kubernetes --all
```

Eventually the Hosted Cluster will start self healing and the ClusterOperator will come back.

**Cause:** The cause of this issue is after the Hosted Cluster Migration the KAS (Kube API Server) uses the same DNS name, but it points to different load balancer in AWS platform. Sometimes OVN does not behave correctly facing this situation.

## The migration gets blocked in ETCD recovery

The context around it's basically "I've edited the Hosted Cluster adding the `ETCDSnapshotURL` but the modification disappears and does not continue".

The first symptom is the status of the Hypershift Operator pod, in this case the pod is usually in `CrashLoopBackOff` status.

To solve this issue we need to:

1. Kill the Hypershift Operator pod
```
oc delete pod -n hypershift -lapp=operator
```

2. Continue editing the HostedCluster, in order to add the `ETCDSnapshotURL` to the Hosted Cluster spec.

3. Now the ETCD pod will raise up using the snapshot from S3 bucket.

**Cause:** This issue happens when the Hypershift operator is down and the Hosted Cluster controller cannot handle the modifications in the objects which belong to it.

## The nodes cannot join the new Hosted Cluster and stay in the older one

We have 2 paths to follow, and it depends on if this code is in your Hypershift Operator.

### The PR is merged and my Hypershift Operator has that code running

If that's the case, you need to make sure your Hosted Cluster is paused:
```
oc get hostedcluster -n <HC Namespace> <HC Name> -ojsonpath={.Spec.pausedUntil}
```

If this command does not give you any output, make sure you've followed properly the "Disaster Recovery" procedure, more concretelly pausing the Hosted Cluster and NodePool.

Even if it's paused and is still in that situation, please **continue to the next section** because it's highly probable that you don't have the code which manages this situation properly.

### The PR is not merged or my Hypershift Operator does not have that code running

If that's not the case, the only way to solve it is executing the teardown of the old Hosted Cluster prior the full restoration in the new Management cluster. Make sure you already have all the Manifests and the ETCD backed up.

Once you followed the Teardown procedure of the old Hosted Cluster, you will see how the migrated Hosted Cluster begins to self-recover.

**Cause:** This issue occurs when the old Hosted Cluster has a conflict with the AWSPrivateLink object. The old one is still running and the new one cannot handle it because the `hypershift.local` AWS internal DNS entry still points to the old LoadBalancer.

## Dependent resources block the old Hosted Cluster teardown

To solve this issue you need to check all the objects in the HostedControlPlane Namespace and make sure all of them are being terminated. To do that we recommend to use an external tool called ketall which gives you a complete overview of all resources in a kubernetes cluster.

You need to know what object is preventing the Hosted Cluster from being deleted and ensure that the finalizer finishes successfully.

If you don't care about the stability of the old Management cluster, this script could help you to delete all the components in the **HostedControlPlane Namespace** (you need the ketall tool):

```
#!/bin/bash

####
# Execution sample:
# ./delete_ns.sh $NAMESPACE
####

NAMESPACE=$1

if [[ -z $1 ]];then
        echo "Specify the Namespace!"
        exit 1
fi

for object in $(ketall -n $NAMESPACE -o name | grep -v packa)
do
    oc -n $NAMESPACE patch $object -p '{"metadata":{"finalizers":null}}' --type merge
done
```

Eventually, the namespace will be successfully terminated and also the Hosted Cluster.

**Cause:** This is pretty common issue in the Kubernetes/Openshift world. You are trying to delete a resource that has other dependedent objects. The finalizer is still trying to delete them but it cannot progress.

## The Storage ClusterOperator keeps reporting "Waiting for Deployment"

To solve this issue you need to check that all the pods from the **HostedCluster** and the **HostedControlPlane** are running, not blocked and there are no issues in the `cluster-storage-operator` pod. After that you need to delete the **AWS EBS CSI Drivers** from the HCP namespace in the destination management cluster:

- Delete the AWS EBS CSI Drivers deployments
```
oc delete aws-ebs-csi-driver-controller aws-ebs-csi-driver-operator
```

The operator will take a while to raise up again and eventually the driver controller will be deployed by the `aws-ebs-csi-driver-operator`.

**Cause:** This issue probably comes from objects that are deployed by the Operator. In this case, `cluster-storage-operator`, but the controller or the operator does not reconcile over them. If you delete the deployments, you ensure the operator is recreated from scratch.


## The image-registry ClusterOperator keeps reporting a degraded status

When a migration is done and the image-registry clusteroperator is marked as degraded, you will need to figure out how it reaches that status. The message will look like `ImagePrunerDegraded: Job has reached the specified backoff limit`.

Things we need to review:

- Look for failure pods in the HostedControlPlane namespace at the destination management cluster.
- Check the other Cluster operators in the HostedCluster.
- Check if the nodes are ready and working fine in the HostedCluster.

If all three components are working fine, the issue is in the backoff times of the executed job `image-pruner-XXXX`; this job has most likely failed. Once the migrated cluster has already converged and looks fine, you will need to make sure to fix this cluster operator manually; you will need to determine if you want immediate resolution, or you can wait 24h and the cronjob will raise up another job by itself.

To solve it manually, you need to:

- Reexecute the job `image-pruner-xxxx` from the `openshift-image-registry` namespace, using a cronjob called `image-pruner`
```
oc create job -n openshift-image-registry --from=cronjob/image-pruner image-pruner-recover
```

This command creates a new job in that namespace and eventually will report the new status to the cluster operator.

---

## Source: docs/content/how-to/azure/azure-workload-identity-setup.md

# Azure Workload Identity Setup for Self-Managed Clusters

!!! note "Developer Preview in OCP 4.21"
    
    Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.

This document describes how to set up Azure Workload Identities and OIDC issuer for self-managed Azure HostedClusters.

!!! warning "Persistent Resource Groups"
    
    When setting up workload identities and OIDC issuer for the first time, create them in a **persistent resource group** that will not be deleted when individual clusters are destroyed. This allows you to reuse the same workload identities and OIDC issuer across multiple HostedClusters, reducing setup time and avoiding unnecessary resource recreation.
    
    - Use a persistent resource group like `os4-common`.
    - This resource group should be separate from the cluster-specific resource groups that get created and deleted with each HostedCluster.
    - The OIDC issuer storage account should also be created in this persistent resource group.

## Prerequisites

- Azure CLI (`az`) installed and configured
- `jq` command-line JSON processor
- Cloud Credential Operator (CCO) tool installed
- Appropriate Azure permissions

## Create Azure Workload Identities

!!! note "OIDC Issuer Required"

    Before running this command, you need an OIDC issuer URL. If you haven't set this up yet, see Configure OIDC Issuer below first.

You can create the required managed identities and federated credentials using the HyperShift CLI:

```bash
# Set environment variables
PERSISTENT_RG_NAME="os4-common"  # Use persistent resource group
LOCATION="eastus"
CLUSTER_NAME="my-self-managed-cluster"
INFRA_ID="${CLUSTER_NAME}-$(openssl rand -hex 4)"
AZURE_CREDS="/path/to/azure-creds.json"

# Create persistent resource group (if it doesn't exist)
az group create --name $PERSISTENT_RG_NAME --location $LOCATION

# Create workload identities using the HyperShift CLI
# (requires OIDC issuer URL - see next section if you haven't set this up yet)
hypershift create iam azure \
    --name $CLUSTER_NAME \
    --infra-id $INFRA_ID \
    --azure-creds $AZURE_CREDS \
    --location $LOCATION \
    --resource-group-name $PERSISTENT_RG_NAME \
    --oidc-issuer-url $OIDC_ISSUER_URL \
    --output-file workload-identities.json
```

This creates 7 managed identities with federated credentials for:

- Disk CSI driver
- File CSI driver
- Image Registry
- Ingress Operator
- Cloud Provider
- NodePool Management
- Network Operator

For complete documentation on the IAM commands, see Create Azure IAM Resources Separately.

## Configure OIDC Issuer

Use the Cloud Credential Operator (CCO) tool to create the OIDC issuer:

```bash
# Set OIDC issuer variables (reusing variables from previous steps)
OIDC_STORAGE_ACCOUNT_NAME="yourstorageaccount"
TENANT_ID="your-tenant-id"
# SUBSCRIPTION_ID and PERSISTENT_RG_NAME already set from previous section
# Create an RSA key pair and save the private and public key
ccoctl azure create-key-pair  

SA_TOKEN_ISSUER_PRIVATE_KEY_PATH="/path/to/serviceaccount-signer.private"
SA_TOKEN_ISSUER_PUBLIC_KEY_PATH="/path/to/serviceaccount-signer.public"

# Create OIDC issuer using CCO tool in os4-common resource group
ccoctl azure create-oidc-issuer \
    --oidc-resource-group-name ${PERSISTENT_RG_NAME} \
    --tenant-id ${TENANT_ID} \
    --region ${LOCATION} \
    --name ${OIDC_STORAGE_ACCOUNT_NAME} \
    --subscription-id ${SUBSCRIPTION_ID} \
    --public-key-file ${SA_TOKEN_ISSUER_PUBLIC_KEY_PATH}

# Set OIDC issuer URL
OIDC_ISSUER_URL="https://${OIDC_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${OIDC_STORAGE_ACCOUNT_NAME}"
```

## Verification

Verify the setup:

```bash
# List created managed identities
az identity list --resource-group $PERSISTENT_RG_NAME --output table

# Verify federated credentials for one identity
az identity federated-credential list \
    --identity-name "${AZURE_DISK_MI_NAME}" \
    --resource-group $PERSISTENT_RG_NAME

# Test OIDC issuer accessibility
curl -s "${OIDC_ISSUER_URL}/.well-known/openid-configuration" | jq .
```

## Next Steps

After setting up workload identities, you can proceed to:

- Setup Azure Management Cluster for HyperShift

---

## Source: docs/content/how-to/azure/create-azure-cluster-on-aks.md

# Create an Azure Hosted Cluster on AKS
## General
This document describes how to set up an Azure Hosted Cluster on an AKS management cluster with an ExternalDNS setup. 
Azure HostedClusters on AKS are supported from OCP 4.19.0+.

This guide provides both automated script-based setup and manual step-by-step instructions. The automated scripts are located in the /contrib/managed-azure folder in the HyperShift repo and can significantly simplify the setup process.

## Prerequisites
- Azure CLI
- jq
- oc (OpenShift CLI)
- kubectl
- ccoctl tool
  - The ccoctl tool provides various commands to assist with the creating and maintenance of cloud credentials from outside
    a cluster. More information on the tool can be found here.

## Automated Setup (Recommended)

For the quickest setup, you can use the automated scripts:

### Quick Start with Automation Scripts

1. **Create and enter a dev directory in the root of the Hypershift project**:
   ```sh
   mkdir dev && cd dev
   ```
2. **Create your user configuration file**:
   ```sh
   cat <<EOF > user-vars.sh
   # User variables.
   export PREFIX="YOUR-management"
   export PULL_SECRET="/path/to/pull-secret.txt"
   export HYPERSHIFT_BINARY_PATH="/path/to/hypershift/bin/"
   export HYPERSHIFT_IMAGE="quay.io/hypershift/hypershift-operator:latest"
   export RELEASE_IMAGE="quay.io/openshift-release-dev/ocp-release:4.20.0-ec.3-multi"
   export LOCATION="eastus"
   export AZURE_CREDS="/path/to/azure-creds.json"
   # Azure storage account names must be between 3 and 24 characters in length and may contain numbers and lowercase letters only.
   export OIDC_ISSUER_NAME="YOURmanagement"
   EOF
   ```

3. **Create Azure credentials file** (see Manual Setup Step 2 below for details)

4. **Run the complete automated setup** (authentication is automatic):
   
   For your **first cluster** (includes one-time resource setup):
   ```sh
   ../contrib/managed-azure/setup_all.sh --first-time
   ```
   
   For **additional clusters** (reuses existing resources):
   ```sh
   ../contrib/managed-azure/setup_all.sh
   ```
   
   View the script: setup_all.sh

!!! note "Automatic Authentication"
    
    The setup script automatically logs you into Azure if you're not already authenticated. No separate login step is required!

!!! warning "Important: One-Time Setup Components"
    
    Three scripts create resources that should be **reused across multiple clusters** to avoid quota issues: setup_MIv3_kv.sh (service principals and Key Vault), setup_oidc_provider.sh (OIDC issuer), and setup_dataplane_identities.sh (data plane identities). Use the `--first-time` flag only for your first cluster setup. For subsequent clusters, run the script without this flag to skip the one-time setup and reuse existing resources.

!!! tip
    The `--first-time` flag automatically handles the one-time setup resources. If you need to run individual scripts for troubleshooting, you can execute them directly from the contrib/managed-azure folder. For manual Azure authentication, a `login.sh` script is also available.

The automated setup runs these scripts in sequence:

- vars.sh - Sources variables from user-vars.sh and sets up a few more required env variables such as PERSISTENT_RG_NAME
- setup_MIv3_kv.sh - Sets up control plane identities and Key Vault
- setup_aks_cluster.sh - Creates the AKS management cluster  
- setup_external_dns.sh - Configures DNS zones and external DNS
- setup_install_ho_on_aks.sh - Installs the HyperShift operator
- setup_oidc_provider.sh - Sets up OIDC issuer for workload identity
- setup_dataplane_identities.sh - Creates data plane managed identities
- create_basic_hosted_cluster.sh - Creates the hosted cluster

## Manual Setup (Step-by-Step)

If you prefer to understand each step or need to customize the process, follow these manual steps:

## Configuration Notes

!!! tip

    For most users, the automated scripts handle all variable configuration automatically. The variables you need to set are defined in your `user-vars.sh` file as shown in the Quick Start section above.

    The `PARENT_DNS_ZONE` value may be different for different teams. Check the `os4-common` resource group associated with your subscription for pre-existing DNS zones. If there are multiple DNS zones, the parent DNS zone is typically the common suffix among them (e.g., `hypershift.azure.devcluster.openshift.com`).

## Manual Steps

### 1. Retrieve User Account Details
**Goal**: Get your Azure subscription and tenant information for authentication setup.

```sh
ACCOUNT_DETAILS=$(az account show --query '{subscriptionId: id, tenantId: tenantId}' -o json)
SUBSCRIPTION_ID=$(echo "$ACCOUNT_DETAILS" | jq -r '.subscriptionId')
TENANT_ID=$(echo "$ACCOUNT_DETAILS" | jq -r '.tenantId')
```

### 2. Create Service Principal for Authentication
**Goal**: Create a service principal that will be used by the HyperShift CLI to create cluster infrastructure.

```sh
SP_DETAILS=$(az ad sp create-for-rbac --name "$PERSONAL_SP_NAME" --role Contributor --scopes "/subscriptions/$SUBSCRIPTION_ID" -o json)
CLIENT_ID=$(echo "$SP_DETAILS" | jq -r '.appId')
CLIENT_SECRET=$(echo "$SP_DETAILS" | jq -r '.password')

cat <<EOF > azure-creds.json
{
  "subscriptionId": "$SUBSCRIPTION_ID",
  "tenantId": "$TENANT_ID",
  "clientId": "$CLIENT_ID",
  "clientSecret": "$CLIENT_SECRET"
}
EOF
```
!!! warning
      
    In order for your Hypershift cluster to create properly, the Microsoft Graph `Application.ReadWrite.OwnedBy` 
    permission must be added to your Service Principal and it also must be assigned to User Access Administrator at the 
    subscription level. 

    In most cases, you'll need to submit a DPTP request to have this done.

### 3. Control Plane Identity and Key Vault Setup
**Goal**: Set up managed identities, key vault, and service principals required for the control plane components. This includes creating certificate-based authentication for various OpenShift services and storing credentials securely.

!!! warning "One-Time Setup Only (Steps 3-5)"
    
    Steps 3-5 create resources that should be **reused across multiple clusters** to avoid Azure quota limits: service principals and Key Vault (setup_MIv3_kv.sh), OIDC issuer (setup_oidc_provider.sh), and data plane identities (setup_dataplane_identities.sh). Only run these steps once per environment. For subsequent clusters, use `setup_all.sh` without the `--first-time` flag to skip these one-time setup steps.

**Automated Script**: setup_MIv3_kv.sh

This script handles:

- Creating managed identities for AKS cluster components
- Setting up Azure Key Vault with proper RBAC authorization
- Creating service principals for control plane services (cloud provider, disk, file, image registry, etc.)
- Generating and storing certificate JSON in Key Vault
- Creating a JSON credential file for the service principals

### 4. OIDC Provider Setup
**Goal**: Create RSA keys and OIDC issuer for workload identity authentication.

**Automated Script**: setup_oidc_provider.sh

This script handles:

- Creating RSA key pairs for service account token signing
- Setting up the OIDC issuer URL in Azure storage
- Configuring the issuer for workload identity federation

### 5. Data Plane Identity Configuration
**Goal**: Create managed identities for data plane components and configure federated identity credentials for workload identity.

**Automated Script**: setup_dataplane_identities.sh

This script handles:

- Creating managed identities for Azure Disk CSI, Azure File CSI, and Image Registry
- Setting up federated identity credentials linking these identities to specific service accounts
- Generating the data plane identities configuration file

### 6. AKS Management Cluster Creation
**Goal**: Create and configure the AKS cluster that will host the HyperShift operator and manage hosted clusters.

**Automated Script**: setup_aks_cluster.sh

This script handles:

- Creating a resource group for the AKS cluster
- Creating managed identities for AKS cluster and kubelet
- Creating the AKS cluster with required features (Key Vault secrets provider, FIPS, autoscaling, etc.)
- Configuring kubeconfig access
- Setting up role assignments for the Key Vault secrets provider

### 7. DNS Configuration Setup
**Goal**: Create DNS zones and configure external DNS for cluster ingress and API access.

**Automated Script**: setup_external_dns.sh

This script handles:

- Creating a DNS zone for your cluster
- Configuring name server delegation to the parent DNS zone
- Creating a service principal for external DNS management
- Setting up proper DNS permissions and role assignments

### 8. HyperShift Operator Installation
**Goal**: Install the HyperShift operator on the AKS management cluster with proper external DNS and Azure integration.

**Automated Script**: setup_install_ho_on_aks.sh

This script handles:

- Installing required CRDs (ServiceMonitors, PrometheusRules, PodMonitors, Routes)
- Installing the HyperShift operator with Azure-specific configuration
- Configuring external DNS integration
- Setting up managed service configuration for ARO-HCP

### 9. Hosted Cluster Creation
**Goal**: Create the actual hosted OpenShift cluster using all the previously configured infrastructure.

**Automated Script**: create_basic_hosted_cluster.sh

This script handles:

- Creating the hosted cluster with all required Azure integrations
- Configuring networking (VNet, subnets, NSGs)
- Setting up marketplace image references
- Applying FIPS configuration and security settings
- Enabling workload identity and managed identity integration

### 10. Cleanup and Deletion

#### Automated Cleanup (Recommended)

If you used the automated setup scripts, you can use the corresponding deletion scripts for easy cleanup:

**Complete Cleanup** - Delete both hosted cluster and AKS management cluster:
```shell
../contrib/managed-azure/delete_all.sh
```

This is the recommended approach as it will clean up all cluster-specific resources in the correct order:
1. Hosted cluster and its managed resources
2. AKS management cluster and resource group  
3. Customer VNet and NSG resource groups
4. AKS-specific Key Vault role assignments

Note: Managed identities, service principals, and Key Vault itself are preserved for reuse across multiple clusters

**Individual Cleanup** - For more granular control:

Delete only the hosted cluster:
```shell
../contrib/managed-azure/delete_hosted_cluster.sh
```

Delete only the AKS management cluster:
```shell
../contrib/managed-azure/delete_aks_cluster.sh
```

#### Manual Deletion

You can also delete the hosted cluster manually using the hypershift CLI:
```shell
${HYPERSHIFT_BINARY_PATH}/hypershift destroy cluster azure \
--name $CLUSTER_NAME \
--azure-creds $AZURE_CREDS \
--resource-group-name ${MANAGED_RG_NAME}
```

!!! tip
    If you used the automated scripts, `CLUSTER_NAME` is set to `"${PREFIX}-hc"`, `AZURE_CREDS` matches your `user-vars.sh` configuration, and `MANAGED_RG_NAME` is `"${PREFIX}-managed-rg"`.

!!! warning
    The manual deletion command only removes the hosted cluster. You'll need to manually clean up the AKS management cluster and other Azure resources created during setup.

!!! note
    The automated deletion scripts include safety confirmations and handle resource dependencies correctly. Some deletions may take several minutes to complete in the background.

!!! important
    **Resource Reuse**: The deletion scripts preserve managed identities, service principals, and the Key Vault itself for reuse across multiple clusters. AKS-specific role assignments are removed, but the underlying identities and vault remain for reuse.

## Troubleshooting

If you encounter issues with the automated scripts:

1. **Check prerequisites**: Ensure all required tools are installed and configured
2. **Verify permissions**: Confirm your service principal has the required permissions
3. **Review logs**: The scripts use `set -x` for detailed logging
4. **Partial execution**: Run individual scripts directly from the contrib/managed-azure folder if you need to resume from a specific point
5. **Manual verification**: Use the Azure portal to verify resources were created correctly

For additional help, reach out to #project-hypershift on Red Hat Slack.


---

## Source: docs/content/how-to/azure/create-azure-cluster-with-options.md

# Create an Azure cluster with Additional Options
This document describes how to set up an Azure cluster with Hypershift with additional options.

Creating an Azure cluster with Hypershift without any additional flag options can be found here.

## Prerequisites
All sections assume you are:
1. Using an AKS management cluster
2. Set up external DNS
3. Installed the HyperShift Operator

## Encrypting the OS Disks on Azure VMs
There are a few prerequisites for encrypting the OS disks on the Azure VMs:

1. Create your own resource group
2. Create an Azure Key Vault, with purge protection required, within the resource group
3. Create a key in the vault to use to create a DiskEncryptionSet
4. Create a DiskEncryptionSet with key in the vault and grant it permissions to assess the key vault

!!! note

    You will need to use the `resource-group-name` flag when using the `DiskEncryptionSetID` flag.

After performing these steps, you just need to provide the DiskEncryptionSet ID when creating a hosted cluster.

### CLI Example
```
${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \
--name "$CLUSTER_NAME" \
--azure-creds $AZURE_CREDS \
--location ${LOCATION} \
--node-pool-replicas 2 \
--base-domain $AZURE_BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--generate-ssh \
--release-image ${RELEASE_IMAGE} \
--external-dns-domain ${MGMT_DNS_ZONE_NAME} \
--resource-group-name "${MANAGED_RG_NAME}" \
--vnet-id "${GetVnetID}" \
--subnet-id "${GetSubnetID}" \
--network-security-group-id "${GetNsgID}" \
--annotations hypershift.openshift.io/pod-security-admission-label-override=baseline \
--managed-identities-file ${SP_FILE} \
--assign-service-principal-roles \
--dns-zone-rg-name ${DNS_ZONE_RG_NAME} \
--fips=true \
--marketplace-publisher azureopenshift \
--marketplace-offer aro4 \
--marketplace-sku aro_417 \
--marketplace-version 417.94.20240701 \
--disk-encryption-set-id <disk_encryption_set_id>
```

You can also pass in the DiskEncryptionSet ID when creating a NodePool.

```
hypershift create nodepool azure \
--name <name_of_nodepool> \
--cluster-name <cluster_name> \
--resource-group-name <resource_group_name> \
--disk-encryption-set-id <disk_encryption_set_id>
```

### NodePool CR Example
The DiskEncryptionSet ID can also be set directly through the NodePool CR.

```
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  creationTimestamp: null
  name: <nodepool_name>
  namespace: clusters
spec:
  arch: amd64
  clusterName: <cluster_name>
  management:
    autoRepair: false
    upgradeType: Replace
  platform:
    azure:
      diskEncryptionSetID: <disk_encryption_set_id>
      diskSizeGB: 120
      vmsize: Standard_D4s_v4
    type: Azure
  release:
    image: <release_image>
  replicas: <number_of_replicas>
status:
  replicas: 0
```

## Enabling Ephemeral OS Disks on Azure VMs
To enable the ephemeral OS disk option on the Azure VMs in your HostedCluster, set the `enable-ephemeral-disk` flag to true.

!!! important

    Ephermeral OS disks are not available in every region or for every instance type.

    You may need to adjust the disk storage account type; to adjust the disk storage account type,
    use the `disk-storage-account-type` flag as shown in the example below.

    You may need to adjust the disk size depending on the instance type used; to adjust the disk size, use the
    `root-disk-size` flag.

    See Ephemeral OS disks for Azure VMs for more details.

```
${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \
--name "$CLUSTER_NAME" \
--azure-creds $AZURE_CREDS \
--location ${LOCATION} \
--node-pool-replicas 2 \
--base-domain $AZURE_BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--generate-ssh \
--release-image ${RELEASE_IMAGE} \
--external-dns-domain ${MGMT_DNS_ZONE_NAME} \
--resource-group-name "${MANAGED_RG_NAME}" \
--vnet-id "${GetVnetID}" \
--subnet-id "${GetSubnetID}" \
--network-security-group-id "${GetNsgID}" \
--annotations hypershift.openshift.io/pod-security-admission-label-override=baseline \
--managed-identities-file ${MANAGED_IDENTITIES_FILE} \
--assign-service-principal-roles \
--dns-zone-rg-name ${DNS_ZONE_RG_NAME} \
--fips=true \
--marketplace-publisher azureopenshift \
--marketplace-offer aro4 \
--marketplace-sku aro_417 \
--marketplace-version 417.94.20240701 \
--enable-ephemeral-disk true \
--instance-type Standard_DS4_v2 \
--disk-storage-account-type Standard_LRS
```

You can also set the `enable-ephemeral-disk` flag when creating a NodePool.
```
hypershift create nodepool azure \
--name <name_of_nodepool> \
--cluster-name <cluster_name> \
--replicas <number_of_replicas> \
--release-image <release_image> \
--enable-ephemeral-disk true \
--instance-type Standard_DS4_v2 \
--disk-storage-account-type Standard_LRS
```

## Enabling KMS encryption
This section walks through how to:

1. Set up a new resource group, key vault, and key for etcd encryption using KMSv2
1. Set up the role assignment between the KMS managed identity (MI) and the key vault
1. Set up the flags needed when creating the Azure HostedCluster
1. Verify the etcd encryption is setup and working properly 

There is a `setup_etcd_kv.sh` script in the contrib folder in the HyperShift repo to help automate the first couple of 
steps mentioned above. However, this guide will manually walk through those steps.

1a) Create a resource group for the key vault that will house the key used for etcd encryption. 

!!! note

    It is assumed this key vault is a different key vault, let's call it MI KV, than the one containing all of the 
    managed identities for the control plane. However, the managed identity for KMS is assumed to be in the MI KV.

```bash
az group create --name example-kms --location eastus
```

1b) Create the etcd encryption key vault
```bash
az keyvault create --name example-kms --resource-group example-kms --location eastus --enable-rbac-authorization
```

1c) Create a key in the etcd encryption key vault and capture the ID in a variable, KEY_ID. This will be passed when 
creating the Azure HostedCluster in a later step below.
```bash
KEY_ID=$(az keyvault key create \
  --vault-name example-kms \
  --name example-key \
  --protection software \
  --kty RSA \
  --query key.kid \
  -o tsv)
```

2) Create a role assignment between the KMS MI and the resource group where the etcd encryption key vault is located so 
that it can encrypt & decrypt objects.

```bash
OBJECT_ID="the object ID of the KMS Managed Identity. This object ID can be found under the enterprise application for your KMS Managed Identity"

az role assignment create --assignee $OBJECT_ID --role "Key Vault Crypto User" \
--scope $(az keyvault show --name example-kms --query "resourceGroup" -o tsv | xargs -I{} az group show --name {} --query "id" -o tsv)
```

3) Add these flags to your HyperShift CLI command when creating the Azure HostedCluster. KEY_ID is from step 1d.
```
`--encryption-key-id $KEY_ID` \
`--kms-credentials-secret-name <your KMS credentials secret name>`
```

4) Here are some different things you can do to confirm etcd encryption using KMSv2 is set up properly on the 
HCP/HostedCluster:

First, confirm the kube-apiserver pod is using the `encryption-provider-config` flag such as:
```
--encryption-provider-config=/etc/kubernetes/secret-encryption/config.yaml 
```

If you look at this data, it should contain something like this:
```
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
- providers:
  - kms:
      apiVersion: v2
      endpoint: unix:///opt/azurekmsactive.socket
      name: azure-20514bc7
      timeout: 35s
  - identity: {}
  resources:
  - secrets
  - configmaps
  - routes.route.openshift.io
  - oauthaccesstokens.oauth.openshift.io
  - oauthauthorizetokens.oauth.openshift.io
```

Next, confirm the ` azure-kms-provider-active` container in the kube-apiserver pod is running properly, there are no 
errors in the log, and the config file is using the KMS MI. The config file path can be found in the flag on the 
container spec:
```
--config-file-path=/etc/kubernetes/azure.json 
```

If you review this data, you should see the KMS MI credentials secret used within it.

Finally, you can create a secret on the HostedCluster and then check the secret on etcd in the etcd pod on the HCP 
directly:

1) Create a secret on the HostedCluster. Example `kubectl create secret generic kms-test --from-literal=foo=bar`.

2) Verify you can see the secret contents on the HostedCluster unencrypted.

3) Switch back to the AKS management cluster and exec into the etcd pod, `kubectl exec -it pod/etcd-0 -- /bin/bash`.

4) Run these commands in the etcd pod
```
export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/etcd/tls/etcd-ca/ca.crt
export ETCDCTL_CERT=/etc/etcd/tls/client/etcd-client.crt
export ETCDCTL_KEY=/etc/etcd/tls/client/etcd-client.key
export ETCDCTL_ENDPOINTS=https://etcd-client:2379
```
5) Get the secret created on the HostedCluster `etcdctl get /kubernetes.io/secrets/default/kms-test`. You should see it 
is encrypted with KMSv2 by the azure provider:
```
k8s:enc:kms:v2:azure-8298bce7:
ÔøΩd2%&G
      	EÔøΩÔøΩk(ÔøΩBÔøΩ	ÔøΩHÔøΩÔøΩÔøΩÔøΩÔøΩ6#ÔøΩ]ÔøΩ[ÔøΩÔøΩÔøΩI
...
ÔøΩ=ÔøΩsÔøΩÔøΩhÔøΩÔøΩÔøΩFqÔøΩÔøΩa^(ÔøΩÔøΩz ÔøΩÔøΩwI»´ÔøΩ›πÔøΩÔøΩ,ÔøΩ’¢RaÔøΩAÌôüÔøΩÔøΩ5uÔøΩŒÄÔøΩÔøΩ*ÔøΩÔøΩ·•ÉÔøΩÔøΩ∆öLÔøΩ$L1Y'ÔøΩVÔøΩ”¶iÔøΩÔøΩ.ÔøΩ	RÔøΩ"                                            ÔøΩ
version.azure.akv.io1"&
algorithm.azure.akv.io
                      RSA-OAEP-256(
```


---

## Source: docs/content/how-to/azure/create-iam-separately.md

---
title: Create Azure IAM resources separately
---

# Create Azure IAM resources separately

The `hypershift create iam azure` command creates Azure workload identities separately from infrastructure,
following the same pattern as AWS and GCP. This enables you to manage IAM resources independently from
your cluster infrastructure lifecycle.

## Overview

For self-managed Azure HyperShift clusters, workload identities authenticate cluster components to Azure
services using OIDC federation. You must create identities separately using `create iam azure` and then
consume them during infrastructure or cluster creation via the `--workload-identities-file` flag.

This approach provides control over the IAM lifecycle and follows the same pattern as AWS and GCP platforms.

## Prerequisites

Before creating Azure IAM resources, ensure you have:

- An Azure credentials file with the following format:
    ```json
    {
      "subscriptionId": "your-subscription-id",
      "tenantId": "your-tenant-id",
      "clientId": "your-client-id",
      "clientSecret": "your-client-secret"
    }
    ```
- An existing resource group where the managed identities will be created
- An OIDC issuer URL for workload identity federation

## Creating Workload Identities

Use the `hypershift create iam azure` command:

```bash
hypershift create iam azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --location LOCATION \
    --resource-group-name RESOURCE_GROUP \
    --oidc-issuer-url OIDC_ISSUER_URL \
    --output-file workload-identities.json
```

where:

* `CLUSTER_NAME` is the name of the hosted cluster you intend to create.
* `INFRA_ID` is a unique identifier used to name Azure resources. Typically this is the cluster name
    with a random suffix appended.
* `AZURE_CREDENTIALS_FILE` points to an Azure credentials file with permission to create
    managed identities and federated credentials.
* `LOCATION` is the Azure region for the managed identities (e.g., `eastus`, `westus2`).
* `RESOURCE_GROUP` is the name of an existing resource group where identities will be created.
* `OIDC_ISSUER_URL` is the URL of the OIDC identity provider used for workload identity federation.

Running this command creates:

* 7 User-Assigned Managed Identities (one per cluster component):
    - Disk CSI driver
    - File CSI driver
    - Image Registry
    - Ingress Operator
    - Cloud Provider
    - NodePool Management
    - Network Operator
* Federated Identity Credentials for each identity, configured with the OIDC issuer

## Output Format

The output file contains the workload identities in JSON format, directly consumable by the
`--workload-identities-file` flag in `create cluster azure` and `create infra azure` commands:

```json
{
  "disk": {
    "tenantID": "...",
    "clientID": "...",
    "resourceID": "/subscriptions/.../providers/Microsoft.ManagedIdentity/userAssignedIdentities/my-cluster-abc123-disk"
  },
  "file": {
    "tenantID": "...",
    "clientID": "...",
    "resourceID": "..."
  },
  "imageRegistry": { ... },
  "ingress": { ... },
  "cloudProvider": { ... },
  "nodePoolManagement": { ... },
  "network": { ... }
}
```

## Using Pre-created Identities

### With Infrastructure Creation

Pass the output file to `create infra azure`:

```bash
hypershift create infra azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --base-domain BASE_DOMAIN \
    --location LOCATION \
    --workload-identities-file workload-identities.json \
    --output-file infra-output.yaml
```

### With Cluster Creation

Or pass directly to `create cluster azure`:

```bash
hypershift create cluster azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --base-domain BASE_DOMAIN \
    --location LOCATION \
    --pull-secret PULL_SECRET_FILE \
    --workload-identities-file workload-identities.json
```

## Destroying Workload Identities

To destroy the workload identities that were created:

```bash
hypershift destroy iam azure \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --workload-identities-file workload-identities.json
```

The destroy command reads the output file from create to identify which identities to delete.
Both the managed identities and their federated credentials are removed.

!!! warning "Destroy Order"

    If you created infrastructure using these identities, destroy the infrastructure first
    before destroying the IAM resources.

## Command Reference

### Required Flags for `create iam azure`

| Flag | Description |
|------|-------------|
| `--name` | Name of the HostedCluster |
| `--infra-id` | Unique infrastructure identifier |
| `--azure-creds` | Path to Azure credentials JSON file |
| `--location` | Azure region for identities |
| `--oidc-issuer-url` | OIDC issuer URL for federation |

### Optional Flags for `create iam azure`

| Flag | Description | Default |
|------|-------------|---------|
| `--resource-group-name` | Resource group for identities | `{name}-{infra-id}` |
| `--output-file` | Output file path | `{name}-iam-output.json` |
| `--cloud` | Azure cloud environment | `AzurePublicCloud` |

### Required Flags for `destroy iam azure`

| Flag | Description |
|------|-------------|
| `--azure-creds` | Path to Azure credentials JSON file |
| `--workload-identities-file` | Path to workload identities JSON file |

### Optional Flags for `destroy iam azure`

| Flag | Description | Default |
|------|-------------|---------|
| `--cloud` | Azure cloud environment | `AzurePublicCloud` |

## Workflow Example

Here's a complete workflow for creating a self-managed Azure cluster with separate IAM management:

```bash
# 1. Set variables
export NAME="my-cluster"
export INFRA_ID="${NAME}-$(openssl rand -hex 4)"
export LOCATION="eastus"
export BASE_DOMAIN="example.com"
export AZURE_CREDS="/path/to/azure-creds.json"
export OIDC_ISSUER_URL="https://my-oidc-issuer.com"

# 2. Create a resource group for identities
az group create --name ${NAME}-rg --location ${LOCATION}

# 3. Create workload identities
hypershift create iam azure \
    --name ${NAME} \
    --infra-id ${INFRA_ID} \
    --azure-creds ${AZURE_CREDS} \
    --location ${LOCATION} \
    --resource-group-name ${NAME}-rg \
    --oidc-issuer-url ${OIDC_ISSUER_URL} \
    --output-file workload-identities.json

# 4. Create infrastructure using pre-created identities
hypershift create infra azure \
    --name ${NAME} \
    --infra-id ${INFRA_ID} \
    --azure-creds ${AZURE_CREDS} \
    --base-domain ${BASE_DOMAIN} \
    --location ${LOCATION} \
    --workload-identities-file workload-identities.json \
    --output-file infra-output.yaml

# 5. Create the cluster
hypershift create cluster azure \
    --name ${NAME} \
    --infra-id ${INFRA_ID} \
    --azure-creds ${AZURE_CREDS} \
    --base-domain ${BASE_DOMAIN} \
    --location ${LOCATION} \
    --pull-secret /path/to/pull-secret \
    --infra-json infra-output.yaml

# --- Cleanup ---

# 6. Destroy the cluster
hypershift destroy cluster azure --name ${NAME}

# 7. Destroy infrastructure
hypershift destroy infra azure \
    --name ${NAME} \
    --infra-id ${INFRA_ID} \
    --azure-creds ${AZURE_CREDS}

# 8. Destroy IAM resources
hypershift destroy iam azure \
    --azure-creds ${AZURE_CREDS} \
    --workload-identities-file workload-identities.json
```

## See Also

- Create Azure Infrastructure Separately
- Azure Workload Identity Setup
- Self-Managed Azure Overview


---

## Source: docs/content/how-to/azure/create-infra-separately.md

---
title: Create Azure infrastructure separately
---

# Create Azure infrastructure separately

The default behavior of the `hypershift create cluster azure` command is to create cloud infrastructure
along with the HostedCluster and apply it. It is possible to create the cloud infrastructure portion
separately so that the `hypershift create cluster azure` command can be used to create the cluster
with pre-existing infrastructure.

In order to do this, you need to:

1. Create the Azure infrastructure
2. Create the cluster using the infrastructure output

## Prerequisites

Before creating Azure infrastructure, ensure you have:

- An Azure credentials file with the following format:
    ```json
    {
      "subscriptionId": "your-subscription-id",
      "tenantId": "your-tenant-id",
      "clientId": "your-client-id",
      "clientSecret": "your-client-secret"
    }
    ```
- An existing public DNS zone in your Azure subscription for your base domain
- Workload identities created via `hypershift create iam azure` (for self-managed Azure with workload identity)

## Creating the Azure Infrastructure

Use the `hypershift create infra azure` command:

```bash
hypershift create infra azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --base-domain BASE_DOMAIN \
    --location LOCATION \
    --workload-identities-file WORKLOAD_IDENTITIES_FILE \
    --output-file OUTPUT_INFRA_FILE
```

where:

* `CLUSTER_NAME` is the name of the hosted cluster you intend to create.
* `INFRA_ID` is a unique name that will be used to identify your infrastructure. It is used
    to name and tag Azure resources. Typically this is the name of your cluster with a random
    suffix appended to it.
* `AZURE_CREDENTIALS_FILE` points to an Azure credentials file that has permission to create
    infrastructure resources such as VNets, subnets, load balancers, etc.
* `BASE_DOMAIN` is the base domain for your hosted cluster's ingress. It must correspond to
    an existing public DNS zone in your Azure subscription.
* `LOCATION` is the Azure region where you want to create the infrastructure (e.g., `eastus`, `westus2`).
* `WORKLOAD_IDENTITIES_FILE` is the path to the JSON file containing workload identity configuration,
    created via `hypershift create iam azure`.
* `OUTPUT_INFRA_FILE` is the file where details of the infrastructure will be stored in YAML format.

Running this command creates the following resources:

* 1 Resource Group (main cluster resource group)
* 1 Resource Group for VNet (if not using existing VNet)
* 1 Resource Group for NSG (if not using existing NSG)
* 1 Virtual Network with a default subnet
* 1 Network Security Group
* 1 Private DNS Zone
* 1 Private DNS Zone Virtual Network Link
* 1 Public IP Address for load balancer
* 1 Load Balancer with outbound rule for egress

All resources are tagged with the infrastructure ID for identification and cleanup.

!!! note "Workload Identities"

    Managed identities and federated credentials must be created separately using
    `hypershift create iam azure`. See Create Azure IAM Resources Separately.

## Using Existing Network Resources

You can use existing VNet, subnet, and network security group instead of creating new ones:

```bash
hypershift create infra azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --base-domain BASE_DOMAIN \
    --location LOCATION \
    --workload-identities-file WORKLOAD_IDENTITIES_FILE \
    --vnet-id /subscriptions/SUB_ID/resourceGroups/RG/providers/Microsoft.Network/virtualNetworks/VNET_NAME \
    --subnet-id /subscriptions/SUB_ID/resourceGroups/RG/providers/Microsoft.Network/virtualNetworks/VNET_NAME/subnets/SUBNET_NAME \
    --network-security-group-id /subscriptions/SUB_ID/resourceGroups/RG/providers/Microsoft.Network/networkSecurityGroups/NSG_NAME \
    --output-file OUTPUT_INFRA_FILE
```

## Enabling RBAC Role Assignment

To automatically assign required Azure RBAC roles to the workload identities:

```bash
hypershift create infra azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --base-domain BASE_DOMAIN \
    --location LOCATION \
    --workload-identities-file WORKLOAD_IDENTITIES_FILE \
    --assign-identity-roles \
    --dns-zone-rg-name DNS_ZONE_RG \
    --output-file OUTPUT_INFRA_FILE
```

where:

* `--assign-identity-roles` enables automatic RBAC role assignment for workload identities
* `DNS_ZONE_RG` is the name of the resource group containing your public DNS zone

## Create Workload Identities Separately

If you want to create workload identities separately before creating infrastructure, use the
dedicated IAM command:

```bash
hypershift create iam azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --location LOCATION \
    --resource-group-name EXISTING_RG \
    --oidc-issuer-url OIDC_ISSUER_URL \
    --output-file WORKLOAD_IDENTITIES_FILE
```

This creates the managed identities and federated credentials, saving them to a JSON file.
You can then use this file with `--workload-identities-file` during infrastructure or cluster creation.

For complete documentation, see Create Azure IAM Resources Separately.

## Creating the Cluster

After the infrastructure is created, use the output file with the cluster creation command:

```bash
hypershift create cluster azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --location LOCATION \
    --base-domain BASE_DOMAIN \
    --pull-secret PULL_SECRET_FILE \
    --node-pool-replicas 3 \
    --infra-json OUTPUT_INFRA_FILE
```

The `--infra-json` flag points to the infrastructure output file generated by the
`create infra azure` command.

## Destroying the Infrastructure

To destroy the infrastructure that was created:

```bash
hypershift destroy infra azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE
```

### Preserving the Resource Group

If you want to preserve the main resource group but delete only cluster-specific resources:

```bash
hypershift destroy infra azure \
    --name CLUSTER_NAME \
    --infra-id INFRA_ID \
    --azure-creds AZURE_CREDENTIALS_FILE \
    --preserve-resource-group
```

This is useful when you have other resources in the same resource group that should not be deleted.

## Command Reference

### Required Flags for `create infra azure`

| Flag | Description |
|------|-------------|
| `--name` | Name of the HostedCluster |
| `--infra-id` | Unique infrastructure identifier |
| `--azure-creds` | Path to Azure credentials JSON file |
| `--base-domain` | Base domain for cluster ingress |

### Identity Configuration

| Option | Flags Required |
|--------|----------------|
| Self-managed with workload identities | `--workload-identities-file` |

Workload identities must be created separately using `hypershift create iam azure`.
See Create Azure IAM Resources Separately.

### Optional Flags

| Flag | Description |
|------|-------------|
| `--location` | Azure region (default: `eastus`) |
| `--cloud` | Azure cloud environment (default: `AzurePublicCloud`) |
| `--resource-group-name` | Custom resource group name |
| `--resource-group-tags` | Tags to apply to resource group |
| `--vnet-id` | Use existing VNet |
| `--subnet-id` | Use existing subnet |
| `--network-security-group-id` | Use existing NSG |
| `--assign-identity-roles` | Enable RBAC role assignment |
| `--dns-zone-rg-name` | DNS zone resource group (for role assignment) |
| `--assign-custom-hcp-roles` | Use custom HCP roles instead of Contributor |
| `--disable-cluster-capabilities` | Disable specific cluster capabilities |
| `--output-file` | Output file for infrastructure details |


---

## Source: docs/content/how-to/azure/create-self-managed-azure-cluster.md

# Create a Self-Managed Azure HostedCluster

!!! note "Developer Preview in OCP 4.21"
    
    Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.

This document describes how to create a self-managed Azure HostedCluster using workload identities for authentication.

## Overview

Self-managed Azure HostedClusters use Azure Workload Identity for authentication.

## Prerequisites

Before creating a self-managed Azure HostedCluster, ensure you have:

- Azure CLI (`az`) installed and configured
- HyperShift CLI binary
- OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`)
- `jq` command-line JSON processor
- An Azure OpenShift management cluster with HyperShift operator installed
- Azure workload identities and OIDC issuer configured
- Appropriate Azure permissions (see Permission Requirements)
- **Optional**: External DNS configured (only if you want automatic DNS management)

!!! note "Setup Requirements"

    This guide assumes you have already completed the workload identity configuration and management cluster setup. Follow these guides in order:

    1. Azure Workload Identity Setup - Workload identities and OIDC issuer configuration
    2. Setup Azure Management Cluster for HyperShift - HyperShift operator installation (with or without External DNS)

### Permission Requirements

Your Azure service principal must have the following permissions:

- **Subscription Level**:
    - `Contributor` role
    - `User Access Administrator` role
- **Microsoft Graph API**:
    - `Application.ReadWrite.OwnedBy` permission (requires DPTP request in most cases)

## Creating the Self-Managed Azure HostedCluster

### Infrastructure Setup

Before creating the HostedCluster, set up the necessary Azure infrastructure:

!!! note "About PERSISTENT_RG_NAME"
    In Red Hat environments, a periodic Azure resource "reaper" deletes resources that are not properly tagged or not located in an approved resource group. We frequently use the `os4-common` resource group for shared, long-lived assets (for example, public DNS zones) to avoid accidental cleanup. If you are not in Red Hat infrastructure, set `PERSISTENT_RG_NAME` to any long-lived resource group in your subscription that will not be automatically reaped, or ensure your organization's required tags/policies are applied. The name does not have to be `os4-common`‚Äîuse whatever persistent resource group fits your environment.

```bash
# Set cluster configuration variables
PREFIX="your-prefix-sm"
RELEASE_IMAGE="quay.io/openshift-release-dev/ocp-release:XYZ"
TAG="latest"

LOCATION="eastus"
MANAGED_RG_NAME="${PREFIX}-managed-rg"
VNET_RG_NAME="${PREFIX}-customer-vnet-rg"
NSG_RG_NAME="${PREFIX}-customer-nsg-rg"
VNET_NAME="${PREFIX}-customer-vnet"
VNET_SUBNET1="${PREFIX}-customer-subnet-1"
NSG="${PREFIX}-customer-nsg"
DNS_ZONE_NAME="your-subdomain.your-parent.dns.zone.com"
CLUSTER_NAMESPACE="clusters"
CLUSTER_NAME="${PREFIX}-hc"
AZURE_CREDS="/path/to/azure/credentials"
PULL_SECRET="/path/to/pull-secret.json"
HYPERSHIFT_BINARY_PATH="/path/to/hypershift/bin"
OIDC_ISSUER_URL="https://yourstorageaccount.blob.core.windows.net/yourstorageaccount"
SA_TOKEN_ISSUER_PRIVATE_KEY_PATH="/path/to/serviceaccount-signer.private"
PERSISTENT_RG_NAME="os4-common"
PARENT_DNS_ZONE="your-parent.dns.zone.com"

# Clean up any previous instances (optional)
az group delete -n "${VNET_RG_NAME}" --yes --no-wait || true
az group delete -n "${NSG_RG_NAME}" --yes --no-wait || true

# Create managed resource group
az group create --name "${MANAGED_RG_NAME}" --location ${LOCATION}

# Create VNET & NSG resource groups
az group create --name "${VNET_RG_NAME}" --location ${LOCATION}
az group create --name "${NSG_RG_NAME}" --location ${LOCATION}

# Create network security group
az network nsg create \
    --resource-group "${NSG_RG_NAME}" \
    --name "${NSG}"

# Get NSG ID
GetNsgID=$(az network nsg list --query "[?name=='${NSG}'].id" -o tsv)

# Create VNet with subnet
az network vnet create \
    --name "${VNET_NAME}" \
    --resource-group "${VNET_RG_NAME}" \
    --address-prefix 10.0.0.0/16 \
    --subnet-name "${VNET_SUBNET1}" \
    --subnet-prefixes 10.0.0.0/24 \
    --nsg "${GetNsgID}"

# Get VNet and Subnet IDs
GetVnetID=$(az network vnet list --query "[?name=='${VNET_NAME}'].id" -o tsv)
GetSubnetID=$(az network vnet subnet show \
    --vnet-name "${VNET_NAME}" \
    --name "${VNET_SUBNET1}" \
    --resource-group "${VNET_RG_NAME}" \
    --query id --output tsv)
```

### Create the HostedCluster

!!! note "Federated Identity Prerequisites"

    Before creating the cluster, ensure that all federated identity credentials have been set up for your workload identities as described in the Azure Workload Identity Setup guide. The cluster creation will fail if these are not properly configured.

!!! note "Azure Marketplace Images"

    For OpenShift 4.20 and later, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. You no longer need to specify `--marketplace-*` flags unless you want to use a specific custom image. See Configuring Azure Marketplace Images for more details.

Create the HostedCluster:

```bash
# Create the HostedCluster
${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \
    --name "$CLUSTER_NAME" \
    --namespace "$CLUSTER_NAMESPACE" \
    --azure-creds $AZURE_CREDS \
    --location ${LOCATION} \
    --node-pool-replicas 2 \
    --base-domain $PARENT_DNS_ZONE \
    --pull-secret $PULL_SECRET \
    --generate-ssh \
    --release-image ${RELEASE_IMAGE} \
    --external-dns-domain ${DNS_ZONE_NAME} \
    --resource-group-name "${MANAGED_RG_NAME}" \
    --vnet-id "${GetVnetID}" \
    --subnet-id "${GetSubnetID}" \
    --network-security-group-id "${GetNsgID}" \
    --sa-token-issuer-private-key-path "${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH}" \
    --oidc-issuer-url "${OIDC_ISSUER_URL}" \
    --control-plane-operator-image="quay.io/hypershift/hypershift:${TAG}" \
    --dns-zone-rg-name ${PERSISTENT_RG_NAME} \
    --assign-service-principal-roles \
    --workload-identities-file ./workload-identities.json \
    --diagnostics-storage-account-type Managed
```

### Configuring Azure Marketplace Images

HyperShift supports multiple approaches for configuring Azure Marketplace images for your cluster nodes. The recommended approach varies based on your OpenShift version and requirements.

#### For OpenShift 4.20 and Later (Recommended)

**Pattern 1: Use Release Payload Defaults (Simplest)**

For OpenShift 4.20+, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. Simply omit all marketplace-related flags:

```bash
# No marketplace flags needed - HyperShift will auto-select the image
# Gen2 VM generation is used by default
${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \
    --name "$CLUSTER_NAME" \
    # ... other flags ...
```

This is the **recommended approach** as it ensures your nodes use the officially tested and supported image for your OpenShift version.

**Pattern 2: Specify VM Generation Only**

If you need to use a specific VM generation (Gen1 or Gen2), you can specify only the `--image-generation` flag:

```bash
${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \
    --name "$CLUSTER_NAME" \
    --image-generation Gen2 \  # Or Gen1 (case-sensitive)
    # ... other flags ...
```

!!! note "VM Generation"

    - Valid values: `Gen1` or `Gen2` (case-sensitive)
    - Default: `Gen2` (recommended for new clusters)
    - Gen2 VMs offer better performance and support for newer Azure features

**Pattern 3: Use Custom Marketplace Image**

If you need to use a specific custom marketplace image, provide all marketplace details:

```bash
${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \
    --name "$CLUSTER_NAME" \
    --marketplace-publisher azureopenshift \
    --marketplace-offer aro4 \
    --marketplace-sku aro_419 \
    --marketplace-version 419.6.20250523 \
    --image-generation Gen2 \  # Optional, defaults to Gen2
    # ... other flags ...
```

!!! important "Marketplace Flag Requirements"

    When specifying marketplace details, you must provide **all four** flags (`--marketplace-publisher`, `--marketplace-offer`, `--marketplace-sku`, `--marketplace-version`) together. Partial specification is not allowed.

#### For OpenShift Versions Before 4.20

For OpenShift versions prior to 4.20, you must explicitly specify marketplace image details (Pattern 3 above) or provide a custom image ID. The automatic image selection from release payload is not available.

#### Adding NodePools with Custom Images

When creating additional NodePools, you can specify image configuration in the same way:

```bash
# Use default from release payload (OCP 4.20+)
${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \
    --cluster-name "$CLUSTER_NAME" \
    # ... other flags ...

# Or specify generation
${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \
    --cluster-name "$CLUSTER_NAME" \
    --image-generation Gen1 \
    # ... other flags ...

# Or use custom marketplace image
${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \
    --cluster-name "$CLUSTER_NAME" \
    --marketplace-publisher azureopenshift \
    --marketplace-offer aro4 \
    --marketplace-sku aro_419 \
    --marketplace-version 419.6.20250523 \
    # ... other flags ...
```

!!! important "Key Configuration Options"
    
    - `--workload-identities-file`: References the workload identities configuration created in the setup guide
    - `--assign-service-principal-roles`: Automatically assigns required Azure roles to workload identities
    - `--sa-token-issuer-private-key-path`: Path to the private key for service account token signing
    - `--oidc-issuer-url`: URL of the OIDC issuer created in the workload identity setup
    - `--vnet-id`, `--subnet-id`, `--network-security-group-id`: Custom networking infrastructure
    - `--image-generation`: (Optional) VM generation (`Gen1` or `Gen2`, defaults to `Gen2`). For OCP 4.20+, omit to use release payload defaults. See Configuring Azure Marketplace Images
    - `--marketplace-publisher/offer/sku/version`: (Optional) Explicit Azure Marketplace image. Must specify all four flags together, or omit all to use defaults (OCP 4.20+)
    - `--dns-zone-rg-name`: Resource group containing the DNS zone (os4-common)
    - `--diagnostics-storage-account-type Managed`: Use Azure managed storage for diagnostics
    - `--control-plane-operator-image`: Custom HyperShift operator image (optional)

## Verification

Check the cluster status and access:

```bash
# Check HostedCluster status
oc get hostedcluster $CLUSTER_NAME -n clusters

# Wait for cluster to be available
oc wait --for=condition=Available hostedcluster/$CLUSTER_NAME -n clusters --timeout=30m

# Get kubeconfig and access the cluster
hypershift create kubeconfig --name $CLUSTER_NAME > $CLUSTER_NAME-kubeconfig
export KUBECONFIG=$CLUSTER_NAME-kubeconfig
oc get nodes
oc get clusterversion
```

## Cleanup

To delete the HostedCluster:

```bash
# Delete the HostedCluster
hypershift destroy cluster azure \
    --name $CLUSTER_NAME \
    --azure-creds $AZURE_CREDS \
    --resource-group-name $MANAGED_RG_NAME
```

!!! note "Resource Cleanup"
    
    The HyperShift destroy command will clean up the cluster resources. Workload identities and OIDC issuer created during setup can be reused for other clusters or cleaned up separately if no longer needed.

## Related Documentation

1. Azure Workload Identity Setup - Workload identities and OIDC issuer setup
2. Setup Azure Management Cluster for HyperShift - DNS and HyperShift operator setup

---

## Source: docs/content/how-to/azure/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/azure/index.md

# Azure

This section provides guides for deploying HyperShift hosted clusters on Microsoft Azure. There are two deployment models available, each with different management cluster platforms and authentication mechanisms.

## Deployment Models

### ARO HCP (Managed Azure)

ARO HCP (Azure Red Hat OpenShift Hosted Control Planes) uses an AKS (Azure Kubernetes Service) cluster as the management platform. This model uses Azure Managed Identities with certificate-based authentication, with credentials stored in Azure Key Vault.

**Guides:**

- Create an Azure Hosted Cluster on AKS - Step-by-step setup guide
- Azure Hosted Cluster with Options - Advanced configuration options

### Self-Managed Azure

Self-managed Azure uses an OpenShift cluster (running on any platform - AWS, Azure, bare metal, etc.) as the management platform. This model uses Azure Workload Identity with OIDC federation for tokenless authentication.

!!! note "Developer Preview in OCP 4.21"

    Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.

**Guides:**

- Self-Managed Azure Overview - Architecture and deployment workflow
- Azure Workload Identity Setup - Set up managed identities and OIDC federation
- Setup Azure Management Cluster - Install HyperShift operator
- Create a Self-Managed Azure HostedCluster - Deploy your first hosted cluster
- Create Azure IAM Resources Separately - Manage workload identities independently
- Create Azure Infrastructure Separately - Create infrastructure before cluster

## Comparison

| Aspect | ARO HCP | Self-Managed Azure |
|--------|---------|-------------------|
| **Management Cluster** | AKS | OpenShift (any platform) |
| **Control Plane Auth** | Certificate-based (Key Vault) | Workload Identity (OIDC) |
| **Data Plane Auth** | Federated Identity (OIDC) | Workload Identity (OIDC) |
| **Credential Storage** | Azure Key Vault | None (tokenless via OIDC) |
| **Identity Configuration** | Managed identities file + data plane identities file | Workload identities file |
| **Secrets Access** | Secrets Store CSI Driver | Projected ServiceAccount tokens |
| **Setup Complexity** | Higher (Key Vault, service principals, CSI driver) | Moderate (OIDC federation only) |
| **Automation Scripts** | Available in `contrib/managed-azure/` | Available in `contrib/self-managed-azure/` |

## Infrastructure Reference

For detailed information about the Azure infrastructure resources required for each deployment model, see:

- ARO HCP Infrastructure
- Self-Managed Azure Infrastructure

## Additional Resources

- Azure Workload Identity Documentation
- Azure Managed Identities Documentation
- Secrets Store CSI Driver


---

## Source: docs/content/how-to/azure/scheduler.md

# Using Azure Scheduler

The Azure Scheduler works with the default `ClusterSizingConfiguration` resource and the `HostedClusterSizing` controller.

## ClusterSizingConfiguration

The `ClusterSizingConfiguration` is an API used for setting tshirt sizes based on the number of nodes a `HostedCluster` has. Each tshirt size can configure different effects that control various aspects of the cluster, such as the Kube API Server (KAS), etcd, etc. Additionally, it allows controlling the frequency of transitions between cluster sizes.

### Effects

- `kasGoMemLimit`: Specifies the memory limit for the Kube API Server.
- `controlPlanePriorityClassName`: The priority class for most control plane pods.
- `etcdPriorityClassName`: The priority class for etcd pods.
- `apiCriticalPriorityClassName`: The priority class for pods in the API request serving path, including Kube API Server and OpenShift APIServer.
- `resourceRequests`: Allows specifying resource requests for control plane pods.
- `machineHealthCheckTimeout`: Specifies an optional timeout for machine health checks created for `HostedClusters` with this specific size.
- `maximumRequestsInFlight`: Specifies the maximum requests in flight for Kube API Server.
- `maximumMutatingRequestsInflight`: Specifies the maximum mutating requests in flight for Kube API Server.

### ConcurrencyConfiguration

The `ConcurrencyConfiguration` defines the bounds of allowed behavior for clusters transitioning between sizes. It includes:

- `SlidingWindow`: The window over which the concurrency bound is enforced. This is a duration (e.g., `10m` for 10 minutes) that specifies the time frame within which the concurrency limit is applied.
- `Limit`: The maximum allowed number of cluster size transitions during the sliding window. This is an integer (e.g., `5`) that specifies how many transitions can occur within the sliding window.

### TransitionDelayConfiguration

The `TransitionDelayConfiguration` defines the lag between cluster size changing and the assigned tshirt size class being applied. It includes:

- `Increase`: The minimum period of time to wait between a cluster's size increasing and the tshirt size assigned to it being updated to reflect the new size. This is a duration (e.g., `30s` for 30 seconds).
- `Decrease`: The minimum period of time to wait between a cluster's size decreasing and the tshirt size assigned to it being updated to reflect the new size. This is a duration (e.g., `10m` for 10 minutes).

## HostedClusterSizing Controller

The `HostedClusterSizing` controller determines the number of nodes associated with a `HostedCluster` either from the `HostedControlPlane.Status` or by iterating through the nodepools and counting the nodepools associated with the `HostedCluster`. It then compares the number of nodes against the minimum and maximum sizes set for each tshirt size in the `ClusterSizingConfiguration`. Based on this comparison, it applies a label to the `HostedCluster` with the appropriate tshirt size. Depending on the settings in the `ClusterSizingConfiguration`, it can wait a specified amount of time before transitioning between tshirt sizes using a sliding window, ensuring that only a limited number of transitions (e.g., 5 transitions) can occur within a specified time frame (e.g., 20 minutes).

The controller also updates the status of the `HostedCluster`, reporting the computed cluster size, indicating if a tshirt size transition is pending, and specifying if the cluster requires a transition to a different size.

## Azure Scheduler Controller

The Azure scheduler controller is straightforward. It checks the label set by the `HostedClusterSizing` controller and retrieves the cluster sizing configuration associated with the tshirt size. Based on the configuration, it can modify the `HostedCluster` with annotations for the specified fields. These annotations are then used by different controllers to propagate the required changes to the appropriate pods and containers.

## How to Use

### Prerequisites

- AKS cluster with cluster-autoscaler enabled and using Standard\_D4s\_v4 VMs for this example. (--enable-cluster-autoscaler flag when installing AKS cluster, with --min-count 2 --max-count 6)
- Hypershift operator with size tagging enabled. (--enable-size-tagging flag when installing hypershift operator)
- ClusterSizingConfiguration resource created. (A default clusterSizingConfiguration resource is created by the hypershift operator)
- A HostedCluster in the Completed state.
- A Nodepool with 2 nodes associated with the HostedCluster.


### Steps

In the example below we will use a HostedCluster with the name 'pstefans-3' in the 'clusters' namespace and the nodepool 'pstefans-3' in the 'clusters' namespace.

1. The AKS cluster should have only 2 nodes at this point.

    ```shell
    oc get nodes
    NAME                                STATUS   ROLES    AGE     VERSION
    aks-nodepool1-11371333-vmss000000   Ready    <none>   3h43m   v1.31.1
    aks-nodepool1-11371333-vmss000002   Ready    <none>   3h43m   v1.31.1
    ```

2. Edit the `ClusterSizingConfiguration` resource with the following spec:

    ```shell
    oc edit clustersizingconfiguration cluster
    ```

    ```yaml
    spec:
      concurrency:
        limit: 5
        slidingWindow: 0s
      sizes:
      - criteria:
          from: 0
          to: 2
        name: small
      - criteria:
          from: 3
          to: 4
        effects:
          resourceRequests:
          - containerName: kube-apiserver
            cpu: 3
            deploymentName: kube-apiserver
          - containerName: control-plane-operator
            cpu: 3
            deploymentName: control-plane-operator
        name: medium
      - criteria:
          from: 5
        name: large
      transitionDelay:
        decrease: 0s
        increase: 0s
    ```

3. Scale nodepool up to 3 nodes:

    ```shell
    oc scale nodepool pstefans-3 \
      --namespace clusters \
      --replicas 3
    ```

4. Once node pool scales successfully, the `HostedCluster` will be updated with the new tshirt size label and should have the resource request overrides annotations applied to the HC and the relevant controllers should pick this up and set it on the specified pods.

    ```shell
    oc get deployment kube-apiserver -n clusters-pstefans-3 -o json | jq '.spec.template.spec.containers[] | select(.name == "kube-apiserver") | .resources'
    ```

    ```json
    {
      "requests": {
        "cpu": "3",
        "memory": "2Gi"
      }
    }
    ```

    ```shell
    oc get deployment control-plane-operator -n clusters-pstefans-3 -o json | jq '.spec.template.spec.containers[] | select(.name == "control-plane-operator") | .resources'
    ```

    ```json
    {
      "requests": {
        "cpu": "3",
        "memory": "80Mi"
      }
    }
    ```

    ```shell
    oc get hc pstefans-3 -n clusters  -o yaml | grep resource-request-override.hypershift.openshift.io
    resource-request-override.hypershift.openshift.io/control-plane-operator.control-plane-operator: cpu=3
    resource-request-override.hypershift.openshift.io/kube-apiserver.kube-apiserver: cpu=3
    ```

5. You should now see the autoscaler scaled the nodes on the AKS cluster to 3 as we requested 3 CPU cores for the kube-apiserver and control-plane-operator on a nodepool with max 4 cores. So each deployment will nearly request nearly a full node to itself.

    ```shell
    oc get nodes
    NAME                                STATUS   ROLES    AGE     VERSION
    aks-nodepool1-11371333-vmss000000   Ready    <none>   4h8m    v1.31.1
    aks-nodepool1-11371333-vmss000002   Ready    <none>   4h8m    v1.31.1
    aks-nodepool1-11371333-vmss000003   Ready    <none>   9m31s   v1.31.1
    ```

6. You should now see that each of the deployments we changed the resource requests for are running on a different node with sufficient compute.

    ```shell
    kubectl get pods --all-namespaces --field-selector spec.nodeName=aks-nodepool1-11371333-vmss000003
    ```

    ```shell
    NAMESPACE             NAME                                     READY   STATUS    RESTARTS   AGE
    clusters-pstefans-3   kube-apiserver-549c75cb99-jj964          4/4     Running   0          12m
    ```

    ```shell
    kubectl get pods --all-namespaces --field-selector spec.nodeName=aks-nodepool1-11371333-vmss000002
    ```

    ```shell
    NAMESPACE             NAME                                      READY   STATUS    RESTARTS   AGE
    clusters-pstefans-3   control-plane-operator-69b894d9dd-cxv2z   1/1     Running   0          14m
    ```

---

## Source: docs/content/how-to/azure/self-managed-azure-index.md

# Self-Managed Azure HyperShift Overview

!!! note "Developer Preview in OCP 4.21"

    Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.

## Introduction

Self-managed Azure HyperShift enables you to deploy and manage OpenShift hosted control planes on an OpenShift management cluster running in Azure. The key difference from managed Azure HyperShift deployments is the management cluster platform: self-managed uses an OpenShift cluster you provision and manage, while managed Azure uses Azure Kubernetes Service (AKS).

## Architecture Overview

Self-managed Azure HyperShift deployments consist of three key layers:

1. **Management Cluster**: An existing Azure OpenShift cluster that hosts the HyperShift operator and control planes for your hosted clusters
2. **Control Plane**: Kubernetes control plane components running as pods on the management cluster
3. **Data Plane**: Worker nodes running as Azure Virtual Machines in your Azure subscription

The architecture uses Azure Workload Identity for secure, credential-free authentication between OpenShift components and Azure services. This eliminates the need to manage long-lived service principal credentials and provides better security through federated identity credentials.

### Key Differences from Managed Azure

Unlike managed Azure HyperShift deployments, self-managed Azure:

- Uses an OpenShift cluster as the management platform instead of AKS
- Requires you to provision and manage the lifecycle of the management cluster

## DNS Management Options

Self-managed Azure HyperShift supports two DNS management approaches, and both are documented in the same guides:

| Aspect | With External DNS | Without External DNS |
|--------|------------------|---------------------|
| **Best For** | Production, multi-cluster | Development, testing |
| **API Server DNS** | Custom (e.g., `api-cluster.example.com`) | Azure LoadBalancer (e.g., `abc123.region.cloudapp.azure.com`) |
| **Setup Complexity** | Higher (requires DNS zones, service principal) | Lower (minimal configuration) |
| **Management** | Fully automatic | Manual or Azure-provided |

!!! tip "Both Approaches in One Guide"

    Our setup guides include instructions for both approaches. Simply follow the sections that match your choice:

    - **With External DNS**: Follow all sections including DNS Zone Configuration and External DNS Service Principal Setup
    - **Without External DNS**: Skip DNS sections and use the simplified operator installation command

## Deployment Workflow

Setting up self-managed Azure HyperShift is a three-phase process that must be completed in order:

### Phase 1: Azure Workload Identity Setup

**Purpose**: Establish secure authentication infrastructure for OpenShift components to access Azure services.

This phase creates the foundational security infrastructure required for your hosted clusters:

- **Managed Identities**: Creates Azure managed identities for each OpenShift component (image registry, ingress, CSI drivers, cloud provider, network operator, etc.)
- **OIDC Issuer**: Configures an OIDC issuer in Azure Blob Storage for service account token validation
- **Federated Credentials**: Establishes trust relationships between Azure Entra ID and OpenShift service accounts

You can create workload identities using either:

- **HyperShift CLI** (recommended): `hypershift create iam azure` - automates identity and federated credential creation
- **Azure CLI**: Manual `az identity` and `az identity federated-credential` commands

**Why This Matters**: Without these identities and federated credentials, your hosted cluster components cannot authenticate with Azure APIs to provision storage, manage load balancers, configure networking, or perform other essential cloud operations. Using workload identities instead of traditional service principals provides better security, automatic credential rotation, and follows Azure's modern authentication best practices.

**When to Complete**: This is a one-time setup that can be reused across multiple hosted clusters. Complete this before proceeding to Phase 2.

üëâ **Guides**:

- Azure Workload Identity Setup - Overview with CLI and OIDC configuration
- Create Azure IAM Resources Separately - Detailed IAM command reference

### Phase 2: Management Cluster Setup

**Purpose**: Prepare your Azure OpenShift management cluster to host and manage HyperShift control planes.

This phase installs the HyperShift operator with optional External DNS configuration:

- **DNS Zone Configuration** (Optional): Creates Azure DNS zones for automatic DNS record management
- **External DNS** (Optional): Sets up a service principal and deploys ExternalDNS operator
- **HyperShift Operator**: Installs the HyperShift operator

**Why This Matters**: The HyperShift operator orchestrates the entire lifecycle of hosted control planes. If you choose External DNS, it automatically provisions DNS records for each hosted cluster's endpoints.

**When to Complete**: After Phase 1 is complete, but before creating any hosted clusters.

üëâ **Guide**: Setup Azure Management Cluster for HyperShift - Includes both DNS approaches

### Phase 3: Create Hosted Clusters

**Purpose**: Deploy self-managed Azure hosted clusters with custom networking and configuration.

This phase creates your actual hosted OpenShift clusters:

- **Infrastructure Provisioning**: Creates resource groups, VNets, subnets, and network security groups
- **HostedCluster Creation**: Deploys the control plane on the management cluster and worker nodes in your Azure subscription
- **Workload Identity Integration**: Links the hosted cluster to the workload identities created in Phase 1

**Why This Matters**: This is where you deploy the actual OpenShift clusters that your applications will run on. Each hosted cluster gets its own control plane running on the management cluster and its own set of worker node VMs in Azure. The cluster uses the workload identities from Phase 1 to securely access Azure services without storing credentials.

**When to Complete**: After Phases 1 and 2 are fully configured and verified.

üëâ **Guide**: Create a Self-Managed Azure HostedCluster - Includes both DNS approaches

## Prerequisites Summary

Before beginning the deployment process, ensure you have:

- **Azure Resources**:

    - An existing Azure OpenShift management cluster
    - Azure subscription with appropriate permissions (Contributor + User Access Administrator)
    - (Optional) A parent DNS zone in Azure DNS for delegating cluster DNS records (required only if using External DNS)

- **Tools and Access**:

    - Azure CLI (`az`) configured with your subscription
    - OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`)
    - HyperShift CLI binary
    - `jq` command-line JSON processor
    - Cloud Credential Operator (CCO) tool
    - Valid OpenShift pull secret

- **Permissions**:

    - Subscription-level Contributor and User Access Administrator roles
    - Microsoft Graph API permissions (Application.ReadWrite.OwnedBy) for creating service principals

## Resource Group Strategy

Self-managed Azure deployments use multiple resource groups with different lifecycles:

- **Persistent Resource Group** (e.g., `os4-common`): Long-lived resources shared across multiple clusters

    - Workload identities (managed identities)
    - OIDC issuer storage account
    - Azure DNS zones (if using External DNS)
    - External DNS service principal (if using External DNS)

- **Cluster-Specific Resource Groups**: Created and destroyed with each hosted cluster

    - Managed resource group for cluster infrastructure
    - VNet resource group (if using custom networking)
    - NSG resource group (if using custom networking)

!!! tip "Resource Reuse"

    By placing workload identities and OIDC issuer in a persistent resource group, you can:

    - Reuse the same identities across multiple hosted clusters
    - Avoid recreating federated credentials for each cluster
    - Reduce cluster creation time
    - Simplify cleanup when deleting individual clusters

## Security Considerations

Self-managed Azure HyperShift implements several security best practices:

1. **Workload Identity Federation**: Eliminates long-lived credentials by using OIDC-based authentication
2. **Least Privilege Access**: Each component gets its own managed identity with minimal required permissions
3. **Network Isolation**: Custom VNets and NSGs allow you to implement network segmentation and security policies
4. **Federated Credentials**: Trust relationships are scoped to specific service accounts, preventing unauthorized access

## Next Steps

Begin your self-managed Azure HyperShift deployment by following the guides in order:

1. **Azure Workload Identity Setup** - Set up managed identities and OIDC federation (or use Create Azure IAM Resources Separately for CLI-based setup)
2. **Setup Azure Management Cluster for HyperShift** - Install HyperShift operator (with or without External DNS)
3. **Create a Self-Managed Azure HostedCluster** - Deploy your first hosted cluster

Each guide includes sections for both DNS approaches - simply follow the sections that match your choice.

## Additional Resources

- Azure Workload Identity Documentation
- HyperShift Architecture Reference
- Managed Azure Documentation - For comparison with AKS-based deployments


---

## Source: docs/content/how-to/azure/setup-management-cluster.md

# Setup Azure Management Cluster for HyperShift

!!! note "Developer Preview in OCP 4.21"

    Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.

This document describes how to install the HyperShift operator on your Azure management cluster with optional External DNS configuration.

## DNS Management Options

You can set up your management cluster with or without External DNS:

!!! tip "Choose Your Approach"

    **With External DNS (Recommended for Production)**

    - Automatic DNS record management
    - Custom domain names (e.g., `api-cluster.example.com`)
    - Requires: DNS zones, service principal, External DNS operator
    - **Follow sections**: DNS Zone Configuration ‚Üí External DNS Service Principal Setup ‚Üí HyperShift Operator Installation (With External DNS)

    **Without External DNS (Simpler for Dev/Test)**

    - Manual or Azure-provided DNS
    - API server uses Azure LoadBalancer DNS (e.g., `abc123.region.cloudapp.azure.com`)
    - No DNS zones or service principals needed
    - **Skip to**: HyperShift Operator Installation (Without External DNS)

## Prerequisites

- Azure CLI (`az`) installed and configured
- OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`)
- An Azure OpenShift management cluster
- **For External DNS only**: `jq` command-line JSON processor

## DNS Zone Configuration (External DNS Only)

!!! warning "External DNS Only"

    This section is only required if you want automatic DNS management with External DNS. If you prefer a simpler setup, skip to HyperShift Operator Installation (Without External DNS).

Before creating HostedClusters with External DNS, you need to set up DNS zones and delegate DNS records for your clusters.

!!! note "About PERSISTENT_RG_NAME"
    In Red Hat environments, a periodic Azure resource "reaper" deletes resources that are not properly tagged or not located in an approved resource group. We frequently use the `os4-common` resource group for shared, long-lived assets (for example, public DNS zones) to avoid accidental cleanup. If you are not in Red Hat infrastructure, set `PERSISTENT_RG_NAME` to any long-lived resource group in your subscription that will not be automatically reaped, or ensure your organization‚Äôs required tags/policies are applied. The name does not have to be `os4-common`‚Äîuse whatever persistent resource group fits your environment.

```bash
# Set DNS configuration variables
PARENT_DNS_RG="your-parent-dns-rg"
PARENT_DNS_ZONE="your-parent.dns.zone.com"
DNS_RECORD_NAME="your-subdomain"
PERSISTENT_RG_NAME="os4-common"  # Use persistent resource group
DNS_ZONE_NAME="your-subdomain.your-parent.dns.zone.com"

az group create \  
      --name $PERSISTENT_RG_NAME \  
      --location $LOCATION  

az network dns zone create \  
        --resource-group $PERSISTENT_RG_NAME \  
        --name $DNS_ZONE_NAME  

# Delete existing NS record if it exists
az network dns record-set ns delete \
    --resource-group $PARENT_DNS_RG \
    --zone-name $PARENT_DNS_ZONE \
    --name $DNS_RECORD_NAME -y

# Get name servers from your DNS zone
name_servers=$(az network dns zone show \
    --resource-group $PERSISTENT_RG_NAME \
    --name $DNS_ZONE_NAME \
    --query nameServers \
    --output tsv)

# Create array of name servers
ns_array=()
while IFS= read -r ns; do
    ns_array+=("$ns")
done <<< "$name_servers"

# Add NS records to parent zone
for ns in "${ns_array[@]}"; do
    az network dns record-set ns add-record \
        --resource-group $PARENT_DNS_RG \
        --zone-name $PARENT_DNS_ZONE \
        --record-set-name $DNS_RECORD_NAME \
        --nsdname "$ns"
done
```

## External DNS Service Principal Setup (External DNS Only)

!!! warning "External DNS Only"

    This section is only required if you're using External DNS. Skip to HyperShift Operator Installation (Without External DNS) if not.

Create a dedicated service principal for External DNS:

```bash
# Set External DNS configuration
EXTERNAL_DNS_NEW_SP_NAME="ExternalDnsServicePrincipal"
SERVICE_PRINCIPAL_FILEPATH="/path/to/azure_mgmt.json"
PERSISTENT_RG_NAME="hypershift-shared-resources"  # Use persistent resource group

# Create service principal for External DNS
DNS_SP=$(az ad sp create-for-rbac --name ${EXTERNAL_DNS_NEW_SP_NAME})
EXTERNAL_DNS_SP_APP_ID=$(echo "$DNS_SP" | jq -r '.appId')
EXTERNAL_DNS_SP_PASSWORD=$(echo "$DNS_SP" | jq -r '.password')

# Get DNS zone ID
DNS_ID=$(az network dns zone show \
    --name ${DNS_ZONE_NAME} \
    --resource-group ${PERSISTENT_RG_NAME} \
    --query "id" \
    --output tsv)

# Assign roles to the service principal
az role assignment create \
    --role "Reader" \
    --assignee "${EXTERNAL_DNS_SP_APP_ID}" \
    --scope "${DNS_ID}"

az role assignment create \
    --role "Contributor" \
    --assignee "${EXTERNAL_DNS_SP_APP_ID}" \
    --scope "${DNS_ID}"

# Create Azure credentials file
cat <<-EOF > ${SERVICE_PRINCIPAL_FILEPATH}
{
  "tenantId": "$(az account show --query tenantId -o tsv)",
  "subscriptionId": "$(az account show --query id -o tsv)",
  "resourceGroup": "$PERSISTENT_RG_NAME",
  "aadClientId": "$EXTERNAL_DNS_SP_APP_ID",
  "aadClientSecret": "$EXTERNAL_DNS_SP_PASSWORD"
}
EOF

# Create Kubernetes secret for Azure credentials
kubectl delete secret/azure-config-file --namespace "default" || true
kubectl create secret generic azure-config-file \
    --namespace "default" \
    --from-file ${SERVICE_PRINCIPAL_FILEPATH}
```

## HyperShift Operator Installation

Choose the installation method that matches your DNS approach:

### With External DNS

Install the HyperShift operator with External DNS configuration:

```bash
# Set installation variables
PULL_SECRET="/path/to/pull-secret.json"
HYPERSHIFT_BINARY_PATH="/path/to/hypershift/bin"

# Install HyperShift operator with External DNS
${HYPERSHIFT_BINARY_PATH}/hypershift install \
    --external-dns-provider=azure \
    --external-dns-credentials ${SERVICE_PRINCIPAL_FILEPATH} \
    --pull-secret ${PULL_SECRET} \
    --external-dns-domain-filter ${DNS_ZONE_NAME} \
    --limit-crd-install Azure
```

!!! tip "Custom HyperShift Image"

    Add `--hypershift-image quay.io/hypershift/hypershift:TAG` if using a custom operator image.

After installation, verify both the HyperShift operator and External DNS are running:

```bash
# Check HyperShift operator and ExternalDNS pods
oc get pods -n hypershift
```

You should see both `operator-*` and `external-dns-*` pods running.

### Without External DNS

Install the HyperShift operator without External DNS:

```bash
# Set installation variables
PULL_SECRET="/path/to/pull-secret.json"
HYPERSHIFT_BINARY_PATH="/path/to/hypershift/bin"

# Install HyperShift operator (no External DNS)
${HYPERSHIFT_BINARY_PATH}/hypershift install \
    --pull-secret ${PULL_SECRET} \
    --limit-crd-install Azure
```

!!! tip "Custom HyperShift Image"

    Add `--hypershift-image quay.io/hypershift/hypershift:TAG` if using a custom operator image.

After installation, verify the HyperShift operator is running:

```bash
# Check HyperShift operator pods
oc get pods -n hypershift
```

You should see the `operator-*` pod running (no `external-dns` pod).

## Verification

Verify your installation:

=== "With External DNS"

    ```bash
    # Check for both operator and external-dns
    oc get pods -n hypershift

    # Expected output:
    # NAME                           READY   STATUS    RESTARTS   AGE
    # external-dns-xxxxx-xxxxx       1/1     Running   0          1m
    # operator-xxxxx-xxxxx           1/1     Running   0          1m
    ```

=== "Without External DNS"

    ```bash
    # Check for operator only
    oc get pods -n hypershift

    # Expected output:
    # NAME                     READY   STATUS    RESTARTS   AGE
    # operator-xxxxx-xxxxx     1/1     Running   0          1m
    ```

## Next Steps

Once the management cluster is set up, create hosted clusters:

- Create a Self-Managed Azure HostedCluster - Includes guidance for both DNS approaches

---

## Source: docs/content/how-to/azure/troubleshooting/debug-nodes.md

# Debug Why Azure Nodes Have Not Joined
If your control plane API endpoint has become available, but the nodes have not joined the hosted cluster, check the following:

## Verify machines were created
```
HC_NAMESPACE="clusters"
HC_NAME="cluster-name"
CONTROL_PLANE_NAMESPACE="${HC_NAMESPACE}-${HC_NAME}"
oc get machines.cluster.x-k8s.io -n $CONTROL_PLANE_NAMESPACE
oc get azuremachines.infrastructure.cluster.x-k8s.io -n $CONTROL_PLANE_NAMESPACE
```

If machines don't exist, check that a machinedeployment and machineset have been created:
```
oc get machinedeployment -n $CONTROL_PLANE_NAMESPACE
oc get machineset -n $CONTROL_PLANE_NAMESPACE
```

In the case that no machinedeployment was created, look at the logs of the hypershift 
operator:
```
oc logs -l app=operator -n hypershift --tail=$NUMBER_OF_LINES
```

If the machines exist but have not been provisioned, check the log of the cluster API provider:
```
oc logs deployment/capi-provider -c manager -n $CONTROL_PLANE_NAMESPACE
```

## Create a bastion to SSH to a node
If the machines look like they have been provisioned correctly, you can directly access the virtual machines related to your nodes through a bastion. 

### Prerequisites 
- Download the `az` cli
- Add the following extensions to the cli:
  - `az extension update --name bastionaz extension update --name bastion`
  - `az extension add -n ssh`
- Extract the private key for the cluster. If you created the cluster with the --generate-ssh flag, a
  ssh key for your cluster was placed in the same namespace as the hosted cluster (default `clusters`). If you
  specified your own key and know how to access it, you can skip this step.
  - `oc get secret -n clusters ${HC_NAME}-ssh-key -o jsonpath='{ .data.id_rsa }' | base64 -d > /tmp/ssh/id_rsa`
- Set the permissions on the key `chmod 400 /tmp/ssh/id_rsa`

### Create a bastion machine
1. Log into the Azure Portal and go to the resource group where your virtual machine was created
2. Click on the `Connect` button then `Connect to Bastion`
3. Accept the defaults and click `Deploy Bastion`
4. Once the bastion and its related resources are created, you will need to modify some of its configuration
   1. Click on the bastion resource
   2. Click Configuration
   3. Set the Tier to `Standard` and check `Native client support`
5. Wait for the new settings to take effect
6. Log into the bastion via terminal through this command `az login --scope https://management.core.windows.net//.defaultnetwork bastion ssh --name <bastion-name> --resource-group <resource-group-name> --target-resource-id <vm-id> --auth-type ssh-key --username core --ssh-key <path-to-your-rsa-secret>`
7. You should now be able to access various directories and logs to debug why the machine did not join
   1. One suggestion would be to look at the journal logs and look for a repeating error near the bottom that should indicate why the kubelet has not been able to join the cluster: `sudo journalctl`

---

## Source: docs/content/how-to/azure/troubleshooting/index.md

# Troubleshooting HyperShift on Azure
This section of the HyperShift documentation contains pages related to troubleshooting specific issues when using the Azure cloud provider.

- Debug Missing Nodes

---

## Source: docs/content/how-to/ci/ai-assisted-ci-jobs.md

# AI-Assisted CI Jobs

This document describes the AI-assisted CI jobs that help automate issue resolution and PR review handling in the HyperShift repository.

!!! warning "Human Review Required"
    **AI-generated code must not be relied upon without human review.** All PRs created by these jobs are drafts and must go through the standard GitHub PR review process. HyperShift repository OWNERS are responsible for reviewing and approving all changes.

!!! info "Responsible Use"
    Please review the Guidelines on Responsible Use of AI Code Assistants before using these tools.

## Overview

HyperShift uses two AI-assisted CI jobs powered by Claude Code to help with development workflows:

| Job | Purpose | Schedule |
|-----|---------|----------|
| `periodic-jira-agent` | Analyzes Jira issues and creates draft PRs with fixes | Weekly on Mondays at 8:30 AM UTC |
| `periodic-review-agent` | Addresses PR review comments on agent-created PRs | Every 3 hours (8:00-23:00 UTC) daily |
| `address-review-comments` | On-demand job to address review comments on a single PR | Triggered via `/test address-review-comments` |

### Usage Scope

These jobs process **internal Red Hat tickets only** from the following Jira projects:

- **OCPBUGS** - OpenShift bug tracking
- **CNTRLPLANE** - HyperShift/Control Plane team issues

---

## Jira Agent

### Overview

The Jira Agent (`periodic-jira-agent`) automatically analyzes Jira issues and creates draft pull requests with proposed fixes.

- **Job name**: `periodic-jira-agent`
- **Schedule**: Weekly on Mondays at 8:30 AM UTC (`30 8 * * 1`)
- **Max issues per run**: 1 (configurable via `JIRA_AGENT_MAX_ISSUES`)
- **Max agentic turns**: 100 per issue

### How It Works

1. **Queries Jira** for unresolved issues matching the criteria (see JQL below)
2. **Clones repositories**: ai-helpers and hypershift-community/hypershift fork
3. **Runs Claude Code** with the `/jira-solve` command to analyze each issue and implement a fix
4. **Creates draft PR** from the `hypershift-community/hypershift` fork to `openshift/hypershift`
5. **Updates Jira** after successful processing:
   - Adds `agent-processed` label
   - Transitions to "ASSIGNED" (OCPBUGS) or "Code Review" (CNTRLPLANE)
   - Sets assignee to `hypershift-automation`

### JQL Query

Issues are selected for processing using this query:

```sql
project in (OCPBUGS, CNTRLPLANE)
  AND resolution = Unresolved
  AND status in (New, "To Do")
  AND labels = issue-for-agent
  AND labels != agent-processed
```

### Data Flow

```mermaid
flowchart TD
    subgraph "Prow CI Environment"
        A[Periodic Job Trigger<br/>Weekly Monday 8:30 UTC] --> B[Setup Step]
        B --> C[Process Step]

        subgraph "Process Step"
            C --> D[Query Jira API]
            D --> E{Issues Found?}
            E -->|No| F[Exit Successfully]
            E -->|Yes| G[Clone Repositories]
            G --> H[For Each Issue]
            H --> I[Run Claude Code<br/>/jira-solve command]
            I --> J[Create Branch]
            J --> K[Push to Fork]
            K --> L[Create Draft PR]
            L --> M[Update Jira Labels]
            M --> N{More Issues?}
            N -->|Yes| H
            N -->|No| F
        end
    end

    subgraph "External Systems"
        D <--> JIRA[(Jira API<br/>issues.redhat.com)]
        I <--> CLAUDE[Claude API<br/>via Vertex AI]
        K <--> FORK[(GitHub Fork<br/>hypershift-community)]
        L <--> UPSTREAM[(GitHub Upstream<br/>openshift/hypershift)]
    end
```

### Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `JIRA_AGENT_MAX_ISSUES` | 1 | Maximum issues to process per run |
| Rate limit | 60 seconds | Delay between processing issues |

---

## Review Agent

### Overview

The Review Agent (`periodic-review-agent`) automatically addresses PR review comments on PRs created by the Jira Agent.

- **Job name**: `periodic-review-agent`
- **Schedule**: Every 3 hours (8:00-23:00 UTC) daily (`0 8-23/3 * * *`)
- **Max PRs per run**: 10 (configurable via `REVIEW_AGENT_MAX_PRS`)
- **Max agentic turns**: 100 per PR
- **On-demand job**: `address-review-comments` (trigger with `/test address-review-comments`)

### How It Works

1. **Queries GitHub** for open PRs authored by `app/hypershift-jira-solve-ci`
2. **Analyzes review threads** to identify comments needing attention
3. **Runs Claude Code** with the `/utils:address-reviews` command
4. **Pushes changes** back to the PR branch

### Comment Analysis Logic

The agent uses a Python-based comment analyzer to intelligently determine which review threads need attention. This prevents duplicate responses and ensures only actionable feedback is processed.

#### What Gets Processed

| Condition | Action |
|-----------|--------|
| No bot reply in thread | Process (first response needed) |
| Human replied after bot's last comment | Process (follow-up needed) |
| Bot already replied, no human follow-up | Skip (already addressed) |
| Thread is resolved | Skip (marked complete by reviewer) |
| Thread is outdated (code changed) | Skip (likely addressed by code change) |

#### What Counts as an Unresolved Review Thread

A review thread is considered **unresolved** when:

- **Inline code comments**: A reviewer left a comment on a specific line of code in the "Files changed" tab, and no one has clicked "Resolve conversation"
- **Review comments with suggestions**: Comments that include suggested code changes that haven't been resolved
- **Threaded discussions**: Any reply chain started from a code review that remains open

A review thread is **NOT** created by:

- General PR comments (comments in the main "Conversation" tab that aren't attached to code)
- PR reviews that only contain an approval/request changes without inline comments
- Commit comments

#### Author Authorization

The review agent only responds to feedback from authorized authors:

| Author Type | Example |
|-------------|---------|
| OpenShift org members | Members of the `openshift` GitHub organization |
| OWNERS file entries | Users listed in `OWNERS` or `OWNERS_ALIASES` |
| Approved bots | `coderabbitai[bot]` |

Comments from unauthorized users are ignored to prevent abuse.

#### Response Rules

When addressing feedback, the bot follows these rules:

1. **One response per feedback**: Never responds to the same feedback via both inline reply AND general PR comment
2. **Code changes only when requested**: Only modifies code when explicitly asked (imperative language like "change", "fix", "update")
3. **Explanations for questions**: Replies with explanation only for clarifying questions, without code changes

### Data Flow

```mermaid
flowchart TD
    subgraph "Prow CI Environment"
        A[Periodic Job Trigger<br/>Every 3 hours 8:00-23:00 UTC] --> B[Setup Step]
        B --> C[Process Step]

        subgraph "Process Step"
            C --> D[Query GitHub API<br/>for Agent PRs]
            D --> E{PRs Found?}
            E -->|No| F[Exit Successfully]
            E -->|Yes| G[For Each PR]
            G --> H[Analyze Review Threads]
            H --> I{Threads Need<br/>Attention?}
            I -->|No| J{More PRs?}
            I -->|Yes| K[Checkout PR Branch]
            K --> L[Run Claude Code<br/>/utils:address-reviews]
            L --> M[Push Changes]
            M --> J
            J -->|Yes| G
            J -->|No| F
        end
    end

    subgraph "External Systems"
        D <--> GH[(GitHub API<br/>github.com)]
        L <--> CLAUDE[Claude API<br/>via Vertex AI]
        M <--> FORK[(GitHub Fork<br/>hypershift-community)]
    end
```

### Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `REVIEW_AGENT_MAX_PRS` | 10 | Maximum PRs to process per run |

---

## User Guide

### Submitting Issues for Processing

To have an issue processed by the Jira Agent:

1. Ensure the issue is in **OCPBUGS** or **CNTRLPLANE** project
2. Set status to **New** or **To Do**
3. Ensure resolution is **Unresolved**
4. Add the label **`issue-for-agent`**
5. Security set to none

The issue will be picked up on the next weekly run (Mondays at 8:30 AM UTC).

### Viewing AI-Generated Output

Track PRs created by the Jira Agent:

- **PR List**: github.com/openshift/hypershift/pulls?q=is:pr+author:app/hypershift-jira-solve-ci

PRs are created as **drafts** and require human review before merging.

### Reprocessing Issues

To have an issue reprocessed:

1. Remove the **`agent-processed`** label from the Jira issue
2. The issue will be picked up on the next weekly run

### Triggering Review Agent On-Demand

For a single PR, you can trigger the review agent manually:

```
/test address-review-comments
```

This runs the review agent for that specific PR only.

---

## Limitations

- **AI may produce incorrect or incomplete solutions** - always review carefully
- **Complex issues may not be fully addressed** - multi-faceted problems may need human intervention
- **Rate limited**: 1 issue per weekly run (jira-agent), 10 PRs per run (review-agent)
- **Cannot access private resources** - no access to internal systems beyond Jira/GitHub
- **Cannot execute destructive operations** - no ability to delete resources or force-push
- **Maximum agentic turns**: 100 per issue (jira-agent), 100 per PR (review-agent)

---

## Support and Feedback

- **Slack channel**: #project-hypershift
- **Feedback**: File issues in openshift/hypershift with label `ai-feedback`
- **Urgent issues**: Contact HyperShift OWNERS directly

---

## Monitoring and Effectiveness

### Performance Monitoring

- **Prow job logs**: prow.ci.openshift.org
- Track job success/failure rates
- Monitor for recurring authentication errors

### Metrics and Indicators

| Metric | Description |
|--------|-------------|
| Issues processed per week | Number of issues successfully analyzed |
| PRs merged vs. closed | Success rate of generated PRs |
| Review cycles needed | Average iterations before merge |
| Time to PR creation | Duration from issue creation to PR |

### Periodic Review Process

The HyperShift team conducts monthly reviews:

- Review AI-generated PRs for quality and accuracy
- Track false positives and missed solutions
- Adjust issue labeling criteria based on results
- Document lessons learned and improve the `/jira-solve` command

---

## Data Flow and Security

### Authentication

| System | Method |
|--------|--------|
| GitHub | GitHub App tokens (JWT-based) for fork and upstream |
| Claude API | GCP service account via Vertex AI |
| Jira | Personal access token for label management |

### Data Retention

- No persistent storage beyond PR content
- Logs retained per Prow standard retention policy

### External Systems

| System | Purpose |
|--------|---------|
| Jira API (issues.redhat.com) | Issue queries and label updates |
| GitHub API (github.com) | PR creation and management |
| Claude API via Vertex AI (GCP) | AI-powered code analysis and generation |


---

## Source: docs/content/how-to/ci/checking-ci.md

# Daily CI Health Check Procedures
This document outlines the daily checks that should be performed each morning to ensure the health and stability of our CI systems.

!!! tip

    Daily CI checks help identify issues early and ensure that our development pipeline remains stable. These checks should be performed at the start of each workday.

## 1. OCP Release Payload Controllers
Check the status of OpenShift Container Platform release payload controllers for the **current** and **previous** OCP versions to ensure they are functioning properly.

- Review amd64 release payload controller to make sure HyperShift AWS and AKS jobs are passing, thus they are not blocking the CI and nightly payloads (both OCP versions)
- Review multi-arch release payload controller to make sure HyperShift AKS job is passing, thus it is not blocking the nightly payload (both OCP versions)
- Alternatively, you can view the same info on the Sippy Payload streams dashboard. Here is an example for OCP 4.21.

!!! warning - "HyperShift job is failing and blocking a payload release"

    When either HyperShift job is blocking a payload release:
    
    - Open a chat thread in #team-ocp-hypershift to start a dialogue on what is happening and to begin root causing the problem.
    - In addition, alert #forum-ocp-oversight we are aware of the issue and working to root cause the problem.

---

## 2. Periodic & Conformance Jobs
Review periodic job status for the **current** and **previous** OCP versions to ensure long-running validation and maintenance tasks are healthy. We want to be passing 70% or higher.

For each OCP version, click on the Jobs link on the left hand side of the screen in Sippy. Here is an example for OCP 4.21 with the jobs filtered on *hypershift*.

We care about the following jobs (you can filter by these names if desired):

- AWS
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-ovn-conformance
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-upgrade
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-multi
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-ovn
- Azure / ARO HCP
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-aks
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-aks-multi-x-ax
    - periodic-ci-openshift-hypershift-release-*-periodics-e2e-azure-aks-ovn-conformance

!!! tip - "Tip - How to check the job test results"

    For any of these jobs, if you click on the running man emblem, Sippy will show you all the test runs. 
    For each of the test runs, you can click the Prow ship emblem to see the test results of the individual run.

!!! warning - "What to do when a job is permafailing"

    Open a chat thread in #team-ocp-hypershift to start a dialogue on what is happening and to begin root causing the problem.

Alternatively, you can view the job runs in TestGrid.

---

## 3. Presubmit Jobs
Monitor presubmit job health for the **current OCP version only** to catch any systemic issues that could block development.

The best way to check to make sure the presubmit jobs are not permafailing are to look at a recent PR in the HyperShift repo and go to the job history of the specific job you want to review.

The presubmit jobs we most care about are:

- pull-ci-openshift-hypershift-main-e2e-aws
- pull-ci-openshift-hypershift-main-e2e-aws-upgrade-hypershift-operator
- pull-ci-openshift-hypershift-main-e2e-aks
- pull-ci-openshift-hypershift-main-e2e-aks-4-20
- pull-ci-openshift-hypershift-main-e2e-kubevirt-aws-ovn-reduced
- pull-ci-openshift-hypershift-main-verify
- pull-ci-openshift-hypershift-main-unit

!!! tip

    If the job is not solid red, the job is not permafailing.

!!! warning - "What to do when a job is permafailing"

    Open a chat thread in #team-ocp-hypershift to start a dialogue on what is happening and to begin root causing the problem.


---

## Source: docs/content/how-to/ci/ci-infrastructure.md

# CI Infrastructure

This document explains the HyperShift CI testing infrastructure, focusing on how Boskos manages cloud resources and how jobs coordinate access to shared infrastructure.

## Overview

CI jobs often need exclusive access to expensive or limited resources:

- Cloud accounts (AWS, Azure, GCP)
- Quota-limited infrastructure
- Clusters for testing

Without coordination, multiple jobs would conflict trying to use the same resources simultaneously. **Boskos** is a resource management service that works alongside Prow to handle shared, limited resources in CI environments.

## Architecture

The following diagram shows how Prow, Boskos, and cloud resources work together:

```mermaid
flowchart TD
    subgraph Prow["Prow (CI Orchestration)"]
        ProwDesc["Schedules and runs jobs"]
    end

    Prow -->|"Job needs cloud account"| Boskos

    subgraph Boskos["Boskos (Resource Broker)"]
        BoskosDesc["Manages pool of resources"]
        subgraph Resources["Resources"]
            R1["aws-1<br/>(leased)"]
            R2["aws-2<br/>(free)"]
            R3["gcp-1<br/>(leased)"]
            R4["azure-1<br/>(dirty)"]
        end
    end
```

## Key Components

| Component | Role |
|-----------|------|
| **Boskos Server** | Tracks resource state, handles acquire/release requests |
| **Boskos Reaper** | Reclaims resources from jobs that died without releasing |
| **Boskos Janitor** | Cleans "dirty" resources (e.g., deletes leftover cloud resources) |
| **Boskos Client** | Library used by jobs to request/release resources |

## Lease Lifecycle

1. **Acquire**: Job requests a resource of a specific type
2. **Lease**: Boskos grants exclusive access, marks resource as "busy"
3. **Heartbeat**: Job periodically confirms it's still using the resource
4. **Release**: Job finishes, returns resource (may be marked "dirty")
5. **Clean**: Janitor component cleans dirty resources for reuse

## Cluster Profiles and Quota Slices

In OpenShift CI, Prow jobs that need cloud infrastructure use Boskos leases. The **ci-operator** handles Boskos integration automatically when jobs request cluster provisioning.

HyperShift jobs use the `hypershift` cluster profile, which maps to `hypershift-quota-slice` in Boskos:

```yaml
# Example from ci-operator config
cluster_profile: hypershift
```

This cluster profile is defined in the Boskos configuration and controls how many concurrent jobs can run.

### Platform Separation

Each cloud platform (AWS, Azure, GCP, Oracle) should ideally have its own cluster profile and Boskos slice for quota isolation. This ensures:

- **Cost control**: Limits concurrent cloud usage per platform
- **Isolation**: Jobs on different platforms don't compete for the same quota
- **Cleanup**: Resources are properly cleaned between uses
- **Fairness**: Jobs wait in queue rather than failing from conflicts

!!! note
    Currently, HyperShift AWS and Azure/AKS jobs share the same `hypershift` cluster profile. Future improvements may separate these into platform-specific profiles.

## Identifying Leases in Build Logs

When a job acquires a Boskos lease, you'll see entries like this in the build log:

```
[2025-12-10T16:07:50Z] Acquiring leases for test e2e-aws: [hypershift-quota-slice]
[2025-12-10T16:52:55Z] Acquired 1 lease(s) for hypershift-quota-slice: [0ae8b8f2-4072-43b0-9bd0-136ee5bb8ab4]
```

If jobs are queued waiting for resources, you'll see the "Acquiring leases" message without a corresponding "Acquired" message until a lease becomes available.

## CI Dashboard

The HyperShift CI dashboard shows the current state of test clusters:

https://ci-dashboard-route-clusters.apps.rosa.hypershift-ci-2.1xls.p3.openshiftapps.com/

This dashboard displays clusters provisioned by CI jobs.

!!! note
    The number of clusters shown may be less than the maximum Boskos leases if some jobs are running on external infrastructure (e.g., Azure/AKS jobs running directly in Azure).

## Why This Matters

Understanding the CI infrastructure helps when:

- **Debugging slow CI**: Jobs may be waiting for Boskos leases
- **Investigating failures**: Resource cleanup issues can cause test failures
- **Planning capacity**: Understanding quota limits helps predict CI throughput
- **Troubleshooting flakes**: Dirty resources or cleanup failures can cause intermittent issues

## References

- Boskos configuration
- HyperShift CI config
- CI Dashboard


---

## Source: docs/content/how-to/common/exposing-services-from-hcp.md

# Exposing the Hosted Control Plane Services

To publish the services from the Hosted Control Plane, we need to understand the available strategies and their implications. Let's explore them.

## Service Publishing Strategies

Let's delve into the motivations for each of the strategies.

The **NodePort** strategy allows you to expose services without requiring a logical LoadBalancer like MetalLB or similar infrastructure. It is one of the simplest methods to implement. This strategy is supported by all services; however, the limitation arises in high availability (HA) environments where you will be pointing to one of the NodePorts instead of all three.

The **LoadBalancer** strategy enables you to expose certain services through a load balancer. While not all services support this strategy, it is the preferred method for exposing the KubeApiServer, as it allows for a single entry point in an HA configuration without relying in the Ingress Controller of the Manamgement cluster.

The **Route** strategy allows you to expose the HostedControlPlane services using the ingress of the Management OpenShift cluster. This strategy is supported by all services but kubeapi-server.

### NodePort

Exposing a service via NodePort is a method used in OpenShift to make a service accessible from outside the cluster. When you expose a service using NodePort, OpenShift allocates a port on each node in the cluster (if the cluster availability policy is set to HighlyAvailable). This port on each node is mapped to the port of the service, allowing external traffic to reach the service by accessing any node's IP address and the allocated NodePort.

This is the default configuration when you use `Agent` and `None` provider platforms. The services relevant for on-premise platforms are:

- APIServer
- OAuthServer
- Konnectivity
- Ignition

!!! Note
    If any of the services are not relevant for your deployment, it is not necessary to specify them.

Here is how it looks in the HostedCluster CR:

```yaml
spec:
...
...
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: <IP_ADDRESS>
        port: <PORT>
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
...
...
```

### Route

OpenShift routes provide a way to expose services within the cluster to external clients. A route in OpenShift maps an external request (typically HTTP/HTTPS) to an internal service. The route specifies a hostname that external clients will use to access the service. OpenShift‚Äôs router (based on HAProxy) will handle the traffic coming to this hostname.

HostedControlPlanes operate in two domains: the Control Plane and the Data Plane. The Control Plane uses routes through the MGMT Cluster ingress to expose services for each of the HostedControlPlanes, and the routes are created in the HostedControlPlane namespace. For the Data Plane, the Ingress handles `*.apps.subdomain.tld` URLs, and all routes under this wildcard are directed to the Namespace by the OpenShift Router on the worker nodes.

The usual configuration for the Hosted Cluster is similar to the LoadBalancer setup we will discuss next.

### LoadBalancer

The LoadBalancer strategy in OpenShift is used to expose services to external clients using an external load balancer. When you create a service of type LoadBalancer, Kubernetes interacts with the underlying cloud platform or appropriate LoadBalancer controllers to provision an external load balancer, which then routes traffic to the service's endpoints (pods).

This is how looks like the most common configuration to expose the services from HCP side:

```yaml
spec:
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
      Route:
        hostname: <URL>
  - service: Konnectivity
    servicePublishingStrate
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
```

If you wanna know more about how to expose the ingress service in the Data Plane side, please access the Recipes section to see how to do it with MetalLB.


---

## Source: docs/content/how-to/common/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/common/hcp-networking-requirements.md

# HCP Networking Requirements

This document outlines the networking port requirements for Hosted Control Plane (HCP) deployments. Understanding these requirements is essential for properly configuring firewalls, security groups, and network policies.

## Overview

Hosted Control Planes require specific network ports to be open for communication between the management cluster, hosted control plane components, and worker nodes. The ports are categorized into ingress (incoming traffic to HCP) and egress (outgoing traffic from HCP) requirements.

!!! note "Networking Perspective"
    All port requirements in this document are from the perspective of the Hosted Control Plane (HCP) namespace. Ingress means traffic entering the HCP namespace, and egress means traffic leaving the HCP namespace.

## Common Ingress Ports

The following port is required for incoming traffic to the Hosted Control Plane across all platforms:

| Port | Protocol | Service | Description | Code Reference |
|------|----------|---------|-------------|----------------|
| 6443 | TCP | Kubernetes API Server | Primary API server port for kubectl and cluster communication | `support/config/constants.go:35` - `KASSVCPort = 6443` |

!!! note "Service Publishing Strategy Determines Additional Ports"
    Ignition and Konnectivity services are exposed via the configured service publishing strategy:

    - **Route** (default on OpenShift): Traffic flows through the OpenShift router on port **443**. No additional firewall rules needed beyond standard HTTPS.
    - **NodePort**: Direct access required to **port 8091** (Konnectivity) and **port 8443** (Ignition Proxy). See Agent and None platform sections.
    - **LoadBalancer**: Direct access required to **port 8091** (Konnectivity) via the cloud load balancer.

## Common Egress Ports

The following ports must be accessible for outgoing traffic from the Hosted Control Plane across all platforms:

| Port | Protocol | Service | Purpose |
|------|----------|---------|---------|
| 443 | TCP | HTTPS | OLM images, Ignition content, external HTTPS services |
| 6443 | TCP | Kubernetes API Server | Communication with management cluster API |
| 53 | TCP + UDP | DNS | Standard DNS queries |

!!! note
    These are the base requirements. Additional ports may be needed depending on your specific platform and workloads. Refer to the platform-specific sections below for additional port requirements.

## Konnectivity Architecture

Konnectivity uses a reverse tunnel pattern for secure communication between the control plane and worker nodes. Worker nodes initiate outbound connections to the Konnectivity server (port 8091), establishing a tunnel. The control plane then uses these pre-established tunnels to reach back into the data plane without requiring direct network access to worker nodes. This design eliminates the need for worker nodes to have publicly accessible IPs while still allowing the control plane to manage them. For detailed Konnectivity architecture and troubleshooting, see Konnectivity in HyperShift.

## Platform-Specific Port Configurations

Different platforms may use additional or alternative ports for specific services.

=== "AWS"

    AWS platform uses port 9440 for CAPI provider health monitoring:

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 9440 | Ingress | CAPI Health Check | Health and readiness probe endpoint for AWS CAPI provider | `hypershift-operator/controllers/hostedcluster/internal/platform/aws/aws.go:222-223` |

=== "Azure"

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 7443 | Ingress | Kubernetes API Server | Azure uses port 7443 (`KASSVCLBAzurePort`) instead of 6443 for the API server load balancer | `support/config/constants.go:34` - `KASSVCLBAzurePort = 7443` |

=== "GCP"

    GCP platform uses the standard common ports (6443 for API server). For private clusters, GCP uses the same private router configuration as AWS and Azure (ports 8080/8443). See the Private Clusters section.

=== "IBM Cloud"

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 2040 | Ingress | Kubernetes API Server | IBM Cloud uses port 2040 (`KASSVCIBMCloudPort`) instead of 6443 for the API server | `support/config/constants.go:37` - `KASSVCIBMCloudPort = 2040` |

    !!! note
        The ignition-server-proxy (port 8443) is not deployed on IBM Cloud platform. Workers retrieve ignition configuration through a different mechanism. See `control-plane-operator/controllers/hostedcontrolplane/v2/ignitionserver_proxy/component.go:52`.

=== "PowerVS"

    PowerVS platform uses standard ports with health monitoring on port 9440:

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 9440 | Ingress | CAPI Health Check | Health and readiness probe endpoint | `hypershift-operator/controllers/hostedcluster/internal/platform/powervs/powervs.go:157` |

=== "KubeVirt"

    KubeVirt platform uses standard ports with health monitoring on port 9440:

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 9440 | Ingress | CAPI Health Check | Health and readiness probe endpoint | `hypershift-operator/controllers/hostedcluster/internal/platform/kubevirt/kubevirt.go:140` |

=== "OpenStack"

    OpenStack platform uses standard ports with health monitoring on port 9440:

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 9440 | Ingress | CAPI Health Check | Health and readiness probe endpoint | `hypershift-operator/controllers/hostedcluster/internal/platform/openstack/openstack.go:238` |
    | 8081 | Ingress | ORC Health Check | Health and readiness probe endpoint for OpenStack Resource Controller (4.19+) | `hypershift-operator/controllers/hostedcluster/internal/platform/openstack/openstack.go:294,311` |

=== "Agent"

    The Agent platform requires additional ports for the Cluster API (CAPI) provider and uses NodePort service publishing which requires the Ignition Proxy:

    | Port | Direction | Service | Description | Code Reference |
    |------|-----------|---------|-------------|----------------|
    | 8443 | Ingress | Ignition Proxy | HTTPS proxy for ignition content delivery (NodePort publishing) | `hypershift-operator/controllers/hostedcluster/network_policies.go:390` |
    | 8081 | Ingress | Agent CAPI Health Probe | Health check endpoint for Agent platform CAPI provider | `hypershift-operator/controllers/hostedcluster/internal/platform/agent/agent.go:96,105,115` |
    | 8080 | Ingress | Agent CAPI Metrics | Metrics endpoint for Agent platform CAPI provider (binds to localhost only) | `hypershift-operator/controllers/hostedcluster/internal/platform/agent/agent.go:97` |

=== "None"

    The None platform uses NodePort service publishing which requires the Ignition Proxy:

    | Port | Direction | Service | Description |
    |------|-----------|---------|-------------|
    | 8443 | Ingress | Ignition Proxy | HTTPS proxy for ignition content delivery (NodePort publishing) |

### Private Clusters (AWS/Azure/GCP)

For private clusters using the private router:

| Port | Direction | Service | Description | Code Reference |
|------|-----------|---------|-------------|----------------|
| 8080 | Ingress | Private Router HTTP | HTTP traffic through the private router | `hypershift-operator/controllers/hostedcluster/network_policies.go:244` |
| 8443 | Ingress | Private Router HTTPS | HTTPS traffic through the private router | `hypershift-operator/controllers/hostedcluster/network_policies.go:245` |

## Worker Node Egress Requirements

Worker nodes require outbound network access to the following HCP services:

| Port | Protocol | Service | Purpose | When Required |
|------|----------|---------|---------|---------------|
| 443 | TCP | HTTPS | Container registries, Ignition/Konnectivity (via Route), external HTTPS services | Always |
| 6443 | TCP | Kubernetes API Server | Cluster management and kubelet communication | Always |
| 7443 | TCP | Kubernetes API Server (Azure) | Cluster management on Azure platform | Azure only |
| 2040 | TCP | Kubernetes API Server (IBM Cloud) | Cluster management on IBM Cloud platform | IBM Cloud only |
| 8091 | TCP | Konnectivity Server | Establish reverse tunnel for control plane access | NodePort/LoadBalancer publishing only |
| 8443 | TCP | Ignition Proxy | Retrieve bootstrap configuration | NodePort publishing only (Agent/None platforms) |
| 53 | TCP + UDP | DNS | Name resolution | Always |

## Security Considerations

When configuring network access to the Hosted Control Plane, apply the following security principles:

### Source IP Restrictions

- **Restrict to Worker Subnets**: When using NodePort/LoadBalancer publishing, ports 8091 (Konnectivity) and 8443 (Ignition Proxy) should be restricted to worker node subnet ranges only.
- **Public Access Required**: Port 6443/7443/2040 (Kubernetes API Server) may need broader access if external clients (kubectl, CI/CD) need to reach the cluster.
- **Internal Only**: Port 8080 (Agent CAPI metrics) binds to localhost (127.0.0.1) and is not externally accessible.

### TLS Encryption

Most ports use TLS encryption with the following exceptions:

**Unencrypted (HTTP) health check endpoints:**

- Port 8081 (Agent/OpenStack CAPI Health) - HTTP health probes
- Port 9440 (CAPI Health on AWS/KubeVirt/PowerVS/OpenStack) - HTTP health probes

**Other unencrypted ports:**

- Port 53 (DNS) - standard unencrypted DNS protocol

### Sensitive Data Warning

The Ignition Server (accessed via Route on port 443 or Ignition Proxy on port 8443) serves sensitive bootstrap data including:
- Node certificates and keys
- Kubeconfig files
- Machine configuration

Ignition endpoints should be strictly limited to worker node subnets and blocked after initial bootstrap if possible.

## Network Policy Considerations

When deploying Hosted Control Planes, consider the following network policy requirements:

1. **Management Cluster to HCP**: The hypershift-operator and control-plane-operator (running on the management cluster) need unrestricted network access to pods in the HCP namespace so they can reconcile control plane components (API server, etcd, Konnectivity, etc.).

2. **HCP to Worker Nodes**: The Konnectivity tunnel enables secure communication from the control plane to worker nodes without requiring direct network access.

3. **Worker Nodes to HCP**: Worker nodes need access to the Kubernetes API Server (port 6443 or platform-specific port) and Konnectivity Server (port 8091).

4. **External Access**: If exposing services externally, ensure the appropriate service publishing strategy (LoadBalancer, NodePort, or Route) is configured. See Exposing Services from HCP for more details.

## Firewall Configuration Example

For a typical AWS deployment using Route service publishing, ensure the following security group rules:

### Ingress Rules

```text
Port 6443/TCP     - Kubernetes API Server (from worker nodes and external clients)
Port 443/TCP      - OpenShift Router for Ignition/Konnectivity Routes (from worker nodes)
```

### Egress Rules

```text
Port 443/TCP      - HTTPS (to container registries, Routes, external services)
Port 6443/TCP     - Management cluster API (to management cluster)
Port 53/TCP+UDP   - DNS (to DNS servers)
```

!!! note "NodePort/LoadBalancer Publishing"
    If using NodePort or LoadBalancer service publishing instead of Routes, add:

    - Port 8091/TCP - Konnectivity Server (from worker nodes)
    - Port 8443/TCP - Ignition Proxy (from worker nodes during bootstrap, NodePort only)

## Related Documentation

- Exposing Services from HCP
- Service Publishing Strategies


---

## Source: docs/content/how-to/common/multi-arch-on-hcp.md

# Multi-arch on Hosted Control Planes
## General
Several platforms now support multi-arch capable HostedClusters when a multi-arch release image or stream is used for 
the HostedCluster. This means a HostedCluster can manage NodePools with different CPU architectures. 

!!! note

    An individual NodePool only supports one CPU architecture and cannot support multiple CPU architectures within the 
    same NodePool.

The most up-to-date information on what CPU types are supported on a platform can be found 
by looking at the NodePool controller function to validate NodePool CPU and platform here.

As of September 2024:

* AWS supports amd64 & arm64 NodePools
* Azure supports amd64 & arm64 NodePools (through ARO HCP only)
* Agent supports amd64, arm64, & ppc64le NodePools

## Multi-arch Validation
### HyperShift Operator
The HyperShift Operator, through the HostedCluster controller, will update a field, HostedCluster.Status.PayloadArch, 
with the payload type of the HostedCluster release image. The valid options for this field are: Multi, ARM64, AMD64, or 
PPC64LE.

When a NodePool is added to a HostedCluster, the HyperShift Operator, through the NodePool controller, will check the 
NodePool.Spec.Arch against the HostedCluster.Status.PayloadArch to ensure the NodePool can be managed by the 
HostedCluster. If HostedCluster.Status.PayloadArch is not `Multi` and it does not exactly match NodePool.Spec.Arch, the 
NodePool controller will block reconciliation of the NodePool and set a status condition on the NodePool CR stating the 
NodePool cannot be supported by the HostedCluster payload type.

### HCP CLI
Create Cluster CLI commands will check to see if a multi-arch release image or stream is being used for the 
HostedCluster payload. If a multi-arch release image or stream is not used, the CLI will check the management cluster 
and NodePool CPU architectures match; if they do not match, the CLI will return an error and stop creating the cluster.

The Create NodePool CLI commands for AWS and Azure will attempt to validate the NodePool CPU architecture against the 
HostedCluster.Status.PayloadArch if the HostedCluster exists. If a HostedCluster doesn't exist, for instance when 
creating a new HostedCluster, a warning message will be displayed stating there was a failure to get the HostedCluster 
to check the payload status. If the HostedCluster.Status.PayloadArch exists and isn't multi or does not match the 
NodePool CPU architecture, the CLI will return an error and stop creating resources.

---

## Source: docs/content/how-to/configure-ocp-components/custom-kas-kubeconfig.md

# Custom Kube API Server DNS Configuration

This guide explains how to configure a custom DNS domain for your HyperShift cluster's Kube API Server and how to consume the generated Kubeconfig that points to this custom domain. This feature is included in OpenShift 4.19 version and MCE 2.9.

## Prerequisites

- A running HyperShift cluster
- Access to modify the HostedCluster resource
- A custom DNS domain that you want to use for the Kube API Server
- A custom certificate configured in the Kubeapi-server as we already explained here

## Configuration

To configure a custom DNS domain for your cluster's Kube API Server, you need to:

1. First, configure the DNS record in your provider platform (AWS, Azure, etc.). This is your responsibility as the cluster administrator. The DNS record must be properly configured and resolvable from your OpenShift cluster.

2. Then, modify the `kubeAPIServerDNSName` field in your HostedCluster specification. This field accepts a URI that will be used as the API server endpoint.

### Example Configuration

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: example-cluster
  namespace: clusters
spec:
  configuration:
    apiServer:
      servingCerts:
        namedCertificates:
        - names:
          - api-custom-cert-sample-hosted.sample-hosted.example.com
          servingCertificate:
            name: sample-hosted-kas-custom-cert
  kubeAPIServerDNSName: api-custom-cert-sample-hosted.sample-hosted.example.com
  # ... other spec fields ...
```

## Consuming the Kubeconfig

After applying the configuration described above:

1. The HyperShift operator will generate a new Kubeconfig that points to your custom DNS domain
2. You can retrieve the Kubeconfig using the standard methods, accessing the secret directly:
```bash
kubectl get secret <cluster-name>-custom-admin-kubeconfig -n <cluster-namespace> -o jsonpath='{.data.kubeconfig}' | base64 -d
```

## Important Considerations

1. Ensure that your custom DNS domain is properly configured and resolvable
2. The DNS domain should have valid TLS certificates configured
3. Network access to the custom DNS domain should be properly configured in your environment
4. The custom DNS domain should be unique across your HyperShift clusters

## Troubleshooting

If you encounter issues accessing the cluster using the custom DNS:

1. Verify that the DNS record is properly configured and resolving
2. Check that the TLS certificates for the custom domain are valid. (Verify the SAN is correct for your domain)
```
oc get secret -n clusters sample-hosted-kas-custom-cert -o jsonpath='{.data.tls\.crt}' | base64 -d |openssl x509 -text -noout -
```
3. Ensure network connectivity to the custom domain is working
4. Verify that the HostedCluster status shows the custom Kubeconfig properly
```
status:
  customKubeconfig:
    name: sample-hosted-custom-admin-kubeconfig
```
5. Check the `kube-apiserver` logs in the HostedControlPlane namespace
```
oc logs -n <HostedControlPlane namespace> -l app=kube-apiserver -f -c kube-apiserver
```


---

## Source: docs/content/how-to/configure-ocp-components/index.md

# Configure OCP Components

## Overview

In standalone OpenShift, cluster configuration is achieved via cluster-scoped resources in the `config.openshift.io/v1`
API group. Resources such as APIServer, OAuth, and Proxy allow adding additional named certificates to the Kube APIServer, 
adding identity providers, configuring the global proxy, etc. In HyperShift, configuration resources that
impact the control plane need to be specified in the HostedCluster resource instead of inside the guest cluster. The
resources still exist inside the guest cluster, but their source of truth is the HostedCluster and are continuously
reconciled with what is specified in the HostedCluster.

## Configuration in a HostedCluster

The configuration resources that should be specified in the HostedCluster are:

* APIServer - Provides API server configuration such as certificates and certificate authorities.
* Authentication - Controls the identity provider and authentication configuration for the cluster.
* FeatureGate - Enables FeatureGates so that you can use Tech Preview features.
* Ingress - Configuration details related to routing such as the default domain for routes.
* Image - Configures how specific image registries should be treated (allowed, disallowed, insecure, CA details).
* OAuth - Configures identity providers and other behavior related to internal OAuth server flows.
* Proxy - Defines proxies to be used by components needing external network access. Note: not all components currently consume this value.
* Scheduler - Configures scheduler behavior such as policies and default node selectors.
* Network - Configures network properties for initial cluster creation.
Resources that should still be configured inside the guest cluster are:

* Build - Controls default and enforced configuration for all builds on the cluster.
* Console - Configures the behavior of the web console interface, including the logout behavior.
* Project - Configures how projects are created including the project template.

## Specifying Configuration in a HostedCluster

Configuration should be specified in their corresponding fields under the spec.configuration field of a HostedCluster:

```
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: example
  namespace: clusters
spec:
  configuration:
    apiServer:
      ...
    oauth:
      ...
    proxy:
      ...
```

## Referenced ConfigMaps and Secrets

Some configuration resources contain references to secrets or config maps. In standalone OpenShift, these are normally
expected in the `openshift-config` namespace. For HyperShift clusters, these need to be placed in the same namespace as
the `HostedCluster` resource.

For example, when adding an additional serving certificate to the Kube APIServer, the referenced serving certificate is
expected to exist in the same namespace as the HostedCluster:

```
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: example
  namespace: clusters
spec:
  configuration:
    apiServer:
      servingCerts:
        namedCertificates:
        - names:
          - xxx.example.com
          - yyy.example.com
          servingCertificate:
            name: my-serving-cert
```

## Configuration Validation

Configuration embedded in a HostedCluster is validated by HyperShift. If there are any issues reading the configuration
or there are secret or config map references missing, the `ValidConfiguration` condition in the `HostedCluster` status
will be set to `False` and the message will include information on the reason the configuration failed validation.

## OAuth configuration

The Oauth configuration section allows a user to specify the desired Oauth configuration for the internal Openshift Oauth server.
Guest cluster kube admin password will be exposed only when user has not explicitly specified the OAuth configuration. An example
configuration for an openID identity provider is shown below:
```
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: example
  namespace: master
spec:
  configuration:
    oauth:
      identityProviders:
      - openID:
          claims:
            email:
              - email
            name:
              - name
            preferredUsername:
              - preferred_username
          clientID: clientid1
          clientSecret:
            name: clientid1-secret-name
          issuer: https://example.com/identity
        mappingMethod: lookup
        name: IAM
        type: OpenID
```

For more details on the individual identity providers: refer to upstream openshift documentation

---

## Source: docs/content/how-to/configure-ocp-components/kubeapi-server.md

# Configuring Custom API Server Certificate in HostedCluster

This guide explains how to configure a custom certificate for the API server in a HostedCluster.

## Overview

You can configure a custom certificate for the API server by specifying the certificate details in the `spec.configuration.apiServer` section of your HostedCluster configuration.

## Considerations creating the certificate

When creating a custom certificate for the API server, there are important considerations to keep in mind regarding Subject Alternative Names (SANs):

### SAN Conflicts with Internal API

If your HostedCluster configuration includes a service publishing strategy like:
```yaml
services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
      loadBalancer:
        hostname: api-custom-cert-sample-hosted.sample-hosted.example.com
```

You must ensure that the certificate's Subject Alternative Names (SANs) do not conflict with the internal API endpoint (`api-int`). This is because:

- The internal API endpoint is automatically created and managed by the platform
- Using the same hostname in both the custom certificate and the internal API endpoint can cause routing conflicts

!!! Important

    The only exception to this rule is when using AWS as the provider with either Private or PublicAndPrivate configurations. In these specific cases, the SAN conflict is allowed and managed by the platform.

### Certificate Requirements

   - The certificate must be valid for the external API endpoint
   - The certificate should not include SANs that match the internal API endpoint pattern (except for AWS Private/PublicAndPrivate configurations)
   - Ensure the certificate's validity period aligns with your cluster's expected lifecycle

## Configuration Example

Here's an example of how to configure a custom certificate for the API server:

```yaml
spec:
  configuration:
    apiServer:
      servingCerts:
        namedCertificates:
        - names:
          - api-custom-cert-sample-hosted.sample-hosted.example.com
          servingCertificate:
            name: sample-hosted-kas-custom-cert
```

## Configuration Fields

- `names`: List of DNS names that the certificate should be valid for.
- `servingCertificate.name`: Name of the secret containing the custom certificate

Note: In HyperShift, the `namedCertificates` configuration applies to both the Kubernetes API Server (KAS) and the OAuth server. Both components read from
"APIServer.ServingCerts.NamedCertificates". Any certificate whose hostname matches the OAuth endpoint will automatically be served by the managed OAuth server.

This differs from standalone OpenShift, where OAuth serving certificates are provisioned separately using the Ingress Operator‚Äôs `componentRoutes` mechanism.

## Prerequisites

Before configuring a custom certificate:

1. Create a Kubernetes secret containing your custom certificate in the management cluster
2. The secret should contain the following keys:
    * `tls.crt`: The certificate
    * `tls.key`: The private key

## Steps to Configure

1. Create a secret with your custom certificate:
   ```bash
   oc create secret tls sample-hosted-kas-custom-cert \
     --cert=path/to/cert.crt \
     --key=path/to/key.key \
     -n <namespace>
   ```

2. Update your HostedCluster configuration with the custom certificate details as shown in the example above.

3. Apply the changes to your HostedCluster.

## Verification

After applying the configuration, you can verify that the API server is using the custom certificate by:

1. Checking the API server pods to ensure they have the new certificate mounted
2. Testing the connection to the API server using the custom domain name
3. Verifying the certificate details in your browser or using tools like `openssl`

## References

- Additional validation to prevent the SAN conflict PR
- Common error creating a custom certificate for the KAS - Solution 6984698
- OpenShift Documentation

---

## Source: docs/content/how-to/configure-ocp-components/oauth-serving-certificates.md

# Configuring OAuth Server Certificates

This guide explains how to configure a custom serving certificate for the OAuth server in a hosted control plane.

## Overview

In Hosted Control Planes, the OAuth server shares its serving certificate configuration with the Kubernetes API server (KAS). To configure a custom serving certificate for the OAuth server, you must modify the `spec.configuration.apiServer` block in the HostedCluster resource.

**Note:** This configuration method is a deviation from standalone OpenShift behavior. In standalone OpenShift, OAuth certificates are configured separately via the Ingress Operator's `componentRoutes`. In Hosted Control Planes, the `namedCertificates` configuration in the API server section applies to both the Kubernetes API Server and the OAuth server.

## Prerequisites
Before configuring OAuth certificates:
1. Ensure you have cluster-admin access to the management cluster.

2. An existing HostedCluster resource must be present.

3. You have the oc command-line interface (CLI) installed.

4. Create a TLS `Secret` containing your signed certificate and private key in the HostedCluster namespace with the following keys:
    - `tls.crt`
    - `tls.key`

## How HyperShift Uses Certificates for OAuth
HyperShift‚Äôs control-plane-operator reads serving certificates through the shared function `GetNamedCertificates()`:

- API Server implementation:
control-plane-operator/controllers/hostedcontrolplane/v2/kas/config.go#L76
- OAuth Server implementation: control-plane-operator/controllers/hostedcontrolplane/v2/oauth/config.go#L66
- Cluster config source: api/hypershift/v1beta1/clusterconfig.go#L45-L50

Both components read from:
```yaml
APIServer.ServingCerts.NamedCertificates
```

This means:
- Certificates are not configured in an OAuth-specific section of the HostedCluster.
- OAuth server certificates are not provided through OAuth CRD configuration.
- HyperShift automatically injects the selected certificates into the OAuth server deployment.

## Behavior Differences from Standard OpenShift

HyperShift differs from standalone OpenShift in how OAuth certificates behave:

| Area                  | Standard OpenShift                                                       | HyperShift HostedCluster                                                |
| --------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------------------------- |
| Certificate source    | Ingress Operator generates and maps certificates via **componentRoutes** | OAuth uses **apiServer.servingCerts.namedCertificates**                 |
| Certificate selection | Based on Ingress-managed routes                                          | Based on hostname match in namedCertificates                            |
| User responsibility   | No need to manually provide OAuth certs                                  | User is responsible for supplying certs if custom behavior is required  |
| Code path             | Ingress Operator manages OAuth route                                     | control-plane-operator manages OAuth server container runtime arguments |

## Configuration Example
Below is an example HostedCluster configuration where a custom certificate is supplied for the OAuth endpoint.

### 1. Identify your Hosted Cluster namespace

Export the namespace where your hosted cluster is running.

```bash
export HC_NAMESPACE=<hostedcluster_namespace>
export CLUSTER_NAME=<hostedcluster_name>
```

### 2. Generate a quick test certificate (most common for testing, if you don't already have)

```bash
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout tls.key \
  -out tls.crt \
  -subj "/CN=openshift-oauth" \
  -addext "subjectAltName=DNS:oauth-${HC_NAMESPACE}-${CLUSTER_NAME}.apps.rosa.hypershift-ci-2.1xls.p3.openshiftapps.com"
```
**Note:** This example uses a placeholder hostname. After discovering your actual OAuth route in step 4, you must regenerate this certificate with the correct hostname before proceeding to step 5.

Confirm file exists:
```bash
ls tls.crt tls.key
```

### 3. Create the TLS Secret (if not already created) in the HostedCluster namespace

The OAuth and API server namedCertificates configuration only references secrets from the HostedCluster namespace.

```bash
oc create secret tls my-oauth-cert-secret \
  --cert=path/to/tls.crt \
  --key=path/to/tls.key \
  -n $HC_NAMESPACE
```
You should see:
```text
secret/my-oauth-cert-secret created
```
**Note:** Even though the OAuth server runs in the hosted control plane namespace, the serving certificate secret must exist in the HostedCluster namespace. Secrets created in the hosted control plane namespace will not be picked up.

### 4. Discover the correct OAuth Route (this is critical)

Run this using the management cluster kubeconfig -
```bash
oc get routes -n ${HC_NAMESPACE}-${CLUSTER_NAME}
```

You should see something like:
```bash
NAME                  HOST/PORT                                                                                        PATH   SERVICES                PORT    TERMINATION        WILDCARD
oauth                 oauth-${HC_NAMESPACE}-${CLUSTER_NAME}.apps.rosa.hypershift-ci-2.1xls.p3.openshiftapps.com               oauth-openshift         <all>   passthrough/None   None
```

If the route name is `oauth`, confirm it:
```bash
oc get route oauth -n ${HC_NAMESPACE}-${CLUSTER_NAME} -o yaml
```

Now extract the OAuth route host:
```bash
OAUTH_HOST=$(oc get route oauth \
  -n ${HC_NAMESPACE}-${CLUSTER_NAME} \
  -o jsonpath='{.spec.host}')

echo "${OAUTH_HOST}"
```

Example output:
```bash
oauth-${HC_NAMESPACE}-${CLUSTER_NAME}.apps.rosa.hypershift-ci-2.1xls.p3.openshiftapps.com
```

### 5. Edit the HostedCluster resource

Open the HostedCluster resource for editing.

```bash
oc edit hostedcluster $CLUSTER_NAME -n $HC_NAMESPACE
```

Configure the named certificates.

Locate the `spec.configuration.apiServer` section. Add the `servingCerts.namedCertificates` stanza. You must ensure the names list matches the hostname of your OAuth endpoint.

```yaml
spec:
  configuration:
    apiServer:
      audit:
        profile: Default
      servingCerts:
        namedCertificates:
        - names:
          - oauth-${HC_NAMESPACE}-${CLUSTER_NAME}.apps.rosa.hypershift-ci-2.1xls.p3.openshiftapps.com   # [1]
          servingCertificate:
            name: my-oauth-cert-secret   # [2]
```
<1> Replace this with the actual host name of your OAuth route. 

<2> Replace it with the name of the Secret created in step 3.

**Important:** The serving certificate secret referenced by
`spec.configuration.apiServer.servingCerts.namedCertificates`
must exist in the HostedCluster namespace. Creating the secret in the hosted control plane namespace will not apply the certificate.

Save and apply the changes. The Control Plane Operator will reconcile the changes. The configuration will propagate to the control plane, and the OAuth server will begin serving the new certificate.

There is no separate OAuth certificate configuration field in a HostedCluster.

### 6. Verifying the OAuth serving certificate

Verify the certificate served by the route:
```bash
echo | openssl s_client \
  -connect "${OAUTH_HOST}:443" \
  -servername "${OAUTH_HOST}" \
  2>/dev/null \
  | openssl x509 -noout -subject -issuer -ext subjectAltName
```

It would be something like:
```bash
subject=CN=openshift-oauth
issuer=CN=openshift-oauth
X509v3 Subject Alternative Name: 
    DNS:oauth-${HC_NAMESPACE}-${CLUSTER_NAME}.apps.rosa.hypershift-ci-2.1xls.p3.openshiftapps.com
```

This proves:
- The OAuth route is serving the custom certificate
- The cert comes from your `my-oauth-cert-secret` secret
- The change is externally observable


---

## Source: docs/content/how-to/disaster-recovery/backup-and-restore-oadp-1-5.md

# Disaster recovery on Hosted Control Planes with OADP-1.5+

!!! important

    This procedure differs from the regular disaster recovery (DR) process due to the use of an OADP plugin that automates many tasks. If you‚Äôre unable to deploy OADP 1.5 in your management cluster, please refer to the regular DR procedure.

This guide outlines the steps for performing disaster recovery on a Hosted Cluster using the OpenShift API for Data Protection (OADP). It distinguishes between:

- Control Plane: Pods running in the Management cluster, functioning as a Hosted Control Plane.
- Data Plane: The Hosted Cluster where customers deploy workloads and develop core business applications.

## Pre-requisites

Ensure the following prerequisites are met on the Management cluster (connected or disconnected):

- A valid StorageClass.
- Cluster-admin access.
- Access to the openshift-adp version 1.5+ subscription via a CatalogSource.
- Access to online storage compatible with OpenShift ADP cloud storage providers (e.g., S3, Azure, GCP, MinIO).
- HostedControlPlane pods are accessible and functioning correctly.
- The HostedCluster should be PublicAndPrivate or Private.
- The Public only clusters should have at least a hostname.

!!! warning "‚ö†Ô∏è HostedCluster Configuration"

    The HostedCluster must be configured as `PublicAndPrivate`, `Private`, or `Public` with a fixed hostname for the kube-api-server. Public clusters without a hostname will cause restore failures. See HostedCluster Configuration Requirements for details.

!!! Note "Note for Bare Metal Providers"

    Since the InfraEnv has a different lifecycle than the HostedCluster, it should reside in a separate namespace from the HostedControlPlane and must not be deleted during backup or restore procedures.


!!! important

    Before proceeding further, two crucial points must be noted:

    1. Restoration will occur in a green field environment, signifying that after the HostedCluster has been backed up, it must be destroyed to initiate the restoration process.

    2. Node reprovisioning will take place, necessitating the backup of workloads in the Data Plane before deleting the HostedCluster..

## Deploying OpenShift ADP

To deploy the OADP Operator, refer to the official Red Hat documentation, which provides instructions based on your Management cluster version.
After installation, create a DataProtectionApplication (DPA) object, which defines backup locations, Velero pod configurations, and more. This process varies depending on your cloud or remote storage provider. Relevant documentation is available here.
This guide focuses on the following platforms:

- AWS
- Baremetal
- Openstack
- KubeVirt

### Creating Cloud Provider Credentials
Begin by creating credentials for your backup storage platform. Specific instructions are available in the official documentation. For AWS S3 and MinIO, the basic steps are:

```bash
cat << EOF > ./credentials
[default]
aws_access_key_id=<AWS_ACCESS_KEY_ID>
aws_secret_access_key=<AWS_SECRET_ACCESS_KEY>
EOF

oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials
```

!!! note

    If using AWS S3, additional AWS resources must be created to enable data backup and restoration. Follow these instructions to set up the necessary configurations.


### Sample DPA Configurations

Below are some samples of DPA configurations for the mentioned platforms

=== "**AWS**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            provider: aws
            default: true
            objectStorage:
              bucket: <bucket_name>
              prefix: <prefix>
            config:
              region: us-east-1
              profile: "backupStorage"
            credential:
              key: cloud
              name: cloud-credentials
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: us-east-1
              profile: "volumeSnapshot"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - csi
            - hypershift
          resourceTimeout: 2h
    ```

=== "**Bare Metal**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            provider: aws
            default: true
            objectStorage:
              bucket: oadp-backup
              prefix: hcp
            config:
              region: minio
              profile: "default"
              s3ForcePathStyle: "true"
              s3Url: "http://registry.hypershiftbm.lab:9002"
              insecureSkipTLSVerify: "true"
            credential:
              key: cloud
              name: cloud-credentials
              default: true
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: minio
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - csi
            - hypershift
          resourceTimeout: 2h
    ```

=== "**Openstack**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            provider: aws
            default: true
            objectStorage:
              bucket: example-oadp
              prefix: backup-objects
            config:
              region: region-one
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: minio
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - csi
            - hypershift
          resourceTimeout: 2h
    ```


Once you create any of these DPA objects, several pods will be instantiated in the `openshift-adp` namespace. This includes one `node-agent` per node in the Management Cluster and the `velero` deployment.

!!! note

    To follow backup and restore procedures, you can monitor the logs in the velero pod.


## Backup and Upload
=== "**AWS**"
    ### Data Plane workloads backup

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps are the following:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).


    ### Control Plane backup

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hc-clusters-hosted-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
        spec:
      hooks: {}
      includedNamespaces:
      - clusters
      - clusters-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - configmap
      - priorityclasses
      - pdb
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - awscluster
      - awsmachinetemplate
      - awsmachine
      - machinedeployment
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      storageLocation: default
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

    ```yaml
    snapshotMoveData: true
    datamover: "velero"
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**Bare Metal - CSI Support**"

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps are the following:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).

    ### Control Plane backup - For CSI compatible storage class

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hc-clusters-hosted-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
        spec:
      hooks: {}
      includedNamespaces:
      - clusters
      - clusters-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - bmh
      - configmap
      - infraenv
      - priorityclasses
      - pdb
      - agents
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - agentcluster
      - agentmachinetemplate
      - agentmachine
      - machinedeployment
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      storageLocation: default
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

    ```yaml
    snapshotMoveData: true
    datamover: "velero"
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**Bare Metal - Non CSI Support**"

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps are the following:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).

    ### Control Plane backup - For **Non** CSI compatible storage class

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hc-clusters-hosted-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
        spec:
      hooks: {}
      includedNamespaces:
      - clusters
      - clusters-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - bmh
      - configmap
      - infraenv
      - priorityclasses
      - pdb
      - agents
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - agentcluster
      - agentmachinetemplate
      - agentmachine
      - machinedeployment
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      defaultVolumesToFsBackup: true
      storageLocation: default
      ttl: 2h30m0s
    ```

    We will emphasize the most important fields:

    - This particular field is crucial if you utilize a storage class that does not supported by CSI drivers (e.g NFS). It designates fs-backup as the default method for Persistent Volume backup. If you wish to continue using CSI Volume Snapshot (within the same backup manifest), please check the "Bare Metal - CSI Support" tab.

    ```yaml
    defaultVolumesToFsBackup: true
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**Openstack**"

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps are the following:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).

    ### Control Plane backup - Openstack

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hc-clusters-hosted-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
        spec:
      hooks: {}
      includedNamespaces:
      - clusters
      - clusters-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - configmap
      - priorityclasses
      - pdb
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - openstackclusters
      - openstackmachinetemplates
      - openstackmachine
      - machinedeployment
      - openstackfloatingippools
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      storageLocation: default
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

    ```yaml
    snapshotMoveData: true
    datamover: "velero"
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**KubeVirt**"

    ### Backup of a kubeVirt Hosted Cluster
    !!! important
        The restore may only be done on the same management cluster where the backup was created.

    Backup of a hosted cluster, running on a KubeVirt platform may be done on a running hosted cluster, and there is no
    need to pause it.

    The backup will contain the hosted control plane components, the hosted cluster ETCD, and the data stored on the
    hosted cluster PVCs.

    The backup will not contain the KubeVirt VMs, used as worker nodes, and they will be automatically recreated after
    the restore.
    #### Performing the Backup
    The backup itself is done by creating a velero Backup resource. Here is how its manifest looks like:
    ```yaml
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hc-clusters-hosted-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
    spec:
      includedNamespaces:
      - clusters
      - clusters-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - deployment
      - statefulset
      - pv
      - pvc
      - bmh
      - configmap
      - infraenv
      - priorityclasses
      - pdb
      - hostedcluster
      - nodepool
      - secrets
      - hostedcontrolplane
      - cluster
      - datavolume
      - service
      - route
      excludedResources: [ ]
      labelSelector:
        matchExpressions:
        - key: 'hypershift.openshift.io/is-kubevirt-rhcos'
          operator: 'DoesNotExist'
      storageLocation: default
      preserveNodePorts: true
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
    ```
    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

        ```yaml
        snapshotMoveData: true
        datamover: "velero"
        ```

    - We don't want to use this feature. This will allow us to safely backup the PVCs we want

        ```yaml
        defaultVolumesToFsBackup: false
        ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both
      the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-hosted`).


        ```yaml
        includedNamespaces:
        - clusters
        - clusters-hosted
        ```

    !!! hint
        By default, the HostedControlPlane namespace is `clusters-<hosted cluster name>`.

    - The boot image of the KubeVirt VMs, that are used as the hosted cluster nodes, are stored in huge PVCs. We don't
      need these PVCs because the VMs are going to be recreated as new VMs. We want tilter these PVCs out of the backup
      to gain meaningful reduce in backup time and storage size. We'll filter these PVC using this label selector:

        ```yaml
        labelSelector:
          matchExpressions:
          - key: 'hypershift.openshift.io/is-kubevirt-rhcos'
            operator: 'DoesNotExist'
        ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.


## Restore

For the restoration procedure, ensure that there are no Pods/PVCs running in the HostedControlPlane namespace to facilitate a successful restoration. If restoring the HostedCluster within the same Management cluster, delete the following objects:

- HostedCluster
- Nodepools
- PVCs

This will remove the Pods/PVCs, allowing for a proper restoration of the environment.

!!! important

    In the case of the Bare Metal provider (Agent), it's crucial to ensure that we don't delete the InfraEnv object. This object is mandatory for the new nodes that will be reprovisioned, as they need access to it in order to retrieve the Discovery ISO.

The restoration object is provider agnostic. Just make sure you point to the right backupName to restore from

Now let's take a look to the restoration manifest:

```yaml
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: hc-clusters-hosted-restore
  namespace: openshift-adp
spec:
  backupName: hc-clusters-hosted-backup
  restorePVs: true
  existingResourcePolicy: update
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
```

Here we have a couple of important fields

```yaml
restorePVs: true
existingResourcePolicy: update
```

- `restorePvs` will initiate the recovery of pods with the included persistent volumes.
- Setting `existingResourcePolicy` to `update` ensures that any existing objects are overwritten with the backup content. This may cause issues with objects containing immutable fields, which is why we delete them in a previous step. If this policy is not set, the Velero engine will skip the restoration of objects that already exist.

You can monitor the restoration process by checking the restore status field and following the Velero logs mentioned in the Watching section.

The restoration process is considered complete once the `status.phase` is `Completed`.

!!! important

    The restore may only be done on the same management cluster where the backup was created. Depending on the HostedCluster provider you are using, there are important topics to have in mind before the restoration.


=== "**AWS**"

    - Node readoption is not supported in this provider yet, so the worker nodes will be reprovisioned at restoration time

=== "**Openstack**"

    - Node readoption is not supported in this provider yet, so the worker nodes will be reprovisioned at restoration time

=== "**BareMetal**"

    - If you are restoring the HostedCluster in the same Management Cluster, make sure:
        - You don't delete the Infraenv namespace or the objects inside.
        - You don't remove the Assisted Installer Postgres DDBB
    - If you are restoring the HostedCluster in a different Management Cluster, have in mind that you should backup some things too:
        - Backup of the infraenv namespace and their objects
        - You need to backup and restore the Postgres DDBB
        - All of this could be included in the Backup manifest to do this automatically
    - Node readoption is supported in this provider only under certain conditions:
        - For now only in supported in HostedClusters with 4.19 OCP versions
        - MCE 2.9 or ACM 2.14

=== "**Kubevirt**"

    - Restoration in a separated Management cluster is not supported by this provider
    - Node readoption is not supported in this provider yet, so the worker nodes will be reprovisioned at restoration time


## Schedule

OADP provides the ability to schedule backups using the Schedule CR. This is fully compatible with the Hosted Control Planes backup procedure; just make sure you are following the official Red Hat documentation.

- https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/backup_and_restore/oadp-application-backup-and-restore#oadp-scheduling-backups-doc

## Watching and Troubleshooting

### Watching

Here we will describe how to monitor and observe a Backup or Restore process:

- Watch the Backup process

```bash
watch "oc get backup -n openshift-adp <BACKUP_NAME> -o jsonpath='{.status}' | jq"
```

- Watch the Restore process

```bash
watch "oc get restore -n openshift-adp <BACKUP_NAME> -o jsonpath='{.status}' | jq"
```

- Follow the Velero logs

```bash
oc logs -n openshift-adp -ldeploy=velero -f
```

- Watch all the OADP Main objects

```bash
watch "echo BackupRepositories:;echo;oc get backuprepositories.velero.io -A;echo; echo BackupStorageLocations: ;echo; oc get backupstoragelocations.velero.io -A;echo;echo DataUploads: ;echo;oc get datauploads.velero.io -A;echo;echo DataDownloads: ;echo;oc get datadownloads.velero.io -n openshift-adp; echo;echo VolumeSnapshotLocations: ;echo;oc get volumesnapshotlocations.velero.io -A;echo;echo Backups:;echo;oc get backup -A; echo;echo Restores:;echo;oc get restore -A"
```

### Troubleshooting

For troubleshooting purposes, it's crucial to identify where the process is stuck. Here are some tips:

- Review the Velero logs in the `openshift-adp` namespace.
- Utilize the `velero` command with the `--details` flag to describe the backup/restore objects.
- Check the backup/restore status directly in the respective object, which can provide valuable hints.
- Examine the Events in the affected namespaces (in the previous examples, these were `clusters` and `clusters-hosted`).
- Verify the status of all OADP objects using the command provided in the Watching section.

## Tips and Tricks

### Use the velero CLI

To gain more insights into the backup/restore objects or to perform actions such as deleting a backup/restore object, you can utilize the `velero` CLI. This tool provides additional context in each case, and the best part is that you don't need to download anything.

- Create an alias to use the `velero` CLI from a container

```bash
alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'
```

Now you can execute commands using the `velero` CLI. Here are some sample commands:

- Describe a restore called `hc-clusters-hosted-restore`

```bash
velero restore describe hc-clusters-hosted-restore --details
```

- Describe a backup called `hc-clusters-hosted-backup`

```bash
velero backup describe hc-clusters-hosted-backup --details
```

### How to handle backups properly

To handle the `backup` and `restore` objects in the cloud storage effectively, the following command is quite useful. It helps identify issues with the `backuprepositories.velero.io` object if you manually modify the folder structure in the destination storage. Therefore, we recommend avoiding modifications over that storage and managing the backups/restore objects using the CLI.

- Delete a backup created and called `hc-clusters-hosted-backup`

```bash
velero delete backup hc-clusters-hosted-backup
```

!!! Important

    If you modify the folder structure of the remote storage where your backups are hosted, you may encounter issues with `backuprepositories.velero.io`. In such cases, you will need to recreate all the associated objects, including DPAs, backups, restores, etc.


## HostedCluster Configuration Requirements

The HostedCluster must be configured as `PublicAndPrivate`, `Private`, or `Public` with a fixed hostname for backup/restore operations to work correctly. If the HostedCluster is configured as `Public` without a hostname in the ServicePublishingStrategy for the kube-api-server, the restore operation will fail with the following consequences:

- **Nodes remain in NotReady state**
- **NodePool scaling fails to generate new nodes**

### Root Cause

The issue occurs because:

- Nodes store the ELB (Elastic Load Balancer) address in their kubelet configuration, which is ephemeral and changes when the cluster is deleted and restored
- The SAN (Subject Alternative Name) in the certificate fails because the certificate name no longer matches the new ELB
- Original nodes cannot connect to the ControlPlane because they point to the old ELB, and even if they pointed to the new one, the certificate would be incorrect

### Solution

Ensure your HostedCluster is configured with either:

- `PublicAndPrivate` service publishing strategy, OR
- `Private` service publishing strategy, OR
- `Public` service publishing strategy with a **hostname** specified for the kube-api-server

### AWS Self-Managed Platform Requirements

!!! important "AWS Self-Managed Platforms"

    When using AWS platform with self-managed infrastructure, the `Public` endpoint access option with a **Route** service publishing strategy and a **fixed hostname** is required. This is a specific case of the `Public` with hostname option described above.

    This ensures that:
    - Node workloads can be properly migrated to new nodes in the restored NodePools
    - Service continuity is maintained during the disaster recovery process
    - DNS resolution remains consistent for applications

### Example Configuration

**Public endpoint access with Route hostname (required for AWS self-managed platforms)**
```yaml
spec:
  platform:
    aws:
      endpointAccess: Public
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: Route
      route:
        hostname: api.example.com
```

### Fixing OIDC After Restore

After completing the OADP restore, if the control-plane-operator reports `WebIdentityErr` errors or NodePool nodes remain not-ready due to a missing default security group, run the OIDC disaster recovery command:

```bash
hypershift fix dr-oidc-iam \
  --hc-name <cluster-name> \
  --hc-namespace <namespace> \
  --aws-creds ~/.aws/credentials
```

This re-uploads the OIDC discovery documents using the existing cluster signing key and recreates the IAM OIDC provider if needed. See the AWS Disaster Recovery documentation for full details.


---

## Source: docs/content/how-to/disaster-recovery/backup-and-restore-oadp.md

# Disaster recovery on Hosted Control Planes

!!! important

    This procedure uses the Hypershift OADP plugin that automates many tasks. Here we assume you have oadp < 1.5 in your management cluster, if that's not the case, please refer to the OADP-1.5 DR procedure.

In this section, we will outline the procedures for performing disaster recovery tasks on a Hosted Cluster using the Openshift API for Data Protection (OADP). We will differentiate between the Control Plane (consisting of pods running in the Management cluster, which function as a Hosted Control Plane) and the Data Plane (the Hosted Cluster where customers add their workloads and develop their core business).

## Pre-requisites

The first consideration is to ensure we meet the prerequisites. On the Management cluster, whether it is Connected or Disconnected, we require:

- A valid StorageClass.
- Cluster-admin access.
- Access to the openshift-adp subscription through a CatalogSource.
- Access to online storage compatible with the openshift-adp cloud storage providers (S3, Azure, GCP, Minio, etc.).
- The HostedControlPlane pods should be accessible and functioning correctly.
- **(Bare Metal Provider Only)** As the InfraEnv has a different lifecycle than the HostedCluster, it should reside in a namespace separate from that of the HostedControlPlane and should not be deleted during the backup/restore procedures.

!!! warning "‚ö†Ô∏è HostedCluster Configuration"

    The HostedCluster must be configured as `PublicAndPrivate`, `Private`, or `Public` with a fixed hostname for the kube-api-server. Public clusters without a hostname will cause restore failures. See HostedCluster Configuration Requirements for details.


!!! important

    Before proceeding further, two crucial points must be noted:
    1. Restoration will occur in a green field environment, signifying that after the HostedCluster has been backed up, it must be destroyed to initiate the restoration process.
    2. Node reprovisioning will take place, necessitating the backup of workloads in the Data Plane before deleting the HostedCluster..

## Openshift-adp deployment

To deploy the OADP operator, we kindly redirect you to the Official Red Hat documentation, where they provide instructions based on the version of the Management Cluster you're using.

Once installed, you'll need to create an object called DPA (Data Protection Application), which essentially describes the backup locations, Velero pod configurations, etc. This process varies depending on the cloud/remote storage location. All relevant documentation is available here.

This guide will focus on three main platforms:

- AWS
- Baremetal
- KubeVirt

The first step is to create credentials for the platform where you'll upload the backups. Specific instructions can be found in the official documentation, but the basic steps are as follows:

```bash
cat << EOF > ./credentials
[default]
aws_access_key_id=<AWS_ACCESS_KEY_ID>
aws_secret_access_key=<AWS_SECRET_ACCESS_KEY>
EOF

oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials
```

!!! note

    This applies to S3 and Minio platforms. For other providers, you can follow the instructions provided in the official documentation.


If you are using the AWS S3 provider, you will need to create additional objects in AWS to enable the push and pullback of data to S3. To accomplish this, follow these instructions

Below are some samples of DPA configurations for the mentioned platforms

=== "**Bare Metal**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            provider: aws
            default: true
            objectStorage:
              bucket: oadp-backup
              prefix: hcp
            config:
              region: minio
              profile: "default"
              s3ForcePathStyle: "true"
              s3Url: "http://registry.hypershiftbm.lab:9002"
              insecureSkipTLSVerify: "true"
            credential:
              key: cloud
              name: cloud-credentials
              default: true
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: minio
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - csi
          customPlugins:
            - name: hypershift-oadp-plugin
              image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main
          resourceTimeout: 2h
    ```


=== "**AWS**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            provider: aws
            default: true
            objectStorage:
              bucket: <bucket_name>
              prefix: <prefix>
            config:
              region: us-east-1
              profile: "backupStorage"
            credential:
              key: cloud
              name: cloud-credentials
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: us-east-1
              profile: "volumeSnapshot"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - csi
          customPlugins:
            - name: hypershift-oadp-plugin
              image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main
          resourceTimeout: 2h
    ```


=== "**Openstack**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            config:
              region: region-one
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
            default: true
            objectStorage:
              bucket: example-oadp
              prefix: backup-objects
            provider: aws
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: region-one
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - kubevirt
            - csi
          customPlugins:
            - name: hypershift-oadp-plugin
              image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main
          resourceTimeout: 2h
    ```


=== "**Kubevirt**"

    ```yaml
    ---
    apiVersion: oadp.openshift.io/v1alpha1
    kind: DataProtectionApplication
    metadata:
      name: dpa-instance
      namespace: openshift-adp
    spec:
      backupLocations:
        - name: default
          velero:
            config:
              region: minio
              profile: "default"
              s3ForcePathStyle: 'true'
              s3Url: 'http://registry.hypershiftbm-2.lab:9002'
            credential:
              key: cloud
              name: cloud-credentials
            default: true
            objectStorage:
              bucket: example-oadp
              prefix: backup-objects
            provider: aws
      snapshotLocations:
        - velero:
            provider: aws
            config:
              region: region-one
              profile: "default"
            credential:
              key: cloud
              name: cloud-credentials
      configuration:
        nodeAgent:
          enable: true
          uploaderType: kopia
        velero:
          defaultPlugins:
            - openshift
            - aws
            - kubevirt
            - csi
          customPlugins:
            - name: hypershift-oadp-plugin
              image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main
          resourceTimeout: 2h
    ```

Once you create any of these DPA objects, several pods will be instantiated in the `openshift-adp` namespace. This includes one `node-agent` per node in the Management Cluster and the `velero` deployment.

!!! note

    To follow backup and restore procedures, you can monitor the logs in the velero pod.


## Backup and Upload

=== "**Bare Metal**"

    ### Data Plane workloads backup

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps will indeed be quite similar:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).

    ### Control Plane backup

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hcp-minio-backup-csi
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
    spec:
      includedNamespaces:
      - clusters
      - clusters-hosted-ipv6
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - bmh
      - configmap
      - infraenv
      - priorityclasses
      - pdb
      - agents
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - agentcluster
      - agentmachinetemplate
      - agentmachine
      - machinedeployment
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      storageLocation: default
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    We will emphasize the most important fields:

    - These fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

    ```yaml
    snapshotMoveData: true
    datamover: "velero"
    defaultVolumesToFsBackup: false
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-example-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**Bare Metal - Non CSI Compatible**"

    ### Data Plane workloads backup

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps will indeed be quite similar:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).

    ### Control Plane backup

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hcp-minio-backup-no-csi
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
    spec:
      includedNamespaces:
      - clusters
      - clusters-hosted-ipv6
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - bmh
      - configmap
      - infraenv
      - priorityclasses
      - pdb
      - agents
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - agentcluster
      - agentmachinetemplate
      - agentmachine
      - machinedeployment
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      defaultVolumesToFsBackup: true
      storageLocation: default
      ttl: 2h30m0s
    ```

    We will emphasize the most important fields:

    - This particular field is crucial if you utilize a combination of CSI Volume Snapshot and fs-backup. It designates fs-backup as the default method for Persistent Volume backup. If you wish to continue using CSI Volume Snapshot (within the same backup manifest), you will need to add an annotation to the desired pods, including the PVs `backup.velero.io/backup-volumes-excludes=<pvc-name>`. Further information can be found here.

    ```yaml
    defaultVolumesToFsBackup: true
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-example-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**AWS**"

    ### Data Plane workloads backup

    !!! Note

        If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.

    If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications

    The steps will indeed be quite similar:

    - Deploy the OADP operator from OLM.
      - Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the `Prefix` or/and `Bucket` fields to keep the ControlPlane and DataPlane backups separated.
      - Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.

      We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.

    As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize `Kopia` as the backend tool for data snapshots, along with `File System Backup`. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.

    !!! Important

        The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.

    Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).

    ### Control Plane backup

    Now, we will apply the backup manifest. Here is how it looks like:

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hcp-aws-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
    spec:
      includedNamespaces:
      - clusters
      - clusters-example-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - configmap
      - priorityclasses
      - pdb
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - awscluster
      - awsmachinetemplate
      - awsmachine
      - machinedeployment
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      storageLocation: default
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

    ```yaml
    snapshotMoveData: true
    datamover: "velero"
    defaultVolumesToFsBackup: false
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-example-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-example-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.


=== "**Openstack**"

    ```yaml
    ---
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hcp-osp-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
    spec:
      includedNamespaces:
      - clusters
      - clusters-example-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - pod
      - pvc
      - pv
      - configmap
      - priorityclasses
      - pdb
      - hostedcluster
      - nodepool
      - secrets
      - services
      - deployments
      - statefulsets
      - hostedcontrolplane
      - cluster
      - openstackclusters
      - openstackmachinetemplates
      - openstackmachine
      - machinedeployment
      - openstackfloatingippools
      - machineset
      - machine
      - route
      - clusterdeployment
      excludedResources: []
      storageLocation: default
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

    ```yaml
    snapshotMoveData: true
    datamover: "velero"
    defaultVolumesToFsBackup: false
    ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-example-hosted`).

    ```yaml
    includedNamespaces:
    - clusters
    - clusters-example-hosted
    ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.

=== "**Kubevirt**"

    ```yaml
    apiVersion: velero.io/v1
    kind: Backup
    metadata:
      name: hcp-osp-backup
      namespace: openshift-adp
      labels:
        velero.io/storage-location: default
    spec:
      includedNamespaces:
      - clusters
      - clusters-example-hosted
      includedResources:
      - sa
      - role
      - rolebinding
      - deployment
      - statefulset
      - pv
      - pvc
      - bmh
      - configmap
      - infraenv
      - priorityclasses
      - pdb
      - hostedcluster
      - nodepool
      - secrets
      - hostedcontrolplane
      - cluster
      - kubevirtcluster
      - kubevirtmachinetemplate
      - datavolume
      - service
      - route
      excludedResources: []
      storageLocation: default
      preserveNodePorts: true
      ttl: 2h30m0s
      snapshotMoveData: true
      datamover: "velero"
      defaultVolumesToFsBackup: false
      snapshotVolumes: true
    ```

    ### Backup of a kubeVirt Hosted Cluster
    !!! important
        The restore may only be done on the same management cluster where the backup was created.

    Backup of a hosted cluster, running on a KubeVirt platform may be done on a running hosted cluster, and there is no
    need to pause it.

    The backup will contain the hosted control plane components, the hosted cluster ETCD, and the data stored on the
    hosted cluster PVCs.

    The backup will not contain the KubeVirt VMs, used as worker nodes, and they will be automatically recreated after
    the restore.

    We will emphasize the most important fields:

    - These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.

        ```yaml
        snapshotMoveData: true
        datamover: "velero"
        ```

    - We don't want to use this feature. This will allow us to safely backup the PVCs we want

        ```yaml
        defaultVolumesToFsBackup: false
        ```

    - This field selects the namespaces from which objects will be backed up. They should include namespaces from both
      the HostedCluster (in the example `clusters`) and the HostedControlPlane (in the example `clusters-example-hosted`).


        ```yaml
        includedNamespaces:
        - clusters
        - clusters-example-hosted
        ```

    !!! hint
        By default, the HostedControlPlane namespace is `clusters-<hosted cluster name>`.

    - The boot image of the KubeVirt VMs, that are used as the hosted cluster nodes, are stored in huge PVCs. We don't
      need these PVCs because the VMs are going to be recreated as new VMs. We want tilter these PVCs out of the backup
      to gain meaningful reduce in backup time and storage size. We'll filter these PVC using this label selector:

        ```yaml
        labelSelector:
          matchExpressions:
          - key: 'hypershift.openshift.io/is-kubevirt-rhcos'
            operator: 'DoesNotExist'
        ```

    Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.

    The backup process is considered complete when the `status.phase` is `Completed`.


## Restore

For the restoration procedure, ensure that there are no Pods/PVCs running in the HostedControlPlane namespace to facilitate a successful restoration. If restoring the HostedCluster within the same Management cluster, delete the following objects:

- HostedCluster
- Nodepools
- PVCs

This will remove the Pods/PVCs, allowing for a proper restoration of the environment.

!!! important

    In the case of the Bare Metal provider (Agent), it's crucial to ensure that we don't delete the InfraEnv object. This object is mandatory for the new nodes that will be reprovisioned, as they need access to it in order to retrieve the Discovery ISO.

Now let's take a look to the restoration manifest:

```yaml
---
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: hc-clusters-example-hosted-restore
  namespace: openshift-adp
spec:
  backupName: <NAME OF THE REFERRED BACKUP>
  restorePVs: true
  existingResourcePolicy: update
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
```

Here we have a couple of important fields

```yaml
restorePVs: true
existingResourcePolicy: update
```

- `restorePvs` will initiate the recovery of pods with the included persistent volumes.
- Setting `existingResourcePolicy` to `update` ensures that any existing objects are overwritten with the backup content. This may cause issues with objects containing immutable fields, which is why we delete them in a previous step. If this policy is not set, the Velero engine will skip the restoration of objects that already exist.

You can monitor the restoration process by checking the restore status field and following the Velero logs mentioned in the Watching section.

The restoration process is considered complete once the `status.phase` is `Completed`.

## Watching and Troubleshooting

### Watching

Here we will describe how to monitor and observe a Backup or Restore process:

- Watch the Backup process

```bash
watch "oc get backup -n openshift-adp <BACKUP_NAME> -o jsonpath='{.status}' | jq"
```

- Watch the Restore process

```bash
watch "oc get restore -n openshift-adp <BACKUP_NAME> -o jsonpath='{.status}' | jq"
```

- Follow the Velero logs

```bash
oc logs -n openshift-adp -ldeploy=velero -f
```

- Watch all the OADP Main objects

```bash
watch "echo BackupRepositories:;echo;oc get backuprepositories.velero.io -A;echo; echo BackupStorageLocations: ;echo; oc get backupstoragelocations.velero.io -A;echo;echo DataUploads: ;echo;oc get datauploads.velero.io -A;echo;echo DataDownloads: ;echo;oc get datadownloads.velero.io -n openshift-adp; echo;echo VolumeSnapshotLocations: ;echo;oc get volumesnapshotlocations.velero.io -A;echo;echo Backups:;echo;oc get backup -A; echo;echo Restores:;echo;oc get restore -A"
```

### Troubleshooting

For troubleshooting purposes, it's crucial to identify where the process is stuck. Here are some tips:

- Review the Velero logs in the `openshift-adp` namespace.
- Utilize the `velero` command with the `--details` flag to describe the backup/restore objects.
- Check the backup/restore status directly in the respective object, which can provide valuable hints.
- Examine the Events in the affected namespaces (in the previous examples, these were `clusters` and `clusters-example-hosted`).
- Verify the status of all OADP objects using the command provided in the Watching section.

## Tips and Tricks

### Use the velero CLI

To gain more insights into the backup/restore objects or to perform actions such as deleting a backup/restore object, you can utilize the `velero` CLI. This tool provides additional context in each case, and the best part is that you don't need to download anything.

- Create an alias to use the `velero` CLI from a container

```bash
alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'
```

Now you can execute commands using the `velero` CLI. Here are some sample commands:

- Describe a restore called `hc-clusters-hosted-restore`

```bash
velero restore describe hc-clusters-hosted-restore --details
```

- Describe a backup called `hc-clusters-hosted-backup`

```bash
velero backup describe hc-clusters-hosted-backup --details
```

### How to handle backups properly

To handle the `backup` and `restore` objects in the cloud storage effectively, the following command is quite useful. It helps identify issues with the `backuprepositories.velero.io` object if you manually modify the folder structure in the destination storage. Therefore, we recommend avoiding modifications over that storage and managing the backups/restore objects using the CLI.

- Delete a backup created and called `hc-clusters-hosted-backup`

```bash
velero delete backup hc-clusters-hosted-backup
```

!!! Important

    If you modify the folder structure of the remote storage where your backups are hosted, you may encounter issues with `backuprepositories.velero.io`. In such cases, you will need to recreate all the associated objects, including DPAs, backups, restores, etc.

## HostedCluster Configuration Requirements for AWS provider

The HostedCluster must be configured as `PublicAndPrivate`, `Private`, or `Public` with a fixed hostname for backup/restore operations to work correctly. If the HostedCluster is configured as `Public` without a hostname in the ServicePublishingStrategy for the kube-api-server, the restore operation will fail with the following consequences:

- **Nodes remain in NotReady state**
- **NodePool scaling fails to generate new nodes**

### Root Cause

The issue occurs because:

- Nodes store the ELB (Elastic Load Balancer) address in their kubelet configuration, which is ephemeral and changes when the cluster is deleted and restored
- The SAN (Subject Alternative Name) in the certificate fails because the certificate name no longer matches the new ELB
- Original nodes cannot connect to the ControlPlane because they point to the old ELB, and even if they pointed to the new one, the certificate would be incorrect

### Solution

Ensure your HostedCluster is configured with either:

- `PublicAndPrivate` service publishing strategy, OR
- `Private` service publishing strategy, OR
- `Public` service publishing strategy with a **hostname** specified for the kube-api-server

### AWS Self-Managed Platform Requirements

!!! important "AWS Self-Managed Platforms"

    When using AWS platform with self-managed infrastructure, the `Public` endpoint access option with a **Route** service publishing strategy and a **fixed hostname** is required. This is a specific case of the `Public` with hostname option described above.

    This ensures that:
    - Node workloads can be properly migrated to new nodes in the restored NodePools
    - Service continuity is maintained during the disaster recovery process
    - DNS resolution remains consistent for applications

### Example Configuration

**Public endpoint access with Route hostname (required for AWS self-managed platforms)**
```yaml
spec:
  platform:
    aws:
      endpointAccess: Public
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: Route
      route:
        hostname: api.example.com
```

### Fixing OIDC After Restore

After completing the OADP restore, if the control-plane-operator reports `WebIdentityErr` errors or NodePool nodes remain not-ready due to a missing default security group, run the OIDC disaster recovery command:

```bash
hypershift fix dr-oidc-iam \
  --hc-name <cluster-name> \
  --hc-namespace <namespace> \
  --aws-creds ~/.aws/credentials
```

This re-uploads the OIDC discovery documents using the existing cluster signing key and recreates the IAM OIDC provider if needed. See the AWS Disaster Recovery documentation for full details.



---

## Source: docs/content/how-to/disaster-recovery/dr-cli.md

# Disaster Recovery CLI

The HyperShift CLI provides disaster recovery capabilities through integrated commands.

## Backup

The `hypershift create oadp-backup` command creates backups of hosted clusters using OADP (OpenShift API for Data Protection).

### Prerequisites

Before creating backups, ensure that:

1. **OADP Operator is installed**: The OADP operator must be installed and running in your management cluster
2. **DataProtectionApplication (DPA) exists**: A DPA custom resource must be configured and ready
3. **Storage location configured**: A backup storage location must be available (e.g., S3, GCS, Azure Blob)

### Basic Usage

```bash
hypershift create oadp-backup \
  --hc-name my-hosted-cluster \
  --hc-namespace my-hosted-cluster-namespace
```

### Command Options

| Flag | Type | Default | Required | Description |
|------|------|---------|----------|-------------|
| `--hc-name` | string | - | ‚úÖ | Name of the hosted cluster to backup |
| `--hc-namespace` | string | - | ‚úÖ | Namespace of the hosted cluster to backup |
| `--oadp-namespace` | string | `openshift-adp` | ‚ùå | Namespace where OADP operator is installed |
| `--storage-location` | string | `default` | ‚ùå | Storage location for the backup (must exist in DPA config) |
| `--ttl` | duration | `2h0m0s` | ‚ùå | Time to live for the backup before automatic deletion |
| `--snapshot-move-data` | bool | `true` | ‚ùå | Enable snapshot move data feature for CSI volumes |
| `--default-volumes-to-fs-backup` | bool | `false` | ‚ùå | Use filesystem backup for volumes by default instead of snapshots |
| `--render` | bool | `false` | ‚ùå | Render the backup object to STDOUT instead of creating it |
| `--included-resources` | []string | (see below) | ‚ùå | Comma-separated list of resources to include (overrides defaults) |

#### Flag Details

**`--hc-name` and `--hc-namespace`**
These identify the hosted cluster to backup. The namespace typically follows the pattern where control plane resources are in `{hc-namespace}-{hc-name}`.

**`--storage-location`**
Must match a storage location configured in your DataProtectionApplication. Common values:
- `default` - Default storage location
- `aws-s3` - AWS S3 bucket
- `gcp-gcs` - Google Cloud Storage
- `azure-blob` - Azure Blob Storage

**`--ttl`**
Supports Kubernetes duration format:
- `30m` - 30 minutes
- `2h` - 2 hours
- `24h` - 24 hours
- `7d` - 7 days

**`--snapshot-move-data`**
When enabled, moves snapshot data to object storage for long-term retention. Useful for cross-region/cross-cloud disaster recovery.

**`--included-resources`**
Accepts comma-separated resource types. See Resource Types section for complete list.

### Example Commands

#### Basic backup with defaults
```bash
hypershift create oadp-backup \
  --hc-name prod01 \
  --hc-namespace hcp01
```

#### Backup with custom settings
```bash
hypershift create oadp-backup \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --oadp-namespace custom-oadp \
  --storage-location s3-backup \
  --ttl 24h0m0s \
  --snapshot-move-data=false
```

#### Render backup object without creating it
```bash
hypershift create oadp-backup \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --render
```

This will output the backup YAML to STDOUT, allowing you to inspect or pipe it to other commands:
```bash
# Save to file
hypershift create oadp-backup --hc-name prod --hc-namespace hcp01 --render > backup.yaml

# Apply with kubectl
hypershift create oadp-backup --hc-name prod --hc-namespace hcp01 --render | kubectl apply -f -
```

#### Backup with custom resource selection
```bash
hypershift create oadp-backup \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --included-resources configmaps,secrets,hostedclusters.hypershift.openshift.io,nodepools.hypershift.openshift.io
```

This will only backup the specified resources instead of the default comprehensive list.

### What Gets Backed Up

By default, the backup includes the following resources. The exact set of resources depends on the platform type of your HostedCluster, which is automatically detected.

**Core Kubernetes Resources (always included):**
- ServiceAccounts (`serviceaccounts`), Roles (`roles`), RoleBindings (`rolebindings`)
- Pods (`pods`), PVCs (`persistentvolumeclaims`), PVs (`persistentvolumes`), ConfigMaps (`configmaps`)
- Secrets (`secrets`), Services (`services`), Deployments (`deployments`), StatefulSets (`statefulsets`)
- PriorityClasses (`priorityclasses`), PodDisruptionBudgets (`poddisruptionbudgets`)

**HyperShift Specific resources (always included):**
- HostedCluster (`hostedclusters.hypershift.openshift.io`)
- HostedControlPlane (`hostedcontrolplanes.hypershift.openshift.io`)
- NodePool (`nodepools.hypershift.openshift.io`)

**Cluster API Resources (always included):**
- MachineDeployment (`machinedeployments.cluster.x-k8s.io`), MachineSet (`machinesets.cluster.x-k8s.io`), Machine (`machines.cluster.x-k8s.io`)
- Generic cluster resources (`clusters.cluster.x-k8s.io`)

**Additional Resources (always included):**
- Routes (`routes.route.openshift.io`), ClusterDeployments (`clusterdeployments.hive.openshift.io`)

**Platform-Specific Resources (automatically detected):**

The backup command automatically detects your HostedCluster's platform and includes the appropriate platform-specific resources:

- **AWS Platform**: AWSCluster (`awsclusters.infrastructure.cluster.x-k8s.io`), AWSMachineTemplate (`awsmachinetemplates.infrastructure.cluster.x-k8s.io`), AWSMachine (`awsmachines.infrastructure.cluster.x-k8s.io`)
- **Agent Platform**: AgentCluster (`agentclusters.infrastructure.cluster.x-k8s.io`), AgentMachineTemplate (`agentmachinetemplates.infrastructure.cluster.x-k8s.io`), AgentMachine (`agentmachines.infrastructure.cluster.x-k8s.io`)
- **KubeVirt Platform**: KubevirtCluster (`kubevirtclusters.infrastructure.cluster.x-k8s.io`), KubevirtMachineTemplate (`kubevirtmachinetemplates.infrastructure.cluster.x-k8s.io`)
- **OpenStack Platform**: OpenStackClusters (`openstackclusters.infrastructure.cluster.x-k8s.io`), OpenStackMachineTemplates (`openstackmachinetemplates.infrastructure.cluster.x-k8s.io`), OpenStackMachine (`openstackmachines.infrastructure.cluster.x-k8s.io`)
- **Azure Platform**: AzureClusters (`azureclusters.infrastructure.cluster.x-k8s.io`), AzureMachineTemplates (`azuremachinetemplates.infrastructure.cluster.x-k8s.io`), AzureMachine (`azuremachines.infrastructure.cluster.x-k8s.io`)

> **Note**: If the HostedCluster cannot be accessed or the platform type cannot be determined, the command defaults to AWS platform resources.

#### Custom Resource Selection

You can override the default resource list using the `--included-resources` flag:

```bash
# Backup only specific resources
hypershift create oadp-backup \
  --hc-name my-cluster \
  --hc-namespace hcp01 \
  --included-resources hostedclusters.hypershift.openshift.io,nodepools.hypershift.openshift.io,secrets,configmaps
```

When using `--included-resources`, only the specified resources will be included in the backup, completely replacing the default list.

### Resource Types

The following table lists all available resource types for the `--included-resources` flag:

| Category | Resource Type | Description |
|----------|---------------|-------------|
| **Core K8s** | `serviceaccounts` | ServiceAccounts |
| | `roles` | Roles |
| | `rolebindings` | RoleBindings |
| | `pods` | Pods |
| | `persistentvolumeclaims` | PersistentVolumeClaims |
| | `persistentvolumes` | PersistentVolumes |
| | `configmaps` | ConfigMaps |
| | `secrets` | Secrets |
| | `services` | Services |
| | `deployments` | Deployments |
| | `statefulsets` | StatefulSets |
| | `priorityclasses` | PriorityClasses |
| | `poddisruptionbudgets` | PodDisruptionBudgets |
| **HyperShift** | `hostedclusters.hypershift.openshift.io` | HostedCluster resources |
| | `hostedcontrolplanes.hypershift.openshift.io` | HostedControlPlane resources |
| | `nodepools.hypershift.openshift.io` | NodePool resources |
| | `clusters.cluster.x-k8s.io` | Generic cluster resources |
| **AWS Platform** | `awsclusters.infrastructure.cluster.x-k8s.io` | AWSCluster resources |
| | `awsmachinetemplates.infrastructure.cluster.x-k8s.io` | AWSMachineTemplate resources |
| | `awsmachines.infrastructure.cluster.x-k8s.io` | AWSMachine resources |
| **Agent Platform** | `agentclusters.infrastructure.cluster.x-k8s.io` | AgentCluster resources |
| | `agentmachinetemplates.infrastructure.cluster.x-k8s.io` | AgentMachineTemplate resources |
| | `agentmachines.infrastructure.cluster.x-k8s.io` | AgentMachine resources |
| **KubeVirt Platform** | `kubevirtclusters.infrastructure.cluster.x-k8s.io` | KubevirtCluster resources |
| | `kubevirtmachinetemplates.infrastructure.cluster.x-k8s.io` | KubevirtMachineTemplate resources |
| **OpenStack Platform** | `openstackclusters.infrastructure.cluster.x-k8s.io` | OpenStackClusters resources |
| | `openstackmachinetemplates.infrastructure.cluster.x-k8s.io` | OpenStackMachineTemplates resources |
| | `openstackmachines.infrastructure.cluster.x-k8s.io` | OpenStackMachine resources |
| **Azure Platform** | `azureclusters.infrastructure.cluster.x-k8s.io` | AzureClusters resources |
| | `azuremachinetemplates.infrastructure.cluster.x-k8s.io` | AzureMachineTemplates resources |
| | `azuremachines.infrastructure.cluster.x-k8s.io` | AzureMachine resources |
| **Cluster API** | `machinedeployments.cluster.x-k8s.io` | MachineDeployment resources |
| | `machinesets.cluster.x-k8s.io` | MachineSet resources |
| | `machines.cluster.x-k8s.io` | Machine resources |
| **OpenShift** | `routes.route.openshift.io` | OpenShift Routes |
| | `clusterdeployments.hive.openshift.io` | ClusterDeployment resources |

> **Platform Detection**: When using default resources (no `--included-resources` flag), only the platform-specific resources matching your HostedCluster's platform will be included automatically.

### Backup Naming

Backup names are automatically generated using the pattern:
```
{hc-name}-{hc-namespace}-{random-hash}
```

Example: `prod01-hcp01-abc123`

### Validation Process

The command performs the following validations before creating a backup:

1. **HostedCluster Validation**: Verifies that the specified HostedCluster exists and extracts its platform type for platform-specific resource selection
2. **OADP Operator Check**: Verifies that the OADP operator deployment is running
3. **Velero Check**: Confirms that Velero deployment is ready
4. **DPA Status Check**: Ensures at least one DataProtectionApplication is in "Reconciled" state
5. **HyperShift Plugin Check**: Verifies that the 'hypershift' plugin is included in the DPA's defaultPlugins list (shows warning if missing, but doesn't fail)

#### Render Mode Validation

When using the `--render` flag:
- If the cluster is accessible, all validations run normally but failures become warnings
- If the cluster is not accessible, validations are skipped and the backup YAML is still rendered
- Platform detection works in render mode: if the HostedCluster can be accessed, the correct platform-specific resources are included; otherwise, it defaults to AWS platform resources
- This allows you to generate backup manifests without requiring cluster connectivity

### Common Scenarios

#### Scenario 1: Full Cluster Backup
For complete disaster recovery, use the default settings:
```bash
hypershift create oadp-backup \
  --hc-name production \
  --hc-namespace hcp01 \
  --ttl 168h  # 7 days retention
```

#### Scenario 2: Configuration-Only Backup
For quick configuration backups (faster, smaller):
```bash
hypershift create oadp-backup \
  --hc-name production \
  --hc-namespace hcp01 \
  --included-resources hostedclusters.hypershift.openshift.io,hostedcontrolplanes.hypershift.openshift.io,nodepools.hypershift.openshift.io,secrets,configmaps \
  --ttl 72h  # 3 days retention
```

#### Scenario 3: Pre-Upgrade Backup
Before cluster upgrades, create a comprehensive backup:
```bash
hypershift create oadp-backup \
  --hc-name production \
  --hc-namespace hcp01 \
  --storage-location long-term-storage \
  --ttl 720h  # 30 days retention
```

#### Scenario 4: Cross-Region Backup
For cross-region disaster recovery:
```bash
hypershift create oadp-backup \
  --hc-name production \
  --hc-namespace hcp01 \
  --storage-location cross-region-s3 \
  --snapshot-move-data=true \
  --ttl 2160h  # 90 days retention
```

#### Scenario 5: Development Environment
For development hcp01 (minimal backup):
```bash
hypershift create oadp-backup \
  --hc-name dev-cluster \
  --hc-namespace dev \
  --included-resources hostedclusters.hypershift.openshift.io,nodepools.hypershift.openshift.io \
  --ttl 24h  # 1 day retention
```

#### Scenario 6: Platform-Specific Backups
The backup command automatically detects your platform and includes appropriate resources. Here are examples for different platforms:

**AWS HostedCluster:**
```bash
# Automatically includes: awsclusters.infrastructure.cluster.x-k8s.io, awsmachinetemplates.infrastructure.cluster.x-k8s.io, awsmachines.infrastructure.cluster.x-k8s.io
hypershift create oadp-backup \
  --hc-name aws-prod-cluster \
  --hc-namespace hcp01
```

**Agent HostedCluster:**
```bash
# Automatically includes: agentclusters.infrastructure.cluster.x-k8s.io, agentmachinetemplates.infrastructure.cluster.x-k8s.io, agentmachines.infrastructure.cluster.x-k8s.io
hypershift create oadp-backup \
  --hc-name agent-cluster \
  --hc-namespace hcp01
```

**KubeVirt HostedCluster:**
```bash
# Automatically includes: kubevirtclusters.infrastructure.cluster.x-k8s.io, kubevirtmachinetemplates.infrastructure.cluster.x-k8s.io
hypershift create oadp-backup \
  --hc-name kubevirt-cluster \
  --hc-namespace hcp01
```

**Custom platform-specific resource selection:**
```bash
# Force specific platform resources regardless of auto-detection
hypershift create oadp-backup \
  --hc-name my-cluster \
  --hc-namespace hcp01 \
  --included-resources hostedclusters.hypershift.openshift.io,nodepools.hypershift.openshift.io,awsclusters.infrastructure.cluster.x-k8s.io,awsmachines.infrastructure.cluster.x-k8s.io,secrets,configmaps
```

### Best Practices

#### Resource Selection Guidelines

**Minimal Backup (fastest, smallest):**
```bash
--included-resources hostedclusters.hypershift.openshift.io,nodepools.hypershift.openshift.io
```

**Configuration Backup (recommended for most cases):**
```bash
--included-resources hostedclusters.hypershift.openshift.io,hostedcontrolplanes.hypershift.openshift.io,nodepools.hypershift.openshift.io,secrets,configmaps
```

**Full Infrastructure Backup (complete recovery):**
Use default resources (no `--included-resources` flag). This automatically includes platform-specific resources based on your HostedCluster's platform type.

#### TTL Recommendations

| Environment | Recommended TTL | Use Case |
|-------------|-----------------|----------|
| Development | `24h` - `72h` | Quick recovery, frequent changes |
| Staging | `168h` (7 days) | Pre-production testing |
| Production | `720h` (30 days) | Long-term retention |
| Compliance | `2160h` (90 days) | Regulatory requirements |

#### Storage Location Strategy

- **Local backups**: Use `default` for same-region recovery
- **DR backups**: Use separate region/cloud storage locations
- **Compliance**: Use immutable storage locations
- **Development**: Use cost-optimized storage classes

#### Backup Frequency

- **Production**: Daily backups with weekly retention
- **Development**: Weekly or on-demand backups
- **Pre-change**: Always backup before major changes

### Troubleshooting

#### HostedCluster Not Found
```
Error: HostedCluster validation failed: HostedCluster 'my-cluster' not found in namespace 'hcp01'
```
**Solution**: Verify the HostedCluster name and namespace are correct:
```bash
kubectl get hostedcluster -n hcp01
```

#### HostedCluster Platform Detection Failed
```
Error: HostedCluster validation failed: platform type not found in HostedCluster 'my-cluster' spec
```
**Solution**: Check if the HostedCluster has a valid platform configuration:
```bash
kubectl get hostedcluster my-cluster -n hcp01 -o jsonpath='{.spec.platform}'
```

#### Platform Detection in Render Mode
```
Warning: HostedCluster validation failed, using default platform (AWS)
```
**Note**: This is normal behavior in render mode when the HostedCluster cannot be accessed. The backup will be generated with AWS platform resources by default.

#### HyperShift Plugin Missing
```
Warning: HyperShift plugin validation: HyperShift plugin not found in any DataProtectionApplication. Please add 'hypershift' to the defaultPlugins list in your DPA configuration
```
**Solution**: Add the 'hypershift' plugin to your DataProtectionApplication configuration:
```bash
kubectl patch dataprotectionapplication <dpa-name> -n openshift-adp --type='json' \
  -p='[{"op": "add", "path": "/spec/configuration/velero/defaultPlugins/-", "value": "hypershift"}]'
```

Or edit your DPA YAML to include 'hypershift' in the defaultPlugins list:
```yaml
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - aws
      - csi
      - hypershift  # Add this line
```

#### OADP Operator Not Found
```
Error: OADP validation failed: OADP operator deployment not found in namespace openshift-adp
```
**Solution**: Install the OADP operator or check the correct namespace with `--oadp-namespace`. The command looks for the deployment named `openshift-adp-controller-manager`.

#### No DataProtectionApplication Found
```
Error: DPA verification failed: no DataProtectionApplication resources found in namespace openshift-adp
```
**Solution**: Create a DataProtectionApplication custom resource

#### Velero Not Ready
```
Error: OADP validation failed: Velero deployment is not ready in namespace openshift-adp
```
**Solution**: Check Velero deployment status and logs for issues

#### Invalid Storage Location
```
Error: failed to create backup resource: admission webhook "vbackup.kb.io" denied the request:
backup storage location 'invalid-location' not found
```
**Solution**: Verify storage location exists in your DPA configuration:
```bash
kubectl get backupstoragelocations -n openshift-adp
```

#### Permission Denied
```
Error: failed to create backup resource: admission webhook denied the request:
insufficient permissions to create backup
```
**Solution**: Ensure your user has proper RBAC permissions:
```bash
kubectl auth can-i create backups.velero.io -n openshift-adp
```

#### Invalid Resource Type
```
Error: backup generation failed: invalid resource type 'invalidresource'
```
**Solution**: Check the Resource Types table for valid resource names

#### Backup Stuck in Progress
If a backup remains in "InProgress" status:
1. Check backup logs:
   ```bash
   kubectl logs -n openshift-adp deployment/velero
   ```
2. Check backup status:
   ```bash
   kubectl describe backup <backup-name> -n openshift-adp
   ```
3. Common causes:
   - Storage location unreachable
   - Large volume snapshots taking time
   - Network connectivity issues

#### Render Mode Warnings
When using `--render`, you may see warnings like:
```
Warning: Cannot connect to cluster for validation, skipping OADP/DPA checks
```
This is normal behavior when cluster connectivity is not available.

### Monitoring and Verification

#### Check Backup Status
```bash
# List all backups
kubectl get backups -n openshift-adp

# Check specific backup details
kubectl describe backup <backup-name> -n openshift-adp

# View backup logs
kubectl logs -n openshift-adp deployment/velero -f
```

#### Verify Backup Contents
```bash
# Check what was backed up
kubectl get backup <backup-name> -n openshift-adp -o jsonpath='{.status}'
```

#### Monitor Storage Usage
```bash
# Check storage location status
kubectl get backupstoragelocations -n openshift-adp

# View storage usage (if supported by storage provider)
kubectl describe backupstoragelocation default -n openshift-adp
```

### Automation and Scheduling

#### Using CronJobs for Automated Backups

Create a CronJob to automate regular backups:

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hypershift-backup
  namespace: openshift-adp
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-operator
          containers:
          - name: hypershift-backup
            image: registry.redhat.io/openshift4/ose-cli:latest
            command:
            - /bin/bash
            - -c
            - |
              hypershift create oadp-backup \
                --hc-name production \
                --hc-namespace hcp01 \
                --ttl 168h
          restartPolicy: OnFailure
```

#### GitOps Integration

Use the `--render` flag in GitOps workflows:

```yaml
# .github/workflows/backup.yml
name: Generate Backup Manifests
on:
  schedule:
    - cron: '0 1 * * *'
jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Generate backup manifest
      run: |
        hypershift create oadp-backup \
          --hc-name ${{ env.CLUSTER_NAME }} \
          --hc-namespace hcp01 \
          --render > manifests/backup-$(date +%Y%m%d).yaml
    - name: Commit manifests
      run: |
        git add manifests/
        git commit -m "Generated backup manifest for $(date)"
        git push
```

### Security Considerations

#### RBAC Requirements

Ensure your service account has the necessary permissions:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: hypershift-backup
rules:
- apiGroups: ["velero.io"]
  resources: ["backups"]
  verbs: ["create", "get", "list", "delete"]
- apiGroups: ["oadp.openshift.io"]
  resources: ["dataprotectionapplications"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list"]
```

#### Sensitive Data Handling

- **Secrets**: Backed up by default, ensure your storage is encrypted
- **Certificates**: Included in backup, verify rotation policies
- **Keys**: Consider using external secret management

### Performance Considerations

#### Backup Size Optimization

| Resource Selection | Typical Size | Backup Time | Use Case |
|-------------------|--------------|-------------|-----------|
| Minimal (`hostedcluster,nodepool`) | < 1 MB | < 30s | Quick config backup |
| Configuration | < 10 MB | < 2 min | Standard backup |
| Full (default) | 100 MB - 1 GB | 5-15 min | Complete DR backup |

#### Network and Storage Impact

- **Bandwidth**: Consider backup timing during low-traffic periods
- **Storage costs**: Use appropriate storage classes for retention policies
- **Cross-region**: Factor in data transfer costs and time

### Integration Examples

#### Slack Notifications

```bash
#!/bin/bash
BACKUP_NAME=$(hypershift create oadp-backup --hc-name prod --hc-namespace hcp01 2>&1 | grep "created successfully" | awk '{print $6}')
if [ $? -eq 0 ]; then
  curl -X POST -H 'Content-type: application/json' \
    --data '{"text":"‚úÖ Backup successful: '$BACKUP_NAME'"}' \
    $SLACK_WEBHOOK_URL
else
  curl -X POST -H 'Content-type: application/json' \
    --data '{"text":"‚ùå Backup failed for prod cluster"}' \
    $SLACK_WEBHOOK_URL
fi
```

#### Prometheus Metrics

Monitor backup success/failure with custom metrics:

```bash
# Success metric
echo "hypershift_backup_success{cluster=\"prod\"} 1" | curl -X POST --data-binary @- http://pushgateway:9091/metrics/job/hypershift_backup

# Failure metric
echo "hypershift_backup_success{cluster=\"prod\"} 0" | curl -X POST --data-binary @- http://pushgateway:9091/metrics/job/hypershift_backup
```

## Restore

The `hypershift create oadp-restore` command creates restore operations from hosted cluster backups using OADP (OpenShift API for Data Protection).

### Prerequisites

Before restoring from backups, ensure that:

1. **OADP Operator is installed**: The OADP operator must be installed and running in your management cluster
2. **DataProtectionApplication (DPA) exists**: A DPA custom resource must be configured and ready
3. **Backup or Schedule exists**: Either a completed backup or a backup schedule must be available
4. **Storage location configured**: The backup storage location must be accessible

### Basic Usage

#### Restore from a specific backup
```bash
hypershift create oadp-restore \
  --hc-name my-hosted-cluster \
  --hc-namespace my-hosted-cluster-namespace \
  --from-backup my-backup-name
```

#### Restore from a schedule (uses latest backup)
```bash
hypershift create oadp-restore \
  --hc-name my-hosted-cluster \
  --hc-namespace my-hosted-cluster-namespace \
  --from-schedule daily-backup-schedule
```

### Command Options

| Flag | Type | Default | Required | Description |
|------|------|---------|----------|-------------|
| `--hc-name` | string | - | ‚úÖ | Name of the hosted cluster to restore |
| `--hc-namespace` | string | - | ‚úÖ | Namespace of the hosted cluster to restore |
| `--from-backup` | string | - | ‚ö†Ô∏è | Name of the backup to restore from (mutually exclusive with `--from-schedule`) |
| `--from-schedule` | string | - | ‚ö†Ô∏è | Name of the schedule to restore from (uses latest backup, mutually exclusive with `--from-backup`) |
| `--name` | string | - | ‚ùå | Custom name for the restore (auto-generated if not specified) |
| `--oadp-namespace` | string | `openshift-adp` | ‚ùå | Namespace where OADP operator is installed |
| `--existing-resource-policy` | string | `update` | ‚ùå | Policy for handling existing resources (`none`, `update`) |
| `--include-namespaces` | []string | (see below) | ‚ùå | Override included namespaces (replaces defaults completely) |
| `--render` | bool | `false` | ‚ùå | Render the restore object to STDOUT instead of creating it |
| `--restore-pvs` | bool | `true` | ‚ùå | Restore persistent volumes |
| `--preserve-node-ports` | bool | `true` | ‚ùå | Preserve NodePort assignments during restore |

> **Note**: Either `--from-backup` OR `--from-schedule` must be specified, but not both.

#### Flag Details

**`--hc-name` and `--hc-namespace`**
These identify the hosted cluster to restore. These should match the original cluster that was backed up.

**`--from-backup` vs `--from-schedule`**
- `--from-backup`: Restores from a specific backup by name. The backup must exist and be in "Completed" status.
- `--from-schedule`: Restores from the latest successful backup created by the specified schedule. Velero automatically selects the most recent backup.

**`--existing-resource-policy`**
Controls how to handle resources that already exist in the target cluster:
- `update` (default): Update existing resources with backup data
- `none`: Skip existing resources, only restore missing ones

**`--include-namespaces`**
By default, restores include:
- `{hc-namespace}` - The hosted cluster's namespace
- `{hc-namespace}-{hc-name}` - The control plane namespace

When specified, this flag **completely overrides** the default namespaces (it doesn't add to them).

**`--name`**
Specifies a custom name for the restore resource. If not provided, a name is automatically generated using the pattern `{source-name}-{hc-name}-restore-{random-suffix}`.

**`--render`**
Outputs the restore YAML to STDOUT instead of creating the resource. Useful for inspection or GitOps workflows.

### Example Commands

#### Basic restore from backup
```bash
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-backup prod01-hcp01-abc123
```

#### Restore from schedule (latest backup)
```bash
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-schedule daily-prod-backup
```

#### Restore with custom settings
```bash
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-backup prod01-hcp01-abc123 \
  --existing-resource-policy none \
  --restore-pvs=false \
  --preserve-node-ports=false
```

#### Restore to custom namespaces
```bash
# Override default namespaces completely
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-backup prod01-hcp01-abc123 \
  --include-namespaces hcp01,hcp01-prod01,custom-namespace
```

#### Restore with custom name
```bash
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-backup prod01-hcp01-abc123 \
  --name my-custom-restore-name
```

#### Render restore object without creating it
```bash
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-backup prod01-hcp01-abc123 \
  --render
```

This outputs the restore YAML to STDOUT:
```bash
# Save to file
hypershift create oadp-restore --hc-name prod01 --hc-namespace hcp01 --from-backup backup-123 --render > restore.yaml

# Apply with kubectl
hypershift create oadp-restore --hc-name prod01 --hc-namespace hcp01 --from-backup backup-123 --render | kubectl apply -f -
```

### What Gets Restored

The restore process includes the following by default:

**Included Namespaces:**
- `{hc-namespace}` - Contains the HostedCluster resource and related objects
- `{hc-namespace}-{hc-name}` - Contains the HostedControlPlane and control plane components

**Excluded Resources (automatically):**
For safety and compatibility, these resources are always excluded from restore:
- `nodes` - Cluster nodes (managed by the platform)
- `events` - Kubernetes events (ephemeral)
- `events.events.k8s.io` - Extended events
- `backups.velero.io` - Velero backup resources
- `restores.velero.io` - Velero restore resources
- `resticrepositories.velero.io` - Velero repository data
- `csinodes.storage.k8s.io` - CSI node information
- `volumeattachments.storage.k8s.io` - Volume attachment state
- `backuprepositories.velero.io` - Backup repository information

### Restore Naming

#### Automatic Name Generation

When `--name` is not specified, restore names are automatically generated using the pattern:
```
{source-name}-{hc-name}-restore-{random-hash}
```

Examples:
- From backup: `prod01-hcp01-abc123-production-restore-def456`
- From schedule: `daily-backup-production-restore-ghi789`

#### Custom Naming

When using the `--name` flag, the specified name is used exactly as provided:
```bash
# Results in restore named "my-custom-restore"
hypershift create oadp-restore \
  --hc-name prod01 \
  --hc-namespace hcp01 \
  --from-backup backup-123 \
  --name my-custom-restore
```

### Validation Process

The command performs the following validations before creating a restore:

1. **Backup/Schedule Validation**:
   - **From backup**: Verifies the backup exists and is in "Completed" status
   - **From schedule**: Verifies the schedule exists and is not paused
2. **OADP Operator Check**: Verifies that the OADP operator deployment is running
3. **Velero Check**: Confirms that Velero deployment is ready

#### Render Mode Validation

When using the `--render` flag:
- If the cluster is accessible, all validations run normally but failures become warnings
- If the cluster is not accessible, validations are skipped and the restore YAML is still rendered
- This allows you to generate restore manifests without requiring cluster connectivity

### Restore Scenarios

#### Scenario 1: Full Disaster Recovery
Complete restore from backup after cluster loss:
```bash
hypershift create oadp-restore \
  --hc-name production \
  --hc-namespace hcp01 \
  --from-backup production-hcp01-20241022-abc123
```

#### Scenario 2: Schedule-Based Recovery
Restore using the latest backup from a schedule:
```bash
hypershift create oadp-restore \
  --hc-name production \
  --hc-namespace hcp01 \
  --from-schedule daily-production-backup
```

#### Scenario 3: Selective Namespace Restore
Restore only specific namespaces:
```bash
hypershift create oadp-restore \
  --hc-name production \
  --hc-namespace hcp01 \
  --from-backup production-backup-abc123 \
  --include-namespaces hcp01,app-namespace-1,app-namespace-2
```

#### Scenario 4: Configuration-Only Restore
Restore without persistent volumes (faster):
```bash
hypershift create oadp-restore \
  --hc-name production \
  --hc-namespace hcp01 \
  --from-backup config-backup-abc123 \
  --restore-pvs=false
```

#### Scenario 5: Non-Destructive Restore
Restore only missing resources, don't update existing ones:
```bash
hypershift create oadp-restore \
  --hc-name production \
  --hc-namespace hcp01 \
  --from-backup production-backup-abc123 \
  --existing-resource-policy none
```

### Best Practices

#### Pre-Restore Checklist

1. **Verify backup status**:
   ```bash
   kubectl get backup <backup-name> -n openshift-adp -o jsonpath='{.status.phase}'
   ```

2. **Check target namespace availability**:
   ```bash
   kubectl get namespace <hc-namespace>
   ```

3. **Ensure sufficient resources**: Verify the target cluster has adequate CPU, memory, and storage

4. **Review existing resources**: Decide on `--existing-resource-policy` based on what's already present

#### Resource Policy Guidelines

| Scenario | Recommended Policy | Result |
|----------|-------------------|--------|
| **Clean cluster** | `update` (default) | All resources restored |
| **Partial restore** | `none` | Only missing resources restored |
| **Force overwrite** | `update` | Existing resources updated |
| **Incremental restore** | `none` | Preserves local changes |

#### Namespace Strategy

**Default behavior** (recommended for most cases):
```bash
# Restores to: hcp01, hcp01-production
hypershift create oadp-restore --hc-name production --hc-namespace hcp01 --from-backup backup-123
```

**Custom namespace mapping**:
```bash
# Restores to: new-hcp, new-hcp-prod, additional-ns
hypershift create oadp-restore \
  --hc-name production \
  --hc-namespace hcp01 \
  --from-backup backup-123 \
  --include-namespaces new-hcp,new-hcp-prod,additional-ns
```

#### Schedule vs Backup Selection

| Use Case | Recommendation | Command |
|----------|----------------|---------|
| **Latest data** | Use schedule | `--from-schedule daily-backup` |
| **Specific point-in-time** | Use backup | `--from-backup backup-20241022-123` |
| **Automated recovery** | Use schedule | `--from-schedule <schedule-name>` |
| **Manual/precise recovery** | Use backup | `--from-backup <specific-backup>` |

#### Naming Strategy

**Automatic naming** (recommended for most cases):
```bash
# Results in: backup-name-cluster-name-restore-abc123
hypershift create oadp-restore --hc-name cluster-name --hc-namespace hcp01 --from-backup backup-name
```

**Custom naming** (useful for tracking or automation):
```bash
# For DR scenarios
--name "prod-cluster-dr-$(date +%Y%m%d)"

# For testing
--name "test-restore-$(git rev-parse --short HEAD)"

# For scheduled restores
--name "weekly-restore-week-$(date +%U)"
```

### Troubleshooting

#### Backup Not Found
```
Error: backup validation failed: backup 'my-backup' not found in namespace 'openshift-adp'
```
**Solution**: Verify backup exists:
```bash
kubectl get backups -n openshift-adp | grep my-backup
```

#### Backup Not Completed
```
Error: backup validation failed: backup 'my-backup' is not completed, current phase: InProgress
```
**Solution**: Wait for backup to complete or use a different backup:
```bash
kubectl get backup my-backup -n openshift-adp -o jsonpath='{.status.phase}'
```

#### Schedule Not Found
```
Error: schedule validation failed: schedule 'daily-backup' not found in namespace 'openshift-adp'
```
**Solution**: Verify schedule exists:
```bash
kubectl get schedules -n openshift-adp | grep daily-backup
```

#### Schedule Paused
```
Error: schedule validation failed: schedule 'daily-backup' is paused
```
**Solution**: Resume the schedule or use a different one:
```bash
kubectl patch schedule daily-backup -n openshift-adp --type='json' -p='[{"op": "replace", "path": "/spec/paused", "value": false}]'
```

#### Mutually Exclusive Flags
```
Error: --from-backup and --from-schedule are mutually exclusive, specify only one
```
**Solution**: Use only one source flag:
```bash
# Either this:
hypershift create oadp-restore --hc-name prod --hc-namespace hcp01 --from-backup backup-123

# Or this:
hypershift create oadp-restore --hc-name prod --hc-namespace hcp01 --from-schedule daily-backup
```

#### Missing Source
```
Error: either --from-backup or --from-schedule must be specified
```
**Solution**: Specify exactly one source:
```bash
hypershift create oadp-restore --hc-name prod --hc-namespace hcp01 --from-backup backup-123
```

#### Invalid Resource Policy
```
Error: invalid existing-resource-policy 'invalid'. Valid values are: none, update
```
**Solution**: Use a valid policy:
```bash
hypershift create oadp-restore \
  --hc-name prod \
  --hc-namespace hcp01 \
  --from-backup backup-123 \
  --existing-resource-policy update  # or 'none'
```

#### Insufficient Permissions
```
Error: failed to create restore resource: admission webhook denied the request
```
**Solution**: Ensure proper RBAC permissions:
```bash
kubectl auth can-i create restores.velero.io -n openshift-adp
```

#### Restore Stuck in Progress
If a restore remains in "InProgress" status:
1. Check restore logs:
   ```bash
   kubectl logs -n openshift-adp deployment/velero
   ```
2. Check restore status:
   ```bash
   kubectl describe restore <restore-name> -n openshift-adp
   ```

### Monitoring and Verification

#### Check Restore Status
```bash
# List all restores
kubectl get restores -n openshift-adp

# Check specific restore details
kubectl describe restore <restore-name> -n openshift-adp

# View restore logs
kubectl logs -n openshift-adp deployment/velero -f
```

#### Verify Restored Resources
```bash
# Check HostedCluster
kubectl get hostedcluster -n <hc-namespace>

# Check HostedControlPlane
kubectl get hostedcontrolplane -n <hc-namespace>-<hc-name>

# Check NodePools
kubectl get nodepool -n <hc-namespace>
```

#### Monitor Restore Progress
```bash
# Watch restore status
kubectl get restore <restore-name> -n openshift-adp -w

# Check restored namespace
kubectl get all -n <hc-namespace>
```

### Automation Examples

#### GitOps Restore Workflow
```yaml
# .github/workflows/restore.yml
name: Disaster Recovery Restore
on:
  workflow_dispatch:
    inputs:
      backup_name:
        description: 'Backup name to restore from'
        required: true
      cluster_name:
        description: 'Cluster name to restore'
        required: true
      cluster_namespace:
        description: 'Cluster namespace'
        required: true
        default: 'hcp01'

jobs:
  restore:
    runs-on: ubuntu-latest
    steps:
    - name: Generate restore manifest
      run: |
        hypershift create oadp-restore \
          --hc-name ${{ github.event.inputs.cluster_name }} \
          --hc-namespace ${{ github.event.inputs.cluster_namespace }} \
          --from-backup ${{ github.event.inputs.backup_name }} \
          --render > restore-manifest.yaml

    - name: Apply restore
      run: |
        kubectl apply -f restore-manifest.yaml
```

#### Automated Schedule-Based Recovery
```bash
#!/bin/bash
# restore-latest.sh - Restore from the latest backup in a schedule

CLUSTER_NAME="$1"
CLUSTER_NAMESPACE="$2"
SCHEDULE_NAME="$3"

if [[ -z "$CLUSTER_NAME" || -z "$CLUSTER_NAMESPACE" || -z "$SCHEDULE_NAME" ]]; then
    echo "Usage: $0 <cluster-name> <cluster-namespace> <schedule-name>"
    exit 1
fi

echo "Starting restore for cluster $CLUSTER_NAME from schedule $SCHEDULE_NAME..."

hypershift create oadp-restore \
  --hc-name "$CLUSTER_NAME" \
  --hc-namespace "$CLUSTER_NAMESPACE" \
  --from-schedule "$SCHEDULE_NAME"

if [[ $? -eq 0 ]]; then
    echo "‚úÖ Restore initiated successfully"
    echo "Monitor progress with: kubectl get restores -n openshift-adp"
else
    echo "‚ùå Restore failed"
    exit 1
fi
```

### Security Considerations

#### RBAC Requirements
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: hypershift-restore
rules:
- apiGroups: ["velero.io"]
  resources: ["restores", "backups", "schedules"]
  verbs: ["create", "get", "list"]
- apiGroups: ["oadp.openshift.io"]
  resources: ["dataprotectionapplications"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list"]
```

#### Data Security
- **Encryption**: Ensure restored data maintains encryption at rest
- **Access Control**: Verify restored resources have appropriate RBAC
- **Secrets**: Validate that restored secrets are current and secure
- **Certificates**: Check certificate validity after restore

### Performance Considerations

#### Restore Duration Estimates

| Restore Type | Typical Duration | Factors |
|--------------|-----------------|---------|
| **Configuration only** | 1-5 minutes | Cluster size, network |
| **With PVs (small)** | 5-15 minutes | Volume size, storage speed |
| **With PVs (large)** | 15-60 minutes | Volume size, storage speed |
| **Full cluster** | 30-120 minutes | Complete cluster complexity |

#### Optimization Tips
- Use `--restore-pvs=false` for faster configuration-only restores
- Consider `--existing-resource-policy=none` for incremental restores
- Use `--include-namespaces` to restore only required namespaces
- Ensure adequate network bandwidth for cross-region restores

### Testing and Validation

#### Integration Tests

The HyperShift CLI includes comprehensive integration tests for both backup and restore functionality:

```bash
# Run all OADP integration tests
go test -tags integration ./test/integration/oadp/... -v

# Run only backup tests
go test -tags integration ./test/integration/oadp/ -run "TestBackup" -v

# Run only restore tests
go test -tags integration ./test/integration/oadp/ -run "TestRestore" -v
```

The tests validate:
- Manifest generation against Velero CRDs
- CLI configuration and defaults
- Business rules and validation logic
- Platform-specific resource selection
- Namespace configuration

> **Note**: Tests require internet connectivity to download Velero CRDs from GitHub for validation.

### Related Documentation

- OADP Installation Guide
- HyperShift OADP Plugin
- Velero Documentation
- HyperShift Documentation
- OpenShift Backup Best Practices

---

## Source: docs/content/how-to/disaster-recovery/etcd-recovery.md

# Etcd Recovery

### Overview

Etcd pods for hosted clusters run as part of a statefulset (etcd). The statefulset relies on persistent storage to store etcd data per member. In the case of a HighlyAvailable control plane, the size of the statefulset is 3 and each member (etcd-N) has its own PersistentVolumeClaim (etcd-data-N).

### Automatic Recovery of Removed Members

In certain circumstances, an etcd member is removed from the cluster. This could be due to networking issues (sdn or dns of management cluster). The hypershift operator can automatically recover from this situation by enabling automatic etcd recovery (--enable-etcd-recovery), which is set to true by default.

If this is enabled, then the HyperShift operator will attempt to recover the health of an etcd cluster if the following conditions are met:

* The hosted cluster is configured to run HighlyAvailable (`spec.controllerAvailabilityPolicy = HighlyAvailable`)
* Etcd is managed by HyperShift (`spec.etcd.managementType = Managed`)
* Only one member of the etcd cluster is failing (quorum is not lost)

The recovery procedure consists of the following:
* If a member has been removed from the etcd cluster, re-add the missing member by executing the `member add` command
* The administrator should delete the etcd member's pod and pvc, after which the HyperShift operator will automatically provision a replacement etcd member Pod and PersistentVolume.

Once this is done, the `reset-member` init container of the removed pod should be able to complete the recovery.

To disable this default behavior, install HyperShift with `--enable-etcd-recovery=false`

### Checking cluster health

Execute into a running etcd pod:

```
$ oc rsh -n ${CONTROL_PLANE_NAMESPACE} -c etcd etcd-0
```

Setup the etcdctl environment:

```
export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/etcd/tls/etcd-ca/ca.crt
export ETCDCTL_CERT=/etc/etcd/tls/client/etcd-client.crt
export ETCDCTL_KEY=/etc/etcd/tls/client/etcd-client.key
export ETCDCTL_ENDPOINTS=https://etcd-client:2379
```

Print out endpoint health for each cluster member:
```
etcdctl endpoint health --cluster -w table
```

### Single Node Recovery

If a single etcd member of a 3-node cluster has corrupted data, it will most likely start crash looping, as in:

```
$ oc get pods -l app=etcd -n ${CONTROL_PLANE_NAMESPACE}
NAME     READY   STATUS             RESTARTS     AGE
etcd-0   2/2     Running            0            64m
etcd-1   2/2     Running            0            45m
etcd-2   1/2     CrashLoopBackOff   1 (5s ago)   64m
```

To recover the etcd member, delete its persistent volume claim (data-etcd-N) as well as the pod (etcd-N):

```
oc delete pvc/data-etcd-2 pod/etcd-2 --wait=false
```

When the pod restarts, the member should get re-added to the etcd cluster and become healthy again:

```
$ oc get pods -l app=etcd -n $CONTROL_PLANE_NAMESPACE
NAME     READY   STATUS    RESTARTS   AGE
etcd-0   2/2     Running   0          67m
etcd-1   2/2     Running   0          48m
etcd-2   2/2     Running   0          2m2s
```

### Recovery from Quorum Loss

If multiple members of the etcd cluster have lost data or are in a crashloop state, then etcd must be restored from a snapshot. The following procedure requires down time for the control plane as the etcd database is restored.

NOTE: The following instructions require the `oc` and `jq` binaries.

0. Setup environment variables that point to your hosted cluster:

```
CLUSTER_NAME=my-cluster
CLUSTER_NAMESPACE=clusters
CONTROL_PLANE_NAMESPACE="${CLUSTER_NAMESPACE}-${CLUSTER_NAME}"
```

1. Pause reconciliation on the HostedCluster (setting CLUSTER_NAME and CLUSTER_NAMESPACE to values that correspond to your hosted cluster):

```
oc patch -n ${CLUSTER_NAMESPACE} hostedclusters/${CLUSTER_NAME} -p '{"spec":{"pausedUntil":"true"}}' --type=merge
```

2. Take a snapshot of etcd data using one of the following methods:

    a. Use a previously backed up snapshot

    b. Take a snapshot from a running etcd pod (PREFERRED but requires available etcd pod):

        ```
        # List etcd pods
        oc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd

        # If a pod is available:

        # 1. take a snapshot of its database and save it locally
        # Set ETCD_POD to the name of the pod that is available
        ETCD_POD=etcd-0
        oc exec -n ${CONTROL_PLANE_NAMESPACE} -c etcd -t ${ETCD_POD} -- env ETCDCTL_API=3 /usr/bin/etcdctl \
        --cacert /etc/etcd/tls/etcd-ca/ca.crt \
        --cert /etc/etcd/tls/client/etcd-client.crt \
        --key /etc/etcd/tls/client/etcd-client.key \
        --endpoints=https://localhost:2379 \
        snapshot save /var/lib/snapshot.db

        # 2. Verify that the snapshot is good
        oc exec -n ${CONTROL_PLANE_NAMESPACE} -c etcd -t ${ETCD_POD} -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/snapshot.db

        # 3. Make a local copy of the snapshot
        oc cp -c etcd ${CONTROL_PLANE_NAMESPACE}/${ETCD_POD}:/var/lib/snapshot.db /tmp/etcd.snapshot.db
        ```

    c. Make a copy of the snapshot db from etcd persistent storage:

       ```
       # List etcd pods
       oc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd

       # Find a pod that is running and set its name as the value of ETCD_POD
       ETCD_POD=etcd-0

       # Copy the snapshot db from it
       oc cp -c etcd ${CONTROL_PLANE_NAMESPACE}/${ETCD_POD}:/var/lib/data/member/snap/db /tmp/etcd.snapshot.db
       ```

3. Scale down the etcd statefulset:

```
oc scale -n ${CONTROL_PLANE_NAMESPACE} statefulset/etcd --replicas=0
```

4. Delete volumes for 2nd and 3rd members:
```
oc delete -n ${CONTROL_PLANE_NAMESPACE} pvc/data-etcd-1 pvc/data-etcd-2
```

5. Create pod to access the first etcd member's data:

```
# Save etcd image
ETCD_IMAGE=$(oc get -n ${CONTROL_PLANE_NAMESPACE} statefulset/etcd -o jsonpath='{ .spec.template.spec.containers[0].image }')

# Create pod that will allow access to etcd data:
cat << EOF | oc apply -n ${CONTROL_PLANE_NAMESPACE} -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: etcd-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd-data
  template:
    metadata:
      labels:
        app: etcd-data
    spec:
      containers:
      - name: access
        image: $ETCD_IMAGE
        volumeMounts:
        - name: data
          mountPath: /var/lib
        command:
        - /usr/bin/bash
        args:
        - -c
        - |-
          while true; do
            sleep 1000
          done
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: data-etcd-0
EOF

```

6. Clear previous data and restore snapshot

```
# Wait for the etcd-data pod to start running
oc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd-data

# Get the name of the etcd-data pod
DATA_POD=$(oc get -n ${CONTROL_PLANE_NAMESPACE} pods --no-headers -l app=etcd-data -o name | cut -d/ -f2)

# Copy local snapshot into the pod
oc cp /tmp/etcd.snapshot.db ${CONTROL_PLANE_NAMESPACE}/${DATA_POD}:/var/lib/restored.snap.db

# Remove old data
oc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- rm -rf /var/lib/data
oc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- mkdir -p /var/lib/data

# Restore snapshot
oc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- etcdutl snapshot restore /var/lib/restored.snap.db \
     --data-dir=/var/lib/data --skip-hash-check \
     --name etcd-0 \
     --initial-cluster-token=etcd-cluster \
     --initial-cluster etcd-0=https://etcd-0.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380,etcd-1=https://etcd-1.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380,etcd-2=https://etcd-2.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380 \
     --initial-advertise-peer-urls https://etcd-0.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380

# Remove snapshot from etcd-0 data directory
oc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- rm /var/lib/restored.snap.db
```

7. Delete data access deployment:

```
oc delete -n ${CONTROL_PLANE_NAMESPACE} deployment/etcd-data
```

8. Scale up etcd cluster:
```
oc scale -n ${CONTROL_PLANE_NAMESPACE} statefulset/etcd --replicas=3
```

Wait for the all etcd member pods to come up and report available:
```
oc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd -w
```

9. Remove hosted cluster pause:

```
oc patch -n ${CLUSTER_NAMESPACE} hostedclusters/${CLUSTER_NAME} -p '{"spec":{"pausedUntil":null}}' --type=merge
```


---

## Source: docs/content/how-to/disaster-recovery/index.md

---
title: Backup and Restore on HostedControlPlanes
---

This section of the Hypershift documentation contains pages that show how to perform disaster recovery tasks using different methods.

## Available Guides

### DR CLI Domain
Use the HyperShift CLI disaster recovery commands with platform-aware backup creation and OADP integration.

### Backup and Restore with OADP
Manual backup and restore procedures using OADP (OpenShift API for Data Protection) with Velero.

### Backup and Restore with OADP 1.5
Updated procedures and enhanced features for OADP version 1.5.

### ETCD Recovery
ETCD disaster recovery procedures for control plane data backup and restoration.


---

## Source: docs/content/how-to/disconnected/disconnected-workarounds.md

# Disconnected workarounds

## Make ImageContentSourcePolicies work using image tags
The ImageContentSourcePolicies (ICSP) have not exposed the API to handle the parameter `mirror-by-digest-only`, so in order to do that, we need to manually create a MachineConfig change to be applied in all the HostedCluster workers. This change will perform a similar action in the workers like an ICSP.

!!! note

    You don't need to do this with future versions of Openshift because `config.openshift.io/v1` has an exposed API to do this, called `ImageTagMirrorSet`.

!!! note

    This workaround could also be applied in the management cluster first, prior to deploying a HostedCluster from the management cluster.

This is our MachineConfig template:

- `mc-icsp-template.yaml`
```yaml
---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-worker-mirror-by-digest-registries
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,$B64_RAWICSP
        filesystem: root
        mode: 420
        path: /etc/containers/registries.conf.d/99-mirror-by-digest-registries.conf
```

Basically, we create a file inside of `/etc/containers/registries.conf.d/` called `99-mirror-by-digest-registries.conf` which tells the runtime to use the custom registry instead of the external one.

Also, here we have our final file content:

- `icsp-raw-mc.yaml`
```ini
[[registry]]
  prefix = ""
  location = "registry.redhat.io/openshift4/ose-kube-rbac-proxy"
  mirror-by-digest-only = false

[[registry.mirror]]
location = "registry.ocp-edge-cluster-0.qe.lab.redhat.com:5000/openshift4/ose-kube-rbac-proxy"

[[registry]]
  prefix = ""
  location = "quay.io/acm-d"
  mirror-by-digest-only = false

[[registry.mirror]]
location = "registry.ocp-edge-cluster-0.qe.lab.redhat.com:5000/acm-d"

[[registry]]
  prefix = ""
  location = "quay.io/open-cluster-management/addon-manager"
  mirror-by-digest-only = false

[[registry.mirror]]
location = "registry.ocp-edge-cluster-0.qe.lab.redhat.com:5000/open-cluster-management/addon-manager"
```

Now we just need to mix the things up and apply them into our HostedCluster

```bash
export B64_RAWICSP=$(cat icsp-raw-mc.yaml | base64)
envsubst < mc-icsp-template.yaml | oc apply -f -
```

These two commands will create the MachineConfig change in the Openshift cluster, so eventually the worker nodes will get rebooted.

After applying this change, the worker nodes will be able to consume the mirror when only the tags are involved.

---

## Source: docs/content/how-to/disconnected/idms-icsp-for-management-clusters.md

## Configuring disconnected HostedControlPlanes deployments

!!! Note

    This section is mainly focused on the Management Cluster side. If you want to see how to configure the HostedCluster's ImageContentSource, please check out this other documentation section.

HostedControlPlanes operate in two main domains: the Control Plane, which is part of the Management Cluster, and the Data Plane, which resides in the worker nodes deployed by the customer.

The ICSP/IDMS for the Data Plane is managed via the `ImageContentSources` API in the HostedCluster manifests and this is the only source of truth for the DataPlane. It is also possible to modify the registry files directly in the workers using the MachineConfig method, changing configurations not directly supported via the API of those ICSP/IDMS objects.

For the Control Plane, ICSP/IDMS objects are managed in the Management Cluster. These objects are parsed by the Hypershift Operator and shared as `registry-overrides` with the ControlPlaneOperator. These entries will be injected into any of the deployments in the HostedControlPlane namespace as an argument.

In summary, to work with disconnected registries in the HostedControlPlane, you first need to create the appropriate ICSP/IDMS in the Management Cluster. Then, to deploy disconnected workloads in the Data Plane, you need to add the desired entries into the ImageContentSources field inside the HostedCluster manifest.

### Mirroring Prerequisites for the Management Cluster

Deploying HostedControlPlanes in a disconnected environment involves a mirroring process that is straightforward, provided you have the necessary prerequisites:

1. **Private Registry**: Ensure a private registry is deployed and operational.
2. **Credentials**: Have a credentials file to pull from a public registry and push to your private registry.
3. **oc-mirror Tool**: Install the `oc-mirror` tool.

#### Setting Up the Private Registry

For the private registry, robust platforms typically use large-scale solutions like Quay or Artifactory. However, for testing or development environments, you can set up a smaller registry by following these steps.

#### Preparing the Credentials File

Your credentials file, typically located at `${HOME}/.docker/config.json`, must include login credentials for at least two registries:

- The public registry from which you pull images (e.g., `quay.io`).
- Your private registry to which you push the images.

Refer to the official OpenShift documentation for detailed instructions on setting up the registry pull secret here.

#### Installing the oc-mirror Tool

Download the `oc-mirror` plugin for the `oc` CLI and place it in the correct directory. Follow the instructions in the OpenShift documentation here.

### Preparing the ImageSetConfiguration file

With the prerequisites in place, you need to prepare the `ImageSetConfiguration` file. This file specifies the OpenShift Container Platform (OCP) releases, Operator Lifecycle Manager (OLM) catalogs, and additional images you wish to mirror.

Here is a sample `ImageSetConfiguration`:

```yaml
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: registry.sample.lab:5000/openshift/release/metadata:latest
mirror:
  platform:
    channels:
    - name: stable-4.14
      minVersion: 4.14.26
      maxVersion: 4.14.28
      type: ocp
    - name: stable-4.15
      minVersion: 4.15.13
      maxVersion: 4.15.17
      type: ocp
    - name: candidate-4.16
      minVersion: 4.16.0-rc.2
      maxVersion: 4.16.0-rc.4
      type: ocp
  additionalImages:
  - name: quay.io/karmab/origin-keepalived-ipfailover:latest
  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.15
    packages:
    - name: lvms-operator
    - name: local-storage-operator
    - name: odf-csi-addons-operator
    - name: odf-operator
    - name: mcg-operator
    - name: ocs-operator
    - name: metallb-operator
```

For a detailed explanation of each field in the `ImageSetConfiguration`, consult the official documentation.

### Running the Mirroring Process

To start the mirroring process, execute the following command:

```bash
oc-mirror --config imagesetconfig.yaml docker://${PRIVATE_REGISTRY_ADDRESS}
```

!!! Note
    If your private registry uses a self-signed CA or a CA not recognized by the system running the `oc-mirror` command, you can use the `--source-skip-tls` flag to bypass certificate verification.

After several minutes, the process will complete, generating output files in the current directory. The key files are `ImageContentSourcePolicies` and `CatalogSource`, which must be applied to the management cluster to enable a disconnected deployment of the HostedControlPlanes.

!!! Important
    If you have a `Registry-root-url/Namespace` or `Registry-root-url` (E.G: ) IDMS/ICSP entry into the Management cluster for the OCP Metadata and Release images, the Hypershift operator cannot assume where the destination will be, so you need to explicitly create a IDMS/ICSP object with both entries. More info here

    You must run the `oc-mirror` command twice. The first run generates a complete `ImageContentSourcePolicy` file, while the second run provides the differences between the two runs. Always maintain a backup of these files to merge them into a comprehensive `ImageContentSourcePolicy` file if needed. This backup ensures you have a complete policy file.

### Debugging IDMS/ICSP Propagation

You can check if the entries created in the ICSP/IDMS are being propagated properly checking the deployments in the HostedControlPlane. For instance, if you check the ignition-server deployment you can see the command the pod will execute, you need to check if the `--registry-overrides` flag is followed by a series of tuples separated by comma:

```
...
 - --registry-overrides
 - quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:922dd705c2e05c88dc328b7ad4651d5af053851ea6a47dc52840d1abdde3926e=registry.sample.net/quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:922dd705c2e05c88dc328b7ad4651d5af053851ea6a47dc52840d1abdde3926e,quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f4196370cd6348094a11d9384bae3c88a05af2fdd0ed32e919bea26943a939c2=registry.sample.net/quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f4196370cd6348094a11d9384bae3c88a05af2fdd0ed32e919bea26943a939c2
...
```

If that so, means the ICSP/IDMS are being propagated properly, otherwise there is some kind of problem. Please if that's the case, check the known-issues section for more info.


## Technical Implementation Details

### General

In `hypershift-operator/main.go`, the HO will look to see if its management cluster has either ICSP or IDMS capabilities. If so, it will retrieve the image registry information from the appropriate ICSP or IDMS instances and store them in a variable called `imageRegistryOverrides`. This variable is then provided to a custom release image provider called `ProviderWithOpenShiftImageRegistryOverrides`.

### Provider With OpenShift Image Registry Overrides

The `Lookup` function for `ProviderWithOpenShiftImageRegistryOverrides` will use the source and mirror information in `imageRegistryOverrides` when attempting to look up release images.
The `ProviderWithOpenShiftImageRegistryOverrides` is also provided to the `HostedClusterReconciler` as its `ReleaseProvider`. When the `HostedClusterReconciler` is reconciling, it will pass any image registry overrides to the ignition server reconciler and the CPO deployment specification.

### Ignition Server

The ignition server reconciler forwards this information on to the `ignition-server` container as an environment variable called `OPENSHIFT_IMG_OVERRIDES`. `hypershift/ignition-server/cmd/start.go` retrieves this information for the `ReleaseProvider` for the `TokenSecretReconciler`. An important caveat for the ignition server, it cannot follow the ImageContentSourcePolicy or ImageDigestMirrorSet rules because there is not a runtime running inside the pod to do the transformations. So, it's necessary to do the image URL live translation to the custom registry address.

### Control Plane Operator
The `HostedClusterReconciler` passes on the image registry override information as an environment variable in the CPO called `OPENSHIFT_IMG_OVERRIDES`. The CPO will check for the existence of this environment variable when it runs. If the variable exists, it's used to build the HostedControlPlaneReconciler's `releaseProvider`.

#### OLM Catalogs

The imageStream used for the OLM catalogs, when using the `management` (default) OLMCatalogPlacement mode, is not automatically amended with override information detected from ImageContentSourcePolicy (ICSP) on the management cluster.
This because there is not an easy way to validate them in advance for imagestreams.
In case the OLM catalogs got properly mirrored to an internal registry (using the original name and tag), the guest cluster owner can use the `hypershift.openshift.io/olm-catalogs-is-registry-overrides` annotation on the HostedCluster CR.
The format is: `"sr1=dr1,sr2=dr2"` having the source registry string as a key and the destination registry string as value.
OLM catalog image addresses, before being applied to the imagestream, are scanned for the source registry string and if found the string is replaced with the destination registry one.
The cluster admin will also be able to bypass the whole OLM catalogs imagestream mechanism using 4 annotations (`hypershift.openshift.io/certified-operators-catalog-image`, `hypershift.openshift.io/community-operators-catalog-image`, `hypershift.openshift.io/redhat-marketplace-catalog-image`, `hypershift.openshift.io/redhat-operators-catalog-image`) on the HostedCluster CR to directly specify the address (only by digest) of the 4 images to be used for OLM operator catalogs.
In this case the imageStream is not going to be created, and it will be up to the guest cluster owner updating the value of the annotations when the internal mirror will get refreshed to pull in operator updates.
Please notice that if this override mechanism is required, all the 4 values for the 4 default catalog sources are needed.

## References

- How to configure ICSP/IDMS for the Management clusters
- Hypershift ImageContentSource details
- Configuring Machines in HyperShift

---

## Source: docs/content/how-to/disconnected/image-content-sources.md

# Configuring disconnected workloads in the HostedCluster through ImageContentSources

## What is ImageContentSources and why do we need it?

Users can set up alternative image registry information for their guest clusters to use through a field called `ImageContentSources` in a hosted control plane's specification. Alternatively, this field can also be set through the HyperShift CLI by specifying a filepath to a file containing the source and mirrors for the alternative registry information, `--image-content-sources=/path/to/file/with/sources_and_mirrors.yml`.

Here is an example of the expected format for this field, `ImageContentSources`:
```
- mirrors:
    - brew.registry.redhat.io
  source: registry.redhat.io
- mirrors:
    - brew.registry.redhat.io
  source: registry.xyz.redhat.io
- mirrors:
    - brew.registry.redhat.io
  source: registry-proxy.engineering.redhat.com
```
!!! note

    This is also the expected format for the file if you choose to use the HyperShift CLI flag.

## How ImageContentSources are used in the nodes within a NodePool
`ImageContentSources` are reconciled, through the HostedClusterConfigOperator (HCCO), to either an ImageContentSourcePolicy (ICSP) custom resources (CR) or an ImageDigestMirrorSet (IDMS) CR. The CR is then included in the configuration for the Nodes in a NodePool through the NodePool controller's functions, `reconcile > getConfig > defaultAndValidateConfigManifest`.

!!! important

    ICSPs will be deprecated in future OpenShift releases, likely starting with v4.17. IDMSs are the replacement CR for ICSPs but are only available in OpenShift starting in v4.13.
    The HCCO will automatically delete any ICSPs previously used in the node configuration setup through the Control Plane Operator (CPO) starting in OpenShift v4.13.

---

## Source: docs/content/how-to/disconnected/index.md

---
title: Disconnected Deployments
---

This section of the Hypershift documentation contains pages that explain how disconnected deployments work in the HostedControlPlane area.

## Understanding Disconnected Deployments

HostedControlPlane deployments in disconnected environments function somewhat differently than in a regular OCP.

We have two distinct environments:

- **ControlPlane**: Located in the Management Cluster, where the pods of the HostedControlPlane are running and managed by the ControlPlaneOperator.
- **DataPlane**: Located in the workers of the HostedCluster, where the workloads and some other pods run, all managed by the HostedClusterConfigOperator.

Depending on where the pods are running, they will be affected by the `IDMS`/`ICSP` created in the Management Cluster (Control Plane) or the `ImageContentSources` set in the HostedCluster manifest's spec field. The latter will be translated into an `IDMS` object on the HostedCluster side.

These are the relevant resources for disconnected deployments:

- Configuring the Registry TLS Certificates
- Configuring Disconnected HostedControlPlane Deployments
- Configuring Disconnected Workloads in the HostedCluster through ImageContentSources
- Disconnected Workarounds
- Known Issues

---

## Source: docs/content/how-to/disconnected/known-issues.md

# Known Issues

## OLM default catalog sources in ImagePullBackOff state

When you work in a disconnected environment the OLM catalog sources will be still pointing to their original source, so all of these container images will keep it in ImagePullBackOff state even if the OLMCatalogPlacement is set to `Management` or `Guest`. From this point you have some options ahead:

1. Disable those OLM default catalog sources and using the oc-mirror binary, mirror the desired images into your private registry, creating a new Custom Catalog Source.
2. Mirror all the Container Images from all the catalog sources and apply an ImageContentSourcePolicy to request those images from the private registry.
3. Mirror all (or part of relying on the operator pruning option, see the image pruning section in https://docs.openshift.com/container-platform/4.14/installing/disconnected_install/installing-mirroring-disconnected.html#oc-mirror-updating-registry-about_installing-mirroring-disconnected ) the Container Images from all the catalog sources to the private registry and annotate the hostedCluster object with hypershift.openshift.io/olm-catalogs-is-registry-overrides annotation to get the ImageStream used for operator catalogs on your hosted-cluster pointing to your internal registry.

The most practical one is the first choice. To proceed with this option, you will need to follow these instructions. The process will make sure all the images get mirrored and also the ICSP will be generated properly.

Additionally when you're provisioning the HostedCluster you will need to add a flag to indicate that the OLMCatalogPlacement is set to `Guest` because if that's not set, you will not be able to disable them.

## Hypershift operator is failing to reconcile in Disconnected environments

If you are operating in a disconnected environment and have deployed the Hypershift operator, you may encounter an issue with the UWM telemetry writer. Essentially, it exposes Openshift deployment data in your RedHat account, but this functionality does not operate in a disconnected environments.

**Symptoms:**

- The Hypershift operator appears to be running correctly in the `hypershift` namespace but even if you creates the Hosted Cluster nothing happens.
- There will be a couple of log entries in the Hypershift operator:

```
{"level":"error","ts":"2023-12-20T15:23:01Z","msg":"Reconciler error","controller":"deployment","controllerGroup":"apps","controllerKind":"Deployment","Deployment":{"name":"operator","namespace":"hypershift"},"namespace":"hypershift","name":"operator","reconcileID":"451fde3c-eb1b-4cf0-98cb-ad0f8c6a6288","error":"cannot get telemeter client secret: Secret \"telemeter-client\" not found","stacktrace":"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/hypershift/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:329\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/hypershift/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:266\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\n\t/hypershift/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:227"}

{"level":"debug","ts":"2023-12-20T15:23:01Z","logger":"events","msg":"Failed to ensure UWM telemetry remote write: cannot get telemeter client secret: Secret \"telemeter-client\" not found","type":"Warning","object":{"kind":"Deployment","namespace":"hypershift","name":"operator","uid":"c6628a3c-a597-4e32-875a-f5704da2bdbb","apiVersion":"apps/v1","resourceVersion":"4091099"},"reason":"ReconcileError"}
```

**Solution:**

To resolve this issue, the solution will depend on how you deployed Hypershift:

- **The HO was deployed using ACM/MCE:** In this case you will need to create a ConfigMap in the `local-cluster` namespace (the namespace and ConfigMap name cannot be changed) called `hypershift-operator-install-flags` with this content:

```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypershift-operator-install-flags
  namespace: local-cluster
data:
  installFlagsToRemove: --enable-uwm-telemetry-remote-write
```

- **The HO was deployed using the Hypershift binary:** In this case you will just need to remove the flag `--enable-uwm-telemetry-remote-write` from the hypershift deployment command.

## HCP CLI failing to create an hosted cluster with `failed to extract release metadata: failed to get repo setup: failed to create repository client for <your.internal.registry>

See: CNV-38194
Currently the HCP client is directly (client side) trying to access the OCP release image, this implies some additional requirements on client side:

1. the HCP CLI client should be able to directly reach the internal mirror registry, and this could require additional configuration (a proxied access to the api server of the management server is not enough)
1. the HCP CLI client should explicitly consume the pull secret of the internal mirror registry
1. the HCP CLI client should explicitly consume the internal CA used to sign the TLS cert of the internal mirror registry

**Workaround:**
Explicitly set `--network-type OVNKubernetes` (or other valid SDN provider if needed) when running `hcp create cluster` to skip the auto-detection of the SDN network type.

## HCP: imageRegistryOverrides information is extracted only once on HyperShift operator initialization and never refreshed

See: OCPBUGS-29110
According to https://hypershift.pages.dev/how-to/disconnected/idms-icsp-for-management-clusters/ , the HyperShift Operator (HO) will automatically initialize the control plane operator (CPO) with any image registry override information from any ImageContentSourcePolicy (ICSP) or any ImageDigestMirrorSet (IDMS) instances from an OpenShift management cluster.
But due to a bug, the HyperShift Operator (HO) reads the image registry override information only during its startup and never refreshes them at runtime so ICSP and IDMS added after the initialization of the HyperShift Operator are going to get ignored.

**Workaround:**
Each time you add or amend and ICSP or an IDMS on your management cluster, you have to explicitly kill the HyperShift Operator pods:
```bash
$ oc delete pods -n hypershift -l name=operator
```

## HCP: hypershift-operator on disconnected clusters ignores ImageContentSourcePolicies when a ImageDigestMirrorSet exist on the management cluster

See: OCPBUGS-29466

ICSP (deprecated) and IDMS can coexists on the management cluster but due to a bug if at least one IDMS object exists on the management cluster, the HyperShift Operator will completely ignore all the ICSP.

**Workaround:**
If you have at least an IDMS object, explicitly convert all the ICSPs to IDMSs:
```bash
$ mkdir -p /tmp/idms
$ mkdir -p /tmp/icsp
$ for i in $(oc get imageContentSourcePolicy -o name); do oc get ${i} -o yaml > /tmp/icsp/$(basename ${i}).yaml ; done
$ for f in /tmp/icsp/*; do oc adm migrate icsp ${f} --dest-dir /tmp/idms ; done
$oc apply -f /tmp/idms || true
```

## HCP: hypershift-operator on disconnected clusters ignores RegistryOverrides inspecting the control-plane-operator-image

See: OCPBUGS-29494

**Symptoms:**
Creating an hosted cluster with:
hcp create cluster kubevirt --image-content-sources /home/mgmt_iscp.yaml  --additional-trust-bundle /etc/pki/ca-trust/source/anchors/registry.2.crt --name simone3 --node-pool-replicas 2 --memory 16Gi --cores 4 --root-volume-size 64 --namespace local-cluster --release-image virthost.ostest.test.metalkube.org:5000/localimages/local-release-image@sha256:66c6a46013cda0ad4e2291be3da432fdd03b4a47bf13067e0c7b91fb79eb4539 --pull-secret /tmp/.dockerconfigjson --generate-ssh

on the hostedCluster object we see:
```yaml
status:
conditions:
- lastTransitionTime: "2024-02-14T22:01:30Z"
  message: 'failed to look up image metadata for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6:
  failed to obtain root manifest for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6:
  unauthorized: authentication required'
  observedGeneration: 3
  reason: ReconciliationError
  status: "False"
  type: ReconciliationSucceeded
```

and in the logs of the hypershift operator:
```json
{"level":"info","ts":"2024-02-14T22:18:11Z","msg":"registry override coincidence not found","controller":"hostedcluster","controllerGroup":"hypershift.openshift.io","controllerKind":"HostedCluster","HostedCluster":{"name":"simone3","namespace":"local-cluster"},"namespace":"local-cluster","name":"simone3","reconcileID":"6d6a2479-3d54-42e3-9204-8d0ab1013745","image":"4.14-2024-02-14-135111"}
{"level":"error","ts":"2024-02-14T22:18:12Z","msg":"Reconciler error","controller":"hostedcluster","controllerGroup":"hypershift.openshift.io","controllerKind":"HostedCluster","HostedCluster":{"name":"simone3","namespace":"local-cluster"},"namespace":"local-cluster","name":"simone3","reconcileID":"6d6a2479-3d54-42e3-9204-8d0ab1013745","error":"failed to look up image metadata for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6: failed to obtain root manifest for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6: unauthorized: authentication required","stacktrace":"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/remote-source/app/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:326\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/remote-source/app/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:273\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\n\t/remote-source/app/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:234"}
```

**Workaround:**
Explicitly use --annotations=hypershift.openshift.io/control-plane-operator-image=<controlPlaneOperatorImage> when creatng the hosted cluster.
```bash
# this will sync with the release image used on the management cluster, replace if needed
$ PAYLOADIMAGE=$(oc get clusterversion version -ojsonpath='{.status.desired.image}')
$ HO_OPERATOR_IMAGE="${PAYLOADIMAGE//@sha256:[^ ]*/@\$(oc adm release info -a /tmp/.dockerconfigjson "$PAYLOADIMAGE" | grep hypershift | awk '{print $2}')}"
$ hcp create cluster ... --annotations=hypershift.openshift.io/control-plane-operator-image=${HO_OPERATOR_IMAGE} ...
```

## (KubeVirt specific) oc-mirror does not mirror the RHCOS image for HyperShift KubeVirt provider from the OCP release payload

See: OCPBUGS-29408

**Workaround:**
Explicitly mirror the RHCOS image for HyperShift KubeVirt provider.
This snippet assumes that `yq-v4` tool is already available on the bastion host.
```bash
mirror_registry=$(oc get imagecontentsourcepolicy -o json | jq -r '.items[].spec.repositoryDigestMirrors[0].mirrors[0]')
mirror_registry=${mirror_registry%%/*}
LOCALIMAGES=localimages
# this will sync with the release image used on the management cluster, replace if needed
PAYLOADIMAGE=$(oc get clusterversion version -ojsonpath='{.status.desired.image}')
oc image extract --file /release-manifests/0000_50_installer_coreos-bootimages.yaml ${PAYLOADIMAGE} --confirm
RHCOS_IMAGE=$(cat 0000_50_installer_coreos-bootimages.yaml | yq -r .data.stream | jq -r '.architectures.x86_64.images.kubevirt."digest-ref"')
RHCOS_IMAGE_NO_DIGEST=${RHCOS_IMAGE%@sha256*}
RHCOS_IMAGE_NAME=${RHCOS_IMAGE_NO_DIGEST##*/}
RHCOS_IMAGE_REPO=${RHCOS_IMAGE_NO_DIGEST%/*}
oc image mirror ${RHCOS_IMAGE} ${mirror_registry}/${LOCALIMAGES}/${RHCOS_IMAGE_NAME}
```

## HCP: recycler pods are not starting on hostedcontrolplane in disconnected environments ( ImagePullBackOff on quay.io/openshift/origin-tools:latest )

See: OCPBUGS-31398

**Symptoms:**
Recycler pods are not starting on hosted control plane with ImagePullBackOff error on `quay.io/openshift/origin-tools:latest`.
The recycler pod template in the recycler-config config map on the hostedcontrolplane namespace refers to `quay.io/openshift/origin-tools:latest`:
```bash
$ oc get cm -n clusters-guest recycler-config -o json | jq -r '.data["recycler-pod.yaml"]' | grep "image"
image: quay.io/openshift/origin-tools:latest
```

**Workaround:**
Not available.
The recycler-config configmap is continuously reconciled by the control plane operator
pointing it back to `quay.io/openshift/origin-tools:latest` which will never be available for a disconnected cluster.

## HCP: imagesStreams on hosted-clusters pointing to image on private registries are failing due to tls verification, although the registry is correctly trusted

See: OCPBUGS-31446
Probably a side effect of https://issues.redhat.com/browse/RFE-3093 - imagestream to trust CA added during the installation

**Symptoms:**
On the imageStream conditions on the the hosted cluster we see something like:
```yaml
tags:
- conditions:
    - generation: 2
      lastTransitionTime: "2024-03-27T12:43:56Z"
      message: 'Internal error occurred: virthost.ostest.test.metalkube.org:5000/localimages/local-test-image:e2e-7-registry-k8s-io-e2e-test-images-busybox-1-29-4-4zE9mRvED4RQoUxQ:
      Get "https://virthost.ostest.test.metalkube.org:5000/v2/": tls: failed to
      verify certificate: x509: certificate signed by unknown authority'
```
although the same image can be directly consumed by a pod on the same cluster

**Workaround:**
Patch all the image streams on the hostedcluster pointing to the internal registry setting insecure mode.
```bash
# KUBECONFIG points to the hosted cluster
IMAGESTREAMS=$(oc get imagestreams -n openshift -o=name)
for isName in ${IMAGESTREAMS}; do
  echo "#### Patching ${isName} using insecure registry"
  oc patch -n openshift ${isName} --type json -p '[{"op": "replace", "path": "/spec/tags/0/importPolicy/insecure", "value": true}]'
done
```

## IDMS/ICSP with only root registry are not propagated to the registry-override flag under ignition server

See: OCPBUGS-33951

**Symptoms:**
Even having the ICSP/IDMS well set in the Management cluster and working fine in that side, the HostedControlPlane deployment is not capable of extract the metadata release from the OCP payload images. Something like this log entry will show up in the ControlPlaneOperator or HypershiftOperator pod.

```
failed to lookup release image: failed to extract release metadata: failed to get repo setup: failed to create repository client for https://registry.sample.net: Get "https://registry.sample.net/v2/": tls: failed to verify certificate: x509: certificate signed by unknown authority
```

**Root Cause**:
Once the ICSP/IDMS are created in the Management cluster and they are only using in the "sources" side a root registry instead of pointing a registry namespace, the image-overrides are not filled with the explicit destination OCP Metadata and Release images, so the Hypeshift operator cannot infer the exact location of the images in the private registry.

This is a sample:

```yaml
apiVersion: config.openshift.io/v1
kind: ImageDigestMirrorSet
metadata:
  name: image-policy
spec:
  imageDigestMirrors:
  - mirrors:
    - registry.sample.net/redhat.io
    source: registry.redhat.io
  - mirrors:
    - registry.sample.net/connect.redhat.com
    source: registry.connect.redhat.com
  - mirrors:
    - registry.sample.net/gcr.io
    source: gcr.io
  - mirrors:
    - registry.sample.net/docker.io
    source: docker.io
```

**Solution:**
In order to solve the issue and perform a successful HostedControlPlane disconnected deployment you need at least to create the OCP namespaced reference in a IDMS/ICSP. A sample will look like this:

```yaml
apiVersion: config.openshift.io/v1
kind: ImageDigestMirrorSet
metadata:
  name: ocp-release
spec:
  imageDigestMirrors:
  - mirrors:
    - registry.sample.net/quay.io/openshift-release-dev/ocp-v4.0-art-dev
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  - mirrors:
    - registry.sample.net/quay.io/openshift-release-dev/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
```

---

## Source: docs/content/how-to/disconnected/tls-certificates.md

# How to Configure TLS Certificates for Disconnected Deployments

In the HostedControlPlane area, it is necessary to configure the registry CA certificates in several areas to ensure proper functioning of disconnected deployments.

## Adding the Registry CA to the Management Cluster

There are numerous methods to accomplish this within the OpenShift environment. However, we have chosen a less intrusive approach.

1. First, create a ConfigMap with a name of your choosing. In this example, we will use the name `registry-config`. The content should resemble the following:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: registry-config
  namespace: openshift-config
data:
  registry1.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
  registry2.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
  registry3.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
```

!!! note

    The `data` field should contain the registry name, while the value should include the registry certificate. As shown, the ":" character is replaced by ".."; ensure this correction is made.

Ensure your data in the ConfigMap is defined using only `"|"` instead of other methods like `"|-"`. Using other methods can cause issues when the Pod reads the certificates loaded in the system.

2. Now, we need to patch the cluster-wide object `image.config.openshift.io` to include this:

```yaml
spec:
  additionalTrustedCA:
    - name: registry-config
```

This modification will result in two significant outcomes:

- Granting master nodes the capability to retrieve images from the private registry.
- Allowing the Hypershift Operator to extract the OpenShift payload for the HostedCluster deployments.

!!! note

    The modification may take several minutes to be successfully executed.

## Adding the Registry CA to the HostedCluster Worker Nodes

This configuration allows the HostedCluster to inject the CA into the DataPlane workers, which is necessary for pulling images from the private registry.

This is defined in the field `hc.spec.additionalTrustBundle` as follows:

```yaml
spec:
  additionalTrustBundle:
    name: user-ca-bundle
```

This `user-ca-bundle` entry is a ConfigMap created by the user in the HostedCluster namespace (the same namespace where the HostedCluster object is created).

The ConfigMap should look like this:

```yaml
apiVersion: v1
data:
  ca-bundle.crt: |
    // Registry1 CA
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----

    // Registry2 CA
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----

    // Registry3 CA
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----

kind: ConfigMap
metadata:
  name: user-ca-bundle
  namespace: <HOSTEDCLUSTER NAMESPACE>
```

This modification will result in two significant outcomes:

- Granting HostedCluster workers the capability to retrieve images from the private registry.


---

## Source: docs/content/how-to/distribute-hosted-cluster-workloads.md

---
title: Distribute Hosted Cluster workloads
---

# Distribute Hosted Cluster workloads

## Topology

HyperShift enables implementing the strategy to colocate and isolate pods for Hosted Clusters.
As a management cluster operator you can leverage the following Node labels and taints:

`hypershift.openshift.io/control-plane: true`

`hypershift.openshift.io/cluster: ${HostedControlPlane Namespace}`

- Pods for a Hosted Cluster tolerate taints for "control-plane" and "cluster".
- Pods for a Hosted Cluster prefer to be scheduled into "control-plane" Nodes.
- Pods for a Hosted Cluster prefer to be scheduled into their own "cluster" Nodes.

In addition:

- Pods for a Hosted Cluster prefer to be scheduled into the same Node.
- If the `ControllerAvailabilityPolicy` is `HighlyAvailable` Pods for each Deployment within a Hosted Cluster will require to be scheduled across different failure domains by setting `topology.kubernetes.io/zone` as the topology key.
- A HostedCluster can require their Pods to be scheduled into particular Nodes by setting `HostedCluster.spec.nodeSelector`. E.g
```yaml
  spec:
    nodeSelector:
      role.kubernetes.io/infra: "" 
```

## Custom Taints and Tolerations

By default, pods for a Hosted Cluster tolerate the "control-plane" and
"cluster" taints, however it is also possible to use custom taints on
nodes and allow Hosted Clusters to tolerate those taints on a per
Hosted Cluster basis by setting `HostedCluster.spec.tolerations`. E.g
```yaml
  spec:
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/custom
      operator: Exists
```

Tolerations can also be set on the Hosted Cluster while creating a cluster
using the `--tolerations` hcp cli argument. E.g
```bash
--toleration="key=kubernetes.io/custom,operator=Exists,effect=NoSchedule"
```

Usage of custom tolerations in concert with nodeSelectors allows for fine
granular control of Hosted Cluster pod placement on a per Hosted Cluster
basis. This control allows for groups of Hosted Clusters to be colocated
and isolated from other Hosted Clusters. It also allows for custom
placement of Hosted Clusters within infra and master nodes.

## Scheduling Topology Options 

Cluster Service Providers may choose how hosted control planes are isolated or co-located. The three different options are:

 - Shared Everything
 - Shared Nothing 
 - Dedicated Request Serving

These options can be seen as a spectrum of isolation. Shared Everything is the least isolated, Dedicated Request Serving (Shared Some) and then Shared Nothing being the most isolated option.

**NOTE**: Each hosted control plane can run Single Replica or Highly Available. If Highly Available, the control plane will be spread across failure domains via `topology.kubernetes.io/zone` as the topology key.

### Shared Everything

- All hosted control plane pods are scheduled to any node that can run hosted control plane workloads.
- Nodes can be allocated specifically for control plane workloads by tainting and labeling them with `hypershift.openshift.io/control-plane: true`.



### Shared Nothing

- To confine nodes to a specific hosted cluster taint and label them with `hypershift.openshift.io/cluster` value.
- No other control plane pods will land on those nodes.



### Dedicated Request Serving

- Two nodes in different zones are dedicated to a specific hosted cluster‚Äôs front end serving components.
- The rest of the hosted cluster‚Äôs control plane pods can co-exist with other clusters‚Äô control plane pods running on shared nodes.
- When running a Highly Available control plane, there will only be 2 replicas of request serving workloads instead of 3.

**NOTE**: A HostedCluster must have:

- `hypershift.openshift.io/topology: dedicated-request-serving-components` annotation to honor dedicated serving content workloads affinity opinions.
-  nodeSelector set as `hypershift.openshift.io/control-plane: true` for it to be a hard requirement for workloads to be scheduled. Without it that label is a soft requirement meaning workloads will try to find any suitable node if there‚Äôs none with this label.



## Priority

HyperShift leverages PriorityClasses for driving Priority and Preemption of their managed Pods.
It will install four priority classes in a management cluster with the following order of priority from highest to lowest:

- `hypershift-operator`: Hypershift operator pods
- `hypershift-etcd`: Pods for etcd.
- `hypershift-api-critical`: Pods that are required for API calls and resource admission to succeed. This includes pods like kube-apiserver, aggregated API servers, and webhooks.
- `hypershift-control-plane`: pods in the HyperShift Control Plane that are not API critical but still need elevated priority. E.g Cluster Version Operator.


---

## Source: docs/content/how-to/feature-gates.md

# Feature Gates

Feature gates in OpenShift allows to ensure everything new works together and optimizes for rapid evaluation in CI to promote without ever releasing a public preview.
There are no guarantees that your fleet continues to be operational long term after you enable a feature gate.

In the HCP context, there are multiple non exclusive scenarios where features might need to be gated:

1 - A feature that impacts HO install
E.g. New CRDs are required for CPOv2 (control plane operator version 2)

2 - A feature that impacts the whole cluster fleet / HO
E.g. Introduce fleet wide shared ingress to be validated in targeted environments

3 - A feature that impacts individual clusters
E.g. Introduce using CPOv2 for some HC to develop feedback

4 - A feature that impacts API
E.g. Introduce a new provider like Openstack
E.g. Introduce a new field/feature like AWS tenancy

5 - A feature specific for an OCP component
Components honour existing standalone in-cluster OCP feature gate mechanisim

## Users
All the feature gates are grouped in a single TechPreviewNoUpgrade feature set. Current implementation exposes this --tech-preview-no-upgrade flag in the CLI at install time

```
hypershift install --help
```
Will show among other flags:
```
--tech-preview-no-upgrade                        If true, the HyperShift operator runs with TechPreviewNoUpgrade features enabled
```

In a follow up we'll consider to introduce support to also signal --tech-preview-no-upgrade at the HC level.
Eventually support for at least 1, 2, 3 and 4 afromentioned scenarios will most likely converge into a single API.

## Devs

We rely on openshift/api tooling for generating CRDs with openshift markers. See this PR as an example of a adding a field behind a feature gate.

The currently ongoing implementation of feature gates for the controllers business logic relies on "k8s.io/component-base/featuregate". This enables devs to declare granular gates for their features.
See this PR as an example.

### Promoting a feature gated API field and feature to sable

Generally speaking any new field should start by being feature gated.
The minimum criteria for promotion is:

- Provide clear context and analysis on the PR about how the field might impact the different GA products. This includes but it is not limited to ROSA, ARO, IBM Cloud and MCE (self hosted).

- Document the field with the expected behaviour for day 1 and day 2 changes.

- There is e2e test coverage for the feature that include day 2 changes of the field.

- There is e2e test coverage for on creation UX failure expectation via this e2e test

- There is e2e test coverage for day 2 on update UX failure expectations via this e2e test

In general we aim to adhere and converge with stand alone principles in openshift/api

---

## Source: docs/content/how-to/gcp-platform.md

# GCP Platform Implementation for HyperShift

This document describes the implementation of Google Cloud Platform (GCP) support in HyperShift with Cluster API Provider GCP (CAPG) integration.

## Overview

The GCP platform implementation enables HyperShift to create and manage OpenShift hosted clusters on Google Cloud Platform using CAPG (Cluster API Provider GCP) for NodePool support. This implementation follows established patterns from AWS and Azure platforms.

## Architecture

### Key Components

1. **Platform Interface Implementation** (`/hypershift-operator/controllers/hostedcluster/internal/platform/gcp/gcp.go`)
   - Implements the HyperShift Platform interface for GCP
   - Provides CAPG controller deployment specification
   - Manages credential validation and condition management
   - Handles CAPI infrastructure resource reconciliation

2. **API Types** (`/api/hypershift/v1beta1/gcp.go`)
   - `GCPPlatformSpec`: Main configuration for GCP clusters
   - `GCPNetworkConfig`: VPC and Private Service Connect configuration
   - `GCPWorkloadIdentityConfig`: Workload Identity Federation preparation
   - `GCPResourceReference`: GCP resource naming standards

3. **Condition Management** (`/api/hypershift/v1beta1/hostedcluster_conditions.go`)
   - `ValidGCPCredentials`: GCP credential validation status
   - `ValidGCPWorkloadIdentity`: WIF configuration validation status

## Features

### Credential Management

The GCP platform uses **Workload Identity Federation (WIF)** for secure, short-lived token-based authentication. This eliminates the need for long-term service account keys and provides better security.

#### Workload Identity Federation (Required)
```yaml
spec:
  platform:
    gcp:
      workloadIdentity:
        projectNumber: "123456789012"
        poolID: "hypershift-pool"
        providerID: "hypershift-provider"
        serviceAccountsRef:
          nodePoolEmail: "capg-controller@my-gcp-project.iam.gserviceaccount.com"
```

**Note**: Workload Identity Federation is mandatory for GCP clusters. The token minter sidecar is automatically deployed to handle token exchange and service account impersonation for CAPG controllers. The specified Google Service Account (GSA) will be impersonated to access GCP APIs for node pool management.

**Getting the Project Number**: The `projectNumber` is the numeric identifier for your GCP project (different from the project ID). You can get it with:
```bash
gcloud projects describe PROJECT_ID --format="value(projectNumber)"
```

### Network Configuration

Supports VPC and Private Service Connect configuration:

```yaml
spec:
  platform:
    gcp:
      networkConfig:
        network:
          name: "my-vpc-network"
        privateServiceConnectSubnet:
          name: "my-psc-subnet"
```

### Resource Labeling

Automatic resource labeling for organization and billing. Labels can be updated after cluster creation and will be propagated to GCP resources where supported:

```yaml
spec:
  platform:
    gcp:
      resourceLabels:
        - key: environment
          value: production
        - key: team
          value: platform
        - key: cost-center
          value: engineering
```

## CAPG Integration

### Controller Deployment

The GCP platform deploys CAPG controllers with:

- **Manager Container**: Main CAPG controller with proper image handling
- **Token Minter Sidecar**: Prepared for Workload Identity Federation
- **Comprehensive RBAC**: Full permissions for CAPI resource management
- **Feature Gates**: Version-conditional settings for compatibility

### RBAC Policies

Comprehensive policy rules covering:

- **CAPG Resources**: `gcpclusters`, `gcpmachines`, `gcpmachinetemplates`
- **CAPI Resources**: `clusters`, `machines`, `machinesets`
- **Kubernetes Resources**: `nodes`, `pods`, `secrets`, `configmaps`
- **HyperShift Resources**: `nodepools`, `hostedclusters`
- **Bootstrap Resources**: `kubeadmconfigs`, `kubeadmconfigtemplates`
- **Certificate Management**: CSRs and approval handling
- **Coordination**: Leader election and distributed coordination

### Infrastructure Reconciliation

Creates and manages `GCPCluster` CAPI resources with:

- **External Management**: Marked with `cluster.x-k8s.io/managed-by: external`
- **Project/Region Configuration**: From HostedCluster specification
- **Network Settings**: VPC and subnet configuration for Private Service Connect
- **Resource Labels**: Applied to all created GCP resources
- **Failure Domains**: Multi-zone deployment support
- **Control Plane Endpoint**: API server endpoint configuration

## Configuration Examples

### Complete GCP Cluster Example
```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: example-gcp
  namespace: clusters
spec:
  platform:
    type: GCP
    gcp:
      project: "my-gcp-project"
      region: "us-central1"
      networkConfig:
        network:
          name: "hypershift-vpc"
        privateServiceConnectSubnet:
          name: "hypershift-psc"
      workloadIdentity:
        projectNumber: "123456789012"
        poolID: "hypershift-pool"
        providerID: "hypershift-provider"
        serviceAccountsRef:
          nodePoolEmail: "capg-controller@my-gcp-project.iam.gserviceaccount.com"
      resourceLabels:
        - key: environment
          value: production
        - key: team
          value: platform
```

## Validation and Defaulting

### API Validation
- **Project ID**: RFC-compliant format with length and character restrictions
- **Region**: Valid GCP region format
- **Resource Labels**: GCP label requirements (63 char max, valid characters)
- **Cross-Field Validation**: Either credentials or WIF required
- **Network Naming**: GCP resource naming standards

### Default Conditions
- **ValidGCPCredentials**: Set to `True` by default
- **ValidGCPWorkloadIdentity**: Set when WIF is configured
- **Platform-Agnostic**: Standard HyperShift conditions apply

## Implementation Status

### ‚úÖ Completed (Phase 1)
- [x] Platform interface implementation
- [x] CAPG controller deployment specification
- [x] API types with comprehensive validation
- [x] Credential validation and condition management
- [x] CAPI infrastructure resource reconciliation
- [x] Comprehensive RBAC policy rules
- [x] Default condition management
- [x] Integration testing framework
- [x] Code generation and documentation

### üöß Future Work (Not in Current Scope)
- [ ] NodePool implementation and CAPG machine management
- [ ] Workload Identity Federation token exchange implementation
- [ ] GCP KMS secret encryption integration
- [ ] External infrastructure creation (VPC, IAM) via CLI
- [ ] Production deployment and testing

## Testing

Integration tests are provided in `/test/integration/gcp_test.go` covering:

1. **Platform Creation**: Basic constructor and instance validation
2. **Deployment Specification**: CAPG controller deployment configuration
3. **Credential Validation**: Condition management and validation logic
4. **Infrastructure Reconciliation**: CAPI resource creation and configuration
5. **Policy Rules**: RBAC rule completeness and correctness
6. **Default Conditions**: Platform-specific condition initialization

Run tests with:
```bash
go test ./test/integration/gcp_test.go -v
```

## Technical Details

### Image Handling Priority
1. Annotation override: `hypershift.openshift.io/capi-provider-gcp-image`
2. Environment variable: `IMAGE_GCP_CAPI_PROVIDER`
3. Payload discovery: From OpenShift release payload

### Naming Conventions
- **ServiceAccount**: `capi-gcp-controller-manager`
- **ClusterRole**: `capi-gcp-controller-manager-<namespace>`
- **Secret**: `capg-manager-bootstrap-credentials`
- **GCPCluster**: Same as HostedCluster name

### Error Handling
- Comprehensive credential validation with detailed error messages
- Condition-based status reporting following HyperShift patterns
- Graceful handling of optional configuration (WIF, resource labels)
- Proper error aggregation and propagation

## References

- CAPG Documentation
- GCP Resource Naming Standards
- HyperShift Platform Development Guide
- Workload Identity Federation

---

## Source: docs/content/how-to/index.md

---
title: How-to guides
---

This section of the HyperShift documentation contains pages that show how to do individual tasks.
A how-to page shows how to do a single thing, typically by giving a short sequence of steps.


---

## Source: docs/content/how-to/install/index.md

# HyperShift Operator Installation

This document describes different installation flags or methods for HyperShift Operator (HO).

## Limiting the CAPI CRDs installed
The HO uses the Cluster API (CAPI) to manage the nodes in the NodePool. By default, the HO installation will install all 
CAPI related CRDs. If you want to limit the CRDs installed, you can set the `--limit-crd-install` flag to a 
comma-separated list of CRDs to install. The valid values for this flag are: AWS, Azure, IBMCloud, KubeVirt, Agent, 
OpenStack.

For example, to only install the AWS and Azure related CAPI CRDs, you would use 
the following flag in your HO install command:

```bash
--limit-crd-install=AWS,Azure
```

!!! important

    Limiting the CAPI CRDs installed means the HO will only be able to manage HostedClusters of the same platform.
    For example, in the above example, if you limit the CRDs to AWS and Azure, the HO will only be able to manage 
    AWS and Azure HostedClusters.

---

## Source: docs/content/how-to/kubevirt/configuring-network.md

# Configuring Network

By default, the NodePools for the KubeVirt platform generate VMs which are
attached to the default pod network. It is possible to attach additional
networks to the NodePools VMs as well though.

## Attaching Additional Networks to KubeVirt VMs

Attaching additional networks to a KubeVirt NodePool is accomplished through
the usage of multus and NetworkAttachmentDefinitions. More information about
how to configure NetworkAttachementDefinitions can be found here. Below is an example
of adding multiple additional networks to a guest cluster at creation time
using the hcp command line tool with the --additional-network argument.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
‚Äìadditional-network name:my-namespace/network1 \
‚Äìadditional-network name:my-namespace/network2
```

In this example, the KubeVirt VMs will have interfaces attached to the networks
for the NetworkAttachmentDefinitions network1 and network2 which reside in
namespace my-namespace.

## Using Secondary Network as Default

Users managing a network (DHCP, routing, etc...) can use that network 
as the default one for the kubevirt hosted clusters, to do so
disable pod default network and attach an additional one that connects to it 
with the hcp command line tool arguments `--attach-default-network=false` and 
`--additional-network`.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--attach-default-network=false \
--additional-network name:my-namespace/network1 \
```

## Avoiding Network CIDR Collisions between Hosting and Guest Clusters.

When creating a guest cluster, it's crucial to ensure that the hosting and guest clusters do not share the same subnet (cidr) for the cluster network.
This prevents DNS-related conflicts and avoids issues during the deployment of guest clusters.


### Guidelines to Avoid CIDR Collisions:


1. Detecting Hosting Cluster Network CIDR Range.

Determine the cluster network CIDR range from the underlying Hosting cluster using the command:

```
oc --kubeconfig <hosting cluster kubeconfig> get network cluster -o=jsonpath='{.spec.clusterNetwork}'
```


2. Identifying CIDR Conflicts with HCP Default Range

Compare the default CIDR range used by HCP, which is typically set to 10.132.0.0/14,
with the CIDR range obtained in Step 1 to detect any conflicts with the hosting CIDR range.


3. Setting Custom Cluster CIDR Range:

If a conflict is detected, use the HCP CLI to specify a custom CIDR range for the guest cluster using
the --cluster-cidr flag. The default CIDR range used by the HCP CLI is designed to be offset from the
typical OCP deployment range, so under normal operation, this offset should prevent collisions.

For example, if the hosting cluster's clusterNetwork CIDR is set to 10.132.0.0/14, it's imperative to
select a different CIDR range for the guest cluster to prevent conflicts.

For instance, consider specifying a different CIDR range, such as 10.128.0.0/14 (this range is just an example),
for the guest cluster.

```
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--cluster-cidr 10.128.0.0/14
```

By following these guidelines, you can ensure successful deployment of guest clusters, avoid network CIDR collisions,
and prevent DNS-related issues between hosting and guest clusters.


---

## Source: docs/content/how-to/kubevirt/configuring-storage.md

# Storage Overview

Configuring storage for KubeVirt Hosted Control Planes falls within three
categories, each of which have differing requirements.

**ETCD Storage**

ETCD requires usage of high performance persistent storage on the management
cluster hosting the ETCD pods. Due to the performance requirements, usage of a
local storage csi driver such as LVM Storage is preferred.
When a guest cluster is created in HighAvailability mode, ETCD is replicated
in pods across three separate management cluster nodes. This replication
ensures data resiliency even when a local storage csi driver is in use.

More information about ETCD storage configuration can be found in the
ETCD Storage Configuration section.

**Node Root Volume Storage**

In a KubeVirt Hosted Control Plane, the worker nodes are hosted in KubeVirt VMs.
It is recommended to use a csi driver capable of providing ReadWriteMany
access mode and Block volume mode for the VM root volume storage. This allows
the KubeVirt VMs to live migrate and remain available even when the underlying
infra cluster nodes are disrupted. Ceph is an example of a csi driver that
would meet these requirements.

More information about configuring root volume storage can be found in the
KubeVirt VM Root Volume Configuration section.

**KubeVirt CSI Storage**

The KubeVirt CSI driver allows storage classes present on the infra cluster (the cluster the KubeVirt VMs run
on)
to be mapped directly to storage classes within the KubeVirt guest cluster.
This lets the guest cluster utilize the same storage that is available on the
infra cluster.

It is recommended to use an underlying infra storage class capable of
ReadWriteMany access mode and Block volume mode for KubeVirt CSI. This allows
KubeVirt CSI to pass storage to the VMs in a way that still allows for the VMs
to live migrate and be portable across infra nodes.

Below is a chart that outlines the current features of KubeVirt CSI as they map
to the infra cluster's storage class.

| Infra CSI Capability  | Guest CSI Capability               | VM Live Migration Support | Notes                                           |
|-----------------------|------------------------------------|---------------------------|-------------------------------------------------|
| RWX Block             | RWO (Block/Filesystem) RWX (Block) | Supported                 |                                                 |
| RWO Block             | RWO (Block/Filesystem)             | Not Supported             |                                                 |
| RWO Filesystem        | RWO (Block/Filesystem)             | Not Supported             | suboptimal guest block volume mode performance. |

More information about configuring KubeVirt CSI can be found in the
KubeVirt CSI StorageClass Mapping section.

# Configuring Storage

By default, if no advanced configuration is provided, the default storageclass
is used for the the KubeVirt VM images, the kubevirt csi mapping, and the
etcd volumes.

## KubeVirt CSI StorageClass Mapping

KubeVirt CSI permits any infra storage class with the `ReadWriteMany` access
mode to be exposed to the guest cluster. This mapping of infra cluster storage
class to guest cluster storage class can be configured during cluster creation
using the `hcp` cli tool and the `--infra-storage-class-mapping` cli
argument.

Below is an example of how to map two infra storage classes called `infra-sc1`
and `infra-sc2` to guest storage classes called `guest-sc1` and `guest-sc2`.
Note that `--infra-storage-class-mapping` argument can be used multiple times
within the create command.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--infra-storage-class-mapping=infra-sc1/guest-sc1 \
--infra-storage-class-mapping=infra-sc2/guest-sc2
```

Once the guest cluster is created, the `guest-sc1` and `guest-sc2` storage
classes will be visible within the guest cluster. When users create a PVC
within the guest cluster that utilizes one of these storage classes, KubeVirt
CSI will facilitate provisioning that volume using the infra storage class
mapping that was configured during cluster creation.

!!! note

    KubeVirt CSI only supports mapping an infrastructure storage class that is
    capable of `ReadWriteMany` (RWX) access.

## KubeVirt CSI VolumeSnapshotClass Mapping

KubeVirt CSI permits infra VolumeSnapshotClasses to be exposed to the guest
cluster. Since VolumeSnapshotClasses are tied to a particular provisioner the
mapping between VolumeSnapshotClasses and StorageClasses needs to expressed to
hypershift during guest cluster creation. using the `hcp` cli tool and the
`--infra-volumesnapshot-class-mapping` cli argument.

Below is an example of a simple setup with a single infra storage class and a
single matching volume snapshot class in the infra cluster being mapped to a
single storage class and single volume snapshot class in the guest cluster.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--infra-storage-class-mapping=infra-sc1/guest-sc1 \
--infra-volumesnapshot-class-mapping=infra-vsc1/guest-vsc1
```

If you omit the `--infra-storage-class-mapping` and the
`--infra-volumesnapshot-class-mapping`. The system will use the default
storage class and the default volume snapshot class in the infra cluster. If
the default is not set, then the snapshot functionality will not work and the
snapshot request will never reach ready state. This is because it not possible
to create a correct snapshot in the infra cluster.

A more complex setup could contain multiple storage classes with multiple
volume snapshot classes. In particular in this setup one volume snapshot class
is only compatible with certain storage classes but not all. So we have infra
storage class a and b, and infra snapshot volume class a and b. Only the a's
are compatible with each other and only the b's are compatible with each other.

To properly group them together use the 'group' option of the
`--infra-volumesnapshot-class-mapping` and `group` option of the
`--infra-storage-class-mapping`.
Below is an example of this setup

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--infra-storage-class-mapping=infra-sca/guest-sca,group=a \
--infra-storage-class-mapping=infra-scb/guest-scb,group=b \
--infra-storage-class-mapping=infra-scc/guest-scc,group=a \
--infra-volumesnapshot-class-mapping=infra-vsca/guest-vsca,group=a \
--infra-volumesnapshot-class-mapping=infra-vscb/guest-vscb,group=b
```

Since both storage class `infra-sca` and volume snapshot class `infra-vsca`
are in the same group, this indicates to KubeVirt CSI that they are
compatible and be used to create snapshots of volumes from storage class
`guest-sca` using the guest volume snapshot class `guest-vsca`. The same
is true with with the `b` grouping as well. Since `infra-scc` is also in
the `a` group, creating snapshots of volumes from storage class `guest-scc`
will use the same volume snapshot class in the infra cluster as making a
snapshot of volumes that use storage class `guest-sca`

!!! note

   KubeVirt CSI passes snapshot requests to the underlying infra. This means
   that snapshots will only work for compatible volumes. Please ensure the
   proper mapping is configured before attempting to create a snapshot in the
   guest cluster.

## Disabling KubeVirt CSI

By default KubeVirt CSI maps the default storage class on the underlying
infrastructure cluster to a storage class in the guest cluster.

To disable KubeVirt CSI entirely, set the hostedCluster.spec.platform.kubeVirt.storageDriver.type
to the value `None` at cluster creation time. Below is an example HostedCluster
spec that outlines this behavior.

```yaml linenums="1"
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: example
  namespace: clusters
spec:
  platform:
    type: KubeVirt
    storageDriver:
      type: None
  pullSecret:
    name: hcp-pull-secret
```

## KubeVirt CSI Storage Security and Isolation

While KubeVirt CSI is extending storage capabilities of the underlying
infrastructure cluster to guest HCP clusters, the csi driver is doing so in a
controlled way. This ensures each guest cluster's storage is both isolated
from other guest clusters, and that the guest cluster can't access arbitrary
storage volumes on the infrastructure cluster that are not associated with the
guest cluster.

This isolation is achieved through a depth of security enforcements

1. Direct API access to the infrastructure cluster is never given directly to
the HCP guest cluster worker nodes. This means the guest cluster does not have
a means to provision storage on the infrastructure cluster except through the
controlled KubeVirt CSI interface.
2. The KubeVirt CSI cluster controller runs in a pod in the HCP namespace, and
is not accessible from within the guest cluster. This component ensures PVCs on
the infrastructure cluster can only be passed into the guest cluster if those
PVCs are associated with the guest cluster.
3. By default, the RBAC provided to the KubeVirt CSI cluster controller limits
PVC access only to the HCP namespace. This prevents the possibility of cross
namespace storage access by any KubeVirt CSI component.

These security enforcements ensure safe and isolated multitenant access to
shared infrastructure storage classes are possible for multiple HCP KubeVirt
guest clusters.

## KubeVirt VM Root Volume Configuration

The storage class used to host the KubeVirt Virtual Machine root volumes can be
configured at cluster creation time using the `hcp` cli tool and the
`--root-volume-storage-class` argument. Likewise, the size of the volume can be
configured using the `--root-volume-size` cli argument.

Below is an example of setting a custom storage class and volume size for the
KubeVirt VMs. The result will be a guest cluster with VMs hosted on 64Gi PVCs
hosted by the `ocs-storagecluster-ceph-rdb` storage class

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--root-volume-storage-class ocs-storagecluster-ceph-rbd \
--root-volume-size 64
```

## KubeVirt VM Image Caching

KubeVirt VM image caching is an advanced feature that can be used to optimize
both cluster startup time and storage utilization. This feature requires the
usage of a storage class capable of smart cloning and the `ReadWriteMany`
access mode.

Image caching works by importing the VM image once to a single PVC associated
with the HostedCluster, and then making a unique clone of that PVC for every
KubeVirt VM added as a worker node to the cluster. This reduces VM startup time
by only requiring a single image import, and can further reduce overall cluster
storage usage when the storage class supports copy on write cloning.

Image caching can be enabled during cluster creation using the `hcp` cli
tool with the `--root-volume-cache-strategy=PVC` argument. Below is an example.


```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--root-volume-cache-strategy=PVC
```

## ETCD Storage Configuration

The storage class used to host the etcd data can be customized at cluster
creation time using the `hcp` cli and the `--etcd-storage-class` cli
argument. When no `--etcd-storage-class` argument is provided, the default
storage class will be used.

Below is an example of how to configure usage of a storage class for etcd.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--etcd-storage-class="local"
```


---

## Source: docs/content/how-to/kubevirt/create-kubevirt-cluster.md

# Create a Kubevirt cluster

Install an OCP cluster running on VMs within a management OCP cluster

## Prerequisites

* Admin access to an OpenShift cluster (version 4.14+) specified by the `KUBECONFIG` environment variable.
* The management OCP cluster must have wildcard dns routes enabled. `oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'`
* The management OCP cluster must have Openshift Virtualization (4.14+) installed on it. Instructions for installing Openshift Virtualization
* The Management OCP cluster must be configured with OVNKubernetes as the default pod network CNI.
* The Management OCP cluster must have LoadBalancer service support. Instructions for installing MetalLB
* The management OCP cluster must have default storage class. Storage Configuration Documentation Example of how to set a default storage class: `oc patch storageclass ocs-storagecluster-ceph-rbd -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'`
* The OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`).
* A valid pull secret file for the `quay.io/openshift-release-dev` repository.
* A network MTU of 9000 or larger on the OCP cluster hosting the KubeVirt VMs is recommended for optimal network performance. Smaller MTU settings will work, but network latency and throughput of the hosted pods will be impacted. Multiqueue should only be enabled on NodePools when the MTU is 9000 or larger.

## Installing HyperShift Operator and cli tooling

Before creating a guest cluster, the hcp cli, hypershift cli, and HyperShift
Operator must be installed.

The `hypershift` cli tool is a development tool that is used to install
developer builds of the HyperShift Operator.

The `hcp` cli tool is used to manage the creation and destruction of guest
clusters.

### Build the HyperShift and HCP CLI

The command below builds latest hypershift and hcp cli tools from source and
places the cli tool within the /usr/local/bin directory.

!!! note

    The command below is the same if you use docker.

```shell
podman run --rm --privileged -it -v \
$PWD:/output docker.io/library/golang:1.20 /bin/bash -c \
'git clone https://github.com/openshift/hypershift.git && \
cd hypershift/ && \
make hypershift product-cli && \
mv bin/hypershift /output/hypershift && \
mv bin/hcp /output/hcp'

sudo install -m 0755 -o root -g root $PWD/hypershift /usr/local/bin/hypershift
sudo install -m 0755 -o root -g root $PWD/hcp /usr/local/bin/hcp
rm $PWD/hypershift
rm $PWD/hcp
```

## Deploy the HyperShift Operator

Use the hypershift cli tool to install the HyperShift operator into the
management cluster.

```shell
hypershift install
```

You will see the operator running in the `hypershift` namespace:

```sh
oc -n hypershift get pods

NAME                        READY   STATUS    RESTARTS   AGE
operator-755d587f44-lrtrq   1/1     Running   0          114s
operator-755d587f44-qj6pz   1/1     Running   0          114s
```

## Create a HostedCluster

Once all the prerequisites are met, and the HyperShift
operator is installed, it is now possible to create a guest cluster.

Below is an example of how to create a guest cluster using environment
variables and the `hcp` cli tool.

!!! note

    The --release-image flag could be used to provision the HostedCluster with a specific OpenShift Release (the hypershift operator has a support matrix of releases supported by a given version of the operator)


```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU
```

!!! note

    A default NodePool will be created for the cluster with 2 vm worker replicas
    per the `--node-pool-replicas` flag.

After a few moments we should see our hosted control plane pods up and running:

~~~sh
oc -n clusters-$CLUSTER_NAME get pods

NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-5cc7b74f47-n5gkr                        1/1     Running   0          3m
catalog-operator-5f799567b7-fd6jw                     2/2     Running   0          69s
certified-operators-catalog-784b9899f9-mrp6p          1/1     Running   0          66s
cluster-api-6bbc867966-l4dwl                          1/1     Running   0          66s
.
.
.
redhat-operators-catalog-9d5fd4d44-z8qqk              1/1     Running   0          66s
~~~

A guest cluster backed by KubeVirt virtual machines typically takes around 10-15
minutes to fully provision. The status of the guest cluster can be seen by
viewing the corresponding HostedCluster resource. For example, the output below
reflects what a fully provisioned HostedCluster object looks like.

```shell linenums="1"

oc get --namespace clusters hostedclusters

NAME            VERSION   KUBECONFIG                       PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
example         4.14.0    example-admin-kubeconfig         Completed  True        False         The hosted control plane is available
```

## Accessing the HostedCluster

CLI access to the guest cluster is gained by retrieving the guest cluster's
kubeconfig. Below is an example of how to retrieve the guest cluster's
kubeconfig using the hcp cli.

```shell
hcp create kubeconfig --name $CLUSTER_NAME > $CLUSTER_NAME-kubeconfig
```

If we access the cluster, we will see we have two nodes.

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes

NAME                  STATUS   ROLES    AGE   VERSION
example-n6prw         Ready    worker   32m   v1.27.4+18eadca
example-nc6g4         Ready    worker   32m   v1.27.4+18eadca
```

We can also check the ClusterVersion:

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get clusterversion

NAME      VERSION       AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.14.0        True        False         5m39s   Cluster version is 4.14.0
```

## Influencing VM Scheduling

By default, KubeVirt VMs created by a NodePool are scheduled to any available
node with the appropriate capacity to run the VM. A topologySpreadConstraint
is used to try to spread VMs for a NodePool out across multiple underlying
nodes, but in general it is possible for the VMs to be scheduled to any node
that meets the requirements for running KubeVirt VMs.
For availability reasons, the topologySpreadConstraint is only a soft constraint
(`whenUnsatisfiable: ScheduleAnyway` policy) so, under certain circumstances,
VMs for a single nodepool can ‚Äúclump‚Äù together on a single node due to live migration
to available nodes.
The De-Scheduler can optionally be used to continuously redistribute VMs so that
they again satisfy the topologySpreadConstraint as soon as possible.
To achieve that behavior, the cluster-kube-descheduler-operator should be installed
and configured with something like:
```yaml
apiVersion: operator.openshift.io/v1
kind: KubeDescheduler
metadata:
  name: cluster
  namespace: openshift-kube-descheduler-operator
spec:
  mode: Automatic
  managementState: Managed
  deschedulingIntervalSeconds: 60
  profiles:
  - SoftTopologyAndDuplicates
  - EvictPodsWithPVC
  - EvictPodsWithLocalStorage
  profileCustomizations:
    devEnableEvictionsInBackground: true
```

It is possible to influence where the KubeVirt VMs within a NodePool are
scheduled through the use of NodeSelectors. Below is an example of using a
NodeSelector on the VMs to place the VMs only on nodes with a specific label
through usage of the hcp command line and the --vm-node-selector argument.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--vm-node-selector labelKey1=labelVal1,labeKey2=labelVal2
```

In the example above, the KubeVirt VMs will only be scheduled to nodes that
 contain the labels labelKey1=labelVal1 and labelKey2=labelVal2.

## Scaling an existing NodePool

Manually scale a NodePool using the `oc scale` command:

```shell linenums="1"
NODEPOOL_NAME=$CLUSTER_NAME
NODEPOOL_REPLICAS=5

oc scale nodepool/$NODEPOOL_NAME --namespace clusters --replicas=$NODEPOOL_REPLICAS
```

After a while, in our hosted cluster this is what we will see:

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes

NAME                  STATUS   ROLES    AGE     VERSION
example-9jvnf         Ready    worker   97s     v1.27.4+18eadca
example-n6prw         Ready    worker   116m    v1.27.4+18eadca
example-nc6g4         Ready    worker   117m    v1.27.4+18eadca
example-thp29         Ready    worker   4m17s   v1.27.4+18eadca
example-twxns         Ready    worker   88s     v1.27.4+18eadca
```

## Adding Additional NodePools

Create additional NodePools for a guest cluster by specifying a name, number of
replicas, and any additional information such as memory and cpu requirements.

For example, let's create a NodePool with more CPUs assigned to the VMs (4 vs 2):

```shell linenums="1"
export NODEPOOL_NAME=$CLUSTER_NAME-extra-cpu
export WORKER_COUNT="2"
export MEM="6Gi"
export CPU="4"
export DISK="16"

hcp create nodepool kubevirt \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --replicas $WORKER_COUNT \
  --memory $MEM \
  --cores $CPU \
  --root-volume-size $DISK
```

Check the status of the NodePool by listing `nodepool` resources in the `clusters`
namespace:

```shell
oc get nodepools --namespace clusters

NAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
example                   example         5               5               False         False        4.14.0  
example-extra-cpu         example         2                               False         False                  True              True             Minimum availability requires 2 replicas, current 0 available
```

After a while, in our hosted cluster this is what we will see:

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes

NAME                      STATUS   ROLES    AGE     VERSION
example-9jvnf             Ready    worker   97s     v1.27.4+18eadca
example-n6prw             Ready    worker   116m    v1.27.4+18eadca
example-nc6g4             Ready    worker   117m    v1.27.4+18eadca
example-thp29             Ready    worker   4m17s   v1.27.4+18eadca
example-twxns             Ready    worker   88s     v1.27.4+18eadca
example-extra-cpu-zh9l5   Ready    worker   2m6s    v1.27.4+18eadca
example-extra-cpu-zr8mj   Ready    worker   102s    v1.27.4+18eadca
```

And the nodepool will be in the desired state:

```shell
oc get nodepools --namespace clusters

NAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
example                   example         5               5               False         False        4.14.0  
example-extra-cpu         example         2               2               False         False        4.14.0  
```

## Delete a HostedCluster

To delete a HostedCluster:

```shell
hcp destroy cluster kubevirt --name $CLUSTER_NAME
```



---

## Source: docs/content/how-to/kubevirt/external-infrastructure.md

# External Infrastructure

By default, the HyperShift operator hosts both the HostedCluster's control
plane pods and KubeVirt worker VMs within the same cluster.

With the external infrastructure feature, it possible to place the worker node
VMs on a separate cluster from the control plane pods.

## Understanding Hosting Cluster Types

**Management Cluster:** This is the OpenShift cluster that runs the HyperShift
operator and hosts the control plane pods for a HostedCluster.

**Infrastructure Cluster:** This is the OpenShift cluster that runs the
KubeVirt worker VMs for a HostedCluster.

By default, the management cluster also acts as the infrastructure cluster
that hosts VMs. However, for the external infrastructure use case, the
management and infrastructure clusters are distinctly different.

## Create a HostedCluster using External Infrastructure

**Prerequisites**
 * Creation of a namespace on the external infrastructure cluster for the KubeVirt worker nodes to be hosted in.
 * A kubeconfig for the external infrastructure cluster

Once the prerequisites are met, the `hcp` cli tool can be used to create
the guest cluster. In order to place the KubeVirt worker VMs on the
infrastructure cluster, use the `--infra-kubeconfig-file` and `--infra-namespace`
arguments.

Below is an example of creating a guest cluster using external infrastructure.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--infra-namespace=clusters-example \
--infra-kubeconfig-file=$HOME/external-infra-kubeconfig
```

This command will result in the control plane pods being hosted on the
management cluster that the HyperShift Operator runs on, while the KubeVirt
VMs will be hosted on a separate infrastructure cluster.

## Required RBAC for the external infrastructure cluster
It isn't necessary for the user defined in the kubeconfig used for the external infra cluster to be a cluster-admin.  
The user or service account used in the provided kubeconfig should have full permissions over the following resources:
* `virtualmachines.kubevirt.io`
* `virtualmachineinstances.kubevirt.io`
* `virtualmachines.kubevirt.io/finalizers`
* `datavolumes.cdi.kubevirt.io`
* `services`
* `endpointslices`
* `endpointslices/restricted`
* `routes`
The user or service account used in the provided kubeconfig should also have get/create/delete permissions over the following resources:
* `volumesnapshots`
As well as get permission for:
* `persistentvolumeclaims`

All of these permissions are needed only on the target namespace on the infra cluster (passed through the `--infra-namespace` command-line argument).
This can be achieved by binding the following Role to the user used in the external infra kubeconfig:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kv-external-infra-role
  namespace: clusters-example
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachines/finalizers
      - virtualmachineinstances
    verbs:
      - '*'
  - apiGroups:
      - cdi.kubevirt.io
    resources:
      - datavolumes
    verbs:
      - '*'
  - apiGroups:
      - ''
    resources:
      - services
    verbs:
      - '*'
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
      - routes/custom-host
    verbs:
      - '*'
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
      - endpointslices/restricted
    verbs:
      - '*'
  - apiGroups:
      - ''
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
    - snapshot.storage.k8s.io
    resources:
    - volumesnapshots
    verbs:
    - get
    - create
    - delete
  - apiGroups:
    - ''
    resources:
    - persistentvolumeclaims
    verbs:
    - get
```


---

## Source: docs/content/how-to/kubevirt/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/kubevirt/gpu-devices.md

# GPU Devices

It is possible to attach one or more GPUs to a HCP KubeVirt clusters through
the use of the NodePool api.

## Prerequisites

Before a GPU device can be attached to a HCP KubeVirt NodePool, the GPU must
first be exposed as a resource on the Node that the GPU resides on. For
example this Nvidia Operator documentation outlines how to expose Nvidia GPUs
as a resource on an OpenShift cluster. Once the GPU is exposed as an
extended resource
on the node, it can then be assigned to a NodePool. 

## Attaching GPU devices to NodePools Using the CLI

Using the `hcp` cli, GPU devices can be attached to NodePools through the
usage of the `--host-device-name` cli argument.


This argument takes the device's resource name as it has been advertised on
the infrastructure node. The argument also takes an optional count that
represents the number of GPU devices to attach to each VM in the NodePool.
By default, the count is `1`.

In the example below, we are attaching 2 GPUs to a NodePool of size 3. This
means each of the 3 VMs in the NodePool will have 2 GPUs each.


```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="16Gi"
export CPU="2"
export WORKER_COUNT="3"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--host-device-name="nvidia-a100,count:2"
```

Note that the `--host-device-name` argument can be specified multiple times
in order to attach multiple devices of different types.

## Attaching GPU Devices via the NodePool API Directly

The same example provided above with the hcp cli can also be expressed
directly using the NodePool API. Below is an example of what the yaml
would look like to configure a NodePool with 3 replicas with 2 GPU devices
for each replica. Take note of the `nodepool.spec.platform.kubevirt.hostDevices`
section. The `hostDevices` are a list, meaning it is possible to attach
multiple GPU devices of different types as well.

```yaml linenums="1"
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: example
  namespace: clusters
spec:
  arch: amd64
  clusterName: example
  management:
    autoRepair: false
    upgradeType: Replace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    kubevirt:
      attachDefaultNetwork: true
      compute:
        cores: 2
        memory: 16Gi
      hostDevices:
      - count: 2
        deviceName: nvidia-a100
      networkInterfaceMultiqueue: Enable
      rootVolume:
        persistent:
          size: 32Gi
        type: Persistent
    type: KubeVirt
  replicas: 3
```



---

## Source: docs/content/how-to/kubevirt/ingress-and-dns.md

# Ingress and DNS configuration

By default, the HyperShift operator will configure the KubeVirt platform guest
cluster's ingress and DNS behavior to reuse what is provided by the underlying
infra cluster that the KubeVirt VMs are running on. This section describes
that default behavior in greater detail as well as information on advanced usage
options.

## Default Ingress and DNS Behavior

Every OpenShift cluster comes setup with a default application ingress
controller which is expected to have an wildcard DNS record associated with it.
By default, guest clusters created using the Hypershift KubeVirt provider
will automatically become a subdomain of the underlying OCP cluster that
the KubeVirt VMs run on.

For example, if an OCP cluster has a default ingress DNS entry of
`*.apps.mgmt-cluster.example.com`, then the default ingress of a KubeVirt
guest cluster named `guest` running on that underlying OCP cluster will
be `*.apps.guest.apps.mgmt-cluster.example.com`.

!!! note

    For this default ingress DNS to work properly, the underlying cluster
    hosting the KubeVirt VMs must allow wildcard DNS routes. This can be
    configured using the following cli command. ```oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'```

!!! note

    When using the default guest cluster ingress, connectivity is limited to HTTPS
    traffic over port 443. Plain HTTP traffic over port 80 will be rejected. This
    limitation only applies to the default ingress behavior and not the custom ingress
    behavior where manual creation of an ingress LoadBalancer and DNS is performed.

## Customized Ingress and DNS Behavior

In lieu of the default ingress and DNS behavior, it is also possible to
configure a Hypershift KubeVirt guest cluster with a unique base domain
at creation time. This option does require some manual configuration
steps during creation though.

This process involves three steps:

1. Cluster creation
2. LoadBalancer creation
3. Wildcard DNS configuration

### Step 1 - Deploying the HostedCluster specifying our base domain

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"
export BASE_DOMAIN=hypershift.lab

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--base-domain $BASE_DOMAIN
```

With above configuration we will end up having a HostedCluster with an ingress wildcard configured for `*.apps.example.hypershift.lab` (*.apps.<hostedcluster_name\>.<base_domain\>).

This time, the HostedCluster will not finish the deployment (will remain in `Partial` progress) as we saw in the previous section, since we have configured a base domain we need to make sure that the required DNS records and load balancer are in-place:

```shell linenums="1"
oc get --namespace clusters hostedclusters

NAME            VERSION   KUBECONFIG                       PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
example                   example-admin-kubeconfig         Partial    True        False         The hosted control plane is available
```

If we access the HostedCluster this is what we will see:

```shell
hcp create kubeconfig --name $CLUSTER_NAME > $CLUSTER_NAME-kubeconfig
```

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get co

NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.14.0    False       False         False      30m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.example.hypershift.lab): Get "https://console-openshift-console.apps.example.hypershift.lab": dial tcp: lookup console-openshift-console.apps.example.hypershift.lab on 172.31.0.10:53: no such host
.
.
.
ingress                                    4.14.0    True        False         True       28m     The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)
```

In the next section we will fix that.

### Step 2 - Set up the LoadBalancer


!!! note

    If your cluster is on bare-metal you may need MetalLB to be able to provision functional LoadBalancer services. Take a look at the section Optional MetalLB Configuration Steps.

This option requires configuring a new LoadBalancer service that routes to the KubeVirt VMs as well as assign a wildcard DNS entry to the LoadBalancer's IP address.

First, we need to create a LoadBalancer Service that routes ingress traffic to the KubeVirt VMs.

A NodePort Service exposing the HostedCluster ingress already exists, we will grab the NodePorts and create the LoadBalancer service targeting these ports.

1. Grab NodePorts

    ```sh
    export HTTP_NODEPORT=$(oc --kubeconfig $CLUSTER_NAME-kubeconfig get services -n openshift-ingress router-nodeport-default -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}')
    export HTTPS_NODEPORT=$(oc --kubeconfig $CLUSTER_NAME-kubeconfig get services -n openshift-ingress router-nodeport-default -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
    ```

2. Create LoadBalancer Service

    ```sh
    cat << EOF | oc apply -f -
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: $CLUSTER_NAME
      name: $CLUSTER_NAME-apps
      namespace: clusters-$CLUSTER_NAME
    spec:
      ports:
      - name: https-443
        port: 443
        protocol: TCP
        targetPort: ${HTTPS_NODEPORT}
      - name: http-80
        port: 80
        protocol: TCP
        targetPort: ${HTTP_NODEPORT}
      selector:
        kubevirt.io: virt-launcher
      type: LoadBalancer
    EOF
    ```

### Step 3 - Set up a wildcard DNS record for the `*.apps`

Now that we have the ingress exposed, next step is configure a wildcard DNS A record or CNAME that references the LoadBalancer Service's external IP.

1. Get the external IP.

  ```shell
  export EXTERNAL_IP=$(oc -n clusters-$CLUSTER_NAME get service $CLUSTER_NAME-apps -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  ```

2. Configure a wildcard `*.apps.<hostedcluster_name\>.<base_domain\>.` DNS entry referencing the IP stored in $EXTERNAL_IP that is routable both internally and externally of the cluster.

For example, for the cluster used in this example and for an external ip value of `192.168.20.30` this is what DNS resolutions will look like:

```sh
dig +short test.apps.example.hypershift.lab

192.168.20.30
```

### Checking HostedCluster status after having fixed the ingress

Now that we fixed the ingress, we should see our HostedCluster progress moved from `Partial` to `Completed`.

```shell linenums="1"
oc get --namespace clusters hostedclusters

NAME            VERSION   KUBECONFIG                       PROGRESS    AVAILABLE   PROGRESSING   MESSAGE
example         4.14.0    example-admin-kubeconfig         Completed   True        False         The hosted control plane is available
```

## Optional MetalLB Configuration Steps

LoadBalancer type services are required. If MetalLB is in use, here are some example steps
outlining how to configure MetalLB after installing MetalLB using CLI.

1. Create a MetalLB instance:

    ```shell
    oc create -f - <<EOF
    apiVersion: metallb.io/v1beta1
    kind: MetalLB
    metadata:
      name: metallb
      namespace: metallb-system
    EOF
    ```

2. Create address pool with an available range of IP addresses within the node network:

    ```shell
    oc create -f - <<EOF
    apiVersion: metallb.io/v1beta1
    kind: IPAddressPool
    metadata:
      name: metallb
      namespace: metallb-system
    spec:
      addresses:
      - 192.168.216.32-192.168.216.122
    EOF
    ```

3. Advertise the address pool using L2 protocol:

    ```shell
    oc create -f - <<EOF
    apiVersion: metallb.io/v1beta1
    kind: L2Advertisement
    metadata:
      name: l2advertisement
      namespace: metallb-system
    spec:
      ipAddressPools:
       - metallb
    EOF
    ```


---

## Source: docs/content/how-to/kubevirt/performance-tuning.md

# Performance Tuning

By default, KubeVirt VMs use best effort resource requests for CPU. This means
 it‚Äôs possible for a VM to have overcommitted CPU which are in competition
with other resources on the host machine. While best effort CPU will provide
the best utilization of resources on a node, it could impact performance of
the VM in high density applications.

## Guaranteed CPU QOS

To avoid overcommitment of CPU, the KubeVirt NodePool can request guaranteed
CPU qos, which will guarantee unshared CPU resources to the KubeVirt VMs. Below
is an example of using the hcp command line tool to request a KubeVirt platform
guest cluster with guaranteed CPU by using the --qos-class argument.

```shell linenums="1"
export CLUSTER_NAME=example
export PULL_SECRET="$HOME/pull-secret"
export MEM="6Gi"
export CPU="2"
export WORKER_COUNT="2"

hcp create cluster kubevirt \
--name $CLUSTER_NAME \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--memory $MEM \
--cores $CPU \
--qos-class Guaranteed
```


---

## Source: docs/content/how-to/kubevirt/troubleshooting-kubevirt-cluster.md

# Troubleshooting a KubeVirt cluster

Troubleshooting why a Hosted Control Plane with the KubeVirt platform is not
coming fully online should involve starting at the top level HostedCluster
and NodePool resources, and then working down through the stack to gain
a more detailed understanding of what has occurred until a root cause is
found.

Below are some steps to help guide that progression and discover the root
cause for the most common classes of issues encountered with the HCP
KubeVirt platform.

## HostedCluster Stuck in Partial State

* Ensure all prerequisites for the HCP KubeVirt platform are met
* Look at conditions on HostedCluster and NodePool objects for validation errors that prevent progress from being made.
* Using the kubeconfig of the guest cluster, inspect the guest cluster‚Äôs status. Look at the status of the `oc get clusteroperators` output to see what cluster operators are pending. Look at the node status `oc get nodes` to ensure the worker nodes are in a Ready state.

## HCP has no worker nodes registered

* Look at HostedCluster and NodePool conditions for failures that indicate what the problem could be.
* Look at the KubeVirt worker node VM status for the NodePool. `oc get vm -n <namespace>`
* If the VMs are stuck in provisioning, look at the CDI import pods within the VM‚Äôs namespace for clues as to why the importer pods have not completed. `oc get pods -n <namespace> | grep "import"`
* If the VMs are stuck in starting, look to see what the status of the virt-launcher pods are. `oc get pods -n <namespace> -l kubevirt.io=virt-launcher`. If the virt-launcher pods are in a pending state, determine why the pods are not being scheduled. For example, there could be a lack of resources to run the virt-launcher pods.
* If the VMs are running but they have not registered as worker nodes, use the web console to gain VNC access to one of the impacted VMs. The VNC output should indicate that the ignition config was applied. If the VM is unable to access the HCP ignition server on startup, that will prevent the VM from being provisioned correctly.
* If the ignition config has been applied but the VM is still not registering as a node, Proceed to the Fetching VM Bootstrap logs section to gain access to the VM console logs during boot.

## HCP Worker Nodes Stuck in NotReady State

* During cluster creation, nodes will enter the "NotReady" state temporarily while the networking stack is rolling out. This is a part of normal operation. If this time period takes longer than 10-15 minutes, then it's possible an issue has occurred that needs investigation.
* Look at the conditions on the node object to determine why the node is not ready. `oc get nodes -o yaml`
* Look for failing pods within the cluster `oc get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded`

## Ingress and Console cluster operators are not coming online

* If the cluster is using the default ingress behavior, ensure that wildcard DNS routes are enabled on the OCP cluster the VMs are hosted on. `oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'`
* If a custom base domain is used for the HCP, double check that the Load Balancer is targeting the VM pods accurately, and make sure the wildcard DNS entry is targeting the Load Balancer IP.

## Guest Cluster Load Balancer services are not becoming available

* Look for events and details associated with the Load Balancer service within the guest cluster
* Load Balancers for the guest cluster are by default handled by the kubevirt-cloud-controller-manager within the HCP namespace. Ensure the kccm pod is online and look at the pod‚Äôs logs for errors or warnings. The kccm pod can be identified in the HCP namespace with this command `oc get pods -n <hcp namespace> -l app=cloud-controller-manager`

## Guest Cluster PVCs are not available

* Look for events and details associated with the PVC to understand what errors are occurring
* If the PVC is failing to attach to a Pod, look at the logs for the kubevirt-csi-node daemonset component within the guest to gain further insight into why that might be occurring. The kubevirt-csi-node pods can be identified for each node with this command `oc get pods -n openshift-cluster-csi-drivers -o wide -l app=kubevirt-csi-driver`
* If the PVC is unable to bind to a PV, look at the logs of the kubevirt-csi-controller component within the HCP namespace. The kubevirt-csi-controller pod can be identified within the hcp namespace with this command `oc get pods -n <hcp namespace> -l app=kubevirt-csi-driver`

## Fetching VM bootstrap logs

The VM console logs are really useful to troubleshoot issues when the HyperShift/KubeVirt VM nodes are not correctly joining the cluster.
KubeVirt v1.1.0 will expose logs from the serial console of guest VMs in a k8s native way (`kubectl logs -n <namespace> <vmi_pod> -c guest-console-log`) but his is still not available with KubeVirt v1.0.0.

On KubeVirt v1.0.0 you can use a helper script to stream them from interactive console executed in background.

```bash
#!/bin/bash
HC_NAMESPACE="${HC_NAMESPACE:-clusters}"
NAME=$1

if [[ -z "${NAME}" ]]
then
    echo "Please specify the name of the guest cluster."
    exit 1
fi

VMNS="${HC_NAMESPACE}"-"${NAME}"
REPLICAS=$(oc get NodePool -n "${HC_NAMESPACE}" "${NAME}" -o=jsonpath='{.spec.replicas}')
PLATFORMTYPE=$(oc get NodePool -n "${HC_NAMESPACE}" "${NAME}" -o=jsonpath='{.spec.platform.type}')
INFRAID=$(oc get HostedCluster -n "${HC_NAMESPACE}" "${NAME}" -o=jsonpath='{.spec.infraID}')

if [[ "${PLATFORMTYPE}" != "KubeVirt" ]]; then
    echo "This tool is designed for the KubeVirt provider."
    exit 1
fi

if ! which tmux >/dev/null 2>&1;
then
    echo "this tool requires tmux, please install it."
    exit 1
fi

VMNAMES=()

while [[ ${#VMNAMES[@]} < ${REPLICAS}  ]]; do
  for VMNAME in $(oc get vmi -n "${VMNS}" -l hypershift.openshift.io/infra-id="${INFRAID}" -o name 2>/dev/null); do
    SVMNAME=${VMNAME/virtualmachineinstance.kubevirt.io\//}
    if ! [[ " ${VMNAMES[*]} " =~ ${SVMNAME} ]]; then
	   VMNAMES+=("${SVMNAME}")
	   tmux new-session -s "${SVMNAME}" -d "virtctl console --timeout 30 -n ${VMNS} ${SVMNAME} | tee -a ${VMNS}_${SVMNAME}.log"
	   echo "logs for VM ${SVMNAME} will be appended to ${VMNS}_${SVMNAME}.log"
    fi
  done
  sleep 3
done

echo "Log collection will continue in background while the VMs are running."
echo "Please avoid trying to directly connect to VM console with 'virtctl console' to avoid hijacking open sessions:"
echo "you can instead use 'tmux attach -t <vmname>' to reach open session, this will not break file logging."
```

You can locally save the script and execute it as `hypershift_kv_log.sh <guestclustername>` as soon as you created the `hostedcluster` object (the script will not collect logs from the past).
The script will loop until all the expected VMs are created and then log collection will continue in background in `tmux` sessions until the VMs are live.
Please avoid directly connecting to VM console with `virtctl console` to avoid hijacking open sessions braking the logging, you can instead use `tmux attach -t <vmname>` and interactively use the serial console from there.
If the namespace used on the hosted cluster is not named `clusters`, a custom value could be set with the `HC_NAMESPACE` env variable.


---

## Source: docs/content/how-to/metrics-sets.md

# Configure Metrics Sets

HyperShift creates ServiceMonitor resources in each control plane namespace that allow
a Prometheus stack to scrape metrics from the control planes. ServiceMonitors use metrics relabelings
to define which metrics are included or excluded from a particular component (etcd, Kube API server, etc)
The number of metrics produced by control planes has a direct impact on resource requirements of
the monitoring stack scraping them.

Instead of producing a fixed number of metrics that apply to all situations, HyperShift allows
configuration of a "metrics set" that identifies a set of metrics to produce per control plane.

The following metrics sets are supported:

* `Telemetry` - metrics needed for telemetry. This is the default and the smallest
   set of metrics.
* `SRE` - Configurable metrics set, intended to include necessary metrics to produce alerts and
   allow troubleshooting of control plane components.
* `All` - all the metrics produced by standalone OCP control plane components.

The metrics set is configured by setting the `METRICS_SET` environment variable in the HyperShift
operator deployment:

```
oc set env -n hypershift deployment/operator METRICS_SET=All
```

## Configuring the SRE Metrics Set

When the SRE metrics set is specified, the HyperShift operator looks for a ConfigMap named
`sre-metric-set` with a single key: `config`. The value of the `config` key should contain a set
of RelabelConfigs organized by control plane component. An example of this configuration can be
found in `support/metrics/testdata/sreconfig.yaml` in this repository.

The following components can be specified:

* etcd
* kubeAPIServer
* kubeControllerManager
* openshiftAPIServer
* openshiftControllerManager
* openshiftRouteControllerManager
* cvo
* olm
* catalogOperator
* registryOperator
* nodeTuningOperator
* controlPlaneOperator
* hostedClusterConfigOperator

---

## Source: docs/content/how-to/none/create-none-cluster.md

# Create a None cluster

This document explains how to create HostedClusters and NodePools using the 'None' platform to create bare metal worker nodes.

## Hypershift Operator requirements

* cluster-admin access to an OpenShift Cluster (We tested it with 4.9.17) to deploy the CRDs + operator
* 1 x filesystem type Persistent Volume to store the `etcd` database for demo purposes (3x for 'production' environments)

## Versions used

* OCP compact cluster (3 masters) version 4.9.17
* HyperShift Operator built from sources (Commit ID 0371f889)

## Prerequisites: Building the Hypershift Operator

Currently, the HyperShift operator is deployed using the `hypershift` binary, which needs to be compiled manually.
RHEL8 doesn't include go1.18 officially but it can be installed via `gvm` by following the next steps:

~~~sh
# Install prerequisites
sudo dnf install -y curl git make bison gcc glibc-devel
git clone https://github.com/openshift/hypershift.git
pushd hypershift

# Install gvm to install go 1.18
bash < <(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)
source ${HOME}/.gvm/scripts/gvm
gvm install go1.18
gvm use go1.18

# build the binary
make hypershift
popd
~~~

Then, the `hypershift` binary can be moved to a convenient place as:

~~~sh
sudo install -m 0755 -o root -g root hypershift/bin/hypershift /usr/local/bin/hypershift
~~~

Alternatively, it can be compiled using a container as:

~~~sh
# Install prerequisites
sudo dnf install podman -y
# Compile hypershift
mkdir -p ./tmp/ && \
podman run -it -v ${PWD}/tmp:/var/tmp/hypershift-bin/:Z --rm docker.io/golang:1.18 sh -c \
  'git clone --depth 1 https://github.com/openshift/hypershift.git /var/tmp/hypershift/ && \
  cd /var/tmp/hypershift && \
  make hypershift && \
  cp bin/hypershift /var/tmp/hypershift-bin/'
sudo install -m 0755 -o root -g root ./tmp/hypershift /usr/local/bin/hypershift
~~~

> **WARNING**: At the time of writing this document, there were some issues already fixed in HyperShift but unfortunately those weren't included in the latest release of the container.

## Prerequisite (optional): Create a custom HyperShift image

The official container image containing the HyperShift bits is hosted in quay.io/hypershift/hypershift but if creating a custom HyperShift image is needed, the following steps can be performed:

~~~sh
QUAY_ACCOUNT='testuser'
podman login -u ${QUAY_ACCOUNT} -p testpassword quay.io
sudo dnf install -y curl git make bison gcc glibc-devel
git clone https://github.com/openshift/hypershift.git
pushd hypershift

# Install gvm to install go 1.18
bash < <(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)
source ${HOME}/.gvm/scripts/gvm
gvm install go1.18 -B
gvm use go1.18

# Build the binaries and the container
make build
make RUNTIME=podman IMG=quay.io/${QUAY_ACCOUNT}/hypershift:latest docker-build docker-push

sudo install -m 0755 -o root -g root hypershift/bin/hypershift /usr/local/bin/hypershift
popd
~~~

## Deploy HyperShift

Once the binary is in place, the operator deployment is performed as:

~~~sh
hypershift install
~~~

Or if using a custom image:

~~~sh
hypershift install --hypershift-image quay.io/${QUAY_ACCOUNT}/hypershift:latest
~~~

Using `hypershift install --render > hypershift-install.yaml` will create a yaml file with all the assets required to deploy HyperShift, then they can be applied as:

~~~sh
oc apply -f ./hypershift-install.yaml
~~~

## Deploy a hosted cluster

There are two main CRDs to describe a hosted cluster:

* `HostedCluster` defines the control plane hosted in the management OpenShift

* `NodePool` defines the nodes that will be created/attached to a hosted cluster

The `hostedcluster.spec.platform` specifies the underlying infrastructure provider for the cluster and is used to configure platform specific behavior, so depending on the environment it is required to configure it properly.

In this repo we will cover the 'none' provider.

### Requisites

* Proper DNS entries for the workers (if the worker uses `localhost` it won't work)
* DNS entry to point `api.${cluster}.${domain}` to each of the nodes where the hostedcluster will be running. This is because the hosted cluster API is exposed as a `nodeport`. For example:

~~~sh
api.hosted0.example.com.  IN A  10.19.138.32
api.hosted0.example.com.  IN A  10.19.138.33
api.hosted0.example.com.  IN A  10.19.138.37
~~~

* DNS entry to point `*.apps.${cluster}.${domain}` to a load balancer deployed to redirect incoming traffic to the ingresses pod the OpenShift documentation provides some instructions about this)
> **NOTE**: This is not strictly required to deploy a sample cluster but to access the exposed routes there. Also, it can be simply an A record pointing to a worker IP where the ingress pods are running and enabling the `hostedcluster.spec.infrastructureAvailabilityPolicy: SingleReplica` configuration parameter.

* Pull-secret (available at cloud.redhat.com)
* ssh public key already available (it can be created as `ssh-keygen -t rsa -f /tmp/sshkey -q -N ""`)
* Any httpd server available to host a ignition file (text) and a modified RHCOS iso

### Procedure

* Create a file containing all the variables depending on the environment:

~~~sh
cat <<'EOF' > ./myvars
export CLUSTERS_NAMESPACE="clusters"
export HOSTED="hosted0"
export HOSTED_CLUSTER_NS="clusters-${HOSTED}"
export PULL_SECRET_NAME="${HOSTED}-pull-secret"
export MACHINE_CIDR="10.19.138.0/24"
export OCP_RELEASE_VERSION="4.9.17"
export OCP_ARCH="x86_64"
export BASEDOMAIN="example.com"

export PULL_SECRET_CONTENT=$(cat ~/clusterconfigs/pull-secret.txt)
export SSH_PUB=$(cat ~/.ssh/id_rsa.pub)
EOF
source ./myvars
~~~

* Create a namespace to host the HostedCluster and secrets

~~~sh
envsubst <<"EOF" | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
 name: ${CLUSTERS_NAMESPACE}
EOF

export PS64=$(echo -n ${PULL_SECRET_CONTENT} | base64 -w0)
envsubst <<"EOF" | oc apply -f -
apiVersion: v1
data:
 .dockerconfigjson: ${PS64}
kind: Secret
metadata:
 name: ${PULL_SECRET_NAME}
 namespace: ${CLUSTERS_NAMESPACE}
type: kubernetes.io/dockerconfigjson
EOF

envsubst <<"EOF" | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: ${HOSTED}-ssh-key
  namespace: ${CLUSTERS_NAMESPACE}
stringData:
  id_rsa.pub: ${SSH_PUB}
EOF
~~~

* Create the `hostedcluster` and the `nodepool` objects:

~~~sh
envsubst <<"EOF" | oc apply -f -
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: ${HOSTED}
  namespace: ${CLUSTERS_NAMESPACE}
spec:
  release:
    image: "quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE_VERSION}-${OCP_ARCH}"
  pullSecret:
    name: ${PULL_SECRET_NAME}
  sshKey:
    name: "${HOSTED}-ssh-key"
  networking:
    serviceCIDR: "172.31.0.0/16"
    podCIDR: "10.132.0.0/14"
    machineCIDR: "${MACHINE_CIDR}"
  platform:
    type: None
  infraID: ${HOSTED}
  dns:
    baseDomain: ${BASEDOMAIN}
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: api.${HOSTED}.${BASEDOMAIN}
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: api.${HOSTED}.${BASEDOMAIN}
      type: NodePort
  - service: OIDC
    servicePublishingStrategy:
      nodePort:
        address: api.${HOSTED}.${BASEDOMAIN}
      type: None
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: api.${HOSTED}.${BASEDOMAIN}
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: api.${HOSTED}.${BASEDOMAIN}
      type: NodePort
EOF

envsubst <<"EOF" | oc apply -f -
apiVersion: hypershift.openshift.io/v1alpha1
kind: NodePool
metadata:
  name: ${HOSTED}-workers
  namespace: ${CLUSTERS_NAMESPACE}
spec:
  clusterName: ${HOSTED}
  replicas: 0
  management:
    autoRepair: false
    upgradeType: Replace
  platform:
    type: None
  release:
    image: "quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE_VERSION}-${OCP_ARCH}"
EOF
~~~

> **NOTE**: The `HostedCluster` and `NodePool` objects can be created using the `hypershift` binary as `hypershift create cluster`. See the `hypershift create cluster -h` output for more information.

After a while, a number of pods will be created in the `${CLUSTERS_NAMESPACE}` namespace. Those pods are the control plane of the hosted cluster.

~~~sh
oc get pods -n ${HOSTED_CLUSTER_NS}

NAME                                              READY   STATUS     RESTARTS        AGE
catalog-operator-54d47cbbdb-29mzf                 2/2     Running    0               6m24s
certified-operators-catalog-78db79f86-6hlk9       1/1     Running    0               6m30s
cluster-api-655c8ff4fb-598zs                      1/1     Running    1 (5m57s ago)   7m26s
cluster-autoscaler-86d9474fcf-rmwzr               0/1     Running    0               6m11s
cluster-policy-controller-bf87c9858-nnlgw         1/1     Running    0               6m37s
cluster-version-operator-ff9475794-dc9hf          2/2     Running    0               6m37s
community-operators-catalog-6f5797cdc4-2hlcp      1/1     Running    0               6m29s
control-plane-operator-749b94cf54-p2lg2           1/1     Running    0               7m23s
etcd-0                                            1/1     Running    0               6m46s
hosted-cluster-config-operator-6646d8f868-h9r2w   0/1     Running    0               6m34s
ignition-server-7797c5f7-vkb2b                    1/1     Running    0               7m20s
ingress-operator-5dc47b99b7-jttpg                 0/2     Init:0/1   0               6m35s
konnectivity-agent-85f979fcb4-67c5h               1/1     Running    0               6m45s
konnectivity-server-576dc7b8b7-rxgms              1/1     Running    0               6m46s
kube-apiserver-66d99fd9fb-dvslc                   2/2     Running    0               6m43s
kube-controller-manager-68dd9fb75f-mgd22          1/1     Running    0               6m42s
kube-scheduler-748d9f5bcb-mlk52                   0/1     Running    0               6m42s
machine-approver-c8c68ffb9-psc6n                  0/1     Running    0               6m11s
oauth-openshift-7fc7dc9c66-fg258                  1/1     Running    0               6m8s
olm-operator-54d7d78b89-f9dng                     2/2     Running    0               6m22s
openshift-apiserver-64b4669d54-ffpw2              2/2     Running    0               6m41s
openshift-controller-manager-7847ddf4fb-x5659     1/1     Running    0               6m38s
openshift-oauth-apiserver-554c449b8f-lk97w        1/1     Running    0               6m41s
packageserver-6fd9f8479-pbvzl                     0/2     Init:0/1   0               6m22s
redhat-marketplace-catalog-8cc88f5cb-hbxv9        1/1     Running    0               6m29s
redhat-operators-catalog-b749d6945-2bx8k          1/1     Running    0               6m29s
~~~

The hosted cluster's kubeconfig can be extracted as:

~~~sh
oc extract -n ${CLUSTERS_NAMESPACE} secret/${HOSTED}-admin-kubeconfig --to=- > ${HOSTED}-kubeconfig
oc get clusterversion --kubeconfig=${HOSTED}-kubeconfig
~~~

### Adding a bare metal worker

* Download the RHCOS live ISO into the httpd server (for example into `/var/www/html/hypershift-none/` on an apache server hosted at www.example.com)

~~~sh
mkdir -p /var/www/html/hypershift-none/
curl -s -o /var/www/html/hypershift-none/rhcos-live.x86_64.iso https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.9/4.9.0/rhcos-live.x86_64.iso
~~~

* Download the ignition generated in the hostedcluster

~~~sh
IGNITION_ENDPOINT=$(oc get hc ${HOSTED} -o json | jq -r '.status.ignitionEndpoint')
IGNITION_TOKEN_SECRET=$(oc -n clusters-${HOSTED} get secret | grep token-${HOSTED}  | awk '{print $1}')
set +x
IGNITION_TOKEN=$(oc -n clusters-${HOSTED} get secret ${IGNITION_TOKEN_SECRET} -o jsonpath={.data.token})
curl -s -k -H "Authorization: Bearer ${IGNITION_TOKEN}" https://${IGNITION_ENDPOINT}/ignition > /var/www/html/hypershift-none/worker.ign
~~~

* Modify the RHCOS live ISO to install the worker using that ignition file into the `/dev/sda` device (your milleage may vary)

~~~sh
podman run --rm -it -v /var/www/html/hypershift-none/:/data:z --workdir /data quay.io/coreos/coreos-installer:release iso customize --live-karg-append="coreos.inst.ignition_url=http://www.example.com/hypershift-none/worker.ign coreos.inst.install_dev=/dev/sda" -o ./rhcos.iso ./rhcos-live.x86_64.iso
podman run --rm -it -v /var/www/html/hypershift-none/:/data:z --workdir /data quay.io/coreos/coreos-installer:release iso kargs show ./rhcos.iso
chmod a+r /var/www/html/hypershift-none/rhcos.iso
~~~

* (Optionally) Check the ISO can be downloaded

~~~sh
curl -v -o rhcos.iso http://www.example.com/hypershift-none/rhcos.iso
~~~

* Attach the ISO to the BMC and boot from it once.

This step is highly dependent on the hardware used. In this example, using Dell hardware, the following steps can be done, but your milleage may vary:

~~~sh
export IDRACIP=10.19.136.22
export IDRACUSER="root"
export IDRACPASS="calvin"

curl -s -L -k https://raw.githubusercontent.com/dell/iDRAC-Redfish-Scripting/master/Redfish%20Python/SetNextOneTimeBootVirtualMediaDeviceOemREDFISH.py -O
curl -s -L -k https://raw.githubusercontent.com/dell/iDRAC-Redfish-Scripting/master/Redfish%20Python/InsertEjectVirtualMediaREDFISH.py -O
curl -s -L -k https://raw.githubusercontent.com/dell/iDRAC-Redfish-Scripting/master/Redfish%20Python/GetSetPowerStateREDFISH.py -O

# Turn the server off
python3 ./SetPowerStateREDFISH.py -ip ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} -r Off
# Insert the ISO as virtual media
python3 ./InsertEjectVirtualMediaREDFISH.py -ip ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} -o 1 -d 1 -i http://www.example.com/hypershift-none/rhcos.iso
# Set boot once using the Virtual media previously attached
python3 ./SetNextOneTimeBootVirtualMediaDeviceOemREDFISH.py -ip ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} -d 1 -r y
~~~

After a while, the worker will be installed.


* Sign the CSR:

~~~sh
oc get csr --kubeconfig=${HOSTED}-kubeconfig -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve --kubeconfig=${HOSTED}-kubeconfig
~~~

* Then the worker is added to the cluster:

~~~sh
oc get nodes --kubeconfig=${HOSTED}-kubeconfig
NAME                                         STATUS   ROLES    AGE   VERSION
kni1-worker-0.cloud.lab.eng.bos.redhat.com   Ready    worker   28m   v1.22.3+e790d7f

oc get co --kubeconfig=${HOSTED}-kubeconfig
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.9.17    True        False         False      17m
csi-snapshot-controller                    4.9.17    True        False         False      24m
dns                                        4.9.17    True        False         False      24m
image-registry                             4.9.17    False       False         False      4m49s   NodeCADaemonAvailable: The daemon set node-ca does not have available replicas...
ingress                                    4.9.17    True        False         False      14m
kube-apiserver                             4.9.17    True        False         False      3h45m
kube-controller-manager                    4.9.17    True        False         False      3h45m
kube-scheduler                             4.9.17    True        False         False      3h45m
kube-storage-version-migrator              4.9.17    True        False         False      24m
monitoring                                           False       True          True       9m      Rollout of the monitoring stack failed and is degraded. Please investigate the degraded status error.
network                                    4.9.17    True        False         False      25m
node-tuning                                4.9.17    True        False         False      24m
openshift-apiserver                        4.9.17    True        False         False      3h45m
openshift-controller-manager               4.9.17    True        False         False      3h45m
openshift-samples                          4.9.17    True        False         False      23m
operator-lifecycle-manager                 4.9.17    True        False         False      3h45m
operator-lifecycle-manager-catalog         4.9.17    True        False         False      3h45m
operator-lifecycle-manager-packageserver   4.9.17    True        False         False      3h45m
service-ca                                 4.9.17    True        False         False      25m
storage                                    4.9.17    True        False         False      25m

oc get clusterversion --kubeconfig=${HOSTED}-kubeconfig
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version             False       True          3h46m   Unable to apply 4.9.17: some cluster operators have not yet rolled out
~~~

> **NOTE**: Some cluster operators are degraded because there is only a single worker and they require at least 2. However setting the `hostedcluster.spec.infrastructureAvailabilityPolicy: SingleReplica` configuration parameter disables the requirement and will make the clusters operator available with a single worker.

* After adding 2 workers, the hosted cluster is completely available as well as all the cluster operators:

~~~sh
oc get hostedcluster -n clusters hosted0
NAME      VERSION   KUBECONFIG                 PROGRESS    AVAILABLE   REASON
hosted0   4.9.17    hosted0-admin-kubeconfig   Completed   True        HostedClusterAsExpected

KUBECONFIG=./hosted0-kubeconfig oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.9.17    True        False         False      64m
csi-snapshot-controller                    4.9.17    True        False         False      71m
dns                                        4.9.17    True        False         False      71m
image-registry                             4.9.17    True        False         False      11m
ingress                                    4.9.17    True        False         False      61m
kube-apiserver                             4.9.17    True        False         False      4h32m
kube-controller-manager                    4.9.17    True        False         False      4h32m
kube-scheduler                             4.9.17    True        False         False      4h32m
kube-storage-version-migrator              4.9.17    True        False         False      71m
monitoring                                 4.9.17    True        False         False      6m51s
network                                    4.9.17    True        False         False      72m
node-tuning                                4.9.17    True        False         False      71m
openshift-apiserver                        4.9.17    True        False         False      4h32m
openshift-controller-manager               4.9.17    True        False         False      4h32m
openshift-samples                          4.9.17    True        False         False      70m
operator-lifecycle-manager                 4.9.17    True        False         False      4h32m
operator-lifecycle-manager-catalog         4.9.17    True        False         False      4h32m
operator-lifecycle-manager-packageserver   4.9.17    True        False         False      4h32m
service-ca                                 4.9.17    True        False         False      72m
storage                                    4.9.17    True        False         False      72m

KUBECONFIG=./hosted0-kubeconfig oc get clusterversion
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.9.17    True        False         8m42s   Cluster version is 4.9.17
~~~


---

## Source: docs/content/how-to/none/exposing-services-from-hcp.md

# Exposing the Hosted Control Plane Services

To publish the services from the Hosted Control Plane, we need to understand the available strategies and their implications. Let's explore them.

## Service Publishing Strategies

Let's delve into the motivations for each of the strategies.

The **NodePort** strategy allows you to expose services without requiring a logical LoadBalancer like MetalLB or similar infrastructure. It is one of the simplest methods to implement. This strategy is supported by all services; however, the limitation arises in high availability (HA) environments where you will be pointing to one of the NodePorts instead of all three.

The **LoadBalancer** strategy enables you to expose certain services through a load balancer. While not all services support this strategy, it is the preferred method for exposing the KubeApiServer, as it allows for a single entry point in an HA configuration without relying in the Ingress Controller of the Manamgement cluster.

The **Route** strategy allows you to expose the HostedControlPlane services using the ingress of the Management OpenShift cluster. This strategy is supported by all services but kubeapi-server.

### NodePort

Exposing a service via NodePort is a method used in OpenShift to make a service accessible from outside the cluster. When you expose a service using NodePort, OpenShift allocates a port on each node in the cluster (if the cluster availability policy is set to HighlyAvailable). This port on each node is mapped to the port of the service, allowing external traffic to reach the service by accessing any node's IP address and the allocated NodePort.

This is the default configuration when you use `Agent` and `None` provider platforms. The services relevant for on-premise platforms are:

- APIServer
- OAuthServer
- Konnectivity
- Ignition

!!! Note
    If any of the services are not relevant for your deployment, it is not necessary to specify them.

Here is how it looks in the HostedCluster CR:

```yaml
spec:
...
...
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: <IP_ADDRESS>
        port: <PORT>
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: 10.103.101.101
      type: NodePort
...
...
```

### Route

OpenShift routes provide a way to expose services within the cluster to external clients. A route in OpenShift maps an external request (typically HTTP/HTTPS) to an internal service. The route specifies a hostname that external clients will use to access the service. OpenShift‚Äôs router (based on HAProxy) will handle the traffic coming to this hostname.

HostedControlPlanes operate in two domains: the Control Plane and the Data Plane. The Control Plane uses routes through the MGMT Cluster ingress to expose services for each of the HostedControlPlanes, and the routes are created in the HostedControlPlane namespace. For the Data Plane, the Ingress handles `*.apps.subdomain.tld` URLs, and all routes under this wildcard are directed to the Namespace by the OpenShift Router on the worker nodes.

The usual configuration for the Hosted Cluster is similar to the LoadBalancer setup we will discuss next.

### LoadBalancer

The LoadBalancer strategy in OpenShift is used to expose services to external clients using an external load balancer. When you create a service of type LoadBalancer, Kubernetes interacts with the underlying cloud platform or appropriate LoadBalancer controllers to provision an external load balancer, which then routes traffic to the service's endpoints (pods).

This is how looks like the most common configuration to expose the services from HCP side:

```yaml
spec:
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
      Route:
        hostname: <URL>
  - service: Konnectivity
    servicePublishingStrate
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
```

If you wanna know more about how to expose the ingress service in the Data Plane side, please access the Recipes section to see how to do it with MetalLB.


---

## Source: docs/content/how-to/none/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/openstack/additional-ports.md

# Additional ports for Nodepools

This document describes how to attach additional ports to nodes in a HostedCluster on OpenStack.

## Use-cases

- **SR-IOV**: Single Root I/O Virtualization (SR-IOV) is a specification that allows a single Peripheral Component Interconnect Express (PCIe) physical device to appear as multiple separate physical devices. This can be useful for high-performance networking scenarios. The Nodepool can be configured to use SR-IOV by attaching additional ports to the nodes. Workloads that require high-performance networking can use the SR-IOV devices.
- **DPDK**: Data Plane Development Kit (DPDK) is a set of libraries and drivers for fast packet processing. DPDK can be used to improve the performance of network applications. The Nodepool can be configured to use DPDK by attaching additional ports to the nodes. Workloads that require high-performance networking can use the DPDK devices.
- **Manila RWX on NFS**: Manila ReadWriteMany (RWX) volumes can be used by multiple nodes in a HostedCluster. The Nodepool can be configured to use Manila RWX volumes by attaching additional ports to the nodes. Workloads that require shared storage will be able to reach the NFS network configured for the Manila RWX volumes.
- **Multus CNI**: Multus is a meta-plugin for Kubernetes CNI that enables attaching multiple network interfaces to pods. The Nodepool can be configured to use Multus by attaching additional ports to the nodes. Workloads that require multiple network interfaces will be able to use the Multus CNI. For example it's useful if the workload needs to connect to IPv6 networks (dual-stack or single-stack).

## Prerequisites

Additional networks must be created in OpenStack and the project used by the HostedCluster must have access to these networks.

## Available options

The `--openstack-node-additional-port` flag can be used to attach additional ports to nodes in a HostedCluster on OpenStack. The flag takes a list of parameters separated by commas. The parameter can be used multiple times to attach multiple additional ports to the nodes.

The parameters are:

| Parameter       | Description                                                                                          | Required | Default  |
|-----------------|------------------------------------------------------------------------------------------------------|----------|----------|
| `network-id`    | The ID of the network to attach to the node.                                                         | Yes      | N/A      |
| `vnic-type`     | The VNIC type to use for the port. When not specified, Neutron uses the default `normal`.            | No       | N/A      |
| `disable-port-security` | Whether to enable port security for the port. When not specified, Neutron will enable it unless explicitly disabled in the network      | No       | N/A      |
| `address-pairs` | A list of IP address pairs to be assigned to the port. The format is `ip_address=mac_address`. Multiple pairs can be provided, separated by a "-". The `mac_address` portion is optional and in most cases won't be used. | No       | N/A      |

## Example

The following example demonstrates how to create a HostedCluster with additional ports attached to the nodes.

```shell
export NODEPOOL_NAME=$CLUSTER_NAME-ports
export WORKER_COUNT="2"
export FLAVOR="m1.xlarge"

hcp create nodepool openstack \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --replicas $WORKER_COUNT \
  --openstack-node-flavor $FLAVOR \
  --openstack-node-additional-port "network-id=<SRIOV_NET_ID>,vnic-type=direct,disable-port-security=true" \
  --openstack-node-additional-port "network-id=<LB_NET_ID>,address-pairs:192.168.0.1-192.168.0.2"
```

In this example, two additional ports are attached to the nodes in the Nodepool in addition to the network created by the Cluster API provider using for the control-plane. The first additional port is attached to the network with the ID `<SRIOV_NET_ID>`. The port uses the `direct` VNIC type and has port security disabled. The second port is attached to the network with the ID `<LB_NET_ID>`. The port will allow address pairs so services like MetalLB will be able to handle
the traffic on these IPs.


---

## Source: docs/content/how-to/openstack/az.md

# Availability Zones

This example will create a Nodepool that will spawn 2 additional machines in a given Nova availability zone.
Availability Zones do not necessarily correspond to fault domains and do not inherently provide high availability benefits.

```shell
export NODEPOOL_NAME=$CLUSTER_NAME-az1
export WORKER_COUNT="2"
export FLAVOR="m1.xlarge"
export AZ="az1"

hcp create nodepool openstack \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --replicas $WORKER_COUNT \
  --openstack-node-flavor $FLAVOR \
  --openstack-node-availability-zone $AZ \
```

Check the status of the NodePool by listing `nodepool` resources in the `clusters`
namespace:

```shell
oc get nodepools --namespace clusters

NAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
example                   example         5               5               False         False        4.17.0
example-az1               example         2                               False         False                  True              True             Minimum availability requires 2 replicas, current 0 available
```

After a while, in our hosted cluster this is what we will see:

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes

NAME                      STATUS   ROLES    AGE     VERSION
(..)
example-extra-az-zh9l5    Ready    worker   2m6s    v1.27.4+18eadca
example-extra-az-zr8mj    Ready    worker   102s    v1.27.4+18eadca
...
```

And the nodepool will be in the desired state:

```shell
oc get nodepools --namespace clusters

NAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
example                   example         5               5               False         False        4.17.0
example-az1               example         2               2               False         False        4.17.0
```


---

## Source: docs/content/how-to/openstack/destroy.md

# Delete a HostedCluster on OpenStack

To delete a HostedCluster on OpenStack:

```shell
hcp destroy cluster openstack --name $CLUSTER_NAME
```

The process will take a few minutes to complete and will destroy all resources associated with the HostedCluster including OpenStack resources such as servers, the RHCOS image, networks, etc.


---

## Source: docs/content/how-to/openstack/etcd-local-storage.md

In a Hosted Control Plane (HCP) architecture, the etcd database plays a critical role as the core data store for the
Hosted Kubernetes control plane components. By default, Hypershift provisions etcd data on a Persistent Volume Claim (PVC),
which relies on the default StorageClass defined in the Management Cluster.

However, HyperShift allows you to easily choose another storage class when desired. 
On OpenStack, the default RWO StorageClass is generally Cinder via its CSI driver to provision storage.
While this driver is suitable for general workloads, it is not ideal for etcd due to the latency and performance characteristics
of network-attached storage.
Instead, a more optimal approach is to create a StorageClass backed by high performance local disk(s) and use it explicitly for etcd.

This document provides detailed instructions on how to configure and leverage Logical Volume Manager Storage through the TopoLVM CSI
driver to dynamically provision storage for etcd.

Logical Volume Manager (LVM) Storage uses LVM2 through the TopoLVM CSI driver to dynamically provision local storage on a cluster with limited resources.
With this method, we can create volume groups, persistent volume claims (PVCs), volume snapshots, and volume clones for etcd.


1. Follow the official procedure to Install LVM Storage by using the CLI on the Management Cluster.

2. You will need worker nodes with additional ephemeral disk(s) on the Management cluster.
   The tested solution is to create an OpenStack Nova flavor that will create an additional ephemeral local disk attached to the instance.

```shell
openstack flavor create (...) --ephemeral 100 (...)
```

!!! note

    When a server will be created with that flavor, Nova will automatically create and attach a local ephemeral storage device to the VM (and format it in vfat).


3. Now we need to Create a compute machine set that would use this flavor.

4. Scale the MachineSet to the desired replica. Note that if HostedClusters are deployed in High Availability, a minimum of 3 workers has to be deployed so the pods can be distributed accordingly.

5. Add a label to identify the Nodes with the ephemeral disk for etcd so we can identify them as ‚ÄúHypershift ready‚Äù:

!!! note

    This label is arbitrary and can be changed.

6. Now that we have the workers that will be used for Hypershift, we can create the LVMCluster object that will describe the configuration of the local storage used for etcd in Hypershift. Create a file named ‚Äúlvmcluster.yaml‚Äù:

```yaml
---
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMCluster
metadata:
  name: etcd-hcp
  namespace: openshift-storage
spec:
  storage:
    deviceClasses:
    - name: etcd-class
      default: true
      nodeSelector:
    	 nodeSelectorTerms:
    	 - matchExpressions:
      	   - key: hypershift-capable
            operator: In
            values:
            - "true"
      deviceSelector:
        forceWipeDevicesAndDestroyAllData: true
        paths:
        - /dev/vdb
```

!!! note

    * We assume that the ephemeral disk is located on /dev/vdb which is the case in most situations. However this needs to be verified when following this procedure. Using symlinks isn't supported.
    * `forceWipeDevicesAndDestroyAllData` is set to True because the default nova ephemeral disk comes formatted in vfat.
    * `thinPoolConfig` can be used but it will affect the performance therefore we don't recommend it.

Now we create the resource: 

```shell
oc apply -f lvmcluster.yaml
```

We can verify that the resource has been created:

```shell
oc get lvmcluster -A
NAMESPACE       	NAME   	STATUS
openshift-storage   etcd-hcp   Ready
```

Also we can verify that we have a StorageClass for LVM:

```shell
oc get storageclass -A
NAME                 	PROVISIONER            	  RECLAIMPOLICY   VOLUMEBINDINGMODE  	ALLOWVOLUMEEXPANSION   AGE
lvms-etcd-class      	topolvm.io             	  Delete      	  WaitForFirstConsumer  true               	   23m
standard-csi (default)  cinder.csi.openstack.org  Delete      	  WaitForFirstConsumer  true               	   56m
```


7. Now that local storage is available through CSI in the Management Cluster, the HostedCluster can be deployed this way:

```shell
hcp create cluster openstack \
	(...)
	--etcd-storage-class lvms-etcd-class
```

8. You can now verify that etcd is placed on local ephemeral storage:

```shell
oc get pvc -A
NAMESPACE              	NAME                 	STATUS   VOLUME                                 	CAPACITY   ACCESS MODES   STORAGECLASS  	VOLUMEATTRIBUTESCLASS   AGE
clusters-emacchi-hcp   	data-etcd-0          	Bound	pvc-f8b4070f-0d11-48b7-93d3-cc2a56ada7e9   8Gi    	RWO        	lvms-etcd-class   <unset>             	8s
```

In the Hosted Control Plane etcd pod:

```shell
/var/lib/data
bash-5.1$ df -h
Filesystem                                     	Size  Used Avail Use% Mounted on
(...)                                         	3.2G   86M  3.1G   3% /etc/passwd
/dev/topolvm/0b1114b3-f084-4c1c-bda5-85ef197459aa  8.0G  215M  7.8G   3% /var/lib
(...)
```

We can see the 8GB device for etcd.


---

## Source: docs/content/how-to/openstack/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/openstack/hostedcluster.md

# Create a HostedCluster on OpenStack

Once all the prerequisites are met, it is now possible to create a Hosted Cluster on OpenStack.

Here are the available options specific to the OpenStack platform:

| Option                                  | Description                                                                                  | Required | Default       |
|-----------------------------------------|----------------------------------------------------------------------------------------------|----------|---------------|
| `--openstack-ca-cert-file`              | Path to the OpenStack CA certificate file                                                    | No       |               |
| `--openstack-cloud`                     | Name of the cloud in `clouds.yaml`                                                           | No       | `'openstack'` |
| `--openstack-credentials-file`          | Path to the OpenStack credentials file                                                       | No       |               |
| `--openstack-external-network-id`       | ID of the OpenStack external network                                                         | No       |               |
| `--openstack-ingress-floating-ip`       | A floating IP for OpenShift ingress                                                          | No       |               |
| `--openstack-node-additional-port`      | Attach additional ports to nodes. Params: `network-id`, `vnic-type`, `disable-port-security`, `address-pairs`. | No       |               |
| `--openstack-node-availability-zone`    | Availability zone for the nodepool                                                           | No       |               |
| `--openstack-node-flavor`               | Flavor for the nodepool                                                                      | Yes      |               |
| `--openstack-node-image-name`           | Image name for the nodepool                                                                  | No       |               |
| `--openstack-dns-nameservers`           | List of DNS server addresses that will be provided when creating the subnet                  | No       |               |

Below is an example of how to create a cluster using environment variables and the `hcp` cli tool.

!!! note

    The `--release-image` flag could be used to provision the HostedCluster with a specific OpenShift Release (the hypershift operator has a support matrix of releases supported by a given version of the operator).

```shell
export CLUSTER_NAME=example
export BASE_DOMAIN=hypershift.lab
export PULL_SECRET="$HOME/pull-secret"
export WORKER_COUNT="2"

# OpenStack resources for the HostedCluster will be created
# in that project.
export OS_CLOUD="openstack"

# Flavor for the nodepool
export FLAVOR="m1.large"

# Pre-defined floating IP for Ingress
export INGRESS_FLOATING_IP="<ingress-floating-ip>"

# Optional flags:
# External network to use for the Ingress endpoint.
export EXTERNAL_NETWORK_ID="5387f86a-a10e-47fe-91c6-41ac131f9f30"

# CA certificate path to use for the OpenStack API if using self-signed certificates.
# In 4.18, this is not required as the CA cert found in clouds.yaml will be used.
export CA_CERT_PATH="$HOME/ca.crt"

# In 4.18, this is not required as the file will be discovered.
export CLOUDS_YAML="$HOME/clouds.yaml"

# SSH Key for the nodepool VMs
export SSH_KEY="$HOME/.ssh/id_rsa.pub"

# DNS nameserver for the subnet
export DNS_NAMESERVERS="1.1.1.1"

hcp create cluster openstack \
--name $CLUSTER_NAME \
--base-domain $BASE_DOMAIN \
--node-pool-replicas $WORKER_COUNT \
--pull-secret $PULL_SECRET \
--ssh-key $SSH_KEY \
--openstack-credentials-file $CLOUDS_YAML \
--openstack-ca-cert-file $CA_CERT_PATH \
--openstack-external-network-id $EXTERNAL_NETWORK_ID \
--openstack-node-flavor $FLAVOR \
--openstack-ingress-floating-ip $INGRESS_FLOATING_IP \
--openstack-dns-nameservers $DNS_NAMESERVERS
```

!!! note

    A default NodePool will be created for the cluster with 2 VM worker replicas
    per the `--node-pool-replicas` flag.

!!! note

    When using `hcp` CLI, High Availability will be enabled by default.
    Pods will be scheduled across different nodes to ensure that the control plane is highly available.
    When the management cluster worker nodes are spread across different availability zones,
    the hosted control plane will be spread across different availability zones as well in
    `PreferredDuringSchedulingIgnoredDuringExecution` mode for `PodAntiAffinity`.
    If your management cluster doesn't have enough workers (less than 3), which is not recommended nor supported,
    you'll need to specify the `--control-plane-availability-policy` flag to `SingleReplica`.

!!! note

    When not providing the `--openstack-node-image-name` flag, the latest RHCOS image will be used.
    ORC will handle the RHCOS image lifecycle by downloading the image from the OpenShift mirror and deleting it when the HostedCluster is deleted.
    The OpenStack Glance Image will be named like this: `<hosted-cluster-name>-rhcos-<rhcos-version>`.

After a few moments we should see our hosted control plane pods up and running:

```shell
oc -n clusters-$CLUSTER_NAME get pods

NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-5cc7b74f47-n5gkr                        1/1     Running   0          3m
catalog-operator-5f799567b7-fd6jw                     2/2     Running   0          69s
certified-operators-catalog-784b9899f9-mrp6p          1/1     Running   0          66s
cluster-api-6bbc867966-l4dwl                          1/1     Running   0          66s
.
.
.
redhat-operators-catalog-9d5fd4d44-z8qqk              1/1     Running   0          66s
```

A guest cluster backed by OpenStack virtual machines typically takes around 10-15
minutes to fully provision.

## Accessing the HostedCluster

CLI access to the guest cluster is gained by retrieving the guest cluster's
kubeconfig. Below is an example of how to retrieve the guest cluster's
kubeconfig using the hcp cli.

```shell
hcp create kubeconfig --name $CLUSTER_NAME > $CLUSTER_NAME-kubeconfig
```

If we access the cluster, we will see we have two nodes.

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes

NAME                  STATUS   ROLES    AGE   VERSION
example-n6prw         Ready    worker   32m   v1.27.4+18eadca
example-nc6g4         Ready    worker   32m   v1.27.4+18eadca
```

We can also check the ClusterVersion:

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get clusterversion

NAME      VERSION       AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.17.0        True        False         5m39s   Cluster version is 4.17.0
```

## Ingress and DNS (optional)

If you haven't created the HostedCluster with `--openstack-ingress-floating-ip`, you'll need to
update the DNS record with the floating IP address that was assigned to the `router-default` Service.

Once the workload cluster is deploying, the Ingress controller will be installed
and a router named `router-default` will be created in the `openshift-ingress` namespace.

You'll need to update your DNS with the external IP of that router so Ingress (and dependent operators like console) can work.

Once the HostedCluster is created, you need to wait for the `router-default` service to get an external IP:

```shell
oc -w --kubeconfig $CLUSTER_NAME-kubeconfig -n openshift-ingress get service/router-default -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
```

When the external IP exists, you can now create a DNS A record for `*.apps.<cluster-name>.<base-domain>` that matches the returned IP address.
Once this is done, the Ingress operator will become healthy and the console will be accessible shortly after.

!!! note

    The DNS propagation time can vary so you might need to wait a few minutes before your HostedCluster becomes healthy.

## Access to the guest cluster

Once the HostedCluster is healthy, you should be able to access the OpenShift console by navigating
to `https://console-openshift-console.apps.<cluster-name>.<base-domain>` in your browser.

To get the `kubeadmin` password, you can run this command:
```shell
oc get --namespace clusters Secret/${CLUSTER_NAME}-kubeadmin-password -o jsonpath='{.data.password}' | base64 --decode
```

To know whether the HostedCluster is healthy, you can verify with this command:
```shell
oc get --namespace clusters hostedclusters

NAME            VERSION   KUBECONFIG                       PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
example         4.17.0    example-admin-kubeconfig         Completed  True        False         The hosted control plane is available
```

## Scaling an existing NodePool

Manually scale a NodePool using the `oc scale` command:

```shell
NODEPOOL_NAME=$CLUSTER_NAME
NODEPOOL_REPLICAS=5

oc scale nodepool/$NODEPOOL_NAME --namespace clusters --replicas=$NODEPOOL_REPLICAS
```

After a while, in our hosted cluster this is what we will see:

```shell
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes

NAME                  STATUS   ROLES    AGE     VERSION
example-9jvnf         Ready    worker   97s     v1.27.4+18eadca
example-n6prw         Ready    worker   116m    v1.27.4+18eadca
example-nc6g4         Ready    worker   117m    v1.27.4+18eadca
example-thp29         Ready    worker   4m17s   v1.27.4+18eadca
example-twxns         Ready    worker   88s     v1.27.4+18eadca
```

## OpenStack resources tagging

The OpenStack resources created by the CAPI provider are tagged with `openshiftClusterID=<infraID>`
but additional tags can be added to the resources via the `HostedCluster.Spec.Platform.OpenStack.Tags`
field when creating the HostedCluster with a given YAML file.


---

## Source: docs/content/how-to/openstack/index.md

---
title: Hosted Clusters on OpenStack
---

This section of the Hypershift documentation contains pages that explain how Hosted Clusters
and Nodepools can be deployed on the OpenStack platform.

!!! warning
    OpenStack support within Hypershift is currently "dev-preview" and therefore is not
    yet intended for production use. However, it is possible to create and manage
    Hosted Clusters for development and testing purposes. It is expected to work
    as described in this documentation.

!!! note
    When you create a HostedCluster with the OpenStack platform, HyperShift will install the CAPI Provider for OpenStack (CAPO).
    in the Hosted Control Plane (HCP) namespace.
    It will also install OpenStack Resource Controller (ORC) in the HCP namespace to manage the OpenStack resources such as the Image.
    Upon scaling up a NodePool, a Machine will be created, and the CAPI provider will create the necessary resources in OpenStack.
    CAPO created OpenStack resources by leveraging Gophercloud, the OpenStack SDK for Go.

Table of content:

- Prerequisites
- Create a Hosted Cluster and a default Nodepool
- Destroy a Hosted Cluster

Advanced scenarios: additional Nodepools can be created with specific configurations:

- Nodepools distributed over Nova availability zones
- Additional ports for Nodepools
- Node tuning, SR-IOV Network Operator


---

## Source: docs/content/how-to/openstack/performance-tuning.md

# Performance guide

This document describes how to tune the performance of nodes in a HostedCluster on OpenStack

## Use-cases

- **CNF**: Cloud-native network functions (CNFs) are network functions that are designed to run in cloud-native environments. CNFs can be used to provide network services such as routing, firewalling, and load balancing. The Nodepool can be configured to use high-performance computing and networking devices to run CNFs.

## Prerequisite

- An OpenStack flavor with the necessary resources to run the workload: dedicated CPU, memory, and host aggregate
  information so the instance will be scheduled on a host with capable hardware (e.g. Intel or Mellanox NICs capable
  of SR-IOV, DPDK, etc).
- An OpenStack network attached to the SR-IOV or DPDK capable NICs; this network must be usable by the project
  used by the HostedCluster.

## Node tuning

Before creating an additional Nodepool used for performance workloads, we'll create a PerformanceProfile that will be
used for the nodes.

The PerformanceProfile is a custom resource that allows you to define the performance characteristics of a node. The
PerformanceProfile can be used to configure the CPU, memory, hugepages, real-time kernel, and other performance-related
settings of a node.

Here is an example of a PerformanceProfile that can be used to configure a node for high-performance workloads:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: perfprof-1
  namespace: clusters
data:
  tuning: |
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cnf-performanceprofile
      namespace: "${HYPERSHIFT_NAMESPACE}"
    data:
      tuning: |
        apiVersion: performance.openshift.io/v2
        kind: PerformanceProfile
        metadata:
          name: cnf-performanceprofile
        spec:
          additionalKernelArgs:
            - nmi_watchdog=0
            - audit=0
            - mce=off
            - processor.max_cstate=1
            - idle=poll
            - intel_idle.max_cstate=0
            - amd_iommu=on
          cpu:
            isolated: "${CPU_ISOLATED}"
            reserved: "${CPU_RESERVED}"
          hugepages:
            defaultHugepagesSize: "1G"
            pages:
              - count: ${HUGEPAGES}
                node: 0
                size: 1G
          nodeSelector:
            node-role.kubernetes.io/worker: ''
          realTimeKernel:
            enabled: false
          globallyDisableIrqLoadBalancing: true
```

Create this PerformanceProfile (change the namespace field in the example above to specify the clusters namespace) by running:

```shell
oc apply -f perfprof.yaml
```

## Nodepool creation

After creating the PerformanceProfile, we can create a Nodepool that will use the PerformanceProfile for the nodes.

```shell
export NODEPOOL_NAME=$CLUSTER_NAME-cnf
export FLAVOR="m1.xlarge.nfv"

hcp create nodepool openstack \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --replicas 0 \
  --openstack-node-flavor $FLAVOR
```

Now let's patch the Nodepool to use the PerformanceProfile:

```shell
oc patch nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} -p '{"spec":{"tuningConfig":[{"name":"cnf-performanceprofile"}]}}' --type=merge
```

We can scale the Nodepool to the desired number of nodes:

```shell
oc scale NodePool/$CLUSTER_NAME --namespace ${HYPERSHIFT_NAMESPACE} --replicas=1
```

To wait for the node to be ready:

```shell
oc wait --for=condition=UpdatingConfig=True nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} --timeout=5m
oc wait --for=condition=UpdatingConfig=False nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} --timeout=30m
oc wait --for=condition=AllNodesHealthy nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} --timeout=5m
```

Once the nodes have been deployed, you can SSH into the nodes and check the performance settings:

```shell
ssh core@<node-ip>
```
chroot /host
cat /proc/cmdline
```

## SR-IOV Network Operator

The SR-IOV Network Operator is a Kubernetes operator that manages SR-IOV devices on the nodes where workloads will be running. In the context of OpenShift on OpenStack, the operator can be used to create SR-IOV network attachments that can be used by workload pods.

The nodes deployed by the Nodepool must be labeled so the SR-IOV Network Operator can be deployed on these nodes.

If you haven't done it already, we need to create a kubeconfig file for the cluster and export it:
```shell
hcp create kubeconfig --name $CLUSTER_NAME > $CLUSTER_NAME-kubeconfig
export KUBECONFIG=$CLUSTER_NAME-kubeconfig
```

For every node that was deployed by the Nodepool, label it with the following command:

```shell
oc label node "${WORKER_NODE}" feature.node.kubernetes.io/network-sriov.capable="true"
```

The operator can now be installed in the guest cluster by following the SR-IOV Network Operator documentation.
The rest of the operations in order to get the workload running with high performances are the same as on a Standalone OCP cluster.


---

## Source: docs/content/how-to/openstack/prerequisites.md

# Prerequisites for OpenStack

* Admin access to an OpenShift cluster (version 4.17+) specified by the `KUBECONFIG` environment variable.
  This cluster is referred to as the Management OCP cluster.
* The Management OCP cluster can be running on OpenStack (ShiftOnStack), but it could also be running on Baremetal or
  a public cloud such as AWS.
* The Management OCP cluster must be configured with OVNKubernetes as the default pod network CNI.
* The OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`) must be installed.
* The `hcp` CLI must be installed and is the production tool to manage the hosted clusters.
* The `hypershift` CLI must be installed to deploy the HyperShift Operator. In production, it is not recommended to use that CLI to
  manage the hosted clusters.
* The HyperShift Operator must be installed in the Management OCP cluster.
* A load-balancer backend must be installed in the Management OCP cluster (e.g. Octavia) so the kube-api Service can be created for each Hosted Cluster.
* A valid pull secret file for the `quay.io/openshift-release-dev` repository.
* OpenStack Octavia service must be running in the cloud hosting the guest cluster when ingress is configured with an Octavia load balancer.
  In the future, we'll explore other Ingress options like MetalLB.
* The default external network (on which the kube-apiserver LoadBalancer type service is created) of the Management OCP cluster must be reachable from the guest cluster.

## Install the HyperShift and HCP CLI

The `hcp` CLI tool is used to manage the creation and destruction of guest
clusters.

The `hypershift` CLI tool is a development tool that is used to install
developer builds of the HyperShift Operator.
The command below builds latest hypershift and hcp cli tools from source and
places the CLI tool within the `/usr/local/bin` directory.

!!! note

    The command below is the same if you use docker.
  
```shell
podman run --rm --privileged -it -v \
$PWD:/output docker.io/library/golang:1.23 /bin/bash -c \
'git clone https://github.com/openshift/hypershift.git && \
cd hypershift/ && \
make hypershift product-cli && \
mv bin/hypershift /output/hypershift && \
mv bin/hcp /output/hcp'

sudo install -m 0755 -o root -g root $PWD/hypershift /usr/local/bin/hypershift
sudo install -m 0755 -o root -g root $PWD/hcp /usr/local/bin/hcp
rm $PWD/hypershift
rm $PWD/hcp
```

## Deploy the HyperShift Operator

Use the hypershift cli tool to install the HyperShift operator into the
management cluster.

```shell
hypershift install --tech-preview-no-upgrade
```

!!! note

    HyperShift on OpenStack is possible behind a feature gate, which is why we have
    to install the operator with `--tech-preview-no-upgrade`. Once the platform
    is GA, the operator will be able to be installed without that flag.

Once installed, you should see the operator running in the `hypershift` namespace:

```shell
oc -n hypershift get pods

NAME                        READY   STATUS    RESTARTS   AGE
operator-755d587f44-lrtrq   1/1     Running   0          114s
operator-755d587f44-qj6pz   1/1     Running   0          114s
```

## Prepare the management cluster to store etcd locally

HostedClusters will have pod(s) for etcd and its performance is essential for the cluster health.
In production environments, it's required to put etcd data on fast storage and in the case of OpenStack it'll be local storage.
Follow this procedure to leverage a well-known and tested solution.

## Upload RHCOS image in OpenStack (optional)

The user can specify which RHCOS image to use when deploying the node pools
on OpenStack by uploading the image to the OpenStack cloud. If the image is not
uploaded to OpenStack, OpenStack Resource Controller (ORC) will
manage the RHCOS image lifecycle by downloading the image from the OpenShift mirror and deleting it when the HostedCluster is deleted.

Here is an example of how to upload an RHCOS image to OpenStack:

```shell
openstack image create --disk-format qcow2 --file rhcos-openstack.x86_64.qcow2 rhcos
```

!!! note

    The `rhcos-openstack.x86_64.qcow2` file is the RHCOS image that was downloaded from the OpenShift mirror.
    You can download the latest RHCOS image from the Red Hat OpenShift Container Platform mirror.

## Create a floating IP for the Ingress (optional)

To get Ingress healthy in a HostedCluster without manual intervention, you need to create a floating IP that will be used by the Ingress service.

```shell
openstack floating ip create <external-network-id>
```

If you provide the floating IP to the `--openstack-ingress-floating-ip` flag without pre-creating it, cloud-provider-openstack will create it for you
only if the Neutron API policy allows a user to create floating IP with a specific IP address.

## Update the DNS record for the Ingress (optional)

If you use a pre-defined floating IP for ingress, you need to create a DNS record for the following wildcard domain that needs to point to the Ingress floating IP:
`*.apps.<cluster-name>.<base-domain>`


---

## Source: docs/content/how-to/pause-reconciliation.md

# Pause Reconciliation

The `pausedUntil` field allows a hypershift administrator to pause reconciliation of a HostedCluster/HostedControlPlane pair. This is useful in operational scenarios like backing up and restoring an etcd database. It is also useful for debugging problems with HostedClusters/HostedControlPlanes.


Pause reconciliation for a given `HostedCluster/HostedControlPlane` pair by populating the pausedUntil field of the resource:

```
#Can be boolean or RFC339 timestamp
# PAUSED_UNTIL="true"
PAUSED_UNTIL="2022-03-03T03:28:48Z"
kubectl patch -n HOSTED_CLUSTERS_NAMESPACE hostedclusters/HOSTED_CLUSTER_NAME -p '{"spec":{"pausedUntil":"'${PAUSED_UNTIL}'"}}' --type=merge
```

Two formats are accepted:
1) If a RFC3339 formatted date is specified: reconciliation will be paused until that date.
2) If the boolean value of true is passed: reconciliation will be paused until the field is removed from the HostedCluster.

All other values will not pause reconciliation.

The associated downstream HostedControlPlane resource will have the pause reconciliation field added to it automatically. When the user removes the field from the HostedCluster or the specified date is passed: reconciliation will continue on the object

The field can be removed with the following patch command:
```
kubectl patch -n HOSTED_CLUSTERS_NAMESPACE hostedclusters/HOSTED_CLUSTER_NAME -p '{"spec":{"pausedUntil":null}}' --type=merge
```


---

## Source: docs/content/how-to/per-hostedcluster-dashboard.md

# Create Monitoring Dashboard per Hosted Cluster

The HyperShift operator can create/destroy a separate monitoring dashboard in the console of the management cluster for each HostedCluster that it manages. This functionality can be optionally enabled on installation of the HyperShift operator.

### Enable Monitoring Dashboards

To enable monitoring dashboards, use the `--monitoring-dashboards` flag when running `hypershift install`. Alternatively, to enable monitoring dashboards in an existing installation, set the `MONITORING_DASHBOARDS` environment variable to `1` on the hypershift operator deployment:

```
oc set env deployment/operator -n hypershift MONITORING_DASHBOARDS=1
```

### Dashboards

When monitoring dashboards are enabled, the HyperShift operator creates a configmap named `cp-[NAMESPACE]-[NAME]` in the `openshift-config-managed` namespace (where NAMESPACE is the namespace of the HostedCluster and NAME is the name of the HostedCluster) for each HostedCluster that the operator manages. This results in a new dashboard getting added under Observe -> Dashboards in the administrative console of the management cluster. When a HostedCluster is deleted, its corresponding dashboard is also deleted.

### Customize Monitoring Dashboards

To generate per-cluster dashboards, the HyperShift operator uses a template stored in a ConfigMap named `monitoring-dashboard-template` in the operator namespace (`hypershift`). This template contains a set of grafana panels that contain the various metrics that should go on the dashboard. Edit the content of this ConfigMap to customize the dashboards. When a particular HostedCluster's dashboard is generated, the following strings will be replaced with values that correspond to the specific HostedCluster:

| Name                          | Description                                                                      |
|-------------------------------|----------------------------------------------------------------------------------|
| `__NAME__`                    | The name of the HostedCluster                                                    |
| `__NAMESPACE__`               | The namespace of the HostedCluster                                               |
| `__CONTROL_PLANE_NAMESPACE__` | The namespace where the control plane pods of the HostedCluster are placed       |
| `__CLUSTER_ID__`              | The UUID of the HostedCluster (matches the `_id` label of HostedCluster metrics) |

---

## Source: docs/content/how-to/pki/break-glass-credentials.md

# Break-Glass Credentials

This document describes break-glass credential management in HyperShift, including automatic rotation and revocation using the CertificateRevocationRequest API.

## Overview

The `control-plane-pki-operator` runs in each hosted control plane namespace and manages the lifecycle of **break-glass credentials**. These credentials provide emergency access to the hosted cluster's API server for both customers and Site Reliability Engineers (SREs).

Break-glass credentials are separate from internal control plane certificates, which are managed by the control-plane-operator (CPO).

The control-plane-pki-operator handles:

- **Automatic certificate rotation** - Continuous rotation of break-glass signing CAs and client certificates
- **Certificate signing request (CSR) approval** - Processing requests for new break-glass credentials
- **Certificate revocation** - Revoking compromised break-glass certificates through a safe, multi-step workflow

## Architecture

### Control Plane PKI Operator Components

The control-plane-pki-operator consists of several controllers:

| Controller | Purpose |
|------------|---------|
| Certificate Rotation Controller | Automatically rotates signing CAs and client certificates |
| Certificate Loading Controller | Loads signing CA certificates for use by the signing controllers |
| CSR Approval Controller | Approves CertificateSigningRequests when a matching CertificateSigningRequestApproval exists |
| Certificate Signing Controller | Signs approved CSRs to generate client certificates |
| Certificate Revocation Controller | Processes CertificateRevocationRequests to revoke certificates |

### Signer Classes

HyperShift defines two signer classes for break-glass credentials:

| Signer Class | Purpose |
|--------------|---------|
| `customer-break-glass` | Emergency access credentials for customers |
| `sre-break-glass` | Emergency access credentials for SREs |

Each signer class has its own:

- Signing CA secret
- CA trust bundle ConfigMap
- Client certificate secret

### Certificate Resources

For each signer class, the following resources are created in the hosted control plane namespace:

**Customer Break-Glass:**

| Resource | Type | Description |
|----------|------|-------------|
| `customer-system-admin-signer` | Secret | Root signing CA for customer credentials |
| `customer-system-admin-signer-ca` | ConfigMap | Trust bundle containing the CA certificate |
| `customer-system-admin-client-cert-key` | Secret | Client certificate for customer break-glass access |

**SRE Break-Glass:**

| Resource | Type | Description |
|----------|------|-------------|
| `sre-system-admin-signer` | Secret | Root signing CA for SRE credentials |
| `sre-system-admin-signer-ca` | ConfigMap | Trust bundle containing the CA certificate |
| `sre-system-admin-client-cert-key` | Secret | Client certificate for SRE break-glass access |

**Combined Trust Bundle:**

The individual CA bundles from both signer classes are combined into a `total-client-ca` ConfigMap. This combined trust bundle is used by the hosted cluster's kube-apiserver for client certificate authentication.

## Automatic Certificate Rotation

The control-plane-pki-operator automatically rotates break-glass signing CAs and client certificates based on configurable timing parameters.

### Default Rotation Schedule

The rotation timing for break-glass certificates is based on multiples of the rotation scale (default: 24 hours):

| Certificate Type | Validity Period | Refresh Interval |
|------------------|-----------------|------------------|
| Signing CA | 7 √ó scale (7 days) | 2 √ó scale (every 2 days) |
| Client Certificate | 1.5 √ó scale (36 hours) | 0.25 √ó scale (every 6 hours) |

### How It Works

1. The Certificate Rotation Controller monitors the validity of signing CAs and client certificates
2. When a certificate approaches its refresh interval, the controller generates a new certificate
3. New certificates are signed by the current CA and placed in the appropriate secrets
4. The CA trust bundle is updated to include both old and new CAs during the transition period
5. Old certificates are eventually removed from the trust bundle after all clients have transitioned

### Configuration

The rotation timing can be scaled using the `CERT_ROTATION_SCALE` environment variable on the control-plane-pki-operator. The default scale is 24 hours.

!!! warning

    Modifying `CERT_ROTATION_SCALE` is unsupported and should only be used for testing purposes.

## Requesting Break-Glass Credentials

Break-glass credentials provide emergency access to a hosted cluster when normal authentication methods are unavailable.

To request new break-glass credentials, create a `CertificateSigningRequest` with the appropriate signer name:

```yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: my-break-glass-request
spec:
  request: <base64-encoded-csr>
  signerName: hypershift.openshift.io/<hcp-namespace>.customer-break-glass
  usages:
  - client auth
```

The signer name format is: `hypershift.openshift.io/<hosted-control-plane-namespace>.<signer-class>`

### Approving Requests

CSRs are not automatically approved. To approve a CSR, create a matching `CertificateSigningRequestApproval` resource:

```yaml
apiVersion: certificates.hypershift.openshift.io/v1alpha1
kind: CertificateSigningRequestApproval
metadata:
  name: my-break-glass-request
  namespace: <hosted-control-plane-namespace>
spec: {}
```

!!! note

    The CertificateSigningRequestApproval must have the same name as the CertificateSigningRequest.

Once the approval resource exists, the CSR Approval Controller will approve the CSR, and the Certificate Signing Controller will sign it.

### Viewing CSR Status

Check the status of a CertificateSigningRequest:

```bash
oc get csr my-break-glass-request -o yaml
```

The signed certificate will be available in the `.status.certificate` field once approved and signed.

## Certificate Revocation

When certificates are compromised or need to be invalidated, use the `CertificateRevocationRequest` API to trigger a safe revocation workflow.

### Creating a Revocation Request

To revoke all certificates for a signer class:

```yaml
apiVersion: certificates.hypershift.openshift.io/v1alpha1
kind: CertificateRevocationRequest
metadata:
  name: revoke-customer-certs
  namespace: <hosted-control-plane-namespace>
spec:
  signerClass: customer-break-glass
```

Apply the revocation request:

```bash
oc apply -f revocation-request.yaml
```

### Revocation Workflow

The Certificate Revocation Controller processes revocation requests through the following steps:

1. **Commit Revocation Timestamp** - Records the point in time after which certificates must be regenerated
2. **Generate New Signer Certificate** - Creates a new signing CA valid only after the revocation timestamp
3. **Ensure New Signer Propagated** - Verifies the new CA is trusted by the hosted cluster's API server
4. **Regenerate Leaf Certificates** - Re-signs all client certificates with the new CA
5. **Prune Previous Signer** - Removes the old CA from the trust bundle
6. **Ensure Old Signer Revoked** - Verifies that certificates signed by the old CA are rejected

### Monitoring Revocation Progress

Check the status of a CertificateRevocationRequest:

```bash
oc get certificaterevocationrequest -n <hosted-control-plane-namespace> revoke-customer-certs -o yaml
```

The status includes conditions that indicate progress through each step:

| Condition Type | Description |
|----------------|-------------|
| `SignerClassValid` | The specified signer class is recognized |
| `RootCertificatesRegenerated` | New signing CA has been generated |
| `NewCertificatesTrusted` | New CA is trusted by the API server |
| `LeafCertificatesRegenerated` | All client certificates have been re-signed |
| `PreviousCertificatesRevoked` | Old certificates are no longer accepted |

Example output:

```yaml
status:
  revocationTimestamp: "2024-01-15T10:30:00Z"
  previousSigner:
    name: abc123def456
  conditions:
  - type: SignerClassValid
    status: "True"
    reason: AsExpected
    message: Signer class "customer-break-glass" known.
  - type: RootCertificatesRegenerated
    status: "True"
    reason: AsExpected
    message: Signer certificate clusters-example/customer-system-admin-signer regenerated.
  - type: NewCertificatesTrusted
    status: "True"
    reason: AsExpected
    message: New signer certificate clusters-example/customer-system-admin-signer trusted.
  - type: LeafCertificatesRegenerated
    status: "True"
    reason: AsExpected
    message: All leaf certificates are re-generated.
  - type: PreviousCertificatesRevoked
    status: "True"
    reason: AsExpected
    message: Previous signer certificate revoked.
```

### Revocation Events

The controller emits Kubernetes events during revocation. View them with:

```bash
oc get events -n <hosted-control-plane-namespace> --field-selector reason=CertificateRevocationProgressing
```

## Troubleshooting

### Checking Certificate Status

View the signing CA secret:

```bash
oc get secret customer-system-admin-signer -n <hosted-control-plane-namespace> -o yaml
```

View the CA trust bundle:

```bash
oc get configmap customer-system-admin-signer-ca -n <hosted-control-plane-namespace> -o yaml
```

### Viewing Operator Logs

Check the control-plane-pki-operator logs for certificate rotation activity:

```bash
oc logs -n <hosted-control-plane-namespace> deployment/control-plane-pki-operator
```

### Common Issues

**CSR not being approved:**

- Verify a CertificateSigningRequestApproval with the same name exists in the hosted control plane namespace
- Check that the signer name in the CSR matches the expected format

**Revocation stuck:**

- Check the conditions on the CertificateRevocationRequest for the current step
- Review operator logs for errors
- Ensure the hosted cluster's API server is accessible

**Certificate validation errors:**

- Verify the CA trust bundle ConfigMap contains the expected certificates
- Check that the client certificate was signed by a CA in the trust bundle


---

## Source: docs/content/how-to/pki/control-plane-certificates.md

# Internal Control Plane Certificates

This document describes how internal control plane certificates are managed in HyperShift hosted control planes.

## Overview

The control-plane-operator (CPO) manages all internal PKI for the hosted control plane. These certificates secure communication between control plane components and are separate from break-glass credentials, which are managed by the control-plane-pki-operator.

## CA Hierarchy

Each hosted control plane namespace contains **multiple independent self-signed Certificate Authorities**, each governing a distinct trust domain. There is no single shared CA ‚Äî the `root-ca` signs the majority of serving certificates, while specialized CAs exist for etcd, konnectivity, KAS client authentication, and CPOv2 components.

### Two PKI Management Patterns

Certificates are managed through two distinct patterns:

| Pattern | Location | Description |
|---------|----------|-------------|
| **Central PKI Reconciler** | `pki/*.go` in the HCP controller | Manages the `root-ca` and all signers/certs listed in `pki/ca.go`. Runs during the main HCP reconciliation loop. |
| **CPOv2 Manifest Adapters** | Each component's `certs.go` | Components like ignition-server create their own self-signed CA via `WithManifestAdapter`. These are independent of the central PKI reconciler. |

### CA Trust Diagram

```mermaid
graph LR
    subgraph CAS["Central PKI ¬∑ Self-Signed CAs ¬∑ 10-year validity ¬∑ one-shot creation"]
        RCA(["root-ca"])
        ETCD_CA(["etcd-signer"])
        ETCD_M_CA(["etcd-metrics-signer"])
        KONN_CA(["konnectivity-signer"])
        AGG_CA(["aggregator-client-signer"])
        KCP_CA(["kube-control-plane-signer"])
        K2K_CA(["kube-apiserver-to-kubelet-signer"])
        ADMIN_CA(["system-admin-signer"])
        HCCO_CA(["hcco-signer"])
        CSR_CA(["csr-signer"])
    end

    subgraph CPOV2["CPOv2 Component CAs ¬∑ self-signed ¬∑ managed per-component"]
        IGN_CA(["ignition-server-ca"])
    end

    RCA -->|"signs ~24 serving certs"| ROOT_CERTS["kas-server-crt ¬∑ kas-server-private-crt
    openshift-apiserver-cert
    openshift-oauth-apiserver-cert
    openshift-controller-manager-cert
    route-controller-manager-cert
    cluster-policy-controller-cert
    kcm-server ¬∑ cvo-server
    ingress-crt ¬∑ oauth-server-crt ¬∑ mcs-crt
    node-tuning-operator-tls
    olm-operator ¬∑ catalog-operator ¬∑ packageserver
    multus ¬∑ network-node-identity ¬∑ ovn-metrics
    csi-metrics (AWS/Azure) ¬∑ aws-pod-identity"]

    RCA -.->|"public cert via ConfigMap"| TRUST["Consumers:
    kube-controller-manager
    hosted-cluster-config-operator
    ignition-server-proxy
    cluster-network-operator
    cluster-image-registry-operator"]

    ETCD_CA -->|signs| ETCD_CERTS["etcd-client-tls
    etcd-server-tls
    etcd-peer-tls"]
    ETCD_M_CA -->|signs| ETCD_M_CERTS["etcd-metrics-client-tls"]
    KONN_CA -->|signs| KONN_CERTS["konnectivity-server
    konnectivity-cluster
    konnectivity-client
    konnectivity-agent"]
    AGG_CA -->|signs| AGG_CERT["kas-aggregator-crt"]
    KCP_CA -->|signs| KCP_CERTS["kube-scheduler-client
    kube-controller-manager-client"]
    K2K_CA -->|signs| K2K_CERT["kas-kubelet-client-crt"]
    ADMIN_CA -->|signs| ADMIN_CERT["system-admin-client"]
    HCCO_CA -->|signs| HCCO_CERT["hcco-client"]
    CSR_CA -->|signs| CSR_CERTS["kas-bootstrap-client
    openshift-authenticator
    metrics-client"]
    IGN_CA -->|signs| IGN_CERT["ignition-server-serving-cert"]
```

### Root CAs

All root CAs are self-signed with a **10-year validity** and are generated once (never auto-rotated). The `ReconcileSelfSignedCA` function in `support/certs/tls.go` is idempotent ‚Äî if the CA secret already contains `ca.crt` and `ca.key`, reconciliation is a no-op.

| CA Secret | Trust Domain | Certs Signed |
|-----------|-------------|--------------|
| `root-ca` | Most serving certs | ~24 serving certificates (KAS, OpenShift API servers, OAuth, ingress, OLM, network, CSI, etc.) |
| `etcd-signer` | etcd cluster communication | etcd client, server, and peer TLS |
| `etcd-metrics-signer` | etcd metrics scraping | etcd metrics client TLS |
| `konnectivity-signer` | Konnectivity tunnel | server, cluster, client, and agent certs |
| `aggregator-client-signer` | KAS aggregation layer | KAS aggregator client cert |
| `kube-control-plane-signer` | Control plane clients | kube-scheduler and kube-controller-manager client certs |
| `kube-apiserver-to-kubelet-signer` | KAS-to-kubelet auth | KAS kubelet client cert |
| `system-admin-signer` | Admin access | system:admin client cert |
| `hcco-signer` | HCCO auth | hosted-cluster-config-operator client cert |
| `csr-signer` | Bootstrap and authenticator | KAS bootstrap client, openshift-authenticator, metrics client |

### CPOv2 Component CAs

Some CPOv2 components create their own independent self-signed CA via `WithManifestAdapter` in their component definition. These CAs are managed during the component's own reconciliation, not by the central PKI reconciler.

| CA Secret | Component | Serving Cert |
|-----------|-----------|-------------|
| `ignition-server-ca` | ignition-server | `ignition-server-serving-cert` |

### Trust Distribution

The `root-ca` public certificate is distributed via a **ConfigMap** (not the Secret) so pods can verify root-ca-signed serving certs without access to the private key:

| Resource | Content | Mounted By |
|----------|---------|------------|
| Secret `root-ca` | `ca.crt` (public) + `ca.key` (private) | Used by PKI reconciler for signing only |
| ConfigMap `root-ca` | `ca-bundle.crt` (public only) | kube-controller-manager, hosted-cluster-config-operator, ignition-server-proxy, cluster-network-operator, cluster-image-registry-operator |

Other CAs distribute their trust via dedicated ConfigMaps:

| ConfigMap | Contains | Purpose |
|-----------|----------|---------|
| `etcd-ca` | etcd-signer public cert | etcd client verification |
| `etcd-metrics-ca` | etcd-metrics-signer public cert | etcd metrics client verification |
| `konnectivity-ca-bundle` | konnectivity-signer public cert | Konnectivity tunnel verification |
| `aggregator-client-ca` | aggregator-client-signer public cert | KAS aggregation layer |
| `client-ca` | Aggregate of all KAS client signers | KAS client certificate verification |
| `kubelet-client-ca` | kube-apiserver-to-kubelet-signer + csr-signer | Kubelet client verification |

## Certificates Managed

The CPO manages certificates for:

- Kube API server serving certificates (internal and external)
- Client certificates for kubelet, scheduler, controller-manager
- etcd peer and client certificates
- Aggregator proxy certificates
- Service account signing keys

## CA Certificate Lifecycle

Root CA certificates managed by the CPO have a **10-year validity period** and are generated once. They are not automatically rotated unless the certificate or key data is missing from the secret.

## Leaf Certificate Rotation

Leaf certificates (server and client certificates signed by a CA) are automatically rotated by the CPO:

| Setting | Default Value | Description |
|---------|---------------|-------------|
| Validity Period | 1 year | How long the certificate is valid |
| Renewal Threshold | 30 days | Certificate is renewed when less than this time remains |

Rotation is **reconciliation-driven**: during each reconciliation loop, the CPO validates existing certificates and regenerates them if they are approaching expiration or if their configuration has changed.

## Test Configuration Options

Certificate validity and renewal can be customized using environment variables on the CPO:

| Environment Variable | Description |
|---------------------|-------------|
| `CERTIFICATE_VALIDITY` | Custom certificate validity duration (e.g., `8760h` for 1 year) |
| `CERTIFICATE_RENEWAL_PERCENTAGE` | Fraction of validity period at which to renew (e.g., `0.30` renews when 30% of validity remains) |

!!! warning

    Modifying certificate validity settings is advanced configuration. Incorrect values may cause control plane instability or security issues.


---

## Source: docs/content/how-to/pki/index.md

# PKI and Certificate Management

This section covers Public Key Infrastructure (PKI) and certificate management in HyperShift hosted control planes.

HyperShift manages two distinct categories of certificates:

- **Internal Control Plane Certificates** - Managed by the control-plane-operator (CPO), these certificates secure communication between control plane components
- **Break-Glass Credentials** - Managed by the control-plane-pki-operator, these certificates provide emergency access to the hosted cluster's API server

## Topics

- Internal Control Plane Certificates - How the CPO manages certificates for control plane components
- Break-Glass Credentials - Managing emergency access credentials, including rotation and revocation


---

## Source: docs/content/how-to/powervs/create-cluster-powervs.md

---
title: Create PowerVS Hosted Cluster
---

# Create PowerVS Hosted Cluster

Create Hosted cluster in IBM Cloud PowerVS service.

## Prerequisites

Please see prerequisites before setting up the cluster

## Creating the Cluster

Use the `hypershift create cluster powervs` command:

    CLUSTER_NAME=example
    REGION=tok
    ZONE=tok04
    VPC_REGION=jp-tok
    BASEDOMAIN=hypershift-on-power.com
    RESOURCE_GROUP=ibm-hypershift-dev
    RELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.0-0.nightly-multi-2022-09-08-131900
    PULL_SECRET="$HOME/pull-secret"

    ./bin/hypershift create cluster powervs \
        --name $CLUSTER_NAME \
        --region $REGION \
        --zone $ZONE \
        --vpc-region $VPC_REGION \
        --base-domain $BASEDOMAIN \
        --resource-group $RESOURCE_GROUP \
        --release-image $RELEASE_IMAGE \
        --pull-secret $PULL_SECRET \
        --node-pool-replicas=2 \
        --transit-gateway-location $TRANSIT_GATEWAY_LOCATION

where

* CLUSTER_NAME is a name for the cluster.
* REGION is the region where you want to create the PowerVS resources.
* ZONE is the zone under REGION where you want to create the PowerVS resources.
* VPC_REGION is the region where you want to create the VPC resources.
* BASEDOMAIN is the CIS base domain that will be used for your hosted cluster's ingress. It should be an existing CIS domain name.
* RESOURCE_GROUP is the resource group in IBM Cloud where your infrastructure resources will be created.
* RELEASE_IMAGE is the latest multi arch release image.
* PULL_SECRET is a file that contains a valid OpenShift pull secret.
* node-pool-replicas is worker node count. 
* TRANSIT_GATEWAY_LOCATION is the location where you want to create the transit gateway. 

Running this command will create infra and manifests for the hosted cluster and deploys it.

!!! important

        Need to understand --recreate-secrets flag usage before using it. Enabling this flag will result in recreating the creds mentioned here PowerVSPlatformSpec

        This is required when rerunning `hypershift create cluster powervs` command, since API Key once created cannot be retrieved again.

        Please make sure cluster name used is unique across different management cluster before using this flag since this will result in removing the existing cred's service ID and recreate them.


---

## Source: docs/content/how-to/powervs/create-infra-separately.md

---
title: Create PowerVS Infra resources separately
---

# Create IBMCloud PowerVS Infra resources separately

The default behavior of the `hypershift create cluster powervs` command is to create cloud infrastructure
along with the manifests for hosted cluster and apply it.

It is possible to create the cloud infrastructure separately so that the `hypershift create cluster powervs` command can just be used to create the manifests and apply it.

In order to do this, you need to:
1. Create PowerVS infrastructure
2. Create cluster

## Creating the PowerVS infra

Please see prerequisites before setting up the infra

Use the `hypershift create infra powervs` command:

    CLUSTER_NAME=example
    INFRA_ID=example-infra
    REGION=tok
    ZONE=tok04
    VPC_REGION=jp-tok
    BASEDOMAIN=hypershift-on-power.com
    RESOURCE_GROUP=ibm-hypershift-dev
    OUTPUT_INFRA_FILE=infra.json

    ./bin/hypershift create infra powervs \
        --name $CLUSTER_NAME \
        --infra-id $INFRA_ID \
        --region $REGION \
        --zone $ZONE \
        --region $VPC_REGION \
        --base-domain $BASEDOMAIN \
        --resource-group $RESOURCE_GROUP \
        --output-file $OUTPUT_INFRA_FILE

where

* CLUSTER_NAME is a name for the cluster.
* INFRA_ID is a unique name that will be used to name the infrastructure resources.
* REGION is the region where you want to create the PowerVS resources.
* ZONE is the zone under POWERVS_REGION where you want to create the PowerVS resources.
* VPC_REGION is the region where you want to create the VPC resources.
* BASEDOMAIN is the CIS base domain that will be used for your hosted cluster's ingress. It should be an existing CIS domain name.
* RESOURCE_GROUP is the resource group in IBM Cloud where your infrastructure resources will be created.
* OUTPUT_INFRA_FILE is the file where IDs of the infrastructure that has been created will be stored in JSON format.
  This file can then be used as input to the `hypershift create cluster powervs` command to populate
  the appropriate fields in the HostedCluster and NodePool resources.


Running this command should result in the following resources getting created in IBM Cloud:

### PowerVS Cluster Infra Resources

* 1 VPC with Subnet
* 1 PowerVS Cloud Instance
* 1 DHCP Service
  * 1 DHCP Private Network
  * 1 DHCP Public Network
* 1 Cloud Connection

## Creating the Cluster

Once you have the `OUTPUT_INFRA_FILE` generated, can pass this file `hypershift create cluster powervs` to this command with `--infra-json` flag
Running the below command set up the cluster on infra created separately

E.g.:

    CLUSTER_NAME=example
    REGION=tok
    ZONE=tok04
    VPC_REGION=jp-tok
    BASEDOMAIN=hypershift-on-power.com
    RESOURCE_GROUP=ibm-hypershift-dev
    RELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.0-0.nightly-multi-2022-09-08-131900
    PULL_SECRET="$HOME/pull-secret"
    INFRA_JSON=infra.json

    ./bin/hypershift create cluster powervs \
        --name $CLUSTER_NAME \
        --region $REGION \
        --zone $ZONE \
        --vpc-region $VPC_REGION \
        --base-domain $BASEDOMAIN \
        --resource-group $RESOURCE_GROUP \
        --release-image $RELEASE_IMAGE
        --pull-secret $PULL_SECRET \
        --infra-json $INFRA_JSON \
        --node-pool-replicas=2


!!! important

        Need to understand --recreate-secrets flag usage before using it. Enabling this flag will result in recreating the creds mentioned here PowerVSPlatformSpec

        This is required when rerunning `hypershift create cluster powervs` command, since API Key once created cannot be retrieved again.

        Please make sure cluster name used is unique across different management cluster before using this flag since this will result in removing the existing cred's service ID and recreate them.


---

## Source: docs/content/how-to/powervs/global-pull-secret.md

# Global Pull Secret for Hosted Control Planes

## Overview

The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.

The implementation uses a DaemonSet approach that automatically detects when you create an `additional-pull-secret` in the `kube-system` namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.

!!! note

    This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.

## Adding your Pull Secret

!!! important

    All actions described in this section must be performed on the **HostedCluster's workers** (DataPlane), not on the Management Cluster.

To use this functionality, follow these steps:

### 1. Create your additional pull secret

Create a secret named `additional-pull-secret` in the `kube-system` namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: additional-pull-secret
  namespace: kube-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: <base64-encoded-docker-config-json>
```

### 2. Example DockerConfigJSON format

Your `.dockerconfigjson` should follow this structure:

```json
{
  "auths": {
    "registry.example.com": {
      "auth": "base64-encoded-credentials"
    },
    "quay.io/mycompany": {
      "auth": "base64-encoded-credentials"
    }
  }
}
```

!!! tip "Using Namespace-Specific Registry Entries"

    For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., `quay.io/mycompany` instead of just `quay.io`). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.

### 3. Apply the secret

```bash
kubectl apply -f additional-pull-secret.yaml
```

### 4. Verification

After creating the secret, the system will automatically:

1. Validate the secret format
2. Merge it with the original pull secret
3. Deploy a DaemonSet to all nodes
4. Update the kubelet configuration on each node

You can verify the deployment by checking:

```bash
# Check if the DaemonSet is running
kubectl get daemonset global-pull-secret-syncer -n kube-system

# Check the merged pull secret
kubectl get secret global-pull-secret -n kube-system

# Check DaemonSet pods
kubectl get pods -n kube-system -l name=global-pull-secret-syncer
```

## How it works

The Global Pull Secret functionality operates through a multi-component system:

### Automatic Detection
- The Hosted Cluster Config Operator (HCCO) continuously monitors the `kube-system` namespace
- When it detects the creation of `additional-pull-secret`, it triggers the reconciliation process

### Validation and Merging
- The system validates that your secret contains a proper DockerConfigJSON format
- It retrieves the original pull secret from the HostedControlPlane
- Your additional pull secret is merged with the original one
- **If there are conflicting registry entries, the original pull secret takes precedence** (the additional pull secret entry is ignored for conflicting registries)
- The system supports namespace-specific registry entries (e.g., `quay.io/namespace`) for better credential specificity

### Deployment Process
- A `global-pull-secret` is created in the `kube-system` namespace containing the merged result
- RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both `kube-system` and `openshift-config` namespaces
- We use Role and RoleBinding in both namespaces to access secrets in `kube-system` and `openshift-config` namespaces
- A DaemonSet named `global-pull-secret-syncer` is deployed to eligible nodes

!!! warning "NodePool InPlace Strategy Restriction"

    The Global Pull Secret DaemonSet is **not deployed** to nodes that belong to NodePools using the **InPlace upgrade strategy**. This restriction prevents conflicts between the DaemonSet's modifications to `/var/lib/kubelet/config.json` and the Machine Config Daemon (MCD) during InPlace upgrades.

    - **Nodes with Replace strategy**: ‚úÖ Receive Global Pull Secret DaemonSet
    - **Nodes with InPlace strategy**: ‚ùå Do not receive Global Pull Secret DaemonSet

    This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.

### Node-Level Synchronization
- Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace
- When changes are detected, it updates `/var/lib/kubelet/config.json` on the node
- The kubelet service is restarted via DBus to apply the new configuration
- If the restart fails after 3 attempts, the system rolls back the file changes

### Automatic Cleanup
- If you delete the `additional-pull-secret`, the HCCO automatically removes the `global-pull-secret` secret
- The system reverts to using only the original pull secret from the HostedControlPlane
- The DaemonSet continues running but now syncs only the original pull secret to nodes

## Registry Precedence and Conflict Resolution

The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:

### Merge Behavior
- **Original pull secret entries always take precedence** over additional pull secret entries for the same registry
- If both secrets contain an entry for `quay.io`, the original pull secret's credentials will be used
- Your additional pull secret entries are only added if they don't conflict with existing entries
- Warnings are logged when conflicts are detected

### Recommended Approach
To avoid conflicts and ensure your credentials are used, consider these strategies:

1. **Use namespace-specific entries**: Instead of `quay.io`, use `quay.io/your-namespace`
2. **Target specific registries**: Add entries only for registries not already in the original pull secret
3. **Check existing entries**: Review what registries are already configured in the HostedControlPlane

### Example Merge Scenario

**Original Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    }
  }
}
```

**Your Additional Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "your-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

**Resulting Merged Pull Secret:**
```json
{
  "auths": {
    "quay.io": {
      "auth": "original-credentials"
    },
    "quay.io/mycompany": {
      "auth": "your-namespace-credentials"
    }
  }
}
```

Note how the `quay.io` entry keeps the original credentials, but `quay.io/mycompany` is added from your additional secret.

## Implementation details

The implementation consists of several key components working together:

### Core Components

1. **Global Pull Secret Controller** (`globalps` package)
   - Handles validation of user-provided pull secrets
   - Manages the merging logic between original and additional pull secrets
   - Creates and manages RBAC resources
   - Deploys and manages the DaemonSet
   - **Node eligibility assessment**: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions

2. **Sync Global Pull Secret Command** (`sync-global-pullsecret` package)
   - Runs as a DaemonSet on each node
   - Watches for changes to the `global-pull-secret` in `kube-system` namespace
   - Accesses the original `pull-secret` in `openshift-config` namespace
   - Updates the kubelet configuration file
   - Manages kubelet service restarts via DBus

3. **Hosted Cluster Config Operator Integration**
   - Monitors for the presence of `additional-pull-secret`
   - Orchestrates the entire process
   - Handles cleanup when the secret is removed

### Architecture Diagram

```mermaid
graph TB
    %% User Input
    User[User creates additional-pull-secret] --> |kube-system namespace| AdditionalPS[additional-pull-secret Secret]

    %% HCCO Controller
    HCCO[Hosted Cluster Config Operator] --> |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]
    GlobalPSController --> |Validates| AdditionalPS
    GlobalPSController --> |Gets original| OriginalPS[Original pull-secret from HCP]

    %% Secret Processing
    AdditionalPS --> |Validates format| ValidatePS[Validate Additional Pull Secret]
    OriginalPS --> |Extracts data| OriginalPSData[Original Pull Secret Data]
    ValidatePS --> |Extracts data| AdditionalPSData[Additional Pull Secret Data]

    %% Merge Process
    OriginalPSData --> MergeSecrets[Merge Pull Secrets]
    AdditionalPSData --> MergeSecrets
    MergeSecrets --> |Creates merged JSON| GlobalPSData[Global Pull Secret Data]

    %% Secret Creation
    GlobalPSData --> |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]

    %% RBAC Setup
    GlobalPSController --> |Creates RBAC| RBACSetup[Setup RBAC Resources]
    RBACSetup --> ServiceAccount[global-pull-secret-syncer ServiceAccount]
    RBACSetup --> KubeSystemRole[global-pull-secret-syncer Role in kube-system]
    RBACSetup --> KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]
    RBACSetup --> OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]
    RBACSetup --> OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]

    %% DaemonSet Deployment
    GlobalPSController --> |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]
    DaemonSet --> |Runs on each node| DaemonSetPod[DaemonSet Pod]

    %% DaemonSet Pod Details
    DaemonSetPod --> |Mounts host paths| HostMounts[Host Path Mounts]
    HostMounts --> KubeletPath["/var/lib/kubelet"]
    HostMounts --> DbusPath["/var/run/dbus"]

    %% Container Execution
    DaemonSetPod --> |Runs command| Container[control-plane-operator Container]
    Container --> |Executes| SyncCommand[sync-global-pullsecret command]

    %% Sync Process
    SyncCommand --> |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]
    SyncController --> |Reads secret| ReadGlobalPS[Read global-pull-secret]
    SyncController --> |Reads original| ReadOriginalPS[Read original pull-secret]

    %% File Update Process
    ReadGlobalPS --> |Gets data| GlobalPSBytes[Global Pull Secret Bytes]
    ReadOriginalPS --> |Gets data| OriginalPSBytes[Original Pull Secret Bytes]

    %% Decision Logic
    GlobalPSBytes --> |If exists| UseGlobalPS[Use Global Pull Secret]
    OriginalPSBytes --> |If not exists| UseOriginalPS[Use Original Pull Secret]

    %% File Update
    UseGlobalPS --> |Updates file| UpdateKubeletConfig["Update /var/lib/kubelet/config.json"]
    UseOriginalPS --> |Updates file| UpdateKubeletConfig

    %% Kubelet Restart
    UpdateKubeletConfig --> |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]
    RestartKubelet --> |Via dbus| DbusConnection[DBus Connection]

    %% Error Handling
    UpdateKubeletConfig --> |If restart fails| RollbackProcess[Rollback Process]
    RollbackProcess --> |Restore original| RestoreOriginal[Restore Original File Content]

    %% Cleanup Process
    GlobalPSController --> |If additional PS deleted| CleanupProcess[Cleanup Process]
    CleanupProcess --> |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]
    CleanupProcess --> |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]

    %% Styling
    classDef userInput fill:#e1f5fe
    classDef controller fill:#f3e5f5
    classDef secret fill:#e8f5e8
    classDef process fill:#fff3e0
    classDef daemonSet fill:#fce4ec
    classDef fileSystem fill:#f1f8e9

    class User,AdditionalPS userInput
    class HCCO,GlobalPSController,SyncController controller
    class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret
    class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process
    class DaemonSet,DaemonSetPod,Container daemonSet
    class KubeletPath,DbusPath fileSystem
```

### Key Features

- **Security**: Only watches specific secrets in `kube-system` and `openshift-config` namespaces
- **Robustness**: Includes automatic rollback in case of failures
- **Efficiency**
  - Only updates when there are actual changes
  - The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation
- **Security considerations**: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:
  - Write to `/var/lib/kubelet/config.json` (kubelet configuration file)
  - Connect to systemd via DBus for service management
  - Restart kubelet.service, which requires root privileges
- **Smart node targeting**: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts

### InPlace NodePool Handling

To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:

#### Node Labeling Process
1. **MachineSets Discovery**: The controller queries the management cluster for MachineSets with InPlace-specific annotations (`hypershift.openshift.io/nodePoolTargetConfigVersion`)
2. **Machine Enumeration**: For each InPlace MachineSets, it lists all associated Machines
3. **Node Identification**: Maps Machine objects to their corresponding nodes via `machine.Status.NodeRef.Name`
4. **Labeling**: Applies `hypershift.openshift.io/nodepool-inplace-strategy=true` label to identified nodes

#### DaemonSet Scheduling Configuration
The DaemonSet uses NodeAffinity to exclude InPlace nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hypershift.openshift.io/nodepool-inplace-strategy
                operator: DoesNotExist
```

This ensures that:
- **Nodes without the label**: ‚úÖ Are eligible for DaemonSet scheduling
- **Nodes with the label** (any value): ‚ùå Are excluded from DaemonSet scheduling

#### Conflict Prevention Benefits
- **Prevents MCD failures**: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades
- **Maintains upgrade reliability**: InPlace upgrade processes are not interrupted by Global Pull Secret modifications
- **Automatic detection**: No manual intervention required - the system automatically identifies and handles InPlace nodes

### Error Handling

The system includes comprehensive error handling:

- **Validation errors**: Invalid DockerConfigJSON format is caught early
- **Restart failures**: If kubelet restart fails after 3 attempts, the file is rolled back
- **Resource cleanup**: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret

This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.

---

## Source: docs/content/how-to/powervs/prerequisites-and-env-guide.md

# Prerequisites

## Generic
* The HyperShift CLI (`hypershift`).

    Install it using Go 1.18:
        ```shell linenums="1"
        git clone https://github.com/openshift/hypershift.git
        cd hypershift
        make build
        sudo install -m 0755 bin/hypershift /usr/local/bin/hypershift
        ```

* Admin access to an OpenShift cluster (version 4.8+) specified by the `KUBECONFIG` environment variable.
* The OpenShift CLI (`oc`) or Kubernetes CLI (`kubectl`).
* A valid pull secret file for the `quay.io/openshift-release-dev` repository.

# Install Hypershift
Install HyperShift into the management cluster.
Once Hypershift CLI and management cluster is ready, run below command to install Hypershift operator and CRDs which are required to setup the cluster.

```
hypershift install
```

## Authentication
There are two ways to set up authentication

* Authenticate IBM Cloud Clients by setting the `IBMCLOUD_API_KEY` environment var to your API Key.
* Authenticate IBM Cloud Clients by setting the `IBMCLOUD_CREDENTIALS` environment var pointing to a file containing your API Key.

## Authorization:

API Key used should have below services with respective roles for hypershift cluster to get created in IBM Cloud.

| Service                                    | Roles                                                   |
|--------------------------------------------|---------------------------------------------------------|
| Workspace for Power Systems Virtual Server | Manager, Administrator                                  |
| VPC Infrastructure Services                | Manager, Administrator                                  |
| Internet Services                          | Manager, Administrator                                  |
| Direct Link                                | Viewer                                                  |
| IAM Identity Service                       | User API key creator, Service ID creator, Administrator |
| All account management services            | Administrator                                           |
| All Identity and Access enabled services   | Manager, Editor                                         |
| Cloud Object Storage                       | Manager, Administrator                                  |
| Transit Gateway                            | Manager, Editor                                         |


## Base Domain
Need to have existing CIS Domain in IBM Cloud Internet Services which can be used as a `BASEDOMAIN` while creating the cluster.

## Region and Zones
Refer this to get possible region and zone values. Substitute those with `REGION` `ZONE` `VPC_REGION` and `TRANSIT_GATEWAY_LOCATION` while creating the cluster.

## Release Image
Use this to get latest multi arch nightly build as release image. Substitute it with `RELEASE_IMAGE` while creating the cluster.

## Custom Endpoints
Use following environment variables to set custom endpoint.
```
IBMCLOUD_POWER_API_ENDPOINT    - to setup PowerVS custom endpoint
IBMCLOUD_VPC_API_ENDPOINT      - to setup VPC custom endpoint
IBMCLOUD_PLATFORM_API_ENDPOINT - to setup platform services custom endpoint
IBMCLOUD_COS_API_ENDPOINT      - to setup COS custom endpoint, can use this to set up custom endpoints mentioned here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#endpoints-region 
```

---

## Source: docs/content/how-to/resource-based-control-plane-autoscaling.md

# Resource-Based Control Plane Autoscaling

Resource-based control plane autoscaling enables automatic sizing of HostedClusters based on actual Kube API server resource usage rather than worker node count. This feature uses Vertical Pod Autoscaler (VPA) recommendations to determine the optimal cluster size class for a HostedCluster.

## Platform Support

**Important**: This feature is only available for HostedClusters using the request serving isolation architecture on AWS. The feature requires the `dedicated-request-serving-components` topology annotation to be set on the HostedCluster.

## Prerequisites

Before enabling resource-based control plane autoscaling, ensure the following prerequisites are met:

### VPA Operator

The Vertical Pod Autoscaler operator must be installed on the management cluster via Operator Lifecycle Manager (OLM). The VPA operator only supports the `OwnNamespace` install mode, so it must be installed in its own namespace.

#### Step 1: Create the VPA Namespace

Create the namespace for the VPA operator:

```bash
kubectl create namespace openshift-vertical-pod-autoscaler
```

#### Step 2: Create the OperatorGroup

Create an OperatorGroup that targets only the VPA namespace:

```bash
kubectl apply -f - <<EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: vpa-operator-group
  namespace: openshift-vertical-pod-autoscaler
spec:
  targetNamespaces:
    - openshift-vertical-pod-autoscaler
EOF
```

#### Step 3: Create the Subscription

Create a Subscription to install the VPA operator from the `redhat-operators` catalog source:

```bash
kubectl apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: vertical-pod-autoscaler
  namespace: openshift-vertical-pod-autoscaler
spec:
  channel: stable
  name: vertical-pod-autoscaler
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Automatic
EOF
```

#### Step 4: Verify Installation

Wait for the ClusterServiceVersion (CSV) to reach the `Succeeded` phase:

```bash
kubectl wait --for=condition=phase=Succeeded \
  csv -n openshift-vertical-pod-autoscaler \
  -l operators.coreos.com/vertical-pod-autoscaler.openshift-vertical-pod-autoscaler \
  --timeout=15m
```

Verify that the VPA CRD is available:

```bash
kubectl get crd verticalpodautoscalers.autoscaling.k8s.io
```

Verify that the VPA recommender deployment is available:

```bash
kubectl wait --for=condition=available \
  deployment/vpa-recommender-default \
  -n openshift-vertical-pod-autoscaler \
  --timeout=15m
```

### VPA Controller Instance

After the VPA operator is installed, a `VerticalPodAutoscalerController` instance named `default` is automatically created in the `openshift-vertical-pod-autoscaler` namespace. You must configure this controller instance for resource-based autoscaling.

Example `VerticalPodAutoscalerController` configuration:

```yaml
apiVersion: autoscaling.openshift.io/v1
kind: VerticalPodAutoscalerController
metadata:
  name: default
  namespace: openshift-vertical-pod-autoscaler
spec:
  recommendationOnly: true
  deploymentOverrides:
    recommender:
      container:
        args:
          - --memory-aggregation-interval=1h
          - --memory-aggregation-interval-count=12
          - --memory-histogram-decay-half-life=1h
```

For a complete list of VPA recommender configuration flags, refer to the VPA recommender documentation.

### ClusterSizingConfiguration

A `ClusterSizingConfiguration` resource named `cluster` must exist in the management cluster with:

- Optional: Size configurations that include `capacity` specifications with memory values. If `capacity` is not specified for a size, the system will introspect the memory and CPU capacity from existing MachineSets in the `openshift-machine-api` namespace. The system looks for MachineSets with the `hypershift.openshift.io/cluster-size` label matching the size name and reads the `machine.openshift.io/memoryMb` and `machine.openshift.io/vCPU` annotations from those MachineSets. The first MachineSet found for each size label is used as the authoritative source for that size's capacity.
- Optional: `resourceBasedAutoscaling.kubeAPIServerMemoryFraction` configuration

## Enabling Resource-Based Autoscaling

To enable resource-based control plane autoscaling for a HostedCluster, add the following annotation:

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: example-cluster
  namespace: clusters
  annotations:
    hypershift.openshift.io/topology: dedicated-request-serving-components
    hypershift.openshift.io/resource-based-cp-auto-scaling: "true"
```

The feature requires both annotations:
- `hypershift.openshift.io/topology: dedicated-request-serving-components` - Enables request serving isolation architecture
- `hypershift.openshift.io/resource-based-cp-auto-scaling: "true"` - Enables resource-based autoscaling

## Annotations

The following annotations are used by the resource-based control plane autoscaling feature:

### Input Annotations

- **`hypershift.openshift.io/resource-based-cp-auto-scaling`**: Set to `"true"` to enable resource-based autoscaling for the HostedCluster. This annotation must be used in conjunction with the `dedicated-request-serving-components` topology.

- **`hypershift.openshift.io/topology`**: Must be set to `dedicated-request-serving-components` for the feature to work.

### Output Annotations

- **`hypershift.openshift.io/recommended-cluster-size`**: This annotation is automatically set by the autoscaler controller with the recommended cluster size class based on VPA recommendations. The value corresponds to a size name from the `ClusterSizingConfiguration`.

## ClusterSizingConfiguration

The `ClusterSizingConfiguration` resource controls how cluster sizes are determined. For resource-based autoscaling, configure the following:

### Size Capacity Configuration

Each size configuration in `spec.sizes` can include a `capacity` field with memory and cpu specifications:

```yaml
apiVersion: scheduling.hypershift.openshift.io/v1alpha1
kind: ClusterSizingConfiguration
metadata:
  name: cluster
spec:
  sizes:
    - name: small
      criteria:
        from: 0
        to: 10
      capacity:
        memory: 32Gi
        cpu: "8"
    - name: medium
      criteria:
        from: 11
        to: 50
      capacity:
        memory: 64Gi
        cpu: "16"
    - name: large
      criteria:
        from: 51
      capacity:
        memory: 128Gi
        cpu: "32"
```

### Resource-Based Autoscaling Configuration

The `resourceBasedAutoscaling` section allows you to configure how memory recommendations are interpreted:

```yaml
spec:
  resourceBasedAutoscaling:
    kubeAPIServerMemoryFraction: "0.65"
```

- **`kubeAPIServerMemoryFraction`**: A value between 0 and 1 that determines what fraction of a machine's total memory is available for the Kube API server pod. This fraction is used to determine whether a VPA memory recommendation fits within a particular cluster size. If not specified, a default fraction of 0.65 is used.

The autoscaler selects the smallest cluster size for which:
```plaintext
machine_memory * kubeAPIServerMemoryFraction >= VPA_recommended_memory
```

## How It Works

1. When resource-based autoscaling is enabled, the HyperShift operator creates a `VerticalPodAutoscaler` resource targeting the `kube-apiserver` deployment in the control plane namespace.

2. The VPA monitors the kube-apiserver container's resource usage and generates memory recommendations.

3. The autoscaler controller reads the VPA recommendations and determines the appropriate cluster size class based on:
   - The VPA's memory recommendation for the kube-apiserver container
   - The machine memory capacity for each size class (from `ClusterSizingConfiguration`)
   - The configured `kubeAPIServerMemoryFraction`

4. The controller sets the `hypershift.openshift.io/recommended-cluster-size` annotation on the HostedCluster with the recommended size.

5. The hosted cluster sizing controller uses this recommendation (along with other factors) to determine the actual cluster size applied to the HostedCluster.

## Monitoring

You can monitor the autoscaling behavior by checking:

- The `hypershift.openshift.io/recommended-cluster-size` annotation on the HostedCluster
- The VPA resource status in the control plane namespace: `kubectl get vpa kube-apiserver -n <control-plane-namespace>`
- The VPA recommendation conditions and container recommendations

## Troubleshooting

If the recommended cluster size is not being set:

1. Verify that both required annotations are present on the HostedCluster
2. Check that the VPA operator is installed and the VPA controller instance exists
3. Verify that the VPA resource exists in the control plane namespace and has valid recommendations
4. Ensure the `ClusterSizingConfiguration` has size configurations with capacity specifications
5. Check controller logs for errors related to size cache updates or VPA reconciliation



---

## Source: docs/content/how-to/restart-control-plane-components.md

# Restart Control Plane Components

The `hypershift.openshift.io/restart-date` annotation allows a hypershift administrator to restart all control plane components for a particular `HostedCluster`. This is useful for certificate rotation and in various development situations where a control plane restart is desired.

Restart a control plane by annotating the `HostedCluster` resource:

```
oc annotate hostedcluster -n clusters example hypershift.openshift.io/restart-date=$(date --iso-8601=seconds)
```

The restart occurs whenever the value of this annotation changes. The `date` command in the example serves only as a source of a unique string.  The annotation is treated only as a string, not a timestamp.

The list of components restarted are listed below:

* catalog-operator
* certified-operators-catalog
* cluster-api
* cluster-autoscaler
* cluster-policy-controller
* cluster-version-operator
* community-operators-catalog
* control-plane-operator
* hosted-cluster-config-operator
* ignition-server
* ingress-operator
* konnectivity-agent
* konnectivity-server
* kube-apiserver
* kube-controller-manager
* kube-scheduler
* machine-approver
* oauth-openshift
* olm-operator
* openshift-apiserver
* openshift-controller-manager
* openshift-oauth-apiserver
* packageserver
* redhat-marketplace-catalog
* redhat-operators-catalog

---

## Source: docs/content/how-to/sdn/other-sdn-providers.md

This document explains how to create a HostedCluster that runs an SDN provider different from OVNKubernetes. The document assumes that you already have the required infrastructure in place to create HostedClusters.

!!! important

    The work described here is **not supported**. SDN providers **must** certify their software on HyperShift before it becomes a supported solution. The steps described here are just a technical reference for people who wants to try different SDN providers in HyperShift.

Versions used while writing this doc:

- Management cluster running OpenShift `v4.14.5` and HyperShift Operator version `e87182ca75da37c74b371aa0f17aeaa41437561a`.
- HostedCluster release set to OpenShift `v4.14.10`.

!!! important

    To configure a different CNI provider for the Hosted Cluster, you must adjust the `hostedcluster.spec.networking.networkType` to `Other`. By doing so, the Control Plane Operator will skip the deployment of the default CNI provider.

## Calico
### Deployment

In this scenario we are using the Calico version v3.27.0 which is the last one at the time of this writing. The steps followed rely on the docs by Tigera to deploy Calico on OpenShift.

1. Create a `HostedCluster` and set its `HostedCluster.spec.networking.networkType` to `Other`.

2. Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.

3. Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in `NotReady` state.

    ~~~sh
    export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig
    oc get nodes
    ~~~

    ~~~output
    NAME             STATUS     ROLES    AGE     VERSION
    hosted-worker1   NotReady   worker   2m51s   v1.27.8+4fab27b
    hosted-worker2   NotReady   worker   2m52s   v1.27.8+4fab27b
    ~~~

4. Apply the yaml manifests provided by `Tigera` in the HostedCluster:

    ~~~sh
    mkdir calico
    wget -qO- https://github.com/projectcalico/calico/releases/download/v3.27.0/ocp.tgz | tar xvz --strip-components=1 -C calico
    cd calico/
    ls *crd*.yaml | xargs -n1 oc apply -f
    ls 00* | xargs -n1 oc apply -f
    ls 01* | xargs -n1 oc apply -f
    ls 02* | xargs -n1 oc apply -f
    ~~~

    ~~~output
    customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
    customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
    customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
    namespace/calico-apiserver created
    namespace/calico-system created
    namespace/tigera-operator created
    apiserver.operator.tigera.io/default created
    installation.operator.tigera.io/default created
    configmap/calico-resources created
    clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
    clusterrole.rbac.authorization.k8s.io/tigera-operator created
    serviceaccount/tigera-operator created
    deployment.apps/tigera-operator created
    ~~~

### Checks

We should see the following pods running in the `tigera-operator` namespace:

  ~~~sh
  oc -n tigera-operator get pods
  ~~~

  ~~~output
  NAME                              READY   STATUS    RESTARTS   AGE
  tigera-operator-dc7c9647f-fvcvd   1/1     Running   0          2m1s
  ~~~

We should see the following pods running in the `calico-system` namespace:

  ~~~sh
  oc -n calico-system get pods
  ~~~

  ~~~output
  NAME                                       READY   STATUS    RESTARTS   AGE
  calico-kube-controllers-69d6d5ff89-5ftcn   1/1     Running   0          2m1s
  calico-node-6bzth                          1/1     Running   0          2m2s
  calico-node-bl4b6                          1/1     Running   0          2m2s
  calico-typha-6558c4c89d-mq2hw              1/1     Running   0          2m2s
  csi-node-driver-l948w                      2/2     Running   0          2m1s
  csi-node-driver-r6rgw                      2/2     Running   0          2m2s
  ~~~

We should see the following pods running in the `calico-apiserver` namespace:

  ~~~sh
  oc -n calico-apiserver get pods
  ~~~

  ~~~output
  NAME                                READY   STATUS    RESTARTS   AGE
  calico-apiserver-7bfbf8fd7c-d75fw   1/1     Running   0          84s
  calico-apiserver-7bfbf8fd7c-fqxjm   1/1     Running   0          84s
  ~~~

The nodes should've moved to `Ready` state:

  ~~~sh
  oc get nodes
  ~~~

  ~~~output
  NAME             STATUS   ROLES    AGE   VERSION
  hosted-worker1   Ready    worker   10m   v1.27.8+4fab27b
  hosted-worker2   Ready    worker   10m   v1.27.8+4fab27b
  ~~~

The HostedCluster deployment will continue, at this point the SDN is running.

## Cilium
### Deployment

In this scenario we are using the Cilium version v1.14.5 which is the last one at the time of this writing. The steps followed rely on the docs by Cilium project to deploy Cilium on OpenShift.

1. Create a `HostedCluster` and set its `HostedCluster.spec.networking.networkType` to `Other`.

2. Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.

3. Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in `NotReady` state.

    ~~~sh
    export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig
    oc get nodes
    ~~~

    ~~~output
    NAME             STATUS     ROLES    AGE     VERSION
    hosted-worker1   NotReady   worker   2m30s   v1.27.8+4fab27b
    hosted-worker2   NotReady   worker   2m33s   v1.27.8+4fab27b
    ~~~

4. Apply the yaml manifests provided by `Isovalent` in the HostedCluster:

    ~~~sh
    #!/bin/bash

    version="1.14.5"
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-03-cilium-ciliumconfigs-crd.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00000-cilium-namespace.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00001-cilium-olm-serviceaccount.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00002-cilium-olm-deployment.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00003-cilium-olm-service.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00004-cilium-olm-leader-election-role.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00005-cilium-olm-role.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00006-leader-election-rolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00007-cilium-olm-rolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00008-cilium-cilium-olm-clusterrole.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00009-cilium-cilium-clusterrole.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00010-cilium-cilium-olm-clusterrolebinding.yaml
    oc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00011-cilium-cilium-clusterrolebinding.yaml
    ~~~


5. Use the right configuration for each network stack

=== "IPv4"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv4PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: true
      ipv6:
        enabled: false
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv4PodCIDRList:
            - "10.128.0.0/14"
          clusterPoolIPv4MaskSize: "23"
      nativeRoutingCIDR: "10.128.0.0/14"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~

=== "IPv6"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv6PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: false
      ipv6:
        enabled: true
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv6PodCIDRList:
            - "fd01::/48"
          clusterPoolIPv6MaskSize: "48"
      nativeRoutingCIDR: "fd01::/48"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~


=== "Dual stack"

    ~~~yaml
    apiVersion: cilium.io/v1alpha1
    kind: CiliumConfig
    metadata:
      name: cilium
      namespace: cilium
    spec:
      debug:
        enabled: true
      k8s:
        requireIPv4PodCIDR: true
      logSystemLoad: true
      bpf:
        preallocateMaps: true
      etcd:
        leaseTTL: 30s
      ipv4:
        enabled: true
      ipv6:
        enabled: true
      identityChangeGracePeriod: 0s
      ipam:
        mode: "cluster-pool"
        operator:
          clusterPoolIPv4PodCIDRList:
            - "10.128.0.0/14"
          clusterPoolIPv4MaskSize: "23"
      nativeRoutingCIDR: "10.128.0.0/14"
      endpointRoutes: {enabled: true}
      clusterHealthPort: 9940
      tunnelPort: 4789
      cni:
        binPath: "/var/lib/cni/bin"
        confPath: "/var/run/multus/cni/net.d"
        chainingMode: portmap
      prometheus:
        serviceMonitor: {enabled: false}
      hubble:
        tls: {enabled: false}
      sessionAffinity: true
    ~~~

    ~~~sh
    oc apply -f ciliumconfig.yaml
    ~~~

!!! important

    Make sure you've changed the networking values according to your platform details `spec.ipam.operator.clusterPoolIPv4PodCIDRList`, `spec.ipam.operator.clusterPoolIPv4MaskSize` and `nativeRoutingCIDR` in IPv4 and `spec.ipam.operator.clusterPoolIPv6PodCIDRList`, `spec.ipam.operator.clusterPoolIPv6MaskSize` and `nativeRoutingCIDR` in IPv6 case.

### Checks

This will be the output:

  ~~~output
  customresourcedefinition.apiextensions.k8s.io/ciliumconfigs.cilium.io created
  namespace/cilium created
  serviceaccount/cilium-olm created
  Warning: would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), hostPort (container "operator" uses hostPort 9443), allowPrivilegeEscalation != false (container "operator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "operator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "operator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "operator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
  deployment.apps/cilium-olm created
  service/cilium-olm created
  role.rbac.authorization.k8s.io/cilium-olm-leader-election created
  role.rbac.authorization.k8s.io/cilium-olm created
  rolebinding.rbac.authorization.k8s.io/leader-election created
  rolebinding.rbac.authorization.k8s.io/cilium-olm created
  clusterrole.rbac.authorization.k8s.io/cilium-cilium-olm created
  clusterrole.rbac.authorization.k8s.io/cilium-cilium created
  clusterrolebinding.rbac.authorization.k8s.io/cilium-cilium-olm created
  clusterrolebinding.rbac.authorization.k8s.io/cilium-cilium created
  ciliumconfig.cilium.io/cilium created
  ~~~

We should see the following pods running in the `cilium` namespace:

  ~~~sh
  oc -n cilium get pods
  ~~~

  ~~~output
  NAME                               READY   STATUS    RESTARTS   AGE
  cilium-ds5tr                       1/1     Running   0          106s
  cilium-olm-7c9cf7c948-txkvt        1/1     Running   0          2m36s
  cilium-operator-595594bf7d-gbnns   1/1     Running   0          106s
  cilium-operator-595594bf7d-mn5wc   1/1     Running   0          106s
  cilium-wzhdk                       1/1     Running   0          106s
  ~~~

The nodes should've moved to `Ready` state:

  ~~~sh
  oc get nodes
  ~~~

  ~~~output
  NAME             STATUS   ROLES    AGE   VERSION
  hosted-worker1   Ready    worker   8m    v1.27.8+4fab27b
  hosted-worker2   Ready    worker   8m    v1.27.8+4fab27b
  ~~~

The HostedCluster deployment will continue, at this point the SDN is running.

### Validation

Additionally you have some conformance tests that could be deployed into the HostedCluster in order to validate if the Cilium SDN was deployed and working properly.

In order for Cilium connectivity test pods to run on OpenShift, a simple custom SecurityContextConstraints object is required to allow hostPort/hostNetwork which the connectivity test pods relies on. it should only set allowHostPorts and allowHostNetwork without any other privileges.

  ~~~sh
  oc apply -f - <<EOF
  apiVersion: security.openshift.io/v1
  kind: SecurityContextConstraints
  metadata:
    name: cilium-test
  allowHostPorts: true
  allowHostNetwork: true
  users:
    - system:serviceaccount:cilium-test:default
  priority: null
  readOnlyRootFilesystem: false
  runAsUser:
    type: MustRunAsRange
  seLinuxContext:
    type: MustRunAs
  volumes: null
  allowHostDirVolumePlugin: false
  allowHostIPC: false
  allowHostPID: false
  allowPrivilegeEscalation: false
  allowPrivilegedContainer: false
  allowedCapabilities: null
  defaultAddCapabilities: null
  requiredDropCapabilities: null
  groups: null
  EOF
  ~~~

  ~~~sh
  version="1.14.5"
  oc apply -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/${version}/examples/kubernetes/connectivity-check/connectivity-check.yaml
  ~~~

  ~~~sh
  oc get pod -n cilium-test
  ~~~

  ~~~output
  NAME                                                     READY   STATUS    RESTARTS   AGE
  echo-a-846dcb4-kq7zh                                     1/1     Running   0          23h
  echo-b-58f67d5b86-5mrtx                                  1/1     Running   0          23h
  echo-b-host-84d7468c8d-nf4vk                             1/1     Running   0          23h
  host-to-b-multi-node-clusterip-b98ff785c-b9vgf           1/1     Running   0          23h
  host-to-b-multi-node-headless-5c55d85dfc-5xjbc           1/1     Running   0          23h
  pod-to-a-6b996b7675-46kkf                                1/1     Running   0          23h
  pod-to-a-allowed-cnp-c958b55bf-6vskb                     1/1     Running   0          23h
  pod-to-a-denied-cnp-6d9b8cbff5-lbrgp                     1/1     Running   0          23h
  pod-to-b-intra-node-nodeport-5f9c4c866f-mhfs4            1/1     Running   0          23h
  pod-to-b-multi-node-clusterip-7cb4bf5495-hmmtg           1/1     Running   0          23h
  pod-to-b-multi-node-headless-68975fc557-sqbgq            1/1     Running   0          23h
  pod-to-b-multi-node-nodeport-559c54c6fc-2rhvv            1/1     Running   0          23h
  pod-to-external-1111-5c4cfd9497-6slss                    1/1     Running   0          23h
  pod-to-external-fqdn-allow-google-cnp-7d65d9b747-w4cx5   1/1     Running   0          23h
  ~~~


---

## Source: docs/content/how-to/troubleshooting-general.md

# Troubleshooting

## General
### Dump HostedCluster resources from a management cluster
To dump the relevant HostedCluster objects, we will need some prerequisites:

- `cluster-admin` access to the management cluster
- The HostedCluster `name` and the `namespace` where the CR is deployed
- The Hypershift CLI
- The Openshift CLI

Once we have these elements and the shell with the Kubeconfig loaded pointing to the management cluster, we will execute these commands:

```bash
CLUSTERNAME="samplecluster"
CLUSTERNS="clusters"

mkdir clusterDump-${CLUSTERNS}-${CLUSTERNAME}
hypershift dump cluster \
    --name ${CLUSTERNAME} \
    --namespace ${CLUSTERNS} \
    --dump-guest-cluster \
    --artifact-dir clusterDump-${CLUSTERNS}-${CLUSTERNAME}
```

After some time, the output will show something like this:

```bash
2023-06-06T12:18:20+02:00	INFO	Archiving dump	{"command": "tar", "args": ["-cvzf", "hypershift-dump.tar.gz", "cluster-scoped-resources", "event-filter.html", "namespaces", "network_logs", "timestamp"]}
2023-06-06T12:18:21+02:00	INFO	Successfully archived dump	{"duration": "1.519376292s"}
```

This dump contains artifacts that aid in troubleshooting issues with hosted control plane clusters.

#### Contents from Dump Command
The Management's Cluster's dump content:

- **Cluster scoped resources**: Basically nodes definitions of the management cluster.
- **The dump compressed file**: This is useful if you need to share the dump with other people
- **Namespaced resources**: This includes all the objects from all the relevant namespaces, like configmaps, services, events, logs, etc...
- **Network logs**: Includes the OVN northbound and southbound DBs and the statuses for each one.
- **HostedClusters**: Another level of dump, involves all the resources inside of the guest cluster.

The Guest Cluster dump content:

- **Cluster scoped resources**: It contains al the cluster-wide objects, things like nodes, CRDs, etc...
- **Namespaced resources**: This includes all the objects from all the relevant namespaces, like configmaps, services, events, logs, etc...

!!! note

    **The dump will not contain any Secret object** from the cluster, only references to the secret's names.

#### Impersonation as user/service account

The dump command can be used with the flag `--as`, which works in the same way as the `oc` client. If you execute the command with the flag, the CLI will impersonate all the queries against the management cluster, using that username or service account.

The service account should have enough permissions to query all the objects from the namespaces, so cluster-admin is recommended to make sure you have enough permissions. The service account should be located (or have permissions to query) at least the HostedControlPlane namespace.

!!! note

    If your user/sa doesn't have enough permissions, the command will be executed and will dump only the objects you have permissions to get and during that process some `forbidden` errors will be raised.

- Impersonation Sample using a service account:

```bash
CLUSTERNAME="samplecluster"
CLUSTERNS="clusters"
SA="samplesa"
SA_NAMESPACE="default"

mkdir clusterDump-${CLUSTERNS}-${CLUSTERNAME}
hypershift dump cluster \
    --name ${CLUSTERNAME} \
    --namespace ${CLUSTERNS} \
    --dump-guest-cluster \
    --as "system:serviceaccount:${SA_NAMESPACE}:${SA}" \
    --artifact-dir clusterDump-${CLUSTERNS}-${CLUSTERNAME}
```

- Impersonation Sample using a user:

```bash
CLUSTERNAME="samplecluster"
CLUSTERNS="clusters"
CLUSTERUSER="cloud-admin"

mkdir clusterDump-${CLUSTERNS}-${CLUSTERNAME}
hypershift dump cluster \
    --name ${CLUSTERNAME} \
    --namespace ${CLUSTERNS} \
    --dump-guest-cluster \
    --as "${CLUSTERUSER}" \
    --artifact-dir clusterDump-${CLUSTERNS}-${CLUSTERNAME}
```

### How to view the ignition payload

1. Define the HCP namespace where the user-data secret is stored
```shell 
HCP_NAMESPACE="<hcp-namespace>"
```
1. Find the user-data secret in the HCP namespace
```shell
SECRET_NAME=$(oc get secret -n $HCP_NAMESPACE | grep user-data | awk '{print $1}')
```
2. Retrieve the secret and decode the value key
```shell
USER_DATA_VALUE=$(oc get secret $SECRET_NAME -n $HCP_NAMESPACE -o jsonpath='{.data.value}' | base64 -d)
```
3. Extract the bearer token and ignition server from the user-data value
```shell
BEARER_TOKEN=$(echo $USER_DATA_VALUE | jq -r '.ignition.config.merge[0].httpHeaders[] | select(.name=="Authorization") | .value')
IGNITION_SERVER=$(echo $USER_DATA_VALUE | jq -r '.ignition.config.merge[0].source')
```

4. Download the ignition payload from the ignition-server and save it to a file
```shell
curl -k -H "Authorization: $BEARER_TOKEN" $IGNITION_SERVER -o ignition.json
```

#### How to view the files in the ignition payload
Following on from the previous section and the example file in step 4, `ignition.json`, to view the files within the payload, execute the following command:
```
% cat ignition.json | jq '.storage.files[].path'
"/usr/local/bin/nm-clean-initrd-state.sh"
"/etc/NetworkManager/conf.d/01-ipv6.conf"
"/etc/NetworkManager/conf.d/20-keyfiles.conf"
"/etc/pki/ca-trust/source/anchors/openshift-config-user-ca-bundle.crt"
"/etc/kubernetes/apiserver-url.env"
"/etc/audit/rules.d/mco-audit-quiet-containers.rules"
"/etc/tmpfiles.d/cleanup-cni.conf"
"/usr/local/bin/configure-ovs.sh"
"/etc/containers/storage.conf"
"/etc/mco/proxy.env"
...
```

To view the specific contents of a file, execute the following command:
```
% cat ignition.json | jq '.storage.files[] | select(.path=="/etc/kubernetes/apiserver-url.env")'
{
  "overwrite": true,
  "path": "/etc/kubernetes/apiserver-url.env",
  "contents": {
    "source": "data:,KUBERNETES_SERVICE_HOST%3D'52.150.32.156'%0AKUBERNETES_SERVICE_PORT%3D'7443'%0A"
  },
  "mode": 420
}
```

## Troubleshoot By Provider
If you have provider-scoped questions, please take a look at the troubleshooting section for the provider in the list below.
We will keep adding more and more troubleshooting sections and updating the existent ones.

- AWS
- Azure

---

## Source: docs/content/how-to/upgrades.md

# Upgrades
HyperShift enables the decoupling of upgrades between the Control Plane and Nodes.

This allows there to be two separate procedures a Cluster Service Provider can take, giving them flexibility to manage the different components separately.

Control Plane upgrades are driven by the HostedCluster, while Node upgrades are driven by its respective NodePool. Both the HostedCluster and NodePool expose a `.release` field where the OCP release image can be specified.

For a cluster to keep fully operational during an upgrade process, Control Plane and Nodes upgrades need to be orchestrated while satisfying Kubernetes version skew policy at any time. The supported OCP versions are dictated by the running HyperShift Operator see here for more details on versioning.

## HostedCluster
`.spec.release` dictates the version of the Control Plane.

The HostedCluster propagates the intended `.spec.release` to the `HostedControlPlane.spec.release` and runs the appropriate Control Plane Operator version.

The HostedControlPlane orchestrates the rollout of the new version of the Control Plane components along with any OCP component in the data plane through the new version of the cluster version operator (CVO). This includes resources like:

- the CVO itself
- cluster network operator (CNO)
- cluster ingress operator
- manifests for the kube API-server (KAS), scheduler, and manager
- machine approver
- autoscaler
- infra resources needed to enable ingress for control plane endpoints (KAS, ignition, konnectivity, etc.)

The CVO also applies the payload for the `CLUSTER_PROFILE": "ibm-cloud-managed` into the hosted cluster.

Traditionally, in standalone OCP, the CVO has been the sole source of truth for upgrades. In HyperShift, the responsibility is currently split between CPO and CVO. This enabled the flexibility and speed the HyperShift project needed for the CPO to support the management/guest cluster topology and the multiple customizations needed for manifests that otherwise would have needed to be segregated in the payload.

HyperShift exposes available upgrades in HostedCluster.Status by bubbling up the status of the ClusterVersion resource inside a hosted cluster. This info is purely informational and doesn't determine upgradability, which is dictated by the `.spec.release` input in practice. This does result in the loss of some of the builtin features and guardrails from CVO like recommendations, allowed upgrade paths, risks, etc. However, this information is still available in the HostedCluster.Status field for consumers to read.

## NodePools
`.spec.release` dictates the version of any particular NodePool.

A NodePool will perform a Replace/InPlace rolling upgrade according to `.spec.management.upgradeType`. See NodePool Upgrades for details.


---

## Source: docs/content/index.md

---
title: Home
---

# HyperShift

HyperShift is middleware for hosting OpenShift control
planes at scale that solves for cost and time to provision, as well as portability 
cross cloud with strong separation of concerns between management and workloads.
Clusters are fully compliant OpenShift Container Platform (OCP)
clusters and are compatible with standard OCP and Kubernetes toolchains.

Quick start{ .md-button .md-button--primary }
Contribute{ .md-button }




---

## Source: docs/content/labs/Dual/dns.md

The DNS configuration is a critical aspect of our setup. To enable name resolution in our virtualized environment, follow these steps:

1. Create the primary DNS configuration file for the dnsmasq server:

- `/opt/dnsmasq/dnsmasq.conf`
```conf
strict-order
bind-dynamic
#log-queries
bogus-priv
dhcp-authoritative

# BM Network IPv4
dhcp-range=dual,192.168.126.120,192.168.126.250,255.255.255.0,24h
dhcp-option=dual,option:dns-server,192.168.126.1
dhcp-option=dual,option:router,192.168.126.1

# BM Network dual
dhcp-range=dual,2620:52:0:1306::11,2620:52:0:1306::20,64
dhcp-option=dual,option6:dns-server,2620:52:0:1306::1

resolv-file=/opt/dnsmasq/upstream-resolv.conf
except-interface=lo
dhcp-lease-max=81
log-dhcp
no-hosts

# DHCP Reservations
dhcp-leasefile=/opt/dnsmasq/hosts.leases

# Include all files in a directory depending on the suffix
conf-dir=/opt/dnsmasq/include.d/*.dual
```

Create the upstream resolver to delegate the non-local environments queries

- `/opt/dnsmasq/upstream-resolv.conf`
```
nameserver 8.8.8.8
nameserver 8.8.4.4
```

Create the different component DNS configurations

- `/opt/dnsmasq/include.d/hosted-nodeport.dual`
```
host-record=api-int.hosted-dual.hypershiftbm.lab,192.168.126.20
host-record=api-int.hosted-dual.hypershiftbm.lab,192.168.126.21
host-record=api-int.hosted-dual.hypershiftbm.lab,192.168.126.22
host-record=api.hosted-dual.hypershiftbm.lab,192.168.126.20
host-record=api.hosted-dual.hypershiftbm.lab,192.168.126.21
host-record=api.hosted-dual.hypershiftbm.lab,192.168.126.22

## IMPORTANT!: You should point to the node which is exposing the router.
## You can also use MetalLB to expose the Apps wildcard.
address=/apps.hosted-dual.hypershiftbm.lab/192.168.126.30

dhcp-host=aa:aa:aa:aa:04:11,hosted-worker0,192.168.126.30
dhcp-host=aa:aa:aa:aa:04:12,hosted-worker1,192.168.126.31
dhcp-host=aa:aa:aa:aa:04:13,hosted-worker2,192.168.126.32
dhcp-host=aa:aa:aa:aa:11:01,hosted-worker0,[2620:52:0:1306::30]
dhcp-host=aa:aa:aa:aa:11:02,hosted-worker1,[2620:52:0:1306::31]
dhcp-host=aa:aa:aa:aa:11:03,hosted-worker2,[2620:52:0:1306::32]
```

- `/opt/dnsmasq/include.d/hub.dual`
```
host-record=api-int.hub-dual.hypershiftbm.lab,192.168.126.10
host-record=api.hub-dual.hypershiftbm.lab,192.168.126.10
address=/apps.hub-dual.hypershiftbm.lab/192.168.126.11
dhcp-host=aa:aa:aa:aa:10:01,ocp-master-0,192.168.126.20
dhcp-host=aa:aa:aa:aa:10:02,ocp-master-1,192.168.126.21
dhcp-host=aa:aa:aa:aa:10:03,ocp-master-2,192.168.126.22
dhcp-host=aa:aa:aa:aa:10:06,ocp-installer,192.168.126.25
dhcp-host=aa:aa:aa:aa:10:07,ocp-bootstrap,192.168.126.26

host-record=api-int.hub-dual.hypershiftbm.lab,2620:52:0:1306::2
host-record=api.hub-dual.hypershiftbm.lab,2620:52:0:1306::2
address=/apps.hub-dual.hypershiftbm.lab/2620:52:0:1306::3
dhcp-host=aa:aa:aa:aa:10:01,ocp-master-0,[2620:52:0:1306::5]
dhcp-host=aa:aa:aa:aa:10:02,ocp-master-1,[2620:52:0:1306::6]
dhcp-host=aa:aa:aa:aa:10:03,ocp-master-2,[2620:52:0:1306::7]
dhcp-host=aa:aa:aa:aa:10:06,ocp-installer,[2620:52:0:1306::8]
dhcp-host=aa:aa:aa:aa:10:07,ocp-bootstrap,[2620:52:0:1306::9]
```

- `/opt/dnsmasq/include.d/infra.dual`
```
host-record=registry.hypershiftbm.lab,2620:52:0:1306::1
host-record=registry.hypershiftbm.lab,192.168.126.1
```

To proceed, we must create a systemd service for the management of the dnsmasq service and disable the system's default dnsmasq service:

- `/etc/systemd/system/dnsmasq-virt.service`
```
[Unit]
Description=DNS server for Openshift 4 Clusters.
After=network.target

[Service]
User=root
Group=root
ExecStart=/usr/sbin/dnsmasq -k --conf-file=/opt/dnsmasq/dnsmasq.conf

[Install]
WantedBy=multi-user.target
```

The commands to do so:

```
systemctl daemon-reload
systemctl disable --now dnsmasq
systemctl enable --now dnsmasq-virt
```

!!! note

    This step is mandatory for both Disconnected and Connected environments. Additionally, it holds significance for both Virtualized and Bare Metal environments. The key distinction lies in the location where the resources will be configured. In a non-virtualized environment, a more robust solution like Bind is recommended instead of a lightweight dnsmasq.

---

## Source: docs/content/labs/Dual/hostedcluster/baremetalhost.md

## Bare Metal Hosts

A **BareMetalHost** is an openshift-machine-api object that encompasses both physical and logical details, allowing it to be identified by the Metal3 operator. Subsequently, these details are associated with other Assisted Service objects known as Agents. The structure of this object is as follows:

```yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: hosted-dual-worker0-bmc-secret
  namespace: clusters-hosted-dual
data:
  password: YWRtaW4=
  username: YWRtaW4=
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: hosted-dual-worker0
  namespace: clusters-hosted-dual
  labels:
    infraenvs.agent-install.openshift.io: hosted-dual
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: hosted-dual-worker0
spec:
  automatedCleaningMode: disabled
  bmc:
    disableCertificateVerification: true
    address: redfish-virtualmedia://[192.168.126.1]:9000/redfish/v1/Systems/local/hosted-dual-worker0
    credentialsName: hosted-dual-worker0-bmc-secret
  bootMACAddress: aa:aa:aa:aa:02:11
  online: true
```

**Details**:

- We will have at least 1 secret that holds the BMH credentials, so we will need to create at least 2 objects per worker node.
- `spec.metadata.labels["infraenvs.agent-install.openshift.io"]` serves as the link between the Assisted Installer and the BareMetalHost objects.
- `spec.metadata.annotations["bmac.agent-install.openshift.io/hostname"]` represents the node name it will adopt during deployment.
- `spec.automatedCleaningMode` prevents the node from being erased by the Metal3 operator.
- `spec.bmc.disableCertificateVerification` is set to `true` to bypass certificate validation from the client.
- `spec.bmc.address` denotes the BMC address of the worker node.
- `spec.bmc.credentialsName` points to the Secret where User/Password credentials are stored.
- `spec.bootMACAddress` indicates the interface MACAddress from which the node will boot.
- `spec.online` defines the desired state of the node once the BMH object is created.

To deploy this object, simply follow the same procedure as before:

!!! important

    Please create the virtual machines before you create the BareMetalHost and the destination Nodes.

To deploy the BareMetalHost object, execute the following command:

```bash
oc apply -f 04-bmh.yaml
```

This will be the process:

- Preparing (Trying to reach the nodes):
```
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   registering              true             2s
clusters-hosted   hosted-worker1   registering              true             2s
clusters-hosted   hosted-worker2   registering              true             2s
```

- Provisioning (Nodes Booting up)
```
NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioning              true             16s
clusters-hosted   hosted-worker1   provisioning              true             16s
clusters-hosted   hosted-worker2   provisioning              true             16s
```

- Provisioned (Nodes Booted up successfully)
```
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioned              true             67s
clusters-hosted   hosted-worker1   provisioned              true             67s
clusters-hosted   hosted-worker2   provisioned              true             67s
```

## Agents registration

After the nodes have booted up, you will observe the appearance of agents within the namespace.

```
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign
```

These agents represent the nodes available for installation. To assign them to a HostedCluster, scale up the NodePool.

## Scaling Up the Nodepool

Once we have the BareMetalHosts created, the statuses of these BareMetalHosts will transition from `Registering` (Attempting to reach the Node's BMC) to `Provisioning` (Node Booting Up), and finally to `Provisioned` (Successful node boot-up).

The nodes will boot with the Agent's RHCOS LiveISO and a default pod named "agent." This agent is responsible for receiving instructions from the Assisted Service Operator to install the Openshift payload.

To accomplish this, execute the following command:

```bash
oc -n clusters scale nodepool hosted-dual --replicas 3
```

After the NodePool scaling, you will notice that the agents are assigned to a Hosted Cluster.

```
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411   hosted    true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412   hosted    true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413   hosted    true       auto-assign
```

And the NodePool replicas set

```
NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted   hosted    3                               False         False        4.14.0-0.nightly-2023-08-29-102237                                      Minimum availability requires 3 replicas, current 0 available
```

So now, we need to wait until the nodes join the cluster. The Agents will provide updates on their current stage and status. Initially, they may not post any status, but eventually, they will.


---

## Source: docs/content/labs/Dual/hostedcluster/hostedcluster.md

In this section, we will focus on all the related objects necessary to achieve a Disconnected Hosted Cluster deployment.

**Premises**:

- HostedCluster Name: `hosted-dual`
- HostedCluster Namespace: `clusters`
- Disconnected: `true`
- Network Stack: `Dual`

## Openshift Objects

### Namespaces

In a typical situation, the operator would be responsible for creating the HCP (HostedControlPlane) namespace. However, in this case, we want to include all the objects before the operator begins reconciliation over the HostedCluster object. This way, when the operator commences the reconciliation process, it will find all the objects in place.

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: clusters-hosted-dual
spec: {}
status: {}
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: clusters
spec: {}
status: {}
```

!!! note

      We will **not** create objects one by one but will concatenate all of them in the same file and apply them with just one command.

### ConfigMap and Secrets

These are the ConfigMaps and Secrets that we will include in the HostedCluster deployment.

```yaml
---
apiVersion: v1
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
kind: ConfigMap
metadata:
  name: user-ca-bundle
  namespace: clusters
---
apiVersion: v1
data:
  .dockerconfigjson: xxxxxxxxx
kind: Secret
metadata:
  creationTimestamp: null
  name: hosted-dual-pull-secret
  namespace: clusters
---
apiVersion: v1
kind: Secret
metadata:
  name: sshkey-cluster-hosted-dual
  namespace: clusters
stringData:
  id_rsa.pub: ssh-rsa xxxxxxxxx
---
apiVersion: v1
data:
  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=
kind: Secret
metadata:
  creationTimestamp: null
  name: hosted-dual-etcd-encryption-key
  namespace: clusters
type: Opaque
```

### RBAC Roles

While not mandatory, it allows us to have the Assisted Service Agents located in the same HostedControlPlane namespace as the HostedControlPlane and still be managed by CAPI.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: capi-provider-role
  namespace: clusters-hosted-dual
rules:
- apiGroups:
  - agent-install.openshift.io
  resources:
  - agents
  verbs:
  - '*'
```

### Hosted Cluster

This is a sample of the HostedCluster Object

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: hosted-dual
  namespace: clusters
spec:
  additionalTrustBundle:
    name: "user-ca-bundle"
  olmCatalogPlacement: guest
  imageContentSources:
  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
    mirrors:
    - registry.hypershiftbm.lab:5000/openshift/release
  - source: quay.io/openshift-release-dev/ocp-release
    mirrors:
    - registry.hypershiftbm.lab:5000/openshift/release-images
  - mirrors:
  ...
  ...
  autoscaling: {}
  controllerAvailabilityPolicy: SingleReplica
  dns:
    baseDomain: hypershiftbm.lab
  etcd:
    managed:
      storage:
        persistentVolume:
          size: 8Gi
        restoreSnapshotURL: null
        type: PersistentVolume
    managementType: Managed
  fips: false
  networking:
    clusterNetwork:
    - cidr: 10.132.0.0/14
    - cidr: fd01::/48
    networkType: OVNKubernetes
    serviceNetwork:
    - cidr: 172.31.0.0/16
    - cidr: fd02::/112
  platform:
    agent:
      agentNamespace: clusters-hosted-dual
    type: Agent
  pullSecret:
    name: hosted-dual-pull-secret
  release:
    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
  secretEncryption:
    aescbc:
      activeKey:
        name: hosted-dual-etcd-encryption-key
    type: aescbc
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-dual.hypershiftbm.lab
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-dual.hypershiftbm.lab
      type: NodePort
  - service: OIDC
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-dual.hypershiftbm.lab
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-dual.hypershiftbm.lab
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-dual.hypershiftbm.lab
      type: NodePort
  sshKey:
    name: sshkey-cluster-hosted-dual
status:
  controlPlaneEndpoint:
    host: ""
    port: 0
```

!!! note

    The `imageContentSources` section within the `spec` field contains mirror references for user workloads within the HostedCluster.

As you can see, all the objects created before are referenced here. You can also refer to the documentation where all the fields are described.

## Deployment

To deploy these objects, simply concatenate them into the same file and apply them against the management cluster:

```bash
oc apply -f 01-4.14-hosted_cluster-nodeport.yaml
```

This will raise up a functional Hosted Control Plane.

``` yaml
NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-5b57dbd6d5-pxlqc                        1/1     Running   0   	   3m57s
catalog-operator-9694884dd-m7zzv                      2/2     Running   0          93s
cluster-api-f98b9467c-9hfrq                           1/1     Running   0   	   3m57s
cluster-autoscaler-d7f95dd5-d8m5d                     1/1     Running   0          93s
cluster-image-registry-operator-5ff5944b4b-648ht      1/2     Running   0          93s
cluster-network-operator-77b896ddc-wpkq8              1/1     Running   0          94s
cluster-node-tuning-operator-84956cd484-4hfgf         1/1     Running   0          94s
cluster-policy-controller-5fd8595d97-rhbwf            1/1     Running   0          95s
cluster-storage-operator-54dcf584b5-xrnts             1/1     Running   0          93s
cluster-version-operator-9c554b999-l22s7              1/1     Running   0          95s
control-plane-operator-6fdc9c569-t7hr4                1/1     Running   0 	       3m57s
csi-snapshot-controller-785c6dc77c-8ljmr              1/1     Running   0 	       77s
csi-snapshot-controller-operator-7c6674bc5b-d9dtp     1/1     Running   0 	       93s
dns-operator-6874b577f-9tc6b                          1/1     Running   0          94s
etcd-0                                                3/3     Running   0 	       3m39s
hosted-cluster-config-operator-f5cf5c464-4nmbh        1/1     Running   0 	       93s
ignition-server-6b689748fc-zdqzk                      1/1     Running   0          95s
ignition-server-proxy-54d4bb9b9b-6zkg7                1/1     Running   0 	       95s
ingress-operator-6548dc758b-f9gtg                     1/2     Running   0          94s
konnectivity-agent-7767cdc6f5-tw782                   1/1     Running   0 	       95s
kube-apiserver-7b5799b6c8-9f5bp                       4/4     Running   0 	       3m7s
kube-controller-manager-5465bc4dd6-zpdlk              1/1     Running   0 	       44s
kube-scheduler-5dd5f78b94-bbbck                       1/1     Running   0 	       2m36s
machine-approver-846c69f56-jxvfr                      1/1     Running   0          92s
oauth-openshift-79c7bf44bf-j975g                      2/2     Running   0 	       62s
olm-operator-767f9584c-4lcl2                          2/2     Running   0 	       93s
openshift-apiserver-5d469778c6-pl8tj                  3/3     Running   0 	       2m36s
openshift-controller-manager-6475fdff58-hl4f7         1/1     Running   0 	       95s
openshift-oauth-apiserver-dbbc5cc5f-98574             2/2     Running   0          95s
openshift-route-controller-manager-5f6997b48f-s9vdc   1/1     Running   0          95s
packageserver-67c87d4d4f-kl7qh                        2/2     Running   0          93s
```


And this is how the HostedCluster looks like:

```bash
NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
clusters    hosted-dual            hosted-admin-kubeconfig   Partial    True 	      False         The hosted control plane is available
```

After some time, we will have almost all the pieces in place, and the Control Plane operator awaits for the worker nodes to join the cluster. To achieve this, we need to create some more objects. Let's discuss the `InfraEnv` and the `BareMetalHost` in the following sections.

---

## Source: docs/content/labs/Dual/hostedcluster/index.md

A Hosted Cluster, as mentioned in the documentation here, is essentially an OCP API endpoint managed by Hypershift. In this context, we will also include the term HostedControlPlane to enhance readability and comprehension. This terminology is further explained in the same link.

The Hosted Cluster comprises two main components:
- The Control Plane, which runs as pods in the management cluster.
- The Data Plane, consisting of external nodes managed by the end user.

With this foundational understanding, we can commence our Hosted Cluster deployment. Typically, an ACM/MCE user would utilize the web UI to create a cluster. However, in this scenario, we will leverage manifests, providing us with greater flexibility to modify the artifacts.


---

## Source: docs/content/labs/Dual/hostedcluster/infraenv.md

The `InfraEnv` is an Assisted Service object that includes essential details such as the `pullSecretRef` and the `sshAuthorizedKey`. These details are used to create the RHCOS Boot Image customized specifically for the cluster. Below is the structure of this object:

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: hosted-dual
  namespace: clusters-hosted-dual
spec:
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDk7ICaUE+/k4zTpxLk4+xFdHi4ZuDi5qjeF52afsNkw0w/glILHhwpL5gnp5WkRuL8GwJuZ1VqLC9EKrdmegn4MrmUlq7WTsP0VFOZFBfq2XRUxo1wrRdor2z0Bbh93ytR+ZsDbbLlGngXaMa0Vbt+z74FqlcajbHTZ6zBmTpBVq5RHtDPgKITdpE1fongp7+ZXQNBlkaavaqv8bnyrP4BWahLP4iO9/xJF9lQYboYwEEDzmnKLMW1VtCE6nJzEgWCufACTbxpNS7GvKtoHT/OVzw8ArEXhZXQUS1UY8zKsX2iXwmyhw5Sj6YboA8WICs4z+TrFP89LmxXY0j6536TQFyRz1iB4WWvCbH5n6W+ABV2e8ssJB1AmEy8QYNwpJQJNpSxzoKBjI73XxvPYYC/IjPFMySwZqrSZCkJYqQ023ySkaQxWZT7in4KeMu7eS2tC+Kn4deJ7KwwUycx8n6RHMeD8Qg9flTHCv3gmab8JKZJqN3hW1D378JuvmIX4V0=
```

**Details**:

- `pullSecretRef` refers to the ConfigMap reference (in the same Namespace as the InfraEnv) where the PullSecret will be utilized.
- `sshAuthorizedKey` represents the SSH Public key that will be injected into the Boot Image. This SSH key will, by default, allow access to the worker nodes as the `core` user.

To deploy this object, follow the same procedure as before:

```bash
oc apply -f 03-infraenv.yaml
```

Once deployed, it will appear as follows:

```bash
NAMESPACE              NAME     ISO CREATED AT
clusters-hosted-dual   hosted   2023-09-11T15:14:10Z
```

---

## Source: docs/content/labs/Dual/hostedcluster/nodepool.md

A `NodePool` is a scalable set of worker nodes associated with a HostedCluster. NodePool machine architectures remain consistent within a specific pool and are independent of the underlying machine architecture of the control plane.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

!!! warning

    Before a day-1 patch, the release image set in the HostedCluster **should use the digest rather than the tag**. (e.g `quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0`)

This is how one looks like:

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  creationTimestamp: null
  name: hosted-dual
  namespace: clusters
spec:
  arch: amd64
  clusterName: hosted-dual
  management:
    autoRepair: false
    upgradeType: InPlace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    type: Agent
  release:
    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
  replicas: 0
status:
  replicas: 0
```

**Details**:

- All the nodes included in this NodePool will be based on the Openshift version `4.14.0-0.nightly-2023-08-29-102237`.
- The Upgrade type is set to `InPlace`, indicating that the same bare-metal node will be reused during an upgrade.
- Autorepair is set to `false` because the node will not be recreated when it disappears.
- Replicas are set to `0` because we intend to scale them when needed.

You can find more information about NodePool in the NodePool documentation.

To deploy this object, simply follow the same procedure as before:

```bash
oc apply -f 02-nodepool.yaml
```

And this is how the NodePool looks like at this point:

```bash
NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted-dual   hosted    0                               False         False        4.14.0-0.nightly-2023-08-29-102237
```

!!! important

    Keep the nodepool replicas to 0 until all the steps are in place.

---

## Source: docs/content/labs/Dual/hostedcluster/worker-nodes.md

Regarding the worker nodes, if you are working on real bare metal, this step is crucial to ensure that the details set in the `BareMetalHost` are correctly configured. If not, you will need to debug why it's not functioning as expected.

However, if you are working with virtual machines, you can follow these steps to create empty ones that will be consumed by the Metal3 operator. To achieve this, we will utilize Kcli.

## Creating Virtual Machines

If this is not your first attempt, you must first delete the previous setup. To do so, please refer to the Deleting Virtual Machines section.

Now, you can execute the following commands for VM creation:

```bash
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:01\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1101 -P name=hosted-dual-worker0
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:02\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1102 -P name=hosted-dual-worker1
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:03\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1103 -P name=hosted-dual-worker2

sleep 2
systemctl restart ksushy
```

Let's dissect the creation command:

- `start=False`: The VM will not boot automatically upon creation.
- `uefi_legacy=true`: We will use UEFI legacy boot to ensure compatibility with older UEFI implementations.
- `plan=hosted-dual`: The plan name, which identifies a group of machines as a cluster.
- `memory=8192` and `numcpus=16`: These parameters specify the resources for the VM, including RAM and CPU.
- `disks=[200,200]`: We are creating 2 disks (thin provisioned) in the virtual machine.
- `nets=[{"name": "dual", "mac": "aa:aa:aa:aa:11:13"}]`: Network details, including the network name it will be connected to and the MAC address for the primary interface.
- The `ksushy` restart is performed to make our `ksushy` (VM's BMC) aware of the new VMs added.

This is what the command looks like:

```bash
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|    hosted-worker0   |  down  |                   |                                                    | hosted-dual |  kvirt  |
|    hosted-worker1   |  down  |                   |                                                    | hosted-dual |  kvirt  |
|    hosted-worker2   |  down  |                   |                                                    | hosted-dual |  kvirt  |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
```

## Deleting Virtual Machines

To delete the VMs, you simply need to delete the plan, which, in our case, is:

```bash
kcli delete plan hosted-dual
```

```bash
$ kcli delete plan hosted-dual
Are you sure? [y/N]: y
hosted-worker0 deleted on local!
hosted-worker1 deleted on local!
hosted-worker2 deleted on local!
Plan hosted-dual deleted!
```

---

## Source: docs/content/labs/Dual/hypervisor/hypervisor-system-config.md

This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the `firewalld` service and configure SELinux policies to maintain a secure environment.

With these instructions, we allow various types of connections through the different virtual networks within the environment:

```bash
## SELinux
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0

## Firewalld
systemctl disable --now firewalld

## Libvirtd
systemctl restart libvirtd
systemctl enable --now libvirtd
```

---

## Source: docs/content/labs/Dual/hypervisor/index.md

---
title: Hypervisor
---

This section is primarily focused on Virtual Machines. If you are working with real bare metal, you can skip this section and verify whether your laboratory has the necessary components to run the deployment properly. If you find that you've missed something, you can configure it separately to address any gaps.


---

## Source: docs/content/labs/Dual/hypervisor/network-manager-dispatcher.md

This script modifies the system DNS resolver to prioritize pointing to the `dnsmasq` service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.

To enable this, you need to create a script named `forcedns` in `/etc/NetworkManager/dispatcher.d/` with the following content:

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

=== "IPv4"

    ```bash
    #!/bin/bash

    export IP="192.168.125.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "IPv6"

    ```bash
    #!/bin/bash

    export IP="2620:52:0:1306::1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "Dual stack"

    ```bash
    #!/bin/bash

    export IP="192.168.126.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

The `IP` variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.

After creating the file, you need to add execution permissions using the command:

```bash
chmod 755 /etc/NetworkManager/dispatcher.d/forcedns
```

Then, execute it once. The output should indicate `ok`.



---

## Source: docs/content/labs/Dual/hypervisor/packaging.md

---
title: Hypervisor Packaging
---

## System

These are the main packages that are needed to deploy a virtualized Openshift Management cluster.

```bash
sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y
```

Additionally, you need to enable and start the Podman service using the following command:

```bash
systemctl enable --now podman
```

## Kcli

We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:

```bash
sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm
sudo usermod -aG qemu,libvirt $(id -un)
sudo newgrp libvirt
sudo systemctl enable --now libvirtd
sudo dnf -y copr enable karmab/kcli
sudo dnf -y install kcli
sudo kcli create pool -p /var/lib/libvirt/images default
kcli create host kvm -H 127.0.0.1 local
sudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images
kcli create network  -c 192.168.122.0/24 default
```

For more info about Kcli please visit the official documentation.

---

## Source: docs/content/labs/Dual/hypervisor/redfish-for-vms.md

In a bare metal environment, the preferred approach is to utilize the actual BMC (Baseboard Management Controller) of the nodes used for the management cluster, which can be managed by Metal3 for discovery and provisioning. However, in a virtual environment, this approach is not feasible. As a workaround, we will use `ksushy`, which is an implementation of `sushy-tools`, allowing us to simulate BMCs for the virtual machines.

To configure `ksushy`, execute the following commands:


```bash
sudo dnf install python3-pyOpenSSL.noarch python3-cherrypy -y
kcli create sushy-service --ssl --ipv6 --port 9000
sudo systemctl daemon-reload
systemctl enable --now ksushy
```

To test if this service is functioning correctly, you can check the service status with `systemctl status ksushy`. Additionally, you can execute a `curl` command against the exposed interface:

```
curl -Lk https://[2620:52:0:1306::1]:9000/redfish/v1
```


---

## Source: docs/content/labs/Dual/hypervisor/requisites.md

---
title: Hypervisor Prerequisites
---

- **CPU**: The number of CPUs provided determines how many HostedClusters can run concurrently.
  - **Recommended**: 16 CPUs per Node for 3 nodes.
  - **Minimal Dev**: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.

- **Memory**: The amount of RAM impacts how many HostedClusters can be hosted.
  - **Recommended**: 48 GB of RAM per Node.
  - **Minimal Dev**: For minimal development, 18 GB of RAM per Node may suffice.

- **Storage**: Using SSD storage for MCE is crucial.
  - **Management Cluster**: 250 GB.
  - **Registry**: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.
  - **Webserver**: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.

- **Production**: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:
  - **Registry**: 2 TB.
  - **Management Cluster**: 500 GB.
  - **WebServer**: 2 TB.


---

## Source: docs/content/labs/Dual/index.md

---
title: Dual Stack
---

This network configuration is currently designated as disconnected. The primary reason for this designation is because remote registries do not function with IPv6. Consequently, this aspect has been incorporated into the documentation.

All the scripts provided contain partial or complete automation to replicate the environment. For this purpose, you can refer to the repository containing all the scripts for Dual Stack environments.

Please note that this documentation is designed to be followed in a specific sequence:

- Hypervisor
- DNS
- Registry
- Management Cluster
- Webserver
- Mirroring
- Multicluster Engine
- TLS Certificates
- HostedCluster
- Watching Deployment progress

---

## Source: docs/content/labs/Dual/mce/agentserviceconfig.md

The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named `AgentServiceConfig` to configure it.

## Agent Service Config Objects

You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.

In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.

### Custom Registries Configuration

This ConfigMap contains the disconnected details necessary to customize the deployment.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-registries
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/openshift4"
    mirror-by-digest-only = true

    [[registry.mirror]]
      location = "registry.hypershiftbm.lab:5000/openshift4"

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/rhacm2"
    mirror-by-digest-only = true
    ...
    ...
```

This object includes two fields:

1. **Custom CAs**: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.
2. **Registries**: The `Registries.conf` field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.

### Assisted Service Customization

The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false"
```

You can find documentation on how to customize the operator here.

### Assisted Service Config

The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config
  name: agent
  namespace: multicluster-engine
spec:
  mirrorRegistryRef:
    name: custom-registries
  databaseStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  filesystemStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 20Gi
  osImages:
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso
    version: 414.92.202308281054-0
```

If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.

In this section, we will emphasize the important aspects:

- The `metadata.annotations["unsupported.agent-install.openshift.io/assisted-service-configmap"]` Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.
- The `spec.mirrorRegistryRef.name` points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.
- The `spec.osImages` field contains different versions available for deployment by this operator. These fields are mandatory.

Let's fill in this section for the 4.14 dev preview ec4 version (sample):

- Release URL
- RHCOS Info

Assuming you've already downloaded the RootFS and LiveIso files:

```yaml
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso
    version: 414.92.202307250657-0
```

## Deployment

To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.

```bash
oc apply -f agentServiceConfig.yaml
```

This will trigger 2 pods

```bash
assisted-image-service-0                               1/1     Running   2             11d
assisted-service-668b49548-9m7xw                       2/2     Running   5             11d
```

!!! note

      The `Assisted Image Service` pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.

      The `Assisted Service` refers to the operator.


---

## Source: docs/content/labs/Dual/mce/index.md

The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.

## Credentials and Authorization

To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.

ACM/MCE Deployment{ .md-button }
Agent Service Config{ .md-button }


---

## Source: docs/content/labs/Dual/mce/multicluster-engine.md

The deployment of each component will depend on your needs, follow the next links accordingly:

- ACM Deployment
- ACM Downstream builds
- MCE Deployment
- MCE Upgrade
- ACM/MCE Cluster deletion
- MulticlusterEngine deletion
- Last resource: **nuke.sh**

---

## Source: docs/content/labs/Dual/mgmt-cluster/compact-dual.md

In this section, we will discuss how to deploy the Openshift management cluster. To do that, we need to have the following files in place:

1. **Pull Secret**
2. **Kcli Plan**

Ensure that these files are properly set up for the deployment process.

### Pull Secret

The Pull Secret should be located in the same folder as the kcli plan and should be named `openshift_pull.json`.

### Kcli plan

The Kcli plan contains the Openshift definition. This is how looks like:

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

- `mgmt-compact-hub-dual.yaml`

```yaml
plan: hub-dual
force: true
version: nightly
tag: "4.14.0-0.nightly-2023-08-29-102237"
cluster: "hub-dual"
dualstack: true
domain: hypershiftbm.lab
api_ip: 192.168.126.10
ingress_ip: 192.168.126.11
service_networks:
- 172.30.0.0/16
- fd02::/112
cluster_networks:
- 10.132.0.0/14
- fd01::/48
disconnected_url: registry.hypershiftbm.lab:5000
disconnected_update: true
disconnected_user: dummy
disconnected_password: dummy
disconnected_operators_version: v4.13
disconnected_operators:
- name: metallb-operator
- name: lvms-operator
  channels:
  - name: stable-4.13
disconnected_extra_images:
- quay.io/mavazque/trbsht:latest
- quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
- registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
dualstack: true
disk_size: 200
extra_disks: [200]
memory: 48000
numcpus: 16
ctlplanes: 3
workers: 0
manifests: extra-manifests
metal3: true
network: dual
users_dev: developer
users_devpassword: developer
users_admin: admin
users_adminpassword: admin
metallb_pool: dual-virtual-network
metallb_ranges:
- 192.168.126.150-192.168.126.190
metallb_autoassign: true
apps:
- users
- lvms-operator
- metallb-operator
vmrules:
- hub-bootstrap:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:10:07
- hub-ctlplane-0:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:10:01
- hub-ctlplane-1:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:10:02
- hub-ctlplane-2:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:10:03
```

!!! note

    To understand the meaning of each of these parameters, you can refer to the official documentation here.

### Deployment

To initiate the provisioning procedure, execute the following:

```bash
kcli create cluster openshift --pf mgmt-compact-hub-dual.yaml
```

---

## Source: docs/content/labs/Dual/mgmt-cluster/index.md

## Openshift Management Cluster

This section contains the necessary artifacts to set up an Openshift management cluster based on virtual machines using kcli as the primary tool. Another option is to use dev-scripts, which uses a different approach.

Networking{ .md-button }
Openshift Compact Dual{ .md-button }

---

## Source: docs/content/labs/Dual/mgmt-cluster/network.md

Firstly, we need to ensure that we have the right networks prepared for use in the Hypervisor. These networks will be used to host both the Management and Hosted clusters.

To configure these networks, we will use the following `kcli` command:

```
kcli create network -c 192.168.126.0/24 -P dhcp=false -P dns=false -d 2620:52:0:1306::0/64 --domain hypershiftbm.lab --nodhcp dual
```

Where:

- `-c` specifies the CIDR used for that network.
- `-P dhcp=false` configures the network to disable DHCP, which will be handled by the previously configured dnsmasq.
- `-P dns=false` configures the network to disable DNS, which will also be handled by the dnsmasq.
- `--domain` sets the domain to search into.
- `dual` is the name of the network that will be created.

This is what the network will look like once created:

```
[root@hypershiftbm ~]# kcli list network
Listing Networks...
+---------+--------+---------------------+-------+------------------+------+
| Network |  Type  |         Cidr        |  Dhcp |      Domain      | Mode |
+---------+--------+---------------------+-------+------------------+------+
| default | routed |   192.168.122.0/24  |  True |     default      | nat  |
| dual    | routed | 2620:52:0:1306::/64 | False | hypershiftbm.lab | nat  |
| dual    | routed |   192.168.125.0/24  | False | hypershiftbm.lab | nat  |
| ipv6    | routed | 2620:52:0:1305::/64 | False | hypershiftbm.lab | nat  |
+---------+--------+---------------------+-------+------------------+------+
```

```
[root@hypershiftbm ~]# kcli info network ipv6
Providing information about network ipv6...
cidr: 2620:52:0:1306::/64
dhcp: false
domain: hypershiftbm.lab
mode: nat
plan: kvirt
type: routed
```



---

## Source: docs/content/labs/Dual/mirror/ICSP-IDMS.md

Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:

1. ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).
2. Catalog Sources.

Using the `oc-mirror` tool, the output artifacts will be located in a new folder called `oc-mirror-workspace/results-XXXXXX/`.

ICSP/IDMS will trigger a "special" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.

Once all nodes are schedulable and marked as `READY`, you will need to apply the new catalog sources generated.

The catalog sources will trigger some actions in the `openshift-marketplace operator`, such as downloading the catalog image and processing it to retrieve all the `PackageManifests` included in that image. You can check the new sources by executing `oc get packagemanifest` using the new CatalogSource as a source.

## Applying the Artifacts

First, we need to create the ICSP/IDMS artifacts:


```bash
oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml
```

Now, wait for the nodes to become ready again and execute the following command:

```bash
oc apply -f catalogSource-XXXXXXXX-index.yaml
```

---

## Source: docs/content/labs/Dual/mirror/index.md

!!! important

    This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.

Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.

To learn how to perform image mirroring, follow the next sections:

Image Mirroring Process{ .md-button }
ICSP and IDMS{ .md-button }


---

## Source: docs/content/labs/Dual/mirror/mirroring.md

The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.

For this purpose, we will use the `oc-mirror` tool, a binary that utilizes an object called `ImageSetConfiguration`.

In this file, you can specify:

- The OpenShift versions to mirror (they should be located in quay.io).
- The additional operators to mirror, selecting packages individually.
- The extra images you want to add to the repository.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

Here is an example of the `ImageSetConfiguration` that we will use for our mirroring:

```yaml
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest
mirror:
  platform:
    channels:
    - name: candidate-4.14
      minVersion: 4.14.0-ec.1
      maxVersion: 4.14.0-ec.3
      type: ocp
    graph: true
  additionalImages:
  - name: quay.io/karmab/origin-keepalived-ipfailover:latest
  - name: quay.io/karmab/kubectl:latest
  - name: quay.io/karmab/haproxy:latest
  - name: quay.io/karmab/mdns-publisher:latest
  - name: quay.io/karmab/origin-coredns:latest
  - name: quay.io/karmab/curl:latest
  - name: quay.io/karmab/kcli:latest
  - name: quay.io/mavazque/trbsht:latest
  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
    - name: lvms-operator
    - name: local-storage-operator
    - name: odf-csi-addons-operator
    - name: odf-operator
    - name: mcg-operator
    - name: ocs-operator
    - name: metallb-operator
```

Make sure you have your `${HOME}/.docker/config.json` file updated with the registries you are trying to mirror from and your private registry to push the images to.

After that, we can begin the mirroring process:

```bash
oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}
```

Once the mirror finishes, you will have a new folder called `oc-mirror-workspace/results-XXXXXX/` which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.

## Mirroring Nightly and CI releases

The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the `oc adm release mirror` to mirror those versions.

To mirror the nightly versions we need for this deployment, you need to execute this:

```bash
REGISTRY=registry.$(hostname --long):5000

oc adm release mirror \
  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \
  --to=${REGISTRY}/openshift/release \
  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
```

For more detailed and updated information, you can visit the official Documentation or GitHub repository

## Mirror MCE internal releases

In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.

- Red Hat Official Documentation
- Red Hat Internal deployment Brew Registry deployment


---

## Source: docs/content/labs/Dual/registry.md

!!! important

    This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.

In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.

1. As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:

- `registry.sh`
```bash
#!/usr/bin/env bash

set -euo pipefail

PRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)
export PATH=/root/bin:$PATH
export PULL_SECRET="/root/baremetal/hub/openshift_pull.json"

if [[ ! -f $PULL_SECRET ]];then
  echo "Pull Secret not found, exiting..."
  exit 1
fi

dnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel
export IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)
REGISTRY_NAME=registry.$(hostname --long)
REGISTRY_USER=dummy
REGISTRY_PASSWORD=dummy
KEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)
echo "{\"auths\": {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\", \"email\": \"sample-email@domain.ltd\"}}}" > /root/disconnected_pull.json
mv ${PULL_SECRET} /root/openshift_pull.json.old
jq ".auths += {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\",\"email\": \"sample-email@domain.ltd\"}}" < /root/openshift_pull.json.old > $PULL_SECRET
mkdir -p /opt/registry/{auth,certs,data,conf}
cat <<EOF > /opt/registry/conf/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
  delete:
    enabled: true
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
compatibility:
  schema1:
    enabled: true
EOF
openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj "/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME" -addext "subjectAltName=DNS:$REGISTRY_NAME"
cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract
htpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD
podman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e "REGISTRY_AUTH=htpasswd" -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry" -e "REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest
[ "$?" == "0" ] || !!
systemctl enable --now registry
```

2. Kindly modify the location of the `PULL_SECRET` to point to the correct location.
3. Please adjust the permissions by adding the execution flag as follows: `chmod u+x ${HOME}/registry.sh`
4. You can execute the script without any parameters using the following command `${HOME}/registry.sh`

After executing the script `${HOME}/registry.sh`, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.

It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.

The root folder for the registry is situated at /opt/registry, and it's structured as follows:

- `certs` holds the TLS certificates.
- `auth` keeps the credentials.
- `data` contains the registry images.
- `conf` will hold the registry configuration.

---

## Source: docs/content/labs/Dual/tls-certificates.md

!!! important

    This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.

In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.

It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):

- `/etc/pki/ca-trust/extracted/pem/`
- `/etc/pki/ca-trust/source/anchors/`
- `/etc/pki/tls/certs/`

## Adding a CA to the Management Cluster

There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.

1. Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name `registry-config` The content should resemble the following:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: registry-config
  namespace: openshift-config
data:
  registry.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
```

!!! note

        The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the ":" character is being replaced by ".."; therefore, it is imperative to ensure this correction.

2. Now we need to patch the clusterwide object `image.config.openshift.io` including this:

```yaml
spec:
  additionalTrustedCA:
    name: registry-config
```

This modification will result in two significant consequences:

- Granting masters the capability to retrieve images from the private registry.
- Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.

!!! note

        The modification required several minutes to be successfully executed.

## Alternative: Adding a CA to the Management Cluster

We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.

It's described here. This method involves utilizing the `image-registry-operator`, which deploys the CAs to the OCP nodes.

Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.

However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:

- Check if the `openshift-config` namespace in the Management cluster contains a ConfigMap named `user-ca-bundle`.
- If the ConfigMap doesn't exist, execute the following command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt

oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}
```

- Otherwise, if that ConfigMap exists, execute this other command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt
export TMP_FILE=$(mktemp)

oc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\.crt}' > ${TMP_FILE}
echo >> ${TMP_FILE}
echo \#registry.$(hostname --long) >> ${TMP_FILE}
cat ${REGISTRY_CERT_PATH} >> ${TMP_FILE}
oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -
```

You have a functional script located in the `assets/<NetworkStack>/09-tls-certificates/01-config.sh`, this is the sample for IPv6.


---

## Source: docs/content/labs/Dual/watching/index.md

This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:

- Control Plane
- Data Plane

So please follow these links accordingly.

Control Plane perspective{ .md-button }
Data Plane perspective{ .md-button }


---

## Source: docs/content/labs/Dual/watching/watching-cp.md

Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:

```bash
export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig

watch "oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;"
```

This command will give you info about:

- What is the Hypershift Operator status
- The HostedControlPlane pod status
- The BareMetalHosts
- The Agents
- The Infraenv
- The HostedCluster and NodePool

This is how it looks:



---

## Source: docs/content/labs/Dual/watching/watching-dp.md

If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands

```bash
oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d > /root/hc_admin_kubeconfig.yaml
export KUBECONFIG=/root/hc_admin_kubeconfig.yaml

watch "oc get clusterversion,nodes,co"
```

This command will give you info about:

- Check the clusterversion
- Check if the Nodes has joined the cluster
- Check the ClusterOperators

This is how looks like:



---

## Source: docs/content/labs/Dual/webserver.md

!!! important

    This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.

This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.

The script refers to this repository folder and it's the same for all three different network stacks.

To do this, you can use this script:

```bash
#!/bin/bash

WEBSRV_FOLDER=/opt/srv
ROOTFS_IMG_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')"
LIVE_ISO_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')"

mkdir -p ${WEBSRV_FOLDER}/images
curl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}
curl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}
chmod -R 755 ${WEBSRV_FOLDER}/*

## Run Webserver
podman ps --noheading | grep -q websrv-ai
if [[ $? == 0 ]];then
    echo "Launching Registry pod..."
    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080
fi
```

The script will create a folder under `/opt/srv`. This folder will contain the `images` for RHCOS provision in the worker nodes. To be more concrete, we need the `RootFS` and `LiveISO` artifacts found on the Openshift CI Release page.

After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.


---

## Source: docs/content/labs/IPv4/dns.md

The DNS configuration is a critical aspect of our setup. To enable name resolution in our virtualized environment, follow these steps:

1. Create the primary DNS configuration file for the dnsmasq server:

- `/opt/dnsmasq/dnsmasq.conf`
```conf
strict-order
bind-dynamic
#log-queries
bogus-priv
dhcp-authoritative

dhcp-range=ipv4,192.168.125.120,192.168.125.250,255.255.255.0,24h
dhcp-option=ipv4,option:dns-server,192.168.125.1
dhcp-option=ipv4,option:router,192.168.125.1

resolv-file=/opt/dnsmasq/upstream-resolv.conf
except-interface=lo
dhcp-lease-max=81
log-dhcp
no-hosts

# DHCP Reservations
dhcp-leasefile=/opt/dnsmasq/hosts.leases

# Include all files in a directory depending on the suffix
conf-dir=/opt/dnsmasq/include.d/*.ipv4
```

Create the upstream resolver to delegate the non-local environments queries

- `/opt/dnsmasq/upstream-resolv.conf`
```
nameserver 8.8.8.8
nameserver 8.8.4.4
```

Create the different component DNS configurations

- `/opt/dnsmasq/include.d/hosted-nodeport.ipv4`
```
## Nodeport
host-record=api-int.hosted-ipv4.hypershiftbm.lab,192.168.125.20
host-record=api-int.hosted-ipv4.hypershiftbm.lab,192.168.125.21
host-record=api-int.hosted-ipv4.hypershiftbm.lab,192.168.125.22
host-record=api.hosted-ipv4.hypershiftbm.lab,192.168.125.20
host-record=api.hosted-ipv4.hypershiftbm.lab,192.168.125.21
host-record=api.hosted-ipv4.hypershiftbm.lab,192.168.125.22

## Nodeport
## IMPORTANT!: You should point to the node which is exposing the router.
## You can also use MetalLB to expose the Apps wildcard.
address=/apps.hosted-ipv4.hypershiftbm.lab/192.168.125.30

## General
dhcp-host=aa:aa:aa:aa:02:01,hosted-ipv4-worker0,192.168.125.30
dhcp-host=aa:aa:aa:aa:02:02,hosted-ipv4-worker1,192.168.125.31
dhcp-host=aa:aa:aa:aa:02:03,hosted-ipv4-worker2,192.168.125.32
```

- `/opt/dnsmasq/include.d/hub.ipv4`
```
host-record=api-int.hub-ipv4.hypershiftbm.lab,192.168.125.10
host-record=api.hub-ipv4.hypershiftbm.lab,192.168.125.10
address=/apps.hub-ipv4.hypershiftbm.lab/192.168.125.11
dhcp-host=aa:aa:aa:aa:02:01,ocp-master-0,192.168.125.20
dhcp-host=aa:aa:aa:aa:02:02,ocp-master-1,192.168.125.21
dhcp-host=aa:aa:aa:aa:02:03,ocp-master-2,192.168.125.22
dhcp-host=aa:aa:aa:aa:02:06,ocp-installer,192.168.125.25
dhcp-host=aa:aa:aa:aa:02:10,ocp-bootstrap,192.168.125.26
```

- `/opt/dnsmasq/include.d/infra.ipv4`
```
host-record=registry.hypershiftbm.lab,192.168.125.1
```

To proceed, we must create a systemd service for the management of the dnsmasq service and disable the system's default dnsmasq service:

- `/etc/systemd/system/dnsmasq-virt.service`
```
[Unit]
Description=DNS server for Openshift 4 Clusters.
After=network.target

[Service]
User=root
Group=root
ExecStart=/usr/sbin/dnsmasq -k --conf-file=/opt/dnsmasq/dnsmasq.conf

[Install]
WantedBy=multi-user.target
```

The commands to do so:

```
systemctl daemon-reload
systemctl disable --now dnsmasq
systemctl enable --now dnsmasq-virt
```

!!! note

    This step is mandatory for both Disconnected and Connected environments. Additionally, it holds significance for both Virtualized and Bare Metal environments. The key distinction lies in the location where the resources will be configured. In a non-virtualized environment, a more robust solution like Bind is recommended instead of a lightweight dnsmasq.

---

## Source: docs/content/labs/IPv4/hostedcluster/baremetalhost.md

## Bare Metal Hosts

A **BareMetalHost** is an openshift-machine-api object that encompasses both physical and logical details, allowing it to be identified by the Metal3 operator. Subsequently, these details are associated with other Assisted Service objects known as Agents. The structure of this object is as follows:
```yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: hosted-ipv4-worker0-bmc-secret
  namespace: clusters-hosted-ipv4
data:
  password: YWRtaW4=
  username: YWRtaW4=
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: hosted-ipv4-worker0
  namespace: clusters-hosted-ipv4
  labels:
    infraenvs.agent-install.openshift.io: hosted-ipv4
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: hosted-ipv4-worker0
spec:
  automatedCleaningMode: disabled
  bmc:
    disableCertificateVerification: true
    address: redfish-virtualmedia://[192.168.125.1]:9000/redfish/v1/Systems/local/hosted-ipv4-worker0
    credentialsName: hosted-ipv4-worker0-bmc-secret
  bootMACAddress: aa:aa:aa:aa:02:11
  online: true
```

**Details**:

- We will have at least 1 secret that holds the BMH credentials, so we will need to create at least 2 objects per worker node.
- `spec.metadata.labels["infraenvs.agent-install.openshift.io"]` serves as the link between the Assisted Installer and the BareMetalHost objects.
- `spec.metadata.annotations["bmac.agent-install.openshift.io/hostname"]` represents the node name it will adopt during deployment.
- `spec.automatedCleaningMode` prevents the node from being erased by the Metal3 operator.
- `spec.bmc.disableCertificateVerification` is set to `true` to bypass certificate validation from the client.
- `spec.bmc.address` denotes the BMC address of the worker node.
- `spec.bmc.credentialsName` points to the Secret where User/Password credentials are stored.
- `spec.bootMACAddress` indicates the interface MACAddress from which the node will boot.
- `spec.online` defines the desired state of the node once the BMH object is created.

To deploy this object, simply follow the same procedure as before:

!!! important

    Please create the virtual machines before you create the BareMetalHost and the destination Nodes.

To deploy the BareMetalHost object, execute the following command:

```bash
oc apply -f 04-bmh.yaml
```

This will be the process:

- Preparing (Trying to reach the nodes):
```
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   registering              true             2s
clusters-hosted   hosted-worker1   registering              true             2s
clusters-hosted   hosted-worker2   registering              true             2s
```

- Provisioning (Nodes Booting up)
```
NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioning              true             16s
clusters-hosted   hosted-worker1   provisioning              true             16s
clusters-hosted   hosted-worker2   provisioning              true             16s
```

- Provisioned (Nodes Booted up successfully)
```
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioned              true             67s
clusters-hosted   hosted-worker1   provisioned              true             67s
clusters-hosted   hosted-worker2   provisioned              true             67s
```

## Agents registration

After the nodes have booted up, you will observe the appearance of agents within the namespace.

```
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign
```

These agents represent the nodes available for installation. To assign them to a HostedCluster, scale up the NodePool.

## Scaling Up the Nodepool

Once we have the BareMetalHosts created, the statuses of these BareMetalHosts will transition from `Registering` (Attempting to reach the Node's BMC) to `Provisioning` (Node Booting Up), and finally to `Provisioned` (Successful node boot-up).

The nodes will boot with the Agent's RHCOS LiveISO and a default pod named "agent." This agent is responsible for receiving instructions from the Assisted Service Operator to install the Openshift payload.

To accomplish this, execute the following command:

```bash
oc -n clusters scale nodepool hosted-ipv4 --replicas 3
```

After the NodePool scaling, you will notice that the agents are assigned to a Hosted Cluster.

```
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411   hosted    true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412   hosted    true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413   hosted    true       auto-assign
```

And the NodePool replicas set

```
NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted   hosted    3                               False         False        4.14.0-0.nightly-2023-08-29-102237                                      Minimum availability requires 3 replicas, current 0 available
```

So now, we need to wait until the nodes join the cluster. The Agents will provide updates on their current stage and status. Initially, they may not post any status, but eventually, they will.

---

## Source: docs/content/labs/IPv4/hostedcluster/hostedcluster.md

In this section, we will focus on all the related objects necessary to achieve a Disconnected Hosted Cluster deployment.
**Premises**:

- HostedCluster Name: `hosted-ipv4`
- HostedCluster Namespace: `clusters`
- Disconnected: `true`
- Network Stack: `IPv4`

## Openshift Objects

### Namespaces

In a typical situation, the operator would be responsible for creating the HCP (HostedControlPlane) namespace. However, in this case, we want to include all the objects before the operator begins reconciliation over the HostedCluster object. This way, when the operator commences the reconciliation process, it will find all the objects in place.

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: clusters-hosted-ipv4
spec: {}
status: {}
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: clusters
spec: {}
status: {}
```

!!! note

      We will **not** create objects one by one but will concatenate all of them in the same file and apply them with just one command.

### ConfigMap and Secrets

These are the ConfigMaps and Secrets that we will include in the HostedCluster deployment.

```yaml
---
apiVersion: v1
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
kind: ConfigMap
metadata:
  name: user-ca-bundle
  namespace: clusters
---
apiVersion: v1
data:
  .dockerconfigjson: xxxxxxxxx
kind: Secret
metadata:
  creationTimestamp: null
  name: hosted-ipv4-pull-secret
  namespace: clusters
---
apiVersion: v1
kind: Secret
metadata:
  name: sshkey-cluster-hosted-ipv4
  namespace: clusters
stringData:
  id_rsa.pub: ssh-rsa xxxxxxxxx
---
apiVersion: v1
data:
  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=
kind: Secret
metadata:
  creationTimestamp: null
  name: hosted-ipv4-etcd-encryption-key
  namespace: clusters
type: Opaque
```

### RBAC Roles

While not mandatory, it allows us to have the Assisted Service Agents located in the same HostedControlPlane namespace as the HostedControlPlane and still be managed by CAPI.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: capi-provider-role
  namespace: clusters-hosted-ipv4
rules:
- apiGroups:
  - agent-install.openshift.io
  resources:
  - agents
  verbs:
  - '*'
```

### Hosted Cluster

This is a sample of the HostedCluster Object

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: hosted-ipv4
  namespace: clusters
spec:
  additionalTrustBundle:
    name: "user-ca-bundle"
  olmCatalogPlacement: guest
  imageContentSources:
  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
    mirrors:
    - registry.hypershiftbm.lab:5000/openshift/release
  - source: quay.io/openshift-release-dev/ocp-release
    mirrors:
    - registry.hypershiftbm.lab:5000/openshift/release-images
  - mirrors:
  ...
  ...
  autoscaling: {}
  controllerAvailabilityPolicy: SingleReplica
  dns:
    baseDomain: hypershiftbm.lab
  etcd:
    managed:
      storage:
        persistentVolume:
          size: 8Gi
        restoreSnapshotURL: null
        type: PersistentVolume
    managementType: Managed
  fips: false
  networking:
    clusterNetwork:
    - cidr: 10.132.0.0/14
    networkType: OVNKubernetes
    serviceNetwork:
    - cidr: 172.31.0.0/16
  platform:
    agent:
      agentNamespace: clusters-hosted-ipv4
    type: Agent
  pullSecret:
    name: hosted-ipv4-pull-secret
  release:
    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
  secretEncryption:
    aescbc:
      activeKey:
        name: hosted-ipv4-etcd-encryption-key
    type: aescbc
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv4.hypershiftbm.lab
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv4.hypershiftbm.lab
      type: NodePort
  - service: OIDC
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv4.hypershiftbm.lab
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv4.hypershiftbm.lab
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv4.hypershiftbm.lab
      type: NodePort
  sshKey:
    name: sshkey-cluster-hosted-ipv4
status:
  controlPlaneEndpoint:
    host: ""
    port: 0
```

!!! note

    The `imageContentSources` section within the `spec` field contains mirror references for user workloads within the HostedCluster.

As you can see, all the objects created before are referenced here. You can also refer to the documentation where all the fields are described.

## Deployment

To deploy these objects, simply concatenate them into the same file and apply them against the management cluster:

```bash
oc apply -f 01-4.14-hosted_cluster-nodeport.yaml
```

This will raise up a functional Hosted Control Plane.

``` yaml
NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-5b57dbd6d5-pxlqc                        1/1     Running   0   	   3m57s
catalog-operator-9694884dd-m7zzv                      2/2     Running   0          93s
cluster-api-f98b9467c-9hfrq                           1/1     Running   0   	   3m57s
cluster-autoscaler-d7f95dd5-d8m5d                     1/1     Running   0          93s
cluster-image-registry-operator-5ff5944b4b-648ht      1/2     Running   0          93s
cluster-network-operator-77b896ddc-wpkq8              1/1     Running   0          94s
cluster-node-tuning-operator-84956cd484-4hfgf         1/1     Running   0          94s
cluster-policy-controller-5fd8595d97-rhbwf            1/1     Running   0          95s
cluster-storage-operator-54dcf584b5-xrnts             1/1     Running   0          93s
cluster-version-operator-9c554b999-l22s7              1/1     Running   0          95s
control-plane-operator-6fdc9c569-t7hr4                1/1     Running   0 	       3m57s
csi-snapshot-controller-785c6dc77c-8ljmr              1/1     Running   0 	       77s
csi-snapshot-controller-operator-7c6674bc5b-d9dtp     1/1     Running   0 	       93s
dns-operator-6874b577f-9tc6b                          1/1     Running   0          94s
etcd-0                                                3/3     Running   0 	       3m39s
hosted-cluster-config-operator-f5cf5c464-4nmbh        1/1     Running   0 	       93s
ignition-server-6b689748fc-zdqzk                      1/1     Running   0          95s
ignition-server-proxy-54d4bb9b9b-6zkg7                1/1     Running   0 	       95s
ingress-operator-6548dc758b-f9gtg                     1/2     Running   0          94s
konnectivity-agent-7767cdc6f5-tw782                   1/1     Running   0 	       95s
kube-apiserver-7b5799b6c8-9f5bp                       4/4     Running   0 	       3m7s
kube-controller-manager-5465bc4dd6-zpdlk              1/1     Running   0 	       44s
kube-scheduler-5dd5f78b94-bbbck                       1/1     Running   0 	       2m36s
machine-approver-846c69f56-jxvfr                      1/1     Running   0          92s
oauth-openshift-79c7bf44bf-j975g                      2/2     Running   0 	       62s
olm-operator-767f9584c-4lcl2                          2/2     Running   0 	       93s
openshift-apiserver-5d469778c6-pl8tj                  3/3     Running   0 	       2m36s
openshift-controller-manager-6475fdff58-hl4f7         1/1     Running   0 	       95s
openshift-oauth-apiserver-dbbc5cc5f-98574             2/2     Running   0          95s
openshift-route-controller-manager-5f6997b48f-s9vdc   1/1     Running   0          95s
packageserver-67c87d4d4f-kl7qh                        2/2     Running   0          93s
```

And this is how the HostedCluster looks like:

```bash
NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
clusters    hosted-ipv4            hosted-admin-kubeconfig   Partial    True 	      False         The hosted control plane is available
```

After some time, we will have almost all the pieces in place, and the Control Plane operator awaits for the worker nodes to join the cluster. To achieve this, we need to create some more objects. Let's discuss the `InfraEnv` and the `BareMetalHost` in the following sections.


---

## Source: docs/content/labs/IPv4/hostedcluster/index.md

 Hosted Cluster, as mentioned in the documentation here, is essentially an OCP API endpoint managed by Hypershift. In this context, we will also include the term HostedControlPlane to enhance readability and comprehension. This terminology is further explained in the same link.

The Hosted Cluster consists of two main components:
- The Control Plane, which operates within the management cluster as pods.
- The Data Plane, which comprises external nodes managed by the end user.

Now, with this foundational understanding, we can proceed with the deployment of our Hosted Cluster. While ACM/MCE users typically employ the web UI for cluster creation, we will take advantage of manifests, which provide greater flexibility for modifying the artifacts.


---

## Source: docs/content/labs/IPv4/hostedcluster/infraenv.md

The `InfraEnv` is an Assisted Service object that includes essential details such as the `pullSecretRef` and the `sshAuthorizedKey`. These details are used to create the RHCOS Boot Image customized specifically for the cluster. Below is the structure of this object:

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: hosted-ipv4
  namespace: clusters-hosted-ipv4
spec:
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDk7ICaUE+/k4zTpxLk4+xFdHi4ZuDi5qjeF52afsNkw0w/glILHhwpL5gnp5WkRuL8GwJuZ1VqLC9EKrdmegn4MrmUlq7WTsP0VFOZFBfq2XRUxo1wrRdor2z0Bbh93ytR+ZsDbbLlGngXaMa0Vbt+z74FqlcajbHTZ6zBmTpBVq5RHtDPgKITdpE1fongp7+ZXQNBlkaavaqv8bnyrP4BWahLP4iO9/xJF9lQYboYwEEDzmnKLMW1VtCE6nJzEgWCufACTbxpNS7GvKtoHT/OVzw8ArEXhZXQUS1UY8zKsX2iXwmyhw5Sj6YboA8WICs4z+TrFP89LmxXY0j6536TQFyRz1iB4WWvCbH5n6W+ABV2e8ssJB1AmEy8QYNwpJQJNpSxzoKBjI73XxvPYYC/IjPFMySwZqrSZCkJYqQ023ySkaQxWZT7in4KeMu7eS2tC+Kn4deJ7KwwUycx8n6RHMeD8Qg9flTHCv3gmab8JKZJqN3hW1D378JuvmIX4V0=
```

**Details**:

- `pullSecretRef` refers to the ConfigMap reference (in the same Namespace as the InfraEnv) where the PullSecret will be utilized.
- `sshAuthorizedKey` represents the SSH Public key that will be injected into the Boot Image. This SSH key will, by default, allow access to the worker nodes as the `core` user.

To deploy this object, follow the same procedure as before:

```bash
oc apply -f 03-infraenv.yaml
```

And this is how looks like

```bash
NAMESPACE              NAME     ISO CREATED AT
clusters-hosted-ipv4   hosted   2023-09-11T15:14:10Z
```

---

## Source: docs/content/labs/IPv4/hostedcluster/nodepool.md

A `NodePool` is a scalable set of worker nodes associated with a HostedCluster. NodePool machine architectures remain consistent within a specific pool and are independent of the underlying machine architecture of the control plane.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

!!! warning

    Before a day-1 patch, the release image set in the HostedCluster **should use the digest rather than the tag**. (e.g `quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0`)

This is how one looks like:

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  creationTimestamp: null
  name: hosted-ipv4
  namespace: clusters
spec:
  arch: amd64
  clusterName: hosted-ipv4
  management:
    autoRepair: false
    upgradeType: InPlace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    type: Agent
  release:
    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
  replicas: 0
status:
  replicas: 0
```

**Details**:

- All the nodes included in this NodePool will be based on the Openshift version `4.14.0-0.nightly-2023-08-29-102237`.
- The Upgrade type is set to `InPlace`, indicating that the same bare-metal node will be reused during an upgrade.
- Autorepair is set to `false` because the node will not be recreated when it disappears.
- Replicas are set to `0` because we intend to scale them when needed.

You can find more information about NodePool in the NodePool documentation.

To deploy this object, simply follow the same procedure as before:

```bash
oc apply -f 02-nodepool.yaml
```

And this is how the NodePool looks like (at this point):

```bash
NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted-ipv4   hosted    0                               False         False        4.14.0-0.nightly-2023-08-29-102237
```

!!! important

    Keep the nodepool replicas to 0 until all the steps are in place.

---

## Source: docs/content/labs/IPv4/hostedcluster/worker-nodes.md

Regarding the worker nodes, if you are working on real bare metal, this step is crucial to ensure that the details set in the `BareMetalHost` are correctly configured. If not, you will need to debug why it's not functioning as expected.

However, if you are working with virtual machines, you can follow these steps to create empty ones that will be consumed by the Metal3 operator. To achieve this, we will utilize Kcli.

## Creating Virtual Machines

If this is not your first attempt, you must first delete the previous setup. To do so, please refer to the Deleting Virtual Machines section.

Now, you can execute the following commands for VM creation:

```bash
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv4 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"ipv4\", \"mac\": \"aa:aa:aa:aa:02:11\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0211 -P name=hosted-ipv4-worker0
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv4 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"ipv4\", \"mac\": \"aa:aa:aa:aa:02:12\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0212 -P name=hosted-ipv4-worker1
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv4 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"ipv4\", \"mac\": \"aa:aa:aa:aa:02:13\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0213 -P name=hosted-ipv4-worker2

sleep 2
systemctl restart ksushy
```

Let's dissect the creation command:

- `start=False`: The VM will not boot automatically upon creation.
- `uefi_legacy=true`: We will use UEFI legacy boot to ensure compatibility with older UEFI implementations.
- `plan=hosted-dual`: The plan name, which identifies a group of machines as a cluster.
- `memory=8192` and `numcpus=16`: These parameters specify the resources for the VM, including RAM and CPU.
- `disks=[200,200]`: We are creating 2 disks (thin provisioned) in the virtual machine.
- `nets=[{"name": "dual", "mac": "aa:aa:aa:aa:02:13"}]`: Network details, including the network name it will be connected to and the MAC address for the primary interface.
- The `ksushy` restart is performed to make our `ksushy` (VM's BMC) aware of the new VMs added.

This is what the command looks like:

```bash
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|    hosted-worker0   |  down  |                   |                                                    | hosted-ipv4 |  kvirt  |
|    hosted-worker1   |  down  |                   |                                                    | hosted-ipv4 |  kvirt  |
|    hosted-worker2   |  down  |                   |                                                    | hosted-ipv4 |  kvirt  |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
```

## Deleting Virtual Machines

To delete the VMs, you simply need to delete the plan, which, in our case, is:

```bash
kcli delete plan hosted-ipv4
```

```bash
$ kcli delete plan hosted-ipv4
Are you sure? [y/N]: y
hosted-worker0 deleted on local!
hosted-worker1 deleted on local!
hosted-worker2 deleted on local!
Plan hosted-ipv4 deleted!
```

---

## Source: docs/content/labs/IPv4/hypervisor/hypervisor-system-config.md

This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the `firewalld` service and configure SELinux policies to maintain a secure environment.

With these instructions, we allow various types of connections through the different virtual networks within the environment:

```bash
## SELinux
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0

## Firewalld
systemctl disable --now firewalld

## Libvirtd
systemctl restart libvirtd
systemctl enable --now libvirtd
```

---

## Source: docs/content/labs/IPv4/hypervisor/index.md

---
title: Hypervisor
---

This section is entirely dedicated to virtual machine environments. If you are working with physical bare-metal infrastructure, you can skip this section and ensure that your lab has all the required components in place for a proper deployment. If you find any gaps in your setup, you can configure them separately to address those specific needs.


---

## Source: docs/content/labs/IPv4/hypervisor/network-manager-dispatcher.md

This script modifies the system DNS resolver to prioritize pointing to the `dnsmasq` service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.

To enable this, you need to create a script named `forcedns` in `/etc/NetworkManager/dispatcher.d/` with the following content:

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

=== "IPv4"

    ```bash
    #!/bin/bash

    export IP="192.168.125.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "IPv6"

    ```bash
    #!/bin/bash

    export IP="2620:52:0:1306::1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "Dual stack"

    ```bash
    #!/bin/bash

    export IP="192.168.126.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

The `IP` variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.

After creating the file, you need to add execution permissions using the command:

```bash
chmod 755 /etc/NetworkManager/dispatcher.d/forcedns
```

Then, execute it once. The output should indicate `ok`.



---

## Source: docs/content/labs/IPv4/hypervisor/packaging.md

---
title: Hypervisor Packaging
---

## System

These are the main packages that are needed to deploy a virtualized Openshift Management cluster.

```bash
sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y
```

Additionally, you need to enable and start the Podman service using the following command:

```bash
systemctl enable --now podman
```

## Kcli

We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:

```bash
sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm
sudo usermod -aG qemu,libvirt $(id -un)
sudo newgrp libvirt
sudo systemctl enable --now libvirtd
sudo dnf -y copr enable karmab/kcli
sudo dnf -y install kcli
sudo kcli create pool -p /var/lib/libvirt/images default
kcli create host kvm -H 127.0.0.1 local
sudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images
kcli create network  -c 192.168.122.0/24 default
```

For more info about Kcli please visit the official documentation.

---

## Source: docs/content/labs/IPv4/hypervisor/redfish-for-vms.md

In a bare metal environment, the preferred approach is to utilize the actual BMC (Baseboard Management Controller) of the nodes used for the management cluster, which can be managed by Metal3 for discovery and provisioning. However, in a virtual environment, this approach is not feasible. As a workaround, we will use `ksushy`, which is an implementation of `sushy-tools`, allowing us to simulate BMCs for the virtual machines.

To configure `ksushy` we need to execute these commands:

```bash
sudo dnf install python3-pyOpenSSL.noarch python3-cherrypy -y
kcli create sushy-service --ssl --port 9000
sudo systemctl daemon-reload
systemctl enable --now ksushy
```

To test if this service is functioning correctly, you can check the service status with `systemctl status ksushy`. Additionally, you can execute a `curl` command against the exposed interface:
```
curl -Lk https://192.168.125.1:9000/redfish/v1
```


---

## Source: docs/content/labs/IPv4/hypervisor/requisites.md

---
title: Hypervisor Prerequisites
---

- **CPU**: The number of CPUs provided determines how many HostedClusters can run concurrently.
  - **Recommended**: 16 CPUs per Node for 3 nodes.
  - **Minimal Dev**: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.

- **Memory**: The amount of RAM impacts how many HostedClusters can be hosted.
  - **Recommended**: 48 GB of RAM per Node.
  - **Minimal Dev**: For minimal development, 18 GB of RAM per Node may suffice.

- **Storage**: Using SSD storage for MCE is crucial.
  - **Management Cluster**: 250 GB.
  - **Registry**: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.
  - **Webserver**: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.

- **Production**: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:
  - **Registry**: 2 TB.
  - **Management Cluster**: 500 GB.
  - **WebServer**: 2 TB.


---

## Source: docs/content/labs/IPv4/index.md

---
title: IPv4
---

This is one of the simplest network configurations for this type of deployment. We will primarily focus on IPv4 ranges, requiring fewer external components than IPv6 or dual-stack setups.

All the scripts provided contain either partial or complete automation to recreate the environment. To follow along, refer to the repository containing all the scripts for IPv4 environments.

This documentation is structured to be followed in a specific order:

- Hypervisor
- DNS
- Registry
- Management Cluster
- Webserver
- Mirroring
- Multicluster Engine
- TLS Certificates
- HostedCluster
- Watching Deployment progress

---

## Source: docs/content/labs/IPv4/mce/agentserviceconfig.md

The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named `AgentServiceConfig` to configure it.

## Agent Service Config Objects

You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.

In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.

### Custom Registries Configuration

This ConfigMap contains the disconnected details necessary to customize the deployment.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-registries
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/openshift4"
    mirror-by-digest-only = true

    [[registry.mirror]]
      location = "registry.hypershiftbm.lab:5000/openshift4"

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/rhacm2"
    mirror-by-digest-only = true
    ...
    ...
```

This object includes two fields:

1. **Custom CAs**: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.
2. **Registries**: The `Registries.conf` field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.

### Assisted Service Customization

The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false"
```

You can find documentation on how to customize the operator here.

### Assisted Service Config

The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config
  name: agent
  namespace: multicluster-engine
spec:
  mirrorRegistryRef:
    name: custom-registries
  databaseStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  filesystemStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 20Gi
  osImages:
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso
    version: 414.92.202308281054-0
```

If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.

In this section, we will emphasize the important aspects:

- The `metadata.annotations["unsupported.agent-install.openshift.io/assisted-service-configmap"]` Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.
- The `spec.mirrorRegistryRef.name` points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.
- The `spec.osImages` field contains different versions available for deployment by this operator. These fields are mandatory.

Let's fill in this section for the 4.14 dev preview ec4 version (sample):

- Release URL
- RHCOS Info

Assuming you've already downloaded the RootFS and LiveIso files:

```yaml
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso
    version: 414.92.202307250657-0
```

## Deployment

To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.

```bash
oc apply -f agentServiceConfig.yaml
```

This will trigger 2 pods

```bash
assisted-image-service-0                               1/1     Running   2             11d
assisted-service-668b49548-9m7xw                       2/2     Running   5             11d
```

!!! note

      The `Assisted Image Service` pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.

      The `Assisted Service` refers to the operator.


---

## Source: docs/content/labs/IPv4/mce/index.md

The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.

## Credentials and Authorization

To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.

ACM/MCE Deployment{ .md-button }
Agent Service Config{ .md-button }


---

## Source: docs/content/labs/IPv4/mce/multicluster-engine.md

The deployment of each component will depend on your needs, follow the next links accordingly:

- ACM Deployment
- ACM Downstream builds
- MCE Deployment
- MCE Upgrade
- ACM/MCE Cluster deletion
- MulticlusterEngine deletion
- Last resource: **nuke.sh**

---

## Source: docs/content/labs/IPv4/mgmt-cluster/compact-ipv4.md

In this section, we will discuss how to deploy the Openshift management cluster. To do that, we need to have the following files in place:

1. **Pull Secret**
2. **Kcli Plan**

Ensure that these files are properly set up for the deployment process.

### Pull Secret

The Pull Secret should be located in the same folder as the kcli plan and should be named `openshift_pull.json`.

### Kcli plan

The Kcli plan contains the Openshift definition. This is how looks like:

- `mgmt-compact-hub-ipv4.yaml`

```yaml
plan: hub-ipv4
force: true
version: nightly
tag: "4.14.0-0.nightly-2023-08-29-102237"
cluster: "hub-ipv4"
domain: hypershiftbm.lab
api_ip: 192.168.125.10
ingress_ip: 192.168.125.11
disconnected_url: registry.hypershiftbm.lab:5000
disconnected_update: true
disconnected_user: dummy
disconnected_password: dummy
disconnected_operators_version: v4.13
disconnected_operators:
- name: metallb-operator
- name: lvms-operator
  channels:
  - name: stable-4.13
disconnected_extra_images:
- quay.io/mavazque/trbsht:latest
- quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
- registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
dualstack: false
disk_size: 200
extra_disks: [200]
memory: 48000
numcpus: 16
ctlplanes: 3
workers: 0
manifests: extra-manifests
metal3: true
network: ipv4
users_dev: developer
users_devpassword: developer
users_admin: admin
users_adminpassword: admin
metallb_pool: ipv4-virtual-network
metallb_ranges:
- 192.168.125.150-192.168.125.190
metallb_autoassign: true
apps:
- users
- lvms-operator
- metallb-operator
vmrules:
- hub-bootstrap:
    nets:
    - name: ipv4
      mac: aa:aa:aa:aa:02:10
- hub-ctlplane-0:
    nets:
    - name: ipv4
      mac: aa:aa:aa:aa:02:01
- hub-ctlplane-1:
    nets:
    - name: ipv4
      mac: aa:aa:aa:aa:02:02
- hub-ctlplane-2:
    nets:
    - name: ipv4
      mac: aa:aa:aa:aa:02:03
```

!!! note

    To understand the meaning of each of these parameters, you can refer to the official documentation here.

### Deployment

To initiate the provisioning procedure, execute the following:

```bash
kcli create cluster openshift --pf mgmt-compact-hub-ipv4.yaml
```

---

## Source: docs/content/labs/IPv4/mgmt-cluster/index.md

## Openshift Management Cluster

This section contains the necessary artifacts to set up an Openshift management cluster based on virtual machines using kcli as the primary tool. Another option is to use dev-scripts, which uses a different approach.

Networking{ .md-button }
Openshift Compact Dual{ .md-button }

---

## Source: docs/content/labs/IPv4/mgmt-cluster/network.md

Firstly, we need to ensure that we have the right networks prepared for use in the Hypervisor. These networks will be used to host both the Management and Hosted clusters.

To configure these networks, we will use the following `kcli` command:

```
kcli create network -c 192.168.125.0/24 -P dhcp=false -P dns=false --domain hypershiftbm.lab ipv4
```

Where:

- `-c` specifies the CIDR used for that network.
- `-P dhcp=false` configures the network to disable DHCP, which will be handled by the previously configured dnsmasq.
- `-P dns=false` configures the network to disable DNS, which will also be handled by the dnsmasq.
- `--domain` sets the domain to search into.
- `ipv4` is the name of the network that will be created.

This is what the network will look like once created:

```
[root@hypershiftbm ~]# kcli list network
Listing Networks...
+---------+--------+---------------------+-------+------------------+------+
| Network |  Type  |         Cidr        |  Dhcp |      Domain      | Mode |
+---------+--------+---------------------+-------+------------------+------+
| default | routed |   192.168.122.0/24  |  True |     default      | nat  |
| ipv4    | routed |   192.168.125.0/24  | False | hypershiftbm.lab | nat  |
| ipv6    | routed | 2620:52:0:1306::/64 | False | hypershiftbm.lab | nat  |
+---------+--------+---------------------+-------+------------------+------+
```

```
[root@hypershiftbm ~]# kcli info network ipv4
Providing information about network ipv4...
cidr: 192.168.125.0/24
dhcp: false
domain: hypershiftbm.lab
mode: nat
plan: kvirt
type: routed
```



---

## Source: docs/content/labs/IPv4/mirror/ICSP-IDMS.md

Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:

1. ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).
2. Catalog Sources.

Using the `oc-mirror` tool, the output artifacts will be located in a new folder called `oc-mirror-workspace/results-XXXXXX/`.

ICSP/IDMS will trigger a "special" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.

Once all nodes are schedulable and marked as `READY`, you will need to apply the new catalog sources generated.

The catalog sources will trigger some actions in the `openshift-marketplace operator`, such as downloading the catalog image and processing it to retrieve all the `PackageManifests` included in that image. You can check the new sources by executing `oc get packagemanifest` using the new CatalogSource as a source.

## Applying the Artifacts

First, we need to create the ICSP/IDMS artifacts:


```bash
oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml
```

Now, wait for the nodes to become ready again and execute the following command:

```bash
oc apply -f catalogSource-XXXXXXXX-index.yaml
```

---

## Source: docs/content/labs/IPv4/mirror/index.md

!!! important

    This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.

Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.

To learn how to perform image mirroring, follow the next sections:

Image Mirroring Process{ .md-button }
ICSP and IDMS{ .md-button }


---

## Source: docs/content/labs/IPv4/mirror/mirroring.md

The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.

For this purpose, we will use the `oc-mirror` tool, a binary that utilizes an object called `ImageSetConfiguration`.

In this file, you can specify:

- The OpenShift versions to mirror (they should be located in quay.io).
- The additional operators to mirror, selecting packages individually.
- The extra images you want to add to the repository.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

Here is an example of the `ImageSetConfiguration` that we will use for our mirroring:

```yaml
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest
mirror:
  platform:
    channels:
    - name: candidate-4.14
      minVersion: 4.14.0-ec.1
      maxVersion: 4.14.0-ec.3
      type: ocp
    graph: true
  additionalImages:
  - name: quay.io/karmab/origin-keepalived-ipfailover:latest
  - name: quay.io/karmab/kubectl:latest
  - name: quay.io/karmab/haproxy:latest
  - name: quay.io/karmab/mdns-publisher:latest
  - name: quay.io/karmab/origin-coredns:latest
  - name: quay.io/karmab/curl:latest
  - name: quay.io/karmab/kcli:latest
  - name: quay.io/mavazque/trbsht:latest
  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
    - name: lvms-operator
    - name: local-storage-operator
    - name: odf-csi-addons-operator
    - name: odf-operator
    - name: mcg-operator
    - name: ocs-operator
    - name: metallb-operator
```

Make sure you have your `${HOME}/.docker/config.json` file updated with the registries you are trying to mirror from and your private registry to push the images to.

After that, we can begin the mirroring process:

```bash
oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}
```

Once the mirror finishes, you will have a new folder called `oc-mirror-workspace/results-XXXXXX/` which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.

## Mirroring Nightly and CI releases

The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the `oc adm release mirror` to mirror those versions.

To mirror the nightly versions we need for this deployment, you need to execute this:

```bash
REGISTRY=registry.$(hostname --long):5000

oc adm release mirror \
  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \
  --to=${REGISTRY}/openshift/release \
  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
```

For more detailed and updated information, you can visit the official Documentation or GitHub repository

## Mirror MCE internal releases

In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.

- Red Hat Official Documentation
- Red Hat Internal deployment Brew Registry deployment


---

## Source: docs/content/labs/IPv4/registry.md

!!! important

    This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.

In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.

1. As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:

- `registry.sh`
```bash
#!/usr/bin/env bash

set -euo pipefail

PRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)
export PATH=/root/bin:$PATH
export PULL_SECRET="/root/baremetal/hub/openshift_pull.json"

if [[ ! -f $PULL_SECRET ]];then
  echo "Pull Secret not found, exiting..."
  exit 1
fi

dnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel
export IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)
REGISTRY_NAME=registry.$(hostname --long)
REGISTRY_USER=dummy
REGISTRY_PASSWORD=dummy
KEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)
echo "{\"auths\": {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\", \"email\": \"sample-email@domain.ltd\"}}}" > /root/disconnected_pull.json
mv ${PULL_SECRET} /root/openshift_pull.json.old
jq ".auths += {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\",\"email\": \"sample-email@domain.ltd\"}}" < /root/openshift_pull.json.old > $PULL_SECRET
mkdir -p /opt/registry/{auth,certs,data,conf}
cat <<EOF > /opt/registry/conf/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
  delete:
    enabled: true
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
compatibility:
  schema1:
    enabled: true
EOF
openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj "/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME" -addext "subjectAltName=DNS:$REGISTRY_NAME"
cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract
htpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD
podman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e "REGISTRY_AUTH=htpasswd" -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry" -e "REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest
[ "$?" == "0" ] || !!
systemctl enable --now registry
```

2. Kindly modify the location of the `PULL_SECRET` to point to the correct location.
3. Please adjust the permissions by adding the execution flag as follows: `chmod u+x ${HOME}/registry.sh`
4. You can execute the script without any parameters using the following command `${HOME}/registry.sh`

After executing the script `${HOME}/registry.sh`, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.

It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.

The root folder for the registry is situated at /opt/registry, and it's structured as follows:

- `certs` holds the TLS certificates.
- `auth` keeps the credentials.
- `data` contains the registry images.
- `conf` will hold the registry configuration.

---

## Source: docs/content/labs/IPv4/tls-certificates.md

!!! important

    This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.

In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.

It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):

- `/etc/pki/ca-trust/extracted/pem/`
- `/etc/pki/ca-trust/source/anchors/`
- `/etc/pki/tls/certs/`

## Adding a CA to the Management Cluster

There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.

1. Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name `registry-config` The content should resemble the following:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: registry-config
  namespace: openshift-config
data:
  registry.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
```

!!! note

        The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the ":" character is being replaced by ".."; therefore, it is imperative to ensure this correction.

2. Now we need to patch the clusterwide object `image.config.openshift.io` including this:

```yaml
spec:
  additionalTrustedCA:
    name: registry-config
```

This modification will result in two significant consequences:

- Granting masters the capability to retrieve images from the private registry.
- Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.

!!! note

        The modification required several minutes to be successfully executed.

## Alternative: Adding a CA to the Management Cluster

We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.

It's described here. This method involves utilizing the `image-registry-operator`, which deploys the CAs to the OCP nodes.

Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.

However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:

- Check if the `openshift-config` namespace in the Management cluster contains a ConfigMap named `user-ca-bundle`.
- If the ConfigMap doesn't exist, execute the following command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt

oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}
```

- Otherwise, if that ConfigMap exists, execute this other command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt
export TMP_FILE=$(mktemp)

oc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\.crt}' > ${TMP_FILE}
echo >> ${TMP_FILE}
echo \#registry.$(hostname --long) >> ${TMP_FILE}
cat ${REGISTRY_CERT_PATH} >> ${TMP_FILE}
oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -
```

You have a functional script located in the `assets/<NetworkStack>/09-tls-certificates/01-config.sh`, this is the sample for IPv6.


---

## Source: docs/content/labs/IPv4/watching/index.md

This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:

- Control Plane
- Data Plane

So please follow these links accordingly.

Control Plane perspective{ .md-button }
Data Plane perspective{ .md-button }


---

## Source: docs/content/labs/IPv4/watching/watching-cp.md

Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:

```bash
export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig

watch "oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;"
```

This command will give you info about:

- What is the Hypershift Operator status
- The HostedControlPlane pod status
- The BareMetalHosts
- The Agents
- The Infraenv
- The HostedCluster and NodePool

This is how it looks:



---

## Source: docs/content/labs/IPv4/watching/watching-dp.md

If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands

```bash
oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d > /root/hc_admin_kubeconfig.yaml
export KUBECONFIG=/root/hc_admin_kubeconfig.yaml

watch "oc get clusterversion,nodes,co"
```

This command will give you info about:

- Check the clusterversion
- Check if the Nodes has joined the cluster
- Check the ClusterOperators

This is how looks like:



---

## Source: docs/content/labs/IPv4/webserver.md

!!! important

    This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.

This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.

The script refers to this repository folder and it's the same for all three different network stacks.

To do this, you can use this script:

```bash
#!/bin/bash

WEBSRV_FOLDER=/opt/srv
ROOTFS_IMG_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')"
LIVE_ISO_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')"

mkdir -p ${WEBSRV_FOLDER}/images
curl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}
curl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}
chmod -R 755 ${WEBSRV_FOLDER}/*

## Run Webserver
podman ps --noheading | grep -q websrv-ai
if [[ $? == 0 ]];then
    echo "Launching Registry pod..."
    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080
fi
```

The script will create a folder under `/opt/srv`. This folder will contain the `images` for RHCOS provision in the worker nodes. To be more concrete, we need the `RootFS` and `LiveISO` artifacts found on the Openshift CI Release page.

After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.


---

## Source: docs/content/labs/IPv6/dns.md

The DNS configuration is a critical aspect of our setup. To enable name resolution in our virtualized environment, follow these steps:

1. Create the primary DNS configuration file for the dnsmasq server:

- `/opt/dnsmasq/dnsmasq.conf`
```conf
strict-order
bind-dynamic
#log-queries
bogus-priv
dhcp-authoritative

# BM Network IPv6
dhcp-range=ipv6,2620:52:0:1305::11,2620:52:0:1305::20,64
dhcp-option=ipv6,option6:dns-server,2620:52:0:1305::1

resolv-file=/opt/dnsmasq/upstream-resolv.conf
except-interface=lo
dhcp-lease-max=81
log-dhcp
no-hosts

# DHCP Reservations
dhcp-leasefile=/opt/dnsmasq/hosts.leases

# Include all files in a directory depending on the suffix
conf-dir=/opt/dnsmasq/include.d/*.ipv6
```

Create the upstream resolver to delegate the non-local environments queries

- `/opt/dnsmasq/upstream-resolv.conf`
```
nameserver 8.8.8.8
nameserver 8.8.4.4
```

Create the different component DNS configurations

- `/opt/dnsmasq/include.d/hosted-nodeport.ipv6`
```
host-record=api-int.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::5
host-record=api-int.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::6
host-record=api-int.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::7
host-record=api.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::5
host-record=api.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::6
host-record=api.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::7
address=/apps.hosted-ipv6.hypershiftbm.lab/2620:52:0:1305::60
dhcp-host=aa:aa:aa:aa:04:11,hosted-worker0,[2620:52:0:1305::11]
dhcp-host=aa:aa:aa:aa:04:12,hosted-worker1,[2620:52:0:1305::12]
dhcp-host=aa:aa:aa:aa:04:13,hosted-worker2,[2620:52:0:1305::13]
```

- `/opt/dnsmasq/include.d/hub.ipv6`
```
host-record=api-int.hub-ipv6.hypershiftbm.lab,2620:52:0:1305::2
host-record=api.hub-ipv6.hypershiftbm.lab,2620:52:0:1305::2
address=/apps.hub-ipv6.hypershiftbm.lab/2620:52:0:1305::3
dhcp-host=aa:aa:aa:aa:03:01,ocp-master-0,[2620:52:0:1305::5]
dhcp-host=aa:aa:aa:aa:03:02,ocp-master-1,[2620:52:0:1305::6]
dhcp-host=aa:aa:aa:aa:03:03,ocp-master-2,[2620:52:0:1305::7]
dhcp-host=aa:aa:aa:aa:03:06,ocp-installer,[2620:52:0:1305::8]
dhcp-host=aa:aa:aa:aa:03:07,ocp-bootstrap,[2620:52:0:1305::9]
```

- `/opt/dnsmasq/include.d/infra.ipv6`
```
host-record=registry.hypershiftbm.lab,2620:52:0:1305::1
```

To proceed, we must create a systemd service for the management of the dnsmasq service and disable the system's default dnsmasq service:

- `/etc/systemd/system/dnsmasq-virt.service`
```
[Unit]
Description=DNS server for Openshift 4 Clusters.
After=network.target

[Service]
User=root
Group=root
ExecStart=/usr/sbin/dnsmasq -k --conf-file=/opt/dnsmasq/dnsmasq.conf

[Install]
WantedBy=multi-user.target
```

The commands to do so:

```
systemctl daemon-reload
systemctl disable --now dnsmasq
systemctl enable --now dnsmasq-virt
```

!!! note

    This step is mandatory for both Disconnected and Connected environments. Additionally, it holds significance for both Virtualized and Bare Metal environments. The key distinction lies in the location where the resources will be configured. In a non-virtualized environment, a more robust solution like Bind is recommended instead of a lightweight dnsmasq.

---

## Source: docs/content/labs/IPv6/hostedcluster/baremetalhost.md

## Bare Metal Hosts

A **BareMetalHost** is an openshift-machine-api object that encompasses both physical and logical details, allowing it to be identified by the Metal3 operator. Subsequently, these details are associated with other Assisted Service objects known as Agents. The structure of this object is as follows:

```yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: hosted-ipv6-worker0-bmc-secret
  namespace: clusters-hosted-ipv6
data:
  password: YWRtaW4=
  username: YWRtaW4=
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: hosted-ipv6-worker0
  namespace: clusters-hosted-ipv6
  labels:
    infraenvs.agent-install.openshift.io: hosted-ipv6
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: hosted-ipv6-worker0
spec:
  automatedCleaningMode: disabled
  bmc:
    disableCertificateVerification: true
    address: redfish-virtualmedia://[192.168.125.1]:9000/redfish/v1/Systems/local/hosted-ipv6-worker0
    credentialsName: hosted-ipv6-worker0-bmc-secret
  bootMACAddress: aa:aa:aa:aa:03:11
  online: true
```

**Details**:

- We will have at least 1 secret that holds the BMH credentials, so we will need to create at least 2 objects per worker node.
- `spec.metadata.labels["infraenvs.agent-install.openshift.io"]` serves as the link between the Assisted Installer and the BareMetalHost objects.
- `spec.metadata.annotations["bmac.agent-install.openshift.io/hostname"]` represents the node name it will adopt during deployment.
- `spec.automatedCleaningMode` prevents the node from being erased by the Metal3 operator.
- `spec.bmc.disableCertificateVerification` is set to `true` to bypass certificate validation from the client.
- `spec.bmc.address` denotes the BMC address of the worker node.
- `spec.bmc.credentialsName` points to the Secret where User/Password credentials are stored.
- `spec.bootMACAddress` indicates the interface MACAddress from which the node will boot.
- `spec.online` defines the desired state of the node once the BMH object is created.

To deploy this object, simply follow the same procedure as before:

!!! important

    Please create the virtual machines before you create the BareMetalHost and the destination Nodes.

To deploy the BareMetalHost object, execute the following command:

```bash
oc apply -f 04-bmh.yaml
```

This will be the process:

- Preparing (Trying to reach the nodes):
```
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   registering              true             2s
clusters-hosted   hosted-worker1   registering              true             2s
clusters-hosted   hosted-worker2   registering              true             2s
```

- Provisioning (Nodes Booting up)
```
NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioning              true             16s
clusters-hosted   hosted-worker1   provisioning              true             16s
clusters-hosted   hosted-worker2   provisioning              true             16s
```

- Provisioned (Nodes Booted up successfully)
```
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioned              true             67s
clusters-hosted   hosted-worker1   provisioned              true             67s
clusters-hosted   hosted-worker2   provisioned              true             67s
```

## Agents registration

After the nodes have booted up, you will observe the appearance of agents within the namespace.

```
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign
```

These agents represent the nodes available for installation. To assign them to a HostedCluster, scale up the NodePool.

## Scaling Up the Nodepool

Once we have the BareMetalHosts created, the statuses of these BareMetalHosts will transition from `Registering` (Attempting to reach the Node's BMC) to `Provisioning` (Node Booting Up), and finally to `Provisioned` (Successful node boot-up).

The nodes will boot with the Agent's RHCOS LiveISO and a default pod named "agent." This agent is responsible for receiving instructions from the Assisted Service Operator to install the Openshift payload.

To accomplish this, execute the following command:

```bash
oc -n clusters scale nodepool hosted-ipv6 --replicas 3
```

After the NodePool scaling, you will notice that the agents are assigned to a Hosted Cluster.

```
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0211   hosted    true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0212   hosted    true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0213   hosted    true       auto-assign
```

And the NodePool replicas set

```
NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted   hosted    3                               False         False        4.14.0-0.nightly-2023-08-29-102237                                      Minimum availability requires 3 replicas, current 0 available
```

So now, we need to wait until the nodes join the cluster. The Agents will provide updates on their current stage and status. Initially, they may not post any status, but eventually, they will.

---

## Source: docs/content/labs/IPv6/hostedcluster/hostedcluster.md

In this section, we will focus on all the related objects necessary to achieve a Disconnected Hosted Cluster deployment.

**Premises**:

- HostedCluster Name: `hosted-ipv6`
- HostedCluster Namespace: `clusters`
- Disconnected: `true`
- Network Stack: `ipv6`

## Openshift Objects

### Namespaces

In a typical situation, the operator would be responsible for creating the HCP (HostedControlPlane) namespace. However, in this case, we want to include all the objects before the operator begins reconciliation over the HostedCluster object. This way, when the operator commences the reconciliation process, it will find all the objects in place.

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: clusters-hosted-ipv6
spec: {}
status: {}
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: clusters
spec: {}
status: {}
```

!!! note

      We will **not** create objects one by one but will concatenate all of them in the same file and apply them with just one command.

### ConfigMap and Secrets

These are the ConfigMaps and Secrets that we will include in the HostedCluster deployment.

```yaml
---
apiVersion: v1
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
kind: ConfigMap
metadata:
  name: user-ca-bundle
  namespace: clusters
---
apiVersion: v1
data:
  .dockerconfigjson: xxxxxxxxx
kind: Secret
metadata:
  creationTimestamp: null
  name: hosted-ipv6-pull-secret
  namespace: clusters
---
apiVersion: v1
kind: Secret
metadata:
  name: sshkey-cluster-hosted-ipv6
  namespace: clusters
stringData:
  id_rsa.pub: ssh-rsa xxxxxxxxx
---
apiVersion: v1
data:
  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=
kind: Secret
metadata:
  creationTimestamp: null
  name: hosted-ipv6-etcd-encryption-key
  namespace: clusters
type: Opaque
```

### RBAC Roles

While not mandatory, it allows us to have the Assisted Service Agents located in the same HostedControlPlane namespace as the HostedControlPlane and still be managed by CAPI.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: capi-provider-role
  namespace: clusters-hosted-ipv6
rules:
- apiGroups:
  - agent-install.openshift.io
  resources:
  - agents
  verbs:
  - '*'
```

### Hosted Cluster

This is a sample of the HostedCluster Object

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: hosted-ipv6
  namespace: clusters
  annotations:
    hypershift.openshift.io/control-plane-operator-image: registry.ocp-edge-cluster-0.qe.lab.redhat.com:5005/openshift/release@sha256:31149e3e5f8c5e5b5b100ff2d89975cf5f7a73801b2c06c639bf6648766117f8
spec:
  additionalTrustBundle:
    name: "user-ca-bundle"
  olmCatalogPlacement: guest
  imageContentSources:
  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
    mirrors:
    - registry.hypershiftbm.lab:5000/openshift/release
  - source: quay.io/openshift-release-dev/ocp-release
    mirrors:
    - registry.hypershiftbm.lab:5000/openshift/release-images
  - mirrors:
  ...
  ...
  autoscaling: {}
  controllerAvailabilityPolicy: SingleReplica
  dns:
    baseDomain: hypershiftbm.lab
  etcd:
    managed:
      storage:
        persistentVolume:
          size: 8Gi
        restoreSnapshotURL: null
        type: PersistentVolume
    managementType: Managed
  fips: false
  networking:
    clusterNetwork:
    - cidr: 10.132.0.0/14
    networkType: OVNKubernetes
    serviceNetwork:
    - cidr: 172.31.0.0/16
  platform:
    agent:
      agentNamespace: clusters-hosted-ipv6
    type: Agent
  pullSecret:
    name: hosted-ipv6-pull-secret
  release:
    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
  secretEncryption:
    aescbc:
      activeKey:
        name: hosted-ipv6-etcd-encryption-key
    type: aescbc
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv6.hypershiftbm.lab
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv6.hypershiftbm.lab
      type: NodePort
  - service: OIDC
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv6.hypershiftbm.lab
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv6.hypershiftbm.lab
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: api.hosted-ipv6.hypershiftbm.lab
      type: NodePort
  sshKey:
    name: sshkey-cluster-hosted-ipv6
status:
  controlPlaneEndpoint:
    host: ""
    port: 0
```

!!! note

    The `imageContentSources` section within the `spec` field contains mirror references for user workloads within the HostedCluster.

As you can see, all the objects created before are referenced here. You can also refer to the documentation where all the fields are described.

## Deployment

To deploy these objects, simply concatenate them into the same file and apply them against the management cluster:

```bash
oc apply -f 01-4.14-hosted_cluster-nodeport.yaml
```

This will raise up a functional Hosted Control Plane.

``` yaml
NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-5b57dbd6d5-pxlqc                        1/1     Running   0   	   3m57s
catalog-operator-9694884dd-m7zzv                      2/2     Running   0          93s
cluster-api-f98b9467c-9hfrq                           1/1     Running   0   	   3m57s
cluster-autoscaler-d7f95dd5-d8m5d                     1/1     Running   0          93s
cluster-image-registry-operator-5ff5944b4b-648ht      1/2     Running   0          93s
cluster-network-operator-77b896ddc-wpkq8              1/1     Running   0          94s
cluster-node-tuning-operator-84956cd484-4hfgf         1/1     Running   0          94s
cluster-policy-controller-5fd8595d97-rhbwf            1/1     Running   0          95s
cluster-storage-operator-54dcf584b5-xrnts             1/1     Running   0          93s
cluster-version-operator-9c554b999-l22s7              1/1     Running   0          95s
control-plane-operator-6fdc9c569-t7hr4                1/1     Running   0 	       3m57s
csi-snapshot-controller-785c6dc77c-8ljmr              1/1     Running   0 	       77s
csi-snapshot-controller-operator-7c6674bc5b-d9dtp     1/1     Running   0 	       93s
dns-operator-6874b577f-9tc6b                          1/1     Running   0          94s
etcd-0                                                3/3     Running   0 	       3m39s
hosted-cluster-config-operator-f5cf5c464-4nmbh        1/1     Running   0 	       93s
ignition-server-6b689748fc-zdqzk                      1/1     Running   0          95s
ignition-server-proxy-54d4bb9b9b-6zkg7                1/1     Running   0 	       95s
ingress-operator-6548dc758b-f9gtg                     1/2     Running   0          94s
konnectivity-agent-7767cdc6f5-tw782                   1/1     Running   0 	       95s
kube-apiserver-7b5799b6c8-9f5bp                       4/4     Running   0 	       3m7s
kube-controller-manager-5465bc4dd6-zpdlk              1/1     Running   0 	       44s
kube-scheduler-5dd5f78b94-bbbck                       1/1     Running   0 	       2m36s
machine-approver-846c69f56-jxvfr                      1/1     Running   0          92s
oauth-openshift-79c7bf44bf-j975g                      2/2     Running   0 	       62s
olm-operator-767f9584c-4lcl2                          2/2     Running   0 	       93s
openshift-apiserver-5d469778c6-pl8tj                  3/3     Running   0 	       2m36s
openshift-controller-manager-6475fdff58-hl4f7         1/1     Running   0 	       95s
openshift-oauth-apiserver-dbbc5cc5f-98574             2/2     Running   0          95s
openshift-route-controller-manager-5f6997b48f-s9vdc   1/1     Running   0          95s
packageserver-67c87d4d4f-kl7qh                        2/2     Running   0          93s
```

And this is how the HostedCluster looks like:

```bash
NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
clusters    hosted-ipv6            hosted-admin-kubeconfig   Partial    True 	      False         The hosted control plane is available
```

After some time, we will have almost all the pieces in place, and the Control Plane operator awaits for the worker nodes to join the cluster. To achieve this, we need to create some more objects. Let's discuss the `InfraEnv` and the `BareMetalHost` in the following sections.


---

## Source: docs/content/labs/IPv6/hostedcluster/index.md

 Hosted Cluster, as mentioned in the documentation here, is essentially an OCP API endpoint managed by Hypershift. In this context, we will also include the term HostedControlPlane to enhance readability and comprehension. This terminology is further explained in the same link.

The Hosted Cluster comprises two main components:
- The Control Plane, which runs as pods in the management cluster.
- The Data Plane, consisting of external nodes managed by the end user.

With this foundational understanding, we can commence our Hosted Cluster deployment. Typically, an ACM/MCE user would utilize the web UI to create a cluster. However, in this scenario, we will leverage manifests, providing us with greater flexibility to modify the artifacts.


---

## Source: docs/content/labs/IPv6/hostedcluster/infraenv.md

The `InfraEnv` is an Assisted Service object that includes essential details such as the `pullSecretRef` and the `sshAuthorizedKey`. These details are used to create the RHCOS Boot Image customized specifically for the cluster. Below is the structure of this object:

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: hosted-ipv6
  namespace: clusters-hosted-ipv6
spec:
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDk7ICaUE+/k4zTpxLk4+xFdHi4ZuDi5qjeF52afsNkw0w/glILHhwpL5gnp5WkRuL8GwJuZ1VqLC9EKrdmegn4MrmUlq7WTsP0VFOZFBfq2XRUxo1wrRdor2z0Bbh93ytR+ZsDbbLlGngXaMa0Vbt+z74FqlcajbHTZ6zBmTpBVq5RHtDPgKITdpE1fongp7+ZXQNBlkaavaqv8bnyrP4BWahLP4iO9/xJF9lQYboYwEEDzmnKLMW1VtCE6nJzEgWCufACTbxpNS7GvKtoHT/OVzw8ArEXhZXQUS1UY8zKsX2iXwmyhw5Sj6YboA8WICs4z+TrFP89LmxXY0j6536TQFyRz1iB4WWvCbH5n6W+ABV2e8ssJB1AmEy8QYNwpJQJNpSxzoKBjI73XxvPYYC/IjPFMySwZqrSZCkJYqQ023ySkaQxWZT7in4KeMu7eS2tC+Kn4deJ7KwwUycx8n6RHMeD8Qg9flTHCv3gmab8JKZJqN3hW1D378JuvmIX4V0=
```

**Details**:

- `pullSecretRef` refers to the ConfigMap reference (in the same Namespace as the InfraEnv) where the PullSecret will be utilized.
- `sshAuthorizedKey` represents the SSH Public key that will be injected into the Boot Image. This SSH key will, by default, allow access to the worker nodes as the `core` user.

To deploy this object, follow the same procedure as before:

```bash
oc apply -f 03-infraenv.yaml
```

And this is how looks like

```bash
NAMESPACE              NAME     ISO CREATED AT
clusters-hosted-ipv6   hosted   2023-09-11T15:14:10Z
```

---

## Source: docs/content/labs/IPv6/hostedcluster/nodepool.md

A `NodePool` is a scalable set of worker nodes associated with a HostedCluster. NodePool machine architectures remain consistent within a specific pool and are independent of the underlying machine architecture of the control plane.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

!!! warning

    Before a day-1 patch, the release image set in the HostedCluster **should use the digest rather than the tag**. (e.g `quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0`)

This is how one looks like:

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  creationTimestamp: null
  name: hosted-ipv6
  namespace: clusters
spec:
  arch: amd64
  clusterName: hosted-ipv6
  management:
    autoRepair: false
    upgradeType: InPlace
  nodeDrainTimeout: 0s
  nodeVolumeDetachTimeout: 0s
  platform:
    type: Agent
  release:
    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
  replicas: 0
status:
  replicas: 0
```

**Details**:

- All the nodes included in this NodePool will be based on the Openshift version `4.14.0-0.nightly-2023-08-29-102237`.
- The Upgrade type is set to `InPlace`, indicating that the same bare-metal node will be reused during an upgrade.
- Autorepair is set to `false` because the node will not be recreated when it disappears.
- Replicas are set to `0` because we intend to scale them when needed.

You can find more information about NodePool in the NodePool documentation.

To deploy this object, simply follow the same procedure as before:

```bash
oc apply -f 02-nodepool.yaml
```

And this is how the NodePool looks like (at this point):

```bash
NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted-ipv6   hosted    0                               False         False        4.14.0-0.nightly-2023-08-29-102237
```

!!! important

    Keep the nodepool replicas to 0 until all the steps are in place.

---

## Source: docs/content/labs/IPv6/hostedcluster/worker-nodes.md

Regarding the worker nodes, if you are working on real bare metal, this step is crucial to ensure that the details set in the `BareMetalHost` are correctly configured. If not, you will need to debug why it's not functioning as expected.

However, if you are working with virtual machines, you can follow these steps to create empty ones that will be consumed by the Metal3 operator. To achieve this, we will utilize Kcli.

## Creating Virtual Machines

If this is not your first attempt, you must first delete the previous setup. To do so, please refer to the Deleting Virtual Machines section.

Now, you can execute the following commands for VM creation:

```bash
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv6 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"ipv6\", \"mac\": \"aa:aa:aa:aa:02:11\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0211 -P name=hosted-ipv6-worker0
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv6 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"ipv6\", \"mac\": \"aa:aa:aa:aa:02:12\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0212 -P name=hosted-ipv6-worker1
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv6 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"ipv6\", \"mac\": \"aa:aa:aa:aa:02:13\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0213 -P name=hosted-ipv6-worker2

sleep 2
systemctl restart ksushy
```

Let's dissect the creation command:

- `start=False`: The VM will not boot automatically upon creation.
- `uefi_legacy=true`: We will use UEFI legacy boot to ensure compatibility with older UEFI implementations.
- `plan=hosted-dual`: The plan name, which identifies a group of machines as a cluster.
- `memory=8192` and `numcpus=16`: These parameters specify the resources for the VM, including RAM and CPU.
- `disks=[200,200]`: We are creating 2 disks (thin provisioned) in the virtual machine.
- `nets=[{"name": "dual", "mac": "aa:aa:aa:aa:02:13"}]`: Network details, including the network name it will be connected to and the MAC address for the primary interface.
- The `ksushy` restart is performed to make our `ksushy` (VM's BMC) aware of the new VMs added.

This is what the command looks like:

```bash
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|    hosted-worker0   |  down  |                   |                                                    | hosted-ipv6 |  kvirt  |
|    hosted-worker1   |  down  |                   |                                                    | hosted-ipv6 |  kvirt  |
|    hosted-worker2   |  down  |                   |                                                    | hosted-ipv6 |  kvirt  |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
```

## Deleting Virtual Machines

To delete the VMs, you simply need to delete the plan, which, in our case, is:

```bash
kcli delete plan hosted-ipv6
```

```bash
$ kcli delete plan hosted-ipv6
Are you sure? [y/N]: y
hosted-worker0 deleted on local!
hosted-worker1 deleted on local!
hosted-worker2 deleted on local!
Plan hosted-ipv6 deleted!
```

---

## Source: docs/content/labs/IPv6/hypervisor/hypervisor-system-config.md

This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the `firewalld` service and configure SELinux policies to maintain a secure environment.

With these instructions, we allow various types of connections through the different virtual networks within the environment:

```bash
## SELinux
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0

## Firewalld
systemctl disable --now firewalld

## Libvirtd
systemctl restart libvirtd
systemctl enable --now libvirtd
```

---

## Source: docs/content/labs/IPv6/hypervisor/index.md

---
title: Hypervisor
---

This section is primarily focused on Virtual Machines. If you are working with real bare metal, you can skip this section and verify whether your laboratory has the necessary components to run the deployment properly. If you find that you've missed something, you can configure it separately to address any gaps.

---

## Source: docs/content/labs/IPv6/hypervisor/network-manager-dispatcher.md

This script modifies the system DNS resolver to prioritize pointing to the `dnsmasq` service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.

To enable this, you need to create a script named `forcedns` in `/etc/NetworkManager/dispatcher.d/` with the following content:

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

=== "IPv4"

    ```bash
    #!/bin/bash

    export IP="192.168.125.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "IPv6"

    ```bash
    #!/bin/bash

    export IP="2620:52:0:1306::1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "Dual stack"

    ```bash
    #!/bin/bash

    export IP="192.168.126.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

The `IP` variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.

After creating the file, you need to add execution permissions using the command:

```bash
chmod 755 /etc/NetworkManager/dispatcher.d/forcedns
```

Then, execute it once. The output should indicate `ok`.



---

## Source: docs/content/labs/IPv6/hypervisor/packaging.md

---
title: Hypervisor Packaging
---

## System

These are the main packages that are needed to deploy a virtualized Openshift Management cluster.

```bash
sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y
```

Additionally, you need to enable and start the Podman service using the following command:

```bash
systemctl enable --now podman
```

## Kcli

We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:

```bash
sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm
sudo usermod -aG qemu,libvirt $(id -un)
sudo newgrp libvirt
sudo systemctl enable --now libvirtd
sudo dnf -y copr enable karmab/kcli
sudo dnf -y install kcli
sudo kcli create pool -p /var/lib/libvirt/images default
kcli create host kvm -H 127.0.0.1 local
sudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images
kcli create network  -c 192.168.122.0/24 default
```

For more info about Kcli please visit the official documentation.

---

## Source: docs/content/labs/IPv6/hypervisor/redfish-for-vms.md

In a bare metal environment, the preferred approach is to utilize the actual BMC (Baseboard Management Controller) of the nodes used for the management cluster, which can be managed by Metal3 for discovery and provisioning. However, in a virtual environment, this approach is not feasible. As a workaround, we will use `ksushy`, which is an implementation of `sushy-tools`, allowing us to simulate BMCs for the virtual machines.

To configure `ksushy` we need to execute these commands:

```bash
sudo dnf install python3-pyOpenSSL.noarch python3-cherrypy -y
kcli create sushy-service --ssl --ipv6 --port 9000
sudo systemctl daemon-reload
systemctl enable --now ksushy
```

To test if this service is functioning correctly, you can check the service status with `systemctl status ksushy`. Additionally, you can execute a `curl` command against the exposed interface:

```
curl -Lk https://[2620:52:0:1305::1]:9000/redfish/v1
```


---

## Source: docs/content/labs/IPv6/hypervisor/requisites.md

---
title: Hypervisor Prerequisites
---

- **CPU**: The number of CPUs provided determines how many HostedClusters can run concurrently.
  - **Recommended**: 16 CPUs per Node for 3 nodes.
  - **Minimal Dev**: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.

- **Memory**: The amount of RAM impacts how many HostedClusters can be hosted.
  - **Recommended**: 48 GB of RAM per Node.
  - **Minimal Dev**: For minimal development, 18 GB of RAM per Node may suffice.

- **Storage**: Using SSD storage for MCE is crucial.
  - **Management Cluster**: 250 GB.
  - **Registry**: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.
  - **Webserver**: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.

- **Production**: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:
  - **Registry**: 2 TB.
  - **Management Cluster**: 500 GB.
  - **WebServer**: 2 TB.


---

## Source: docs/content/labs/IPv6/index.md

---
title: IPv6
---

This network configuration is (for now) by definition disconnected. The main reason is because the remote registries cannot work with IPv6 so we will include this part in the documentation.

All the scripts provided will hold part or the whole automation to reproduce the environment. To do that this is the repository holding all the scripts for IPv6 environments.

This documentation is prepared to be followed in a concrete order:

- Hypervisor
- DNS
- Registry
- Management Cluster
- Webserver
- Mirroring
- Multicluster Engine
- TLS Certificates
- HostedCluster
- Watching Deployment progress

---

## Source: docs/content/labs/IPv6/mce/agentserviceconfig.md

The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named `AgentServiceConfig` to configure it.

## Agent Service Config Objects

You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.

In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.

### Custom Registries Configuration

This ConfigMap contains the disconnected details necessary to customize the deployment.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-registries
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/openshift4"
    mirror-by-digest-only = true

    [[registry.mirror]]
      location = "registry.hypershiftbm.lab:5000/openshift4"

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/rhacm2"
    mirror-by-digest-only = true
    ...
    ...
```

This object includes two fields:

1. **Custom CAs**: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.
2. **Registries**: The `Registries.conf` field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.

### Assisted Service Customization

The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false"
```

You can find documentation on how to customize the operator here.

### Assisted Service Config

The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config
  name: agent
  namespace: multicluster-engine
spec:
  mirrorRegistryRef:
    name: custom-registries
  databaseStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  filesystemStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 20Gi
  osImages:
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso
    version: 414.92.202308281054-0
```

If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.

In this section, we will emphasize the important aspects:

- The `metadata.annotations["unsupported.agent-install.openshift.io/assisted-service-configmap"]` Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.
- The `spec.mirrorRegistryRef.name` points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.
- The `spec.osImages` field contains different versions available for deployment by this operator. These fields are mandatory.

Let's fill in this section for the 4.14 dev preview ec4 version (sample):

- Release URL
- RHCOS Info

Assuming you've already downloaded the RootFS and LiveIso files:

```yaml
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso
    version: 414.92.202307250657-0
```

## Deployment

To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.

```bash
oc apply -f agentServiceConfig.yaml
```

This will trigger 2 pods

```bash
assisted-image-service-0                               1/1     Running   2             11d
assisted-service-668b49548-9m7xw                       2/2     Running   5             11d
```

!!! note

      The `Assisted Image Service` pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.

      The `Assisted Service` refers to the operator.


---

## Source: docs/content/labs/IPv6/mce/index.md

The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.

## Credentials and Authorization

To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.

ACM/MCE Deployment{ .md-button }
Agent Service Config{ .md-button }


---

## Source: docs/content/labs/IPv6/mce/multicluster-engine.md

The deployment of each component will depend on your needs, follow the next links accordingly:

- ACM Deployment
- ACM Downstream builds
- MCE Deployment
- MCE Upgrade
- ACM/MCE Cluster deletion
- MulticlusterEngine deletion
- Last resource: **nuke.sh**

---

## Source: docs/content/labs/IPv6/mgmt-cluster/compact-ipv6.md

In this section, we will discuss how to deploy the Openshift management cluster. To do that, we need to have the following files in place:

1. **Pull Secret**
2. **Kcli Plan**

Ensure that these files are properly set up for the deployment process.

### Pull Secret

The Pull Secret should be located in the same folder as the kcli plan and should be named `openshift_pull.json`.

### Kcli plan

The Kcli plan contains the Openshift definition. This is how looks like:

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

- `mgmt-compact-hub-ipv6.yaml`

```yaml
plan: hub-ipv6
force: true
version: nightly
tag: "4.14.0-0.nightly-2023-08-29-102237"
cluster: "hub-ipv6"
ipv6: true
domain: hypershiftbm.lab
api_ip: 2620:52:0:1305::2
ingress_ip: 2620:52:0:1305::3
disconnected_url: registry.hypershiftbm.lab:5000
disconnected_update: true
disconnected_user: dummy
disconnected_password: dummy
disconnected_operators_version: v4.13
disconnected_operators:
- name: metallb-operator
- name: lvms-operator
  channels:
  - name: stable-4.13
disconnected_extra_images:
- quay.io/mavazque/trbsht:latest
- quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
- registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
dualstack: false
disk_size: 200
extra_disks: [200]
memory: 48000
numcpus: 16
ctlplanes: 3
workers: 0
manifests: extra-manifests
metal3: true
network: ipv6
users_dev: developer
users_devpassword: developer
users_admin: admin
users_adminpassword: admin
metallb_pool: ipv6-virtual-network
metallb_ranges:
- 2620:52:0:1305::150-2620:52:0:1305::190
metallb_autoassign: true
apps:
- users
- lvms-operator
- metallb-operator
vmrules:
- hub-bootstrap:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:03:10
- hub-ctlplane-0:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:03:01
- hub-ctlplane-1:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:03:02
- hub-ctlplane-2:
    nets:
    - name: ipv6
      mac: aa:aa:aa:aa:03:03
```

!!! note

    To understand the meaning of each of these parameters, you can refer to the official documentation here.

### Deployment

To initiate the provisioning procedure, execute the following:

```bash
kcli create cluster openshift --pf mgmt-compact-hub-ipv6.yaml
```

---

## Source: docs/content/labs/IPv6/mgmt-cluster/index.md

## Openshift Management Cluster

This section contains the necessary artifacts to set up an Openshift management cluster based on virtual machines using kcli as the primary tool. Another option is to use dev-scripts, which uses a different approach.

Networking{ .md-button }
Openshift Compact Dual{ .md-button }

---

## Source: docs/content/labs/IPv6/mgmt-cluster/network.md

Firstly, we need to ensure that we have the right networks prepared for use in the Hypervisor. These networks will be used to host both the Management and Hosted clusters.

To configure these networks, we will use the following `kcli` command:

```
kcli create network -c 2620:52:0:1305::0/64 -P dhcp=false -P dns=false --domain hypershiftbm.lab --nodhcp ipv6
```

Where:

- `-c` remarks the CIDR used for that network
- `-P dhcp=false` configures the network to disable the DHCP, this will be done by the dnsmasq we've configured before.
- `-P dns=false` configures the network to disable the DNS, this will be done by the dnsmasq we've configured before.
- `--domain` sets the domain to search into.
- `ipv6` is the name of the network that will be created.

This is what the network will look like once created:

```
[root@hypershiftbm ~]# kcli list network
Listing Networks...
+---------+--------+---------------------+-------+------------------+------+
| Network |  Type  |         Cidr        |  Dhcp |      Domain      | Mode |
+---------+--------+---------------------+-------+------------------+------+
| default | routed |   192.168.122.0/24  |  True |     default      | nat  |
| ipv6    | routed |   192.168.125.0/24  | False | hypershiftbm.lab | nat  |
| ipv6    | routed | 2620:52:0:1305::/64 | False | hypershiftbm.lab | nat  |
+---------+--------+---------------------+-------+------------------+------+
```

```
[root@hypershiftbm ~]# kcli info network ipv6
Providing information about network ipv6...
cidr: 2620:52:0:1305::/64
dhcp: false
domain: hypershiftbm.lab
mode: nat
plan: kvirt
type: routed
```



---

## Source: docs/content/labs/IPv6/mirror/ICSP-IDMS.md

Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:

1. ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).
2. Catalog Sources.

Using the `oc-mirror` tool, the output artifacts will be located in a new folder called `oc-mirror-workspace/results-XXXXXX/`.

ICSP/IDMS will trigger a "special" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.

Once all nodes are schedulable and marked as `READY`, you will need to apply the new catalog sources generated.

The catalog sources will trigger some actions in the `openshift-marketplace operator`, such as downloading the catalog image and processing it to retrieve all the `PackageManifests` included in that image. You can check the new sources by executing `oc get packagemanifest` using the new CatalogSource as a source.

## Applying the Artifacts

First, we need to create the ICSP/IDMS artifacts:


```bash
oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml
```

Now, wait for the nodes to become ready again and execute the following command:

```bash
oc apply -f catalogSource-XXXXXXXX-index.yaml
```

---

## Source: docs/content/labs/IPv6/mirror/index.md

!!! important

    This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.

Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.

To learn how to perform image mirroring, follow the next sections:

Image Mirroring Process{ .md-button }
ICSP and IDMS{ .md-button }


---

## Source: docs/content/labs/IPv6/mirror/mirroring.md

The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.

For this purpose, we will use the `oc-mirror` tool, a binary that utilizes an object called `ImageSetConfiguration`.

In this file, you can specify:

- The OpenShift versions to mirror (they should be located in quay.io).
- The additional operators to mirror, selecting packages individually.
- The extra images you want to add to the repository.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

Here is an example of the `ImageSetConfiguration` that we will use for our mirroring:

```yaml
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest
mirror:
  platform:
    channels:
    - name: candidate-4.14
      minVersion: 4.14.0-ec.1
      maxVersion: 4.14.0-ec.3
      type: ocp
    graph: true
  additionalImages:
  - name: quay.io/karmab/origin-keepalived-ipfailover:latest
  - name: quay.io/karmab/kubectl:latest
  - name: quay.io/karmab/haproxy:latest
  - name: quay.io/karmab/mdns-publisher:latest
  - name: quay.io/karmab/origin-coredns:latest
  - name: quay.io/karmab/curl:latest
  - name: quay.io/karmab/kcli:latest
  - name: quay.io/mavazque/trbsht:latest
  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
    - name: lvms-operator
    - name: local-storage-operator
    - name: odf-csi-addons-operator
    - name: odf-operator
    - name: mcg-operator
    - name: ocs-operator
    - name: metallb-operator
```

Make sure you have your `${HOME}/.docker/config.json` file updated with the registries you are trying to mirror from and your private registry to push the images to.

After that, we can begin the mirroring process:

```bash
oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}
```

Once the mirror finishes, you will have a new folder called `oc-mirror-workspace/results-XXXXXX/` which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.

## Mirroring Nightly and CI releases

The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the `oc adm release mirror` to mirror those versions.

To mirror the nightly versions we need for this deployment, you need to execute this:

```bash
REGISTRY=registry.$(hostname --long):5000

oc adm release mirror \
  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \
  --to=${REGISTRY}/openshift/release \
  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
```

For more detailed and updated information, you can visit the official Documentation or GitHub repository

## Mirror MCE internal releases

In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.

- Red Hat Official Documentation
- Red Hat Internal deployment Brew Registry deployment


---

## Source: docs/content/labs/IPv6/registry.md

!!! important

    This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.

In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.

1. As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:

- `registry.sh`
```bash
#!/usr/bin/env bash

set -euo pipefail

PRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)
export PATH=/root/bin:$PATH
export PULL_SECRET="/root/baremetal/hub/openshift_pull.json"

if [[ ! -f $PULL_SECRET ]];then
  echo "Pull Secret not found, exiting..."
  exit 1
fi

dnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel
export IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)
REGISTRY_NAME=registry.$(hostname --long)
REGISTRY_USER=dummy
REGISTRY_PASSWORD=dummy
KEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)
echo "{\"auths\": {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\", \"email\": \"sample-email@domain.ltd\"}}}" > /root/disconnected_pull.json
mv ${PULL_SECRET} /root/openshift_pull.json.old
jq ".auths += {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\",\"email\": \"sample-email@domain.ltd\"}}" < /root/openshift_pull.json.old > $PULL_SECRET
mkdir -p /opt/registry/{auth,certs,data,conf}
cat <<EOF > /opt/registry/conf/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
  delete:
    enabled: true
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
compatibility:
  schema1:
    enabled: true
EOF
openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj "/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME" -addext "subjectAltName=DNS:$REGISTRY_NAME"
cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract
htpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD
podman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e "REGISTRY_AUTH=htpasswd" -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry" -e "REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest
[ "$?" == "0" ] || !!
systemctl enable --now registry
```

2. Kindly modify the location of the `PULL_SECRET` to point to the correct location.
3. Please adjust the permissions by adding the execution flag as follows: `chmod u+x ${HOME}/registry.sh`
4. You can execute the script without any parameters using the following command `${HOME}/registry.sh`

After executing the script `${HOME}/registry.sh`, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.

It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.

The root folder for the registry is situated at /opt/registry, and it's structured as follows:

- `certs` holds the TLS certificates.
- `auth` keeps the credentials.
- `data` contains the registry images.
- `conf` will hold the registry configuration.

---

## Source: docs/content/labs/IPv6/tls-certificates.md

!!! important

    This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.

In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.

It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):

- `/etc/pki/ca-trust/extracted/pem/`
- `/etc/pki/ca-trust/source/anchors/`
- `/etc/pki/tls/certs/`

## Adding a CA to the Management Cluster

There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.

1. Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name `registry-config` The content should resemble the following:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: registry-config
  namespace: openshift-config
data:
  registry.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
```

!!! note

        The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the ":" character is being replaced by ".."; therefore, it is imperative to ensure this correction.

2. Now we need to patch the clusterwide object `image.config.openshift.io` including this:

```yaml
spec:
  additionalTrustedCA:
    name: registry-config
```

This modification will result in two significant consequences:

- Granting masters the capability to retrieve images from the private registry.
- Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.

!!! note

        The modification required several minutes to be successfully executed.

## Alternative: Adding a CA to the Management Cluster

We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.

It's described here. This method involves utilizing the `image-registry-operator`, which deploys the CAs to the OCP nodes.

Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.

However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:

- Check if the `openshift-config` namespace in the Management cluster contains a ConfigMap named `user-ca-bundle`.
- If the ConfigMap doesn't exist, execute the following command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt

oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}
```

- Otherwise, if that ConfigMap exists, execute this other command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt
export TMP_FILE=$(mktemp)

oc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\.crt}' > ${TMP_FILE}
echo >> ${TMP_FILE}
echo \#registry.$(hostname --long) >> ${TMP_FILE}
cat ${REGISTRY_CERT_PATH} >> ${TMP_FILE}
oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -
```

You have a functional script located in the `assets/<NetworkStack>/09-tls-certificates/01-config.sh`, this is the sample for IPv6.


---

## Source: docs/content/labs/IPv6/watching/index.md

This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:

- Control Plane
- Data Plane

So please follow these links accordingly.

Control Plane perspective{ .md-button }
Data Plane perspective{ .md-button }


---

## Source: docs/content/labs/IPv6/watching/watching-cp.md

Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:

```bash
export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig

watch "oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;"
```

This command will give you info about:

- What is the Hypershift Operator status
- The HostedControlPlane pod status
- The BareMetalHosts
- The Agents
- The Infraenv
- The HostedCluster and NodePool

This is how it looks:



---

## Source: docs/content/labs/IPv6/watching/watching-dp.md

If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands

```bash
oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d > /root/hc_admin_kubeconfig.yaml
export KUBECONFIG=/root/hc_admin_kubeconfig.yaml

watch "oc get clusterversion,nodes,co"
```

This command will give you info about:

- Check the clusterversion
- Check if the Nodes has joined the cluster
- Check the ClusterOperators

This is how looks like:



---

## Source: docs/content/labs/IPv6/webserver.md

!!! important

    This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.

This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.

The script refers to this repository folder and it's the same for all three different network stacks.

To do this, you can use this script:

```bash
#!/bin/bash

WEBSRV_FOLDER=/opt/srv
ROOTFS_IMG_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')"
LIVE_ISO_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')"

mkdir -p ${WEBSRV_FOLDER}/images
curl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}
curl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}
chmod -R 755 ${WEBSRV_FOLDER}/*

## Run Webserver
podman ps --noheading | grep -q websrv-ai
if [[ $? == 0 ]];then
    echo "Launching Registry pod..."
    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080
fi
```

The script will create a folder under `/opt/srv`. This folder will contain the `images` for RHCOS provision in the worker nodes. To be more concrete, we need the `RootFS` and `LiveISO` artifacts found on the Openshift CI Release page.

After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.


---

## Source: docs/content/labs/common/hypervisor/hypervisor-system-config.md

This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the `firewalld` service and configure SELinux policies to maintain a secure environment.

With these instructions, we allow various types of connections through the different virtual networks within the environment:

```bash
## SELinux
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0

## Firewalld
systemctl disable --now firewalld

## Libvirtd
systemctl restart libvirtd
systemctl enable --now libvirtd
```

---

## Source: docs/content/labs/common/hypervisor/network-manager-dispatcher.md

This script modifies the system DNS resolver to prioritize pointing to the `dnsmasq` service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.

To enable this, you need to create a script named `forcedns` in `/etc/NetworkManager/dispatcher.d/` with the following content:

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

=== "IPv4"

    ```bash
    #!/bin/bash

    export IP="192.168.125.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "IPv6"

    ```bash
    #!/bin/bash

    export IP="2620:52:0:1306::1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

=== "Dual stack"

    ```bash
    #!/bin/bash

    export IP="192.168.126.1"
    export BASE_RESOLV_CONF="/run/NetworkManager/resolv.conf"

    if ! [[ `grep -q "$IP" /etc/resolv.conf` ]]; then
    export TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)
    cp $BASE_RESOLV_CONF $TMP_FILE
    chmod --reference=$BASE_RESOLV_CONF $TMP_FILE
    sed -i -e "s/hypershiftbm.lab//" -e "s/search /& hypershiftbm.lab /" -e "0,/nameserver/s/nameserver/& $IP\n&/" $TMP_FILE
    mv $TMP_FILE /etc/resolv.conf
    fi
    echo "ok"
    ```

The `IP` variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.

After creating the file, you need to add execution permissions using the command:

```bash
chmod 755 /etc/NetworkManager/dispatcher.d/forcedns
```

Then, execute it once. The output should indicate `ok`.



---

## Source: docs/content/labs/common/hypervisor/packaging.md

---
title: Hypervisor Packaging
---

## System

These are the main packages that are needed to deploy a virtualized Openshift Management cluster.

```bash
sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y
```

Additionally, you need to enable and start the Podman service using the following command:

```bash
systemctl enable --now podman
```

## Kcli

We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:

```bash
sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm
sudo usermod -aG qemu,libvirt $(id -un)
sudo newgrp libvirt
sudo systemctl enable --now libvirtd
sudo dnf -y copr enable karmab/kcli
sudo dnf -y install kcli
sudo kcli create pool -p /var/lib/libvirt/images default
kcli create host kvm -H 127.0.0.1 local
sudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images
kcli create network  -c 192.168.122.0/24 default
```

For more info about Kcli please visit the official documentation.

---

## Source: docs/content/labs/common/hypervisor/requisites.md

---
title: Hypervisor Prerequisites
---

- **CPU**: The number of CPUs provided determines how many HostedClusters can run concurrently.
  - **Recommended**: 16 CPUs per Node for 3 nodes.
  - **Minimal Dev**: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.

- **Memory**: The amount of RAM impacts how many HostedClusters can be hosted.
  - **Recommended**: 48 GB of RAM per Node.
  - **Minimal Dev**: For minimal development, 18 GB of RAM per Node may suffice.

- **Storage**: Using SSD storage for MCE is crucial.
  - **Management Cluster**: 250 GB.
  - **Registry**: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.
  - **Webserver**: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.

- **Production**: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:
  - **Registry**: 2 TB.
  - **Management Cluster**: 500 GB.
  - **WebServer**: 2 TB.


---

## Source: docs/content/labs/common/mce/agentserviceconfig.md

The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named `AgentServiceConfig` to configure it.

## Agent Service Config Objects

You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.

In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.

### Custom Registries Configuration

This ConfigMap contains the disconnected details necessary to customize the deployment.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-registries
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/openshift4"
    mirror-by-digest-only = true

    [[registry.mirror]]
      location = "registry.hypershiftbm.lab:5000/openshift4"

    [[registry]]
    prefix = ""
    location = "registry.redhat.io/rhacm2"
    mirror-by-digest-only = true
    ...
    ...
```

This object includes two fields:

1. **Custom CAs**: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.
2. **Registries**: The `Registries.conf` field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.

### Assisted Service Customization

The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.


```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false"
```

You can find documentation on how to customize the operator here.

### Assisted Service Config

The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.

```yaml
---
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config
  name: agent
  namespace: multicluster-engine
spec:
  mirrorRegistryRef:
    name: custom-registries
  databaseStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  filesystemStorage:
    storageClassName: lvms-vg1
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 20Gi
  osImages:
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso
    version: 414.92.202308281054-0
```

If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.

In this section, we will emphasize the important aspects:

- The `metadata.annotations["unsupported.agent-install.openshift.io/assisted-service-configmap"]` Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.
- The `spec.mirrorRegistryRef.name` points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.
- The `spec.osImages` field contains different versions available for deployment by this operator. These fields are mandatory.

Let's fill in this section for the 4.14 dev preview ec4 version (sample):

- Release URL
- RHCOS Info

Assuming you've already downloaded the RootFS and LiveIso files:

```yaml
  - cpuArchitecture: x86_64
    openshiftVersion: "4.14"
    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img
    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso
    version: 414.92.202307250657-0
```

## Deployment

To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.

```bash
oc apply -f agentServiceConfig.yaml
```

This will trigger 2 pods

```bash
assisted-image-service-0                               1/1     Running   2             11d
assisted-service-668b49548-9m7xw                       2/2     Running   5             11d
```

!!! note

      The `Assisted Image Service` pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.

      The `Assisted Service` refers to the operator.


---

## Source: docs/content/labs/common/mce/index.md

The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.

## Credentials and Authorization

To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.

ACM/MCE Deployment{ .md-button }
Agent Service Config{ .md-button }


---

## Source: docs/content/labs/common/mce/multicluster-engine.md

The deployment of each component will depend on your needs, follow the next links accordingly:

- ACM Deployment
- ACM Downstream builds
- MCE Deployment
- MCE Upgrade
- ACM/MCE Cluster deletion
- MulticlusterEngine deletion
- Last resource: **nuke.sh**

---

## Source: docs/content/labs/common/mirror/ICSP-IDMS.md

Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:

1. ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).
2. Catalog Sources.

Using the `oc-mirror` tool, the output artifacts will be located in a new folder called `oc-mirror-workspace/results-XXXXXX/`.

ICSP/IDMS will trigger a "special" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.

Once all nodes are schedulable and marked as `READY`, you will need to apply the new catalog sources generated.

The catalog sources will trigger some actions in the `openshift-marketplace operator`, such as downloading the catalog image and processing it to retrieve all the `PackageManifests` included in that image. You can check the new sources by executing `oc get packagemanifest` using the new CatalogSource as a source.

## Applying the Artifacts

First, we need to create the ICSP/IDMS artifacts:


```bash
oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml
```

Now, wait for the nodes to become ready again and execute the following command:

```bash
oc apply -f catalogSource-XXXXXXXX-index.yaml
```

---

## Source: docs/content/labs/common/mirror/index.md

!!! important

    This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.

Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.

To learn how to perform image mirroring, follow the next sections:

Image Mirroring Process{ .md-button }
ICSP and IDMS{ .md-button }


---

## Source: docs/content/labs/common/mirror/mirroring.md

The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.

For this purpose, we will use the `oc-mirror` tool, a binary that utilizes an object called `ImageSetConfiguration`.

In this file, you can specify:

- The OpenShift versions to mirror (they should be located in quay.io).
- The additional operators to mirror, selecting packages individually.
- The extra images you want to add to the repository.

!!! note

    Please ensure you modify the appropriate fields to align with your laboratory environment.

Here is an example of the `ImageSetConfiguration` that we will use for our mirroring:

```yaml
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest
mirror:
  platform:
    channels:
    - name: candidate-4.14
      minVersion: 4.14.0-ec.1
      maxVersion: 4.14.0-ec.3
      type: ocp
    graph: true
  additionalImages:
  - name: quay.io/karmab/origin-keepalived-ipfailover:latest
  - name: quay.io/karmab/kubectl:latest
  - name: quay.io/karmab/haproxy:latest
  - name: quay.io/karmab/mdns-publisher:latest
  - name: quay.io/karmab/origin-coredns:latest
  - name: quay.io/karmab/curl:latest
  - name: quay.io/karmab/kcli:latest
  - name: quay.io/mavazque/trbsht:latest
  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3
  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
    - name: lvms-operator
    - name: local-storage-operator
    - name: odf-csi-addons-operator
    - name: odf-operator
    - name: mcg-operator
    - name: ocs-operator
    - name: metallb-operator
```

Make sure you have your `${HOME}/.docker/config.json` file updated with the registries you are trying to mirror from and your private registry to push the images to.

After that, we can begin the mirroring process:

```bash
oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}
```

Once the mirror finishes, you will have a new folder called `oc-mirror-workspace/results-XXXXXX/` which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.

## Mirroring Nightly and CI releases

The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the `oc adm release mirror` to mirror those versions.

To mirror the nightly versions we need for this deployment, you need to execute this:

```bash
REGISTRY=registry.$(hostname --long):5000

oc adm release mirror \
  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \
  --to=${REGISTRY}/openshift/release \
  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237
```

For more detailed and updated information, you can visit the official Documentation or GitHub repository

## Mirror MCE internal releases

In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.

- Red Hat Official Documentation
- Red Hat Internal deployment Brew Registry deployment


---

## Source: docs/content/labs/common/registry.md

!!! important

    This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.

In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.

1. As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:

- `registry.sh`
```bash
#!/usr/bin/env bash

set -euo pipefail

PRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)
export PATH=/root/bin:$PATH
export PULL_SECRET="/root/baremetal/hub/openshift_pull.json"

if [[ ! -f $PULL_SECRET ]];then
  echo "Pull Secret not found, exiting..."
  exit 1
fi

dnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel
export IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)
REGISTRY_NAME=registry.$(hostname --long)
REGISTRY_USER=dummy
REGISTRY_PASSWORD=dummy
KEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)
echo "{\"auths\": {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\", \"email\": \"sample-email@domain.ltd\"}}}" > /root/disconnected_pull.json
mv ${PULL_SECRET} /root/openshift_pull.json.old
jq ".auths += {\"$REGISTRY_NAME:5000\": {\"auth\": \"$KEY\",\"email\": \"sample-email@domain.ltd\"}}" < /root/openshift_pull.json.old > $PULL_SECRET
mkdir -p /opt/registry/{auth,certs,data,conf}
cat <<EOF > /opt/registry/conf/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
  delete:
    enabled: true
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
compatibility:
  schema1:
    enabled: true
EOF
openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj "/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME" -addext "subjectAltName=DNS:$REGISTRY_NAME"
cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract
htpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD
podman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e "REGISTRY_AUTH=htpasswd" -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry" -e "REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest
[ "$?" == "0" ] || !!
systemctl enable --now registry
```

2. Kindly modify the location of the `PULL_SECRET` to point to the correct location.
3. Please adjust the permissions by adding the execution flag as follows: `chmod u+x ${HOME}/registry.sh`
4. You can execute the script without any parameters using the following command `${HOME}/registry.sh`

After executing the script `${HOME}/registry.sh`, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.

It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.

The root folder for the registry is situated at /opt/registry, and it's structured as follows:

- `certs` holds the TLS certificates.
- `auth` keeps the credentials.
- `data` contains the registry images.
- `conf` will hold the registry configuration.

---

## Source: docs/content/labs/common/tls-certificates.md

!!! important

    This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.

In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.

It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):

- `/etc/pki/ca-trust/extracted/pem/`
- `/etc/pki/ca-trust/source/anchors/`
- `/etc/pki/tls/certs/`

## Adding a CA to the Management Cluster

There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.

1. Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name `registry-config` The content should resemble the following:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: registry-config
  namespace: openshift-config
data:
  registry.hypershiftbm.lab..5000: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
```

!!! note

        The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the ":" character is being replaced by ".."; therefore, it is imperative to ensure this correction.

2. Now we need to patch the clusterwide object `image.config.openshift.io` including this:

```yaml
spec:
  additionalTrustedCA:
    name: registry-config
```

This modification will result in two significant consequences:

- Granting masters the capability to retrieve images from the private registry.
- Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.

!!! note

        The modification required several minutes to be successfully executed.

## Alternative: Adding a CA to the Management Cluster

We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.

It's described here. This method involves utilizing the `image-registry-operator`, which deploys the CAs to the OCP nodes.

Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.

However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:

- Check if the `openshift-config` namespace in the Management cluster contains a ConfigMap named `user-ca-bundle`.
- If the ConfigMap doesn't exist, execute the following command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt

oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}
```

- Otherwise, if that ConfigMap exists, execute this other command:

```bash
## REGISTRY_CERT_PATH=<PATH/TO/YOUR/CERTIFICATE/FILE>
export REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt
export TMP_FILE=$(mktemp)

oc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\.crt}' > ${TMP_FILE}
echo >> ${TMP_FILE}
echo \#registry.$(hostname --long) >> ${TMP_FILE}
cat ${REGISTRY_CERT_PATH} >> ${TMP_FILE}
oc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -
```

You have a functional script located in the `assets/<NetworkStack>/09-tls-certificates/01-config.sh`, this is the sample for IPv6.


---

## Source: docs/content/labs/common/watching/index.md

This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:

- Control Plane
- Data Plane

So please follow these links accordingly.

Control Plane perspective{ .md-button }
Data Plane perspective{ .md-button }


---

## Source: docs/content/labs/common/watching/watching-cp.md

Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:

```bash
export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig

watch "oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;"
```

This command will give you info about:

- What is the Hypershift Operator status
- The HostedControlPlane pod status
- The BareMetalHosts
- The Agents
- The Infraenv
- The HostedCluster and NodePool

This is how it looks:



---

## Source: docs/content/labs/common/watching/watching-dp.md

If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands

```bash
oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d > /root/hc_admin_kubeconfig.yaml
export KUBECONFIG=/root/hc_admin_kubeconfig.yaml

watch "oc get clusterversion,nodes,co"
```

This command will give you info about:

- Check the clusterversion
- Check if the Nodes has joined the cluster
- Check the ClusterOperators

This is how looks like:



---

## Source: docs/content/labs/common/webserver.md

!!! important

    This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.

This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.

The script refers to this repository folder and it's the same for all three different network stacks.

To do this, you can use this script:

```bash
#!/bin/bash

WEBSRV_FOLDER=/opt/srv
ROOTFS_IMG_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')"
LIVE_ISO_URL="$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')"

mkdir -p ${WEBSRV_FOLDER}/images
curl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}
curl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}
chmod -R 755 ${WEBSRV_FOLDER}/*

## Run Webserver
podman ps --noheading | grep -q websrv-ai
if [[ $? == 0 ]];then
    echo "Launching Registry pod..."
    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080
fi
```

The script will create a folder under `/opt/srv`. This folder will contain the `images` for RHCOS provision in the worker nodes. To be more concrete, we need the `RootFS` and `LiveISO` artifacts found on the Openshift CI Release page.

After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.


---

## Source: docs/content/labs/index.md

---
title: MCE
---

# Multi-Cluster Engine for Bare Metal

Within this section, our primary focus revolves around the various deployment methods available for Hosted Clusters, leveraging MCE as a foundational framework. The inclusion of the Hypershift release is planned in MCE's payload, starting from version 2.4, and will be fully integrated by version 2.9 in ACM.

In this segment of the documentation, we will delve into both Connected and Disconnected deployment modes, exploring their respective configurations related to Network stacks:

- IPv4
- IPv6
- Dual Stack

Each of these modes possesses its unique characteristics and intricacies, which will be thoroughly examined‚Äîfrom the Node/Hypervisor configuration to the artifacts entailed in the deployment process.

For more information on specific topics, please refer to the following links:

- IPv4{ .md-button }
- IPv6{ .md-button }
- Dual Stack{ .md-button }

While our primary focus in this documentation is Virtual Machines, it is important to note that the principles discussed herein are equally applicable to bare metal nodes. We will duly emphasize the distinct considerations associated with each type of deployment.

---

## Source: docs/content/recipes/common/exposing-dataplane-with-metallb.md

## Configure MetalLB for HostedCluster's Data Plane

- Deploy the MetalLB Operator using the OLM, applying this manifest or using the UI Console:

```yaml
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator
  namespace: openshift-operators
spec:
  channel: "stable"
  name: metallb-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Automatic
```

- Deploy the MetalLB CR:

```yaml
---
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: openshift-operators
```

This deploys the metallb-controller-manager and the webhook-server.

- Configure the IPAddressPool and the L2Advertisement:

```yaml
---
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: lab-network
  namespace: openshift-operators
spec:
  autoAssign: true
  addresses:
  - 192.168.126.160-192.168.126.165
  - 2620:52:0:1306::160-2620:52:0:1306::169
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: advertise-lab-network
  namespace: openshift-operators
spec:
  ipAddressPools:
  - lab-network
```

!!! Note
    The sample config is based on a DualStack layout. If your deployment uses only one stack, specify the IPAddressPool for that stack.

- Expose the OpenShift service. This is usually done in both the Control Plane for MGMT configuration and the Data Plane to configure the Ingress:

```yaml
kind: Service
apiVersion: v1
metadata:
  annotations:
    metallb.universe.tf/address-pool: lab-network
  name: metallb-ingress
  namespace: openshift-ingress
spec:
  ipFamilies:
  - IPv4
  - IPv6
  ipFamilyPolicy: PreferDualStack
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
    - name: https
      protocol: TCP
      port: 443
      targetPort: 443
  selector:
    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  type: LoadBalancer
```

!!! Note
    The sample config is based on a DualStack layout. If your deployment uses only one stack, specify the ipFamilies for that stack and modify the ipFamilyPolicy accordingly.

The usual configuration for the Hosted Cluster in the BareMetal case is a mix between Route and LoadBalancer strategies:

```yaml
spec:
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
      Route:
        hostname: <URL>
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
```

This way, the API server is configured as a LoadBalancer, and the rest of the services are exposed via Route.

!!! Note
    You can specify a specific URL to expose the service you want.

---

## Source: docs/content/recipes/common/replace-crio-runtime.md

# Replacing the Worker Node's CRI-O Runtime

Starting with OpenShift version 4.18, the Machine Config Operator (MCO) will switch the default runtime from `runc` to `crun`, simplifying much of the internals, crun is a simplified implementation of runc written in C. You can find more information about this change here.

## Switching from `runc` to `crun`

The most common use case is when someone using OpenShift 4.16 or 4.17 wants to test their workload performance with `crun` before upgrading to 4.18, where `crun` will become the default runtime.

Ensure you follow these steps to make the change.

### Creating the MachineConfig patch for the NodePool

The following manifest sets `crun` as the default runtime.

=== "**HCP Regular Workloads (CRUN)**"

    - Put the crio configuration into a file

    ```bash
    cat <<EOF > crio-config.conf
    [crio.runtime]
    default_runtime="crun"

    [crio.runtime.runtimes.crun]
    runtime_path = "/bin/crun"
    runtime_type = "oci"
    runtime_root = "/run/crun"
    EOF
    ```

    - Get the base64 hash of the file content

    ```bash
    export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)
    ```

    - Create the MachineConfig file with content in Base64

    ```bash
    cat <<EOF > mc-crun-default-runtime.yaml
    apiVersion: machineconfiguration.openshift.io/v1
    kind: MachineConfig
    metadata:
      name: 60-crun-default-runtime
    spec:
      config:
        ignition:
          version: 3.2.0
        storage:
          files:
          - contents:
              source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}
            mode: 420
            path: /etc/crio/crio.conf.d/99-crun-runtime.conf
    EOF
    ```

    - Run this command to create a ConfigMap containing the MachineConfig patch for regular workloads

    ```bash
    oc create -n <hostedcluster_namespace> configmap mcp-crun-default --from-file config=mc-crun-default-runtime.yaml
    ```

    - Use this command to patch the NodePool resource by adding the newly created ConfigMap as a MachineConfig change

    ```bash
    oc patch -n <hostedcluster_namespace> nodepool <nodepool_name> --type=json -p='[{"op": "add", "path": "/spec/config", "value": [{"name": "mcp-crun-default"}]}]'
    ```

=== "**HCP High Performance Workloads (CRUN)**"

    - Put the crio configuration into a file

    ```bash
    cat <<EOF > crio-config.conf
    [crio.runtime]
    default_runtime="crun"
    # You will need to adjust the CPU set assignation depending on the infrastructure
    infra_ctr_cpuset = "0-1,24-25"

    [crio.runtime.runtimes.crun]
    runtime_path = "/bin/crun"
    runtime_type = "oci"
    runtime_root = "/run/crun"

    allowed_annotations = [
        "io.containers.trace-syscall",
        "io.kubernetes.cri-o.Devices",
        "io.kubernetes.cri-o.LinkLogs",
        "cpu-load-balancing.crio.io",
        "cpu-quota.crio.io",
        "irq-load-balancing.crio.io",
    ]
    EOF
    ```

    - Get the base64 hash of the file content

    ```bash
    export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)
    ```

    - Create the MachineConfig file with content in Base64

    ```bash
    cat <<EOF > mc-crun-high-perf-default-runtime.yaml
    apiVersion: machineconfiguration.openshift.io/v1
    kind: MachineConfig
    metadata:
      name: 60-crun-high-perf-default-runtime
    spec:
      config:
        ignition:
          version: 3.2.0
        storage:
          files:
          - contents:
              source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}
            mode: 420
            path: /etc/crio/crio.conf.d/99-crun-high-perf-runtime.conf
    EOF
    ```

    - Create the ConfigMap for high-performance workloads using the following command

    ```bash
    oc create -n <hostedcluster_namespace> configmap mcp-crun-hp-default --from-file config=mc-crun-high-perf-default-runtime.yaml
    ```

    - Patch the NodePool resource with the newly created ConfigMap

    ```bash
    oc patch -n <hostedcluster_namespace> nodepool <nodepool_name> --type=json -p='[{"op": "add", "path": "/spec/config", "value": [{"name": "mcp-crun-hp-default"}]}]'
    ```

## Switching from `crun` to `runc`

If you experience performance issues or similar problems and want to revert to `runc` as the default runtime, follow this guide to set `runc` as the default CRI-O runtime for your worker nodes, even after upgrading to 4.18. This process is also compatible with 4.17 if you plan to upgrade but want to retain `runc` as the runtime.

### Creating the MachineConfig patch for the NodePool

The following manifest sets `runc` as the default runtime.

=== "**HCP Regular Workloads (RUNC)**"

    - Put the crio configuration into a file

    ```bash
    cat <<EOF > crio-config.conf
    [crio.runtime]
    default_runtime="runc"

    [crio.runtime.runtimes.runc]
    runtime_path = "/bin/runc"
    runtime_type = "oci"
    runtime_root = "/run/runc"
    EOF
    ```

    - Get the base64 hash of the file content

    ```bash
    export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)
    ```

    - Create the MachineConfig file with content in Base64

    ```bash
    cat <<EOF > mc-runc-default-runtime.yaml
    apiVersion: machineconfiguration.openshift.io/v1
    kind: MachineConfig
    metadata:
      name: 60-runc-default-runtime
    spec:
      config:
        ignition:
          version: 3.2.0
        storage:
          files:
          - contents:
              source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}
            mode: 420
            path: /etc/crio/crio.conf.d/99-runc-runtime.conf
    EOF
    ```

    - Create the ConfigMap for regular workloads using

    ```bash
    oc create -n <hostedcluster_namespace> configmap mcp-runc-default --from-file config=mc-runc-default-runtime.yaml
    ```

    - Patch the NodePool resource with the newly created ConfigMap

    ```bash
    oc patch -n <hostedcluster_namespace> nodepool <nodepool_name> --type=json -p='[{"op": "add", "path": "/spec/config", "value": [{"name": "mcp-runc-default"}]}]'
    ```

=== "**HCP High Performance Workloads (RUNC)**"

    - Put the crio configuration into a file

    ```bash
    cat <<EOF > crio-config.conf
    [crio.runtime]
    default_runtime="runc"
    # You will need to adjust the CPU set assignation depending on the infrastructure
    infra_ctr_cpuset = "0-1,24-25"

    [crio.runtime.runtimes.runc]
    runtime_path = "/bin/runc"
    runtime_type = "oci"
    runtime_root = "/run/runc"

    allowed_annotations = [
        "io.containers.trace-syscall",
        "io.kubernetes.cri-o.Devices",
        "io.kubernetes.cri-o.LinkLogs",
        "cpu-load-balancing.crio.io",
        "cpu-quota.crio.io",
        "irq-load-balancing.crio.io",
    ]
    EOF
    ```

    - Get the base64 hash of the file content

    ```bash
    export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)
    ```

    ```bash
    cat <<EOF > mc-runc-high-perf-default-runtime.yaml
    apiVersion: machineconfiguration.openshift.io/v1
    kind: MachineConfig
    metadata:
      name: 60-runc-high-perf-default-runtime
    spec:
      config:
        ignition:
          version: 3.2.0
        storage:
          files:
          - contents:
              source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}
            mode: 420
            path: /etc/crio/crio.conf.d/99-runc-high-perf-runtime.conf
    EOF
    ```

    - Create the ConfigMap for high-performance workloads

    ```bash
    oc create -n <hostedcluster_namespace> configmap mcp-runc-hp-default --from-file config=mc-runc-high-perf-default-runtime.yaml
    ```

    - Patch the NodePool resource with the newly created ConfigMap

    ```bash
    oc patch -n <hostedcluster_namespace> nodepool <nodepool_name> --type=json -p='[{"op": "add", "path": "/spec/config", "value": [{"name": "mcp-runc-hp-default"}]}]'
    ```


---

## Source: docs/content/recipes/index.md

---
title: Recipes
---

In this section we will expose the more frequent recipes the people could use for different use cases, separated by providers.

---

## Source: docs/content/reference/SLOs.md

# SLOs
This project is committed to satisfy a number of internal SLOs. 
These SLOs can be taken as reference by consumers to aggregate them and help define their own SLOs.

These SLOs/SLIs are currently just referential and monitored as part of our CI runs.
They are valid under the following circumstances:

- Running raw Hypershift operator in an OCP management cluster with bare HostedCluster and NodePool resources.
- Management cluster has 2 m6i.4xlarge instances with 16 cores to allocate HostedClusters.
- No more than 20 concurrent HostedClusters.

## SLOs/SLIs
- Time for cluster to become available should be < 5min.
- Time for cluster to complete rollout should be < 15min.
- Cluster Memory consumption per single replica HostedCluster should be < 7GB.
- Cluster Memory consumption per high available HostedCluster should be < 18GB.
- Time to NodePool availability should be < 8min.
- Time to cloud resources deletion in a raw cluster should be < 3min.
- Time to cluster deletion in a raw cluster should be < 5min.

TBD:

- Time to instance creation
- Time to node joining cluster
- Time for node to become active after joining

Dashboards with these metrics are currently stored in a temporary Grafana instance:

- Deletion SLIs
- Creation/Running SLIs

---

## Source: docs/content/reference/api.md

---
################# IMPORTANT ########################
# Document generated by `make api-docs`. DO NOT EDIT
################# IMPORTANT ########################
title: API
---
# HyperShift API Reference
<p>Packages:</p>
<ul>
<li>
<a href="#hypershift.openshift.io%2fv1beta1">hypershift.openshift.io/v1beta1</a>
</li>
</ul>
<h2 id="hypershift.openshift.io/v1beta1">hypershift.openshift.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains the HyperShift API.</p>
<p>The HyperShift API enables creating and managing lightweight, flexible, heterogeneous
OpenShift clusters at scale.</p>
<p>HyperShift clusters are deployed in a topology which isolates the &ldquo;control plane&rdquo;
(e.g. etcd, the API server, controller manager, etc.) from the &ldquo;data plane&rdquo; (e.g.
worker nodes and their kubelets, and the infrastructure on which they run). This
enables &ldquo;hosted control plane as a service&rdquo; use cases.</p>
</p>
##CertificateSigningRequestApproval { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval }
<p>
<p>CertificateSigningRequestApproval defines the desired state of CertificateSigningRequestApproval</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>CertificateSigningRequestApproval</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>metadata is standard object metadata.</p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec">
CertificateSigningRequestApprovalSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>spec is the specification of the desired behavior of the CertificateSigningRequestApproval.</p>
<br/>
<br/>
<table>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus">
CertificateSigningRequestApprovalStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>status is the most recently observed status of the CertificateSigningRequestApproval.</p>
</td>
</tr>
</tbody>
</table>
##GCPPrivateServiceConnect { #hypershift.openshift.io/v1beta1.GCPPrivateServiceConnect }
<p>
<p>GCPPrivateServiceConnect represents GCP Private Service Connect infrastructure.
This resource is feature-gated behind the GCPPlatform feature gate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>GCPPrivateServiceConnect</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>metadata is the metadata for the GCPPrivateServiceConnect.</p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectSpec">
GCPPrivateServiceConnectSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>spec is the specification for the GCPPrivateServiceConnect.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>loadBalancerIP</code></br>
<em>
string
</em>
</td>
<td>
<p>loadBalancerIP is the IP address of the Internal Load Balancer
Populated by the observer from service status
This value must be a valid IPv4 or IPv6 address.</p>
</td>
</tr>
<tr>
<td>
<code>forwardingRuleName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>forwardingRuleName is the name of the Internal Load Balancer forwarding rule
Populated by the reconciler via GCP API lookup</p>
</td>
</tr>
<tr>
<td>
<code>consumerAcceptList</code></br>
<em>
[]string
</em>
</td>
<td>
<p>consumerAcceptList specifies which customer projects can connect
Accepts both project IDs (e.g. &ldquo;my-project-123&rdquo;) and project numbers (e.g. &ldquo;123456789012&rdquo;)</p>
</td>
</tr>
<tr>
<td>
<code>natSubnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>natSubnet is the subnet used for NAT by the Service Attachment
Auto-populated by the HyperShift Operator</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectStatus">
GCPPrivateServiceConnectStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>status is the status of the GCPPrivateServiceConnect.</p>
</td>
</tr>
</tbody>
</table>
##HostedCluster { #hypershift.openshift.io/v1beta1.HostedCluster }
<p>
<p>HostedCluster is the primary representation of a HyperShift cluster and encapsulates
the control plane and common data plane configuration. Creating a HostedCluster
results in a fully functional OpenShift control plane with no attached nodes.
To support workloads (e.g. pods), a HostedCluster may have one or more associated
NodePool resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>HostedCluster</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>metadata is the metadata for the HostedCluster.</p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">
HostedClusterSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>spec is the desired behavior of the HostedCluster.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the desired OCP release payload for all the hosted cluster components.
This includes those components running management side like the Kube API Server and the CVO but also the operands which land in the hosted cluster data plane like the ingress controller, ovn agents, etc.
The maximum and minimum supported release versions are determined by the running Hypersfhit Operator.
Attempting to use an unsupported version will result in the HostedCluster being degraded and the validateReleaseImage condition being false.
Attempting to use a release with a skew against a NodePool release bigger than N-2 for the y-stream will result in leaving the NodePool in an unsupported state.
Changing this field will trigger a rollout of the control plane components.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneRelease</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneRelease is like spec.release but only for the components running on the management cluster.
This excludes any operand which will land in the hosted cluster data plane.
It is useful when you need to apply patch management side like a CVE, transparently for the hosted cluster.
Version input for this field is free, no validation is performed against spec.release or maximum and minimum is performed.
If defined, it will dicate the version of the components running management side, while spec.release will dictate the version of the components landing in the hosted cluster data plane.
If not defined, spec.release is used for both.
Changing this field will trigger a rollout of the control plane.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterID uniquely identifies this cluster. This is expected to be an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in hexadecimal digits).
As with a Kubernetes metadata.uid, this ID uniquely identifies this cluster in space and time.
This value identifies the cluster in metrics pushed to telemetry and metrics produced by the control plane operators.
If a value is not specified, a random clusterID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>infraID is a globally unique identifier for the cluster.
It must consist of lowercase alphanumeric characters and hyphens (&lsquo;-&rsquo;) only, and start and end with an alphanumeric character.
It must be no more than 253 characters in length.
This identifier will be used to associate various cloud resources with the HostedCluster and its associated NodePools.
infraID is used to compute and tag created resources with &ldquo;kubernetes.io/cluster/&rdquo;+hcluster.Spec.InfraID which has contractual meaning for the cloud provider implementations.
If a value is not specified, a random infraID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
If omitted we will use the appropriate update service for the cluster and region.
This is used by the control plane operator to determine and signal the appropriate available upgrades in the hostedCluster.status.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster.
If omitted no particular upgrades are suggested.
TODO(alberto): Consider the backend to use the default channel by default. Default channel will contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>kubeAPIServerDNSName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeAPIServerDNSName specifies a desired DNS name to resolve to the KAS.
When set, the controller will automatically generate a secret with kubeconfig and expose it in the hostedCluster Status.customKubeconfig field.
If it&rsquo;s set or removed day 2, the kubeconfig generated secret will be created, recreated or deleted.
The DNS entries should be resolvable from the cluster, so this should be manually configured in the DNS provider.
This field works in conjunction with configuration.APIServer.ServingCerts.NamedCertificates to enable
access to the API server via a custom domain name. The NamedCertificates provide the TLS certificates
for the custom domain, while this field triggers the generation of a kubeconfig that uses those certificates.
This API endpoint only works in OCP version 4.19 or later. Older versions will result in a no-op.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components like the Kube API Server.
Possible values are HighlyAvailable and SingleReplica. The default value is HighlyAvailable.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on the hosted cluster data plane like the ingress controller and image registry controller.
Possible values are HighlyAvailable and SingleReplica. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>dns specifies the DNS configuration for the hosted cluster ingress.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<p>networking specifies network configuration for the hosted cluster.
Defaults to OVNKubernetes with a cluster network of cidr: &ldquo;10.132.0.0/14&rdquo; and a service network of cidr: &ldquo;172.31.0.0/16&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with this HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoNode specifies the configuration for the autoNode feature.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>etcd specifies configuration for the control plane etcd cluster. The
default managementType is Managed. Once set, the managementType cannot be
changed.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>services specifies how individual control plane services endpoints are published for consumption.
This requires APIServer;OAuthServer;Konnectivity;Ignition.
This field is immutable for all platforms but IBMCloud.
Max is 6 to account for OIDC;OVNSbDb for backward compatibility though they are no-op.</p>
<p>-kubebuilder:validation:XValidation:rule=&ldquo;self.all(s, !(s.service == &lsquo;APIServer&rsquo; &amp;&amp; s.servicePublishingStrategy.type == &lsquo;Route&rsquo;) || has(s.servicePublishingStrategy.route.hostname))&rdquo;,message=&ldquo;If serviceType is &lsquo;APIServer&rsquo; and publishing strategy is &lsquo;Route&rsquo;, then hostname must be set&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.platform.type == &lsquo;IBMCloud&rsquo; ? [&lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;].all(requiredType, self.exists(s, s.service == requiredType))&rdquo;,message=&ldquo;Services list must contain at least &lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, and &lsquo;Konnectivity&rsquo; service types&rdquo; : [&lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, &lsquo;Ignition&rsquo;].all(requiredType, self.exists(s, s.service == requiredType))&ldquo;,message=&ldquo;Services list must contain at least &lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, and &lsquo;Ignition&rsquo; service types&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; has(s.servicePublishingStrategy.route) &amp;&amp; has(s.servicePublishingStrategy.route.hostname)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.route) &amp;&amp; has(y.servicePublishingStrategy.route.hostname) &amp;&amp; y.servicePublishingStrategy.route.hostname == x.servicePublishingStrategy.route.hostname)).size() &lt;= 1)&rdquo;,message=&ldquo;Each route publishingStrategy &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; has(s.servicePublishingStrategy.nodePort) &amp;&amp; has(s.servicePublishingStrategy.nodePort.address) &amp;&amp; has(s.servicePublishingStrategy.nodePort.port)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.nodePort) &amp;&amp; has(y.servicePublishingStrategy.nodePort.address) &amp;&amp; y.servicePublishingStrategy.nodePort.address == x.servicePublishingStrategy.nodePort.address &amp;&amp; has(y.servicePublishingStrategy.nodePort.port) &amp;&amp; y.servicePublishingStrategy.nodePort.port == x.servicePublishingStrategy.nodePort.port )).size() &lt;= 1)&rdquo;,message=&ldquo;Each nodePort publishingStrategy &lsquo;nodePort&rsquo; and &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
TODO(alberto): this breaks the cost budget for &lt; 4.17. We should figure why and enable it back. And If not fixable, consider imposing a minimum version on the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>pullSecret is a local reference to a Secret that must have a &ldquo;.dockerconfigjson&rdquo; key whose content must be a valid Openshift pull secret JSON.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This pull secret will be part of every payload generated by the controllers for any NodePool of the HostedCluster
and it will be injected into the container runtime of all NodePools.
Changing this value will trigger a rollout for all existing NodePools in the cluster.
Changing the content of the secret inplace will not trigger a rollout and might result in unpredictable behaviour.
TODO(alberto): have our own local reference type to include our opinions and avoid transparent changes.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>sshKey is a local reference to a Secret that must have a &ldquo;id_rsa.pub&rdquo; key whose content must be the public part of 1..N SSH keys.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
When sshKey is set, the controllers will generate a machineConfig with the sshAuthorizedKeys <a href="https://coreos.github.io/ignition/configuration-v3_2/">https://coreos.github.io/ignition/configuration-v3_2/</a> populated with this value.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>issuerURL is an OIDC issuer URL which will be used as the issuer in all
ServiceAccount tokens generated by the control plane API server via &ndash;service-account-issuer kube api server flag.
<a href="https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/">https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/</a>
<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection</a>
The default value is kubernetes.default.svc, which only works for in-cluster
validation.
If the platform is AWS and this value is set, the controller will update an s3 object with the appropriate OIDC documents (using the serviceAccountSigningKey info) into that issuerURL.
The expectation is for this s3 url to be backed by an OIDC provider in the AWS IAM.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAccountSigningKey is a local reference to a secret that must have a &ldquo;key&rdquo; key whose content must be the private key
used by the service account token issuer.
If not specified, a service account signing key will
be generated automatically for the cluster.
When specifying a service account signing key, an IssuerURL must also be specified.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.</p>
<p>For key rotation, the secret may optionally contain an &ldquo;old-key.pub&rdquo; key whose content is the PEM-encoded
public key of the previous signing key. When present, the kube-apiserver will accept tokens signed by
both the current and previous keys, allowing for graceful key rotation without invalidating existing tokens.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>configuration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
</td>
</tr>
<tr>
<td>
<code>operatorConfiguration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">
OperatorConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>auditWebhook contains metadata for configuring an audit webhook endpoint
for a cluster to process cluster audit events. It references a secret that
contains the webhook information for the audit webhook endpoint. It is a
secret because if the endpoint has mTLS the kubeconfig will contain client
keys. The kubeconfig needs to be stored in the secret with a secret key
name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageContentSources specifies image mirrors that can be used by cluster
nodes to pull content.
When imageContentSources is set, the controllers will generate a machineConfig.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalTrustBundle is a local reference to a ConfigMap that must have a &ldquo;ca-bundle.crt&rdquo; key
whose content must be a PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>secretEncryption specifies a Kubernetes secret encryption strategy for the
control plane.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>fips indicates whether this cluster&rsquo;s nodes will be running in FIPS mode.
If set to true, the control plane&rsquo;s ignition server will be configured to
expect that nodes joining the cluster will be FIPS-enabled.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the HostedCluster controller, resulting in any change to the HostedCluster being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>olmCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeSelector when specified, is propagated to all control plane Deployments and Stateful sets running management side.
It must be satisfied by the management Nodes for the pods to be scheduled. Otherwise the HostedCluster will enter a degraded state.
Changes to this field will propagate to existing Deployments and StatefulSets.
TODO(alberto): add additional validation for the map key/values.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tolerations when specified, define what custom tolerations are added to the hcp pods.</p>
</td>
</tr>
<tr>
<td>
<code>labels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>labels when specified, define what custom labels are added to the hcp pods.
Changing this day 2 will cause a rollout of all hcp pods.
Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
<p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=&ldquo;label key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (<em>), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)&rdquo;
-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=&ldquo;label value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores (</em>) or dots (.), and begin and end with an alphanumeric character&rdquo;
TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p>
</td>
</tr>
<tr>
<td>
<code>capabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">
Capabilities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capabilities allows for disabling optional components at cluster install time.
This field is optional and once set cannot be changed.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">
HostedClusterStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>status is the latest observed status of the HostedCluster.</p>
</td>
</tr>
</tbody>
</table>
##NodePool { #hypershift.openshift.io/v1beta1.NodePool }
<p>
<p>NodePool is a scalable set of worker nodes attached to a HostedCluster.
NodePool machine architectures are uniform within a given pool, and are
independent of the control plane&rsquo;s underlying machine architecture.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiVersion</code></br>
string</td>
<td>
<code>
hypershift.openshift.io/v1beta1
</code>
</td>
</tr>
<tr>
<td>
<code>kind</code></br>
string
</td>
<td><code>NodePool</code></td>
</tr>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>metadata is the metadata for the NodePool.</p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">
NodePoolSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>spec is the desired behavior of the NodePool.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>clusterName</code></br>
<em>
string
</em>
</td>
<td>
<p>clusterName is the name of the HostedCluster this NodePool belongs to.
If a HostedCluster with this name doesn&rsquo;t exist, the controller will no-op until it exists.</p>
</td>
</tr>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the OCP release used for this NodePool. It drives the machine ignition configuration (including
the kubelet version) and other platform-specific properties (e.g. an AMI on AWS).</p>
<p>Version-skew rules and effects:
- The minor-version skew relative to the control-plane release must be &lt;= N-2.
This is not currently enforced, but exceeding this limit is unsupported and
may lead to unpredictable behavior.
- If the specified release is higher than the HostedCluster&rsquo;s release, the
NodePool will be degraded and the ValidReleaseImage condition will be false.
- If the specified release is lower than the NodePool&rsquo;s current y-stream,
the NodePool will be degraded and the ValidReleaseImage condition will be false.</p>
<p>Changing this field triggers a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">
NodePoolPlatform
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the NodePool
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>replicas is the desired number of nodes the pool should maintain. If unset, the controller default value is 0.
replicas is mutually exclusive with autoscaling. If autoscaling is configured, replicas must be omitted and autoscaling will control the NodePool size internally.</p>
</td>
</tr>
<tr>
<td>
<code>management</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">
NodePoolManagement
</a>
</em>
</td>
<td>
<p>management specifies behavior for managing nodes in the pool, such as
upgrade strategies and auto-repair behaviors.</p>
</td>
</tr>
<tr>
<td>
<code>autoScaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolAutoScaling">
NodePoolAutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoScaling specifies auto-scaling behavior for the NodePool.
autoScaling is mutually exclusive with replicas. If replicas is set, this field must be omitted.</p>
</td>
</tr>
<tr>
<td>
<code>config</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>config is a list of references to ConfigMaps containing serialized
MachineConfig resources to be injected into the ignition configurations of
nodes in the NodePool. The MachineConfig API schema is defined here:</p>
<p><a href="https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185">https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</a></p>
<p>Each ConfigMap must have a single key named &ldquo;config&rdquo; whose value is the YML
with one or more serialized machineconfiguration.openshift.io resources:</p>
<ul>
<li>KubeletConfig</li>
<li>ContainerRuntimeConfig</li>
<li>MachineConfig</li>
<li>ClusterImagePolicy</li>
<li>ImageContentSourcePolicy</li>
<li>ImageDigestMirrorSet</li>
</ul>
<p>This is validated in the backend and signaled back via validMachineConfig condition.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeDrainTimeout is the maximum amount of time that the controller will spend on retrying to drain a node until it succeeds.
The default value is 0, meaning that the node can retry drain without any time limitations.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeVolumeDetachTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node.
The default value is 0, meaning that the volumes will be detached from the node without any time limitations.
After the timeout, any remaining attached volumes will be ignored and the removal of the machine will continue.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeLabels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeLabels propagates a list of labels to Nodes, only once on creation.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
</td>
</tr>
<tr>
<td>
<code>taints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Taint">
[]Taint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>taints if specified, propagates a list of taints to Nodes, only once on creation.
These taints are additive to the ones applied by other controllers</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the NodePool controller. Resulting in any change to the NodePool being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tuningConfig is a list of references to ConfigMaps containing serialized
Tuned or PerformanceProfile resources to define the tuning configuration to be applied to
nodes in the NodePool. The Tuned API is defined here:</p>
<p><a href="https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go">https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</a></p>
<p>The PerformanceProfile API is defined here:
<a href="https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2">https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</a></p>
<p>Each ConfigMap must have a single key named &ldquo;tuning&rdquo; whose value is the
JSON or YAML of a serialized Tuned or PerformanceProfile.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>arch</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>arch is the preferred processor architecture for the NodePool. Different platforms might have different supported architectures.
TODO: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery:
<a href="https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215">https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</a></p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">
NodePoolStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>status is the latest observed status of the NodePool.</p>
</td>
</tr>
</tbody>
</table>
###AESCBCSpec { #hypershift.openshift.io/v1beta1.AESCBCSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>AESCBCSpec defines metadata about the AESCBC secret encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>activeKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>backupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
</tbody>
</table>
###APIServerNetworking { #hypershift.openshift.io/v1beta1.APIServerNetworking }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>APIServerNetworking specifies how the APIServer is exposed inside a cluster
node.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>advertiseAddress</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>advertiseAddress is the address that pods within the nodes will use to talk to the API
server. This is an address associated with the loopback adapter of each
node. If not specified, the controller will take default values.
The default values will be set as 172.20.0.1 or fd00::1.
This value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>port</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>port is the port at which the APIServer is exposed inside a node. Other
pods using host networking cannot listen on this port.
If omitted 6443 is used.
This is useful to choose a port other than the default one which might interfere with customer environments e.g. <a href="https://github.com/openshift/hypershift/pull/356">https://github.com/openshift/hypershift/pull/356</a>.
Setting this to 443 is possible only for backward compatibility reasons and it&rsquo;s discouraged.
Doing so, it would result in the controller overriding the KAS endpoint in the guest cluster having a discrepancy with the KAS Pod and potentially causing temporarily network failures.
This value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>allowedCIDRBlocks</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CIDRBlock">
[]CIDRBlock
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>allowedCIDRBlocks is an allow list of CIDR blocks that can access the APIServer.
If not specified, traffic is allowed from all addresses.
This field is enforced for ARO (Azure Red Hat OpenShift) via the shared-ingress HAProxy.
For platforms other than ARO, the enforcement depends on whether the underlying cloud provider supports the Service LoadBalancerSourceRanges field.
If the platform does not support LoadBalancerSourceRanges, this field may have no effect.</p>
</td>
</tr>
</tbody>
</table>
###AWSCloudProviderConfig { #hypershift.openshift.io/v1beta1.AWSCloudProviderConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSCloudProviderConfig specifies AWS networking configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>subnet is the subnet to use for control plane cloud resources.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>zone is the availability zone where control plane cloud resources are
created.</p>
</td>
</tr>
<tr>
<td>
<code>vpc</code></br>
<em>
string
</em>
</td>
<td>
<p>vpc is the VPC to use for control plane cloud resources.</p>
</td>
</tr>
</tbody>
</table>
###AWSEndpointAccessType { #hypershift.openshift.io/v1beta1.AWSEndpointAccessType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSEndpointAccessType specifies the publishing scope of cluster endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Private&#34;</p></td>
<td><p>Private endpoint access allows only private API server access and private
node communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;Public&#34;</p></td>
<td><p>Public endpoint access allows public API server access and public node
communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;PublicAndPrivate&#34;</p></td>
<td><p>PublicAndPrivate endpoint access allows public API server access and
private node communication with the control plane.</p>
</td>
</tr></tbody>
</table>
###AWSKMSAuthSpec { #hypershift.openshift.io/v1beta1.AWSKMSAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">AWSKMSSpec</a>)
</p>
<p>
<p>AWSKMSAuthSpec defines metadata about the management of credentials used to interact and encrypt data via AWS KMS key.
The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:</p>
<pre><code>{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Federated&quot;: &quot;{{ .ProviderARN }}&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRoleWithWebIdentity&quot;,
&quot;Condition&quot;: {
&quot;StringEquals&quot;: {
&quot;{{ .ProviderName }}:sub&quot;: {{ .ServiceAccounts }}
}
}
}
]
}
</code></pre>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>awsKms</code></br>
<em>
string
</em>
</td>
<td>
<p>awsKms is an ARN value referencing a role appropriate for managing the auth via the AWS KMS key.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Encrypt&rdquo;,
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt<em>&rdquo;,
&ldquo;kms:GenerateDataKey</em>&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: %q
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AWSKMSKeyEntry { #hypershift.openshift.io/v1beta1.AWSKMSKeyEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">AWSKMSSpec</a>)
</p>
<p>
<p>AWSKMSKeyEntry defines metadata to locate the encryption key in AWS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code></br>
<em>
string
</em>
</td>
<td>
<p>arn is the Amazon Resource Name for the encryption key</p>
</td>
</tr>
</tbody>
</table>
###AWSKMSSpec { #hypershift.openshift.io/v1beta1.AWSKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>AWSKMSSpec defines metadata about the configuration of the AWS KMS Secret Encryption provider</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>region contains the AWS region</p>
</td>
</tr>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry">
AWSKMSKeyEntry
</a>
</em>
</td>
<td>
<p>activeKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry">
AWSKMSKeyEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>backupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
<tr>
<td>
<code>auth</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSAuthSpec">
AWSKMSAuthSpec
</a>
</em>
</td>
<td>
<p>auth defines metadata about the management of credentials used to interact with AWS KMS</p>
</td>
</tr>
</tbody>
</table>
###AWSNodePoolPlatform { #hypershift.openshift.io/v1beta1.AWSNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AWSNodePoolPlatform specifies the configuration of a NodePool when operating
on AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceType</code></br>
<em>
string
</em>
</td>
<td>
<p>instanceType is an ec2 instance type for node instances (e.g. m5.large).</p>
</td>
</tr>
<tr>
<td>
<code>instanceProfile</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>instanceProfile is the AWS EC2 instance profile, which is a container for an IAM role that the EC2 instance uses.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<p>subnet is the subnet to use for node instances.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ami is the image id to use for node instances. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>imageType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageType">
ImageType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageType specifies the type of image to use for node instances.
Valid values are &ldquo;Linux&rdquo; or &ldquo;Windows&rdquo;
When set to Windows, the controller will automatically populate the AMI field
with a Windows-compatible AMI based on the region and OpenShift version.
When the AMI field is also set, it will take precedence over automatic selection based
on imageType. Also note that Windows ImageType is only compatible with an Arch of amd64</p>
</td>
</tr>
<tr>
<td>
<code>securityGroups</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>securityGroups is an optional set of security groups to associate with node
instances.</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>rootVolume specifies configuration for the root volume of node instances.</p>
</td>
</tr>
<tr>
<td>
<code>resourceTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceTag">
[]AWSResourceTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>resourceTags is an optional list of additional tags to apply to AWS node
instances. Changes to this field will be propagated in-place to AWS EC2 instances and their initial EBS volumes.
Volumes created by the storage operator and attached to instances after they are created do not get these tags applied.</p>
<p>These will be merged with HostedCluster scoped tags, which take precedence in case of conflicts.
These take precedence over tags defined out of band (i.e., tags added manually or by other tools outside of HyperShift) in AWS in case of conflicts.</p>
<p>See <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html</a> for
information on tagging AWS resources. AWS supports a maximum of 50 tags per
resource. OpenShift reserves 25 tags for its use, leaving 25 tags available
for the user.</p>
</td>
</tr>
<tr>
<td>
<code>placement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlacementOptions">
PlacementOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>placement specifies the placement options for the EC2 instances.</p>
</td>
</tr>
</tbody>
</table>
###AWSPlatformSpec { #hypershift.openshift.io/v1beta1.AWSPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AWSPlatformSpec specifies configuration for clusters running on Amazon Web Services.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>region is the AWS region in which the cluster resides. This configures the
OCP control plane cloud integrations, and is used by NodePool to resolve
the correct boot AMI for a given release.</p>
</td>
</tr>
<tr>
<td>
<code>cloudProviderConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig">
AWSCloudProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>cloudProviderConfig specifies AWS networking configuration for the control
plane.
This is mainly used for cloud provider controller config:
<a href="https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364">https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364</a>
TODO(dan): should this be named AWSNetworkConfig?</p>
</td>
</tr>
<tr>
<td>
<code>serviceEndpoints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSServiceEndpoint">
[]AWSServiceEndpoint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceEndpoints specifies optional custom endpoints which will override
the default service endpoint of specific AWS Services.</p>
<p>There must be only one ServiceEndpoint for a given service name.</p>
</td>
</tr>
<tr>
<td>
<code>rolesRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSRolesRef">
AWSRolesRef
</a>
</em>
</td>
<td>
<p>rolesRef contains references to various AWS IAM roles required to enable
integrations such as OIDC.</p>
</td>
</tr>
<tr>
<td>
<code>resourceTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceTag">
[]AWSResourceTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>resourceTags is a list of additional tags to apply to AWS resources created
for the cluster. See
<a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html</a> for
information on tagging AWS resources. AWS supports a maximum of 50 tags per
resource. OpenShift reserves 25 tags for its use, leaving 25 tags available
for the user.
Changes to this field will be propagated in-place to AWS resources (VPC Endpoints, EC2 instances, initial EBS volumes and default/endpoint security groups).
These tags will be propagated to the infrastructure CR in the guest cluster, where other OCP operators might choose to honor this input to reconcile AWS resources created by them.
Please consult the official documentation for a list of all AWS resources that support in-place tag updates.
These take precedence over tags defined out of band (i.e., tags added manually or by other tools outside of HyperShift) in AWS in case of conflicts.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSEndpointAccessType">
AWSEndpointAccessType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>endpointAccess specifies the publishing scope of cluster endpoints. The
default is Public.</p>
</td>
</tr>
<tr>
<td>
<code>additionalAllowedPrincipals</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalAllowedPrincipals specifies a list of additional allowed principal ARNs
to be added to the hosted control plane&rsquo;s VPC Endpoint Service to enable additional
VPC Endpoint connection requests to be automatically accepted.
See <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html">https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html</a>
for more details around VPC Endpoint Service allowed principals.</p>
</td>
</tr>
<tr>
<td>
<code>multiArch</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>multiArch specifies whether the Hosted Cluster will be expected to support NodePools with different
CPU architectures, i.e., supporting arm64 NodePools and supporting amd64 NodePools on the same Hosted Cluster.
Deprecated: This field is no longer used. The HyperShift Operator now performs multi-arch validations
automatically despite the platform type. The HyperShift Operator will set HostedCluster.Status.PayloadArch based
on the HostedCluster release image. This field is used by the NodePool controller to validate the
NodePool.Spec.Arch is supported.</p>
</td>
</tr>
<tr>
<td>
<code>sharedVPC</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPC">
AWSSharedVPC
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>sharedVPC contains fields that must be specified if the HostedCluster must use a VPC that is
created in a different AWS account and is shared with the AWS account where the HostedCluster
will be created.</p>
</td>
</tr>
</tbody>
</table>
###AWSPlatformStatus { #hypershift.openshift.io/v1beta1.AWSPlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">PlatformStatus</a>)
</p>
<p>
<p>AWSPlatformStatus contains status specific to the AWS platform</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>defaultWorkerSecurityGroupID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>defaultWorkerSecurityGroupID is the ID of a security group created by
the control plane operator. It is always added to worker machines in
addition to any security groups specified in the NodePool.</p>
</td>
</tr>
</tbody>
</table>
###AWSResourceReference { #hypershift.openshift.io/v1beta1.AWSResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig">AWSCloudProviderConfig</a>, 
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>AWSResourceReference is a reference to a specific AWS resource by ID or filters.
Only one of ID or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>id of resource</p>
</td>
</tr>
<tr>
<td>
<code>filters</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Filter">
[]Filter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>filters is a set of key/value pairs used to identify a resource
They are applied according to the rules defined by the AWS API:
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html</a></p>
</td>
</tr>
</tbody>
</table>
###AWSResourceTag { #hypershift.openshift.io/v1beta1.AWSResourceTag }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSResourceTag is a tag to apply to AWS resources created for the cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>key is the key of the tag.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<p>value is the value of the tag.</p>
<p>Some AWS service do not support empty values. Since tags are added to
resources in many services, the length of the tag value must meet the
requirements of all services.</p>
</td>
</tr>
</tbody>
</table>
###AWSRoleCredentials { #hypershift.openshift.io/v1beta1.AWSRoleCredentials }
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code></br>
<em>
string
</em>
</td>
<td>
<p>arn is the ARN of the role.</p>
</td>
</tr>
<tr>
<td>
<code>namespace</code></br>
<em>
string
</em>
</td>
<td>
<p>namespace is the namespace of the role.</p>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of the role.</p>
</td>
</tr>
</tbody>
</table>
###AWSRolesRef { #hypershift.openshift.io/v1beta1.AWSRolesRef }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSRolesRef contains references to various AWS IAM roles required for operators to make calls against the AWS API.
The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:</p>
<pre><code>{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Federated&quot;: &quot;{{ .ProviderARN }}&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRoleWithWebIdentity&quot;,
&quot;Condition&quot;: {
&quot;StringEquals&quot;: {
&quot;{{ .ProviderName }}:sub&quot;: {{ .ServiceAccounts }}
}
}
}
]
}
</code></pre>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ingressARN is an ARN value referencing a role appropriate for the Ingress Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:aws:route53:::PUBLIC_ZONE_ID&rdquo;,
&ldquo;arn:aws:route53:::PRIVATE_ZONE_ID&rdquo;
]
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryARN</code></br>
<em>
string
</em>
</td>
<td>
<p>imageRegistryARN is an ARN value referencing a role appropriate for the Image Registry Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;s3:CreateBucket&rdquo;,
&ldquo;s3:DeleteBucket&rdquo;,
&ldquo;s3:PutBucketTagging&rdquo;,
&ldquo;s3:GetBucketTagging&rdquo;,
&ldquo;s3:PutBucketPublicAccessBlock&rdquo;,
&ldquo;s3:GetBucketPublicAccessBlock&rdquo;,
&ldquo;s3:PutEncryptionConfiguration&rdquo;,
&ldquo;s3:GetEncryptionConfiguration&rdquo;,
&ldquo;s3:PutLifecycleConfiguration&rdquo;,
&ldquo;s3:GetLifecycleConfiguration&rdquo;,
&ldquo;s3:GetBucketLocation&rdquo;,
&ldquo;s3:ListBucket&rdquo;,
&ldquo;s3:GetObject&rdquo;,
&ldquo;s3:PutObject&rdquo;,
&ldquo;s3:DeleteObject&rdquo;,
&ldquo;s3:ListBucketMultipartUploads&rdquo;,
&ldquo;s3:AbortMultipartUpload&rdquo;,
&ldquo;s3:ListMultipartUploadParts&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>storageARN</code></br>
<em>
string
</em>
</td>
<td>
<p>storageARN is an ARN value referencing a role appropriate for the Storage Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:CreateSnapshot&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:DeleteSnapshot&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeSnapshots&rdquo;,
&ldquo;ec2:DescribeTags&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DescribeVolumesModifications&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>networkARN</code></br>
<em>
string
</em>
</td>
<td>
<p>networkARN is an ARN value referencing a role appropriate for the Network Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInstanceStatus&rdquo;,
&ldquo;ec2:DescribeInstanceTypes&rdquo;,
&ldquo;ec2:UnassignPrivateIpAddresses&rdquo;,
&ldquo;ec2:AssignPrivateIpAddresses&rdquo;,
&ldquo;ec2:UnassignIpv6Addresses&rdquo;,
&ldquo;ec2:AssignIpv6Addresses&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerARN</code></br>
<em>
string
</em>
</td>
<td>
<p>kubeCloudControllerARN is an ARN value referencing a role appropriate for the KCM/KCC.
Source: <a href="https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies">https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies</a></p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;autoscaling:DescribeAutoScalingGroups&rdquo;,
&ldquo;autoscaling:DescribeLaunchConfigurations&rdquo;,
&ldquo;autoscaling:DescribeTags&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeRegions&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;,
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:DeleteRoute&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:AttachLoadBalancerToSubnets&rdquo;,
&ldquo;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerPolicy&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:ConfigureHealthCheck&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:DetachLoadBalancerFromSubnets&rdquo;,
&ldquo;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:ModifyLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:RegisterInstancesWithLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:CreateListener&rdquo;,
&ldquo;elasticloadbalancing:CreateTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeleteListener&rdquo;,
&ldquo;elasticloadbalancing:DeleteTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeregisterTargets&rdquo;,
&ldquo;elasticloadbalancing:DescribeListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerPolicies&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetGroups&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetHealth&rdquo;,
&ldquo;elasticloadbalancing:ModifyListener&rdquo;,
&ldquo;elasticloadbalancing:ModifyTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:RegisterTargets&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&rdquo;,
&ldquo;iam:CreateServiceLinkedRole&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;*&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementARN</code></br>
<em>
string
</em>
</td>
<td>
<p>nodePoolManagementARN is an ARN value referencing a role appropriate for the CAPI Controller.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;ec2:AssociateRouteTable&rdquo;,
&ldquo;ec2:AttachInternetGateway&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateInternetGateway&rdquo;,
&ldquo;ec2:CreateNatGateway&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:CreateRouteTable&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateSubnet&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:DeleteInternetGateway&rdquo;,
&ldquo;ec2:DeleteNatGateway&rdquo;,
&ldquo;ec2:DeleteRouteTable&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteSubnet&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DescribeAccountAttributes&rdquo;,
&ldquo;ec2:DescribeAddresses&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInternetGateways&rdquo;,
&ldquo;ec2:DescribeNatGateways&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;ec2:DescribeVpcAttribute&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DetachInternetGateway&rdquo;,
&ldquo;ec2:DisassociateRouteTable&rdquo;,
&ldquo;ec2:DisassociateAddress&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:ModifySubnetAttribute&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RunInstances&rdquo;,
&ldquo;ec2:TerminateInstances&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;ec2:CreateLaunchTemplate&rdquo;,
&ldquo;ec2:CreateLaunchTemplateVersion&rdquo;,
&ldquo;ec2:DescribeLaunchTemplates&rdquo;,
&ldquo;ec2:DescribeLaunchTemplateVersions&rdquo;,
&ldquo;ec2:DeleteLaunchTemplate&rdquo;,
&ldquo;ec2:DeleteLaunchTemplateVersions&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;<em>&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Condition&rdquo;: {
&ldquo;StringLike&rdquo;: {
&ldquo;iam:AWSServiceName&rdquo;: &ldquo;elasticloadbalancing.amazonaws.com&rdquo;
}
},
&ldquo;Action&rdquo;: [
&ldquo;iam:CreateServiceLinkedRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Action&rdquo;: [
&ldquo;iam:PassRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/</em>-worker-role&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt&rdquo;,
&ldquo;kms:GenerateDataKeyWithoutPlainText&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;<em>&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:CreateGrant&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;</em>&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;Bool&rdquo;: {
&ldquo;kms:GrantIsForAWSResource&rdquo;: true
}
}
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneOperatorARN</code></br>
<em>
string
</em>
</td>
<td>
<p>controlPlaneOperatorARN  is an ARN value referencing a role appropriate for the Control Plane Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;arn:aws:route53:::%s&rdquo;
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AWSServiceEndpoint { #hypershift.openshift.io/v1beta1.AWSServiceEndpoint }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSServiceEndpoint stores the configuration for services to
override existing defaults of AWS Services.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of the AWS service.
This must be provided and cannot be empty.</p>
</td>
</tr>
<tr>
<td>
<code>url</code></br>
<em>
string
</em>
</td>
<td>
<p>url is fully qualified URI with scheme https, that overrides the default generated
endpoint for a client.
This must be provided and cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###AWSSharedVPC { #hypershift.openshift.io/v1beta1.AWSSharedVPC }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">AWSPlatformSpec</a>)
</p>
<p>
<p>AWSSharedVPC contains fields needed to create a HostedCluster using a VPC that has been
created and shared from a different AWS account than the AWS account where the cluster
is getting created.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rolesRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef">
AWSSharedVPCRolesRef
</a>
</em>
</td>
<td>
<p>rolesRef contains references to roles in the VPC owner account that enable a
HostedCluster on a shared VPC.</p>
</td>
</tr>
<tr>
<td>
<code>localZoneID</code></br>
<em>
string
</em>
</td>
<td>
<p>localZoneID is the ID of the route53 hosted zone for [cluster-name].hypershift.local that is
associated with the HostedCluster&rsquo;s VPC and exists in the VPC owner account.</p>
</td>
</tr>
</tbody>
</table>
###AWSSharedVPCRolesRef { #hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSSharedVPC">AWSSharedVPC</a>)
</p>
<p>
<p>AWSSharedVPCRolesRef contains references to AWS IAM roles required for a shared VPC hosted cluster.
These roles must exist in the VPC owner&rsquo;s account.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code></br>
<em>
string
</em>
</td>
<td>
<p>ingressARN is an ARN value referencing the role in the VPC owner account that allows the
ingress operator in the cluster account to create and manage records in the private DNS
hosted zone.</p>
<p>The referenced role must have a trust relationship that allows it to be assumed by the
ingress operator role in the VPC creator account.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Sid&rdquo;: &ldquo;Statement1&rdquo;,
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;AWS&rdquo;: &ldquo;arn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-openshift-ingress&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRole&rdquo;
}
]
}</p>
<p>The following is an example of the policy document for this role.
(Based on <a href="https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config">https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config</a>)</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;route53:ListHostedZonesByName&rdquo;,
&ldquo;route53:ChangeTagsForResource&rdquo;,
&ldquo;route53:GetAccountLimit&rdquo;,
&ldquo;route53:GetChange&rdquo;,
&ldquo;route53:GetHostedZone&rdquo;,
&ldquo;route53:ListTagsForResource&rdquo;,
&ldquo;route53:UpdateHostedZoneComment&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;tag:UntagResources&rdquo;
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneARN</code></br>
<em>
string
</em>
</td>
<td>
<p>controlPlaneARN is an ARN value referencing the role in the VPC owner account that allows
the control plane operator in the cluster account to create and manage a VPC endpoint, its
corresponding Security Group, and DNS records in the hypershift local hosted zone.</p>
<p>The referenced role must have a trust relationship that allows it to be assumed by the
control plane operator role in the VPC creator account.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Sid&rdquo;: &ldquo;Statement1&rdquo;,
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;AWS&rdquo;: &ldquo;arn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-control-plane-operator&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRole&rdquo;
}
]
}</p>
<p>The following is an example of the policy document for this role.</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
</tbody>
</table>
###AddressPair { #hypershift.openshift.io/v1beta1.AddressPair }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PortSpec">PortSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ipAddress</code></br>
<em>
string
</em>
</td>
<td>
<p>ipAddress is the IP address of the allowed address pair. Depending on
the configuration of Neutron, it may be supported to specify a CIDR
instead of a specific IP address.</p>
</td>
</tr>
</tbody>
</table>
###AgentNodePoolPlatform { #hypershift.openshift.io/v1beta1.AgentNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AgentNodePoolPlatform specifies the configuration of a NodePool when operating
on the Agent platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>agentLabelSelector</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>agentLabelSelector contains labels that must be set on an Agent in order to
be selected for a Machine.</p>
</td>
</tr>
</tbody>
</table>
###AgentPlatformSpec { #hypershift.openshift.io/v1beta1.AgentPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AgentPlatformSpec specifies configuration for agent-based installations.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>agentNamespace</code></br>
<em>
string
</em>
</td>
<td>
<p>agentNamespace is the namespace where to search for Agents for this cluster</p>
</td>
</tr>
</tbody>
</table>
###AllocateNodeCIDRsMode { #hypershift.openshift.io/v1beta1.AllocateNodeCIDRsMode }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>AllocateNodeCIDRsMode specifies whether the KCM manages node CIDR allocation.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disabled&#34;</p></td>
<td><p>AllocateNodeCIDRsDisabled disables node CIDR allocation by the KCM</p>
</td>
</tr><tr><td><p>&#34;Enabled&#34;</p></td>
<td><p>AllocateNodeCIDRsEnabled enables node CIDR allocation by the KCM</p>
</td>
</tr></tbody>
</table>
###AllocationPool { #hypershift.openshift.io/v1beta1.AllocationPool }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SubnetSpec">SubnetSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>start</code></br>
<em>
string
</em>
</td>
<td>
<p>start represents the start of the AllocationPool, that is the lowest IP of the pool.</p>
</td>
</tr>
<tr>
<td>
<code>end</code></br>
<em>
string
</em>
</td>
<td>
<p>end represents the end of the AlloctionPool, that is the highest IP of the pool.</p>
</td>
</tr>
</tbody>
</table>
###AutoNode { #hypershift.openshift.io/v1beta1.AutoNode }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>We expose here internal configuration knobs that won&rsquo;t be exposed to the service.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provisionerConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ProvisionerConfig">
ProvisionerConfig
</a>
</em>
</td>
<td>
<p>provisionerConfig is the implementation used for Node auto provisioning.</p>
</td>
</tr>
</tbody>
</table>
###AvailabilityPolicy { #hypershift.openshift.io/v1beta1.AvailabilityPolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>availabilityPolicy specifies a high level availability policy for components.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;HighlyAvailable&#34;</p></td>
<td><p>HighlyAvailable means components should be resilient to problems across
fault boundaries as defined by the component to which the policy is
attached. This usually means running critical workloads with 3 replicas and
with little or no toleration of disruption of the component.</p>
</td>
</tr><tr><td><p>&#34;SingleReplica&#34;</p></td>
<td><p>SingleReplica means components are not expected to be resilient to problems
across most fault boundaries associated with high availability. This
usually means running critical workloads with just 1 replica and with
toleration of full disruption of the component.</p>
</td>
</tr></tbody>
</table>
###AzureAuthenticationConfiguration { #hypershift.openshift.io/v1beta1.AzureAuthenticationConfiguration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzurePlatformSpec">AzurePlatformSpec</a>)
</p>
<p>
<p>azureAuthenticationConfiguration is a discriminated union type that contains the Azure authentication configuration
for a Hosted Cluster. This configuration is used to determine how the Hosted Cluster authenticates with Azure&rsquo;s API,
either with managed identities or workload identities.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>azureAuthenticationConfigType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureAuthenticationType">
AzureAuthenticationType
</a>
</em>
</td>
<td>
<p>azureAuthenticationConfigType is the type of identity configuration used in the Hosted Cluster. This field is
used to determine which identity configuration is being used. Valid values are &ldquo;ManagedIdentities&rdquo; and
&ldquo;WorkloadIdentities&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>managedIdentities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities">
AzureResourceManagedIdentities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>managedIdentities contains the managed identities needed for HCP control plane and data plane components that
authenticate with Azure&rsquo;s API.</p>
<p>These are required for managed Azure, also known as ARO HCP.</p>
</td>
</tr>
<tr>
<td>
<code>workloadIdentities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureWorkloadIdentities">
AzureWorkloadIdentities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>workloadIdentities is a struct of client IDs for each component that needs to authenticate with Azure&rsquo;s API in
self-managed Azure. These client IDs are used to authenticate with Azure cloud on both the control plane and data
plane.</p>
<p>This is required for self-managed Azure.</p>
</td>
</tr>
</tbody>
</table>
###AzureAuthenticationType { #hypershift.openshift.io/v1beta1.AzureAuthenticationType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureAuthenticationConfiguration">AzureAuthenticationConfiguration</a>)
</p>
<p>
<p>AzureAuthenticationType is a discriminated union type that contains the Azure authentication configuration for an
Azure Hosted Cluster. This type is used to determine which authentication configuration is being used. Valid values
are &ldquo;ManagedIdentities&rdquo; and &ldquo;WorkloadIdentities&rdquo;.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;ManagedIdentities&#34;</p></td>
<td><p>&ldquo;ManagedIdentities&rdquo; means that the Hosted Cluster is using managed identities to authenticate with Azure&rsquo;s API.
This is only valid for managed Azure, also known as ARO HCP.</p>
</td>
</tr><tr><td><p>&#34;WorkloadIdentities&#34;</p></td>
<td><p>&ldquo;WorkloadIdentities&rdquo; means that the Hosted Cluster is using workload identities to authenticate with Azure&rsquo;s API.
This is only valid for self-managed Azure.</p>
</td>
</tr></tbody>
</table>
###AzureClientID { #hypershift.openshift.io/v1beta1.AzureClientID }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">ManagedIdentity</a>, 
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">WorkloadIdentity</a>)
</p>
<p>
<p>AzureClientID is a string that represents the client ID of a managed identity.</p>
</p>
###AzureDiagnosticsStorageAccountType { #hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">Diagnostics</a>)
</p>
<p>
<p>AzureDiagnosticsStorageAccountType specifies the type of storage account for storing Azure VM diagnostics data.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disabled&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Managed&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;UserManaged&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###AzureDiskPersistence { #hypershift.openshift.io/v1beta1.AzureDiskPersistence }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk">AzureNodePoolOSDisk</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Ephemeral&#34;</p></td>
<td><p>EphemeralDiskPersistence is the ephemeral disk type.</p>
</td>
</tr><tr><td><p>&#34;Persistent&#34;</p></td>
<td><p>PersistentDiskPersistence is the persistent disk type.</p>
</td>
</tr></tbody>
</table>
###AzureDiskStorageAccountType { #hypershift.openshift.io/v1beta1.AzureDiskStorageAccountType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk">AzureNodePoolOSDisk</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Premium_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesPremiumLRS - Premium SSD locally redundant storage. Best for production and performance sensitive
workloads.</p>
</td>
</tr><tr><td><p>&#34;PremiumV2_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesPremiumV2LRS - Premium SSD v2 locally redundant storage. Best for production and performance-sensitive
workloads that consistently require low latency and high IOPS and throughput.</p>
</td>
</tr><tr><td><p>&#34;Standard_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesStandardLRS - Standard HDD locally redundant storage. Best for backup, non-critical, and infrequent
access.</p>
</td>
</tr><tr><td><p>&#34;StandardSSD_LRS&#34;</p></td>
<td><p>DiskStorageAccountTypesStandardSSDLRS - Standard SSD locally redundant storage. Best for web servers, lightly used enterprise
applications and dev/test.</p>
</td>
</tr></tbody>
</table>
###AzureKMSKey { #hypershift.openshift.io/v1beta1.AzureKMSKey }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">AzureKMSSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>keyVaultName</code></br>
<em>
string
</em>
</td>
<td>
<p>keyVaultName is the name of the keyvault. Must match criteria specified at <a href="https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name">https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name</a>
Your Microsoft Entra application used to create the cluster must be authorized to access this keyvault, e.g using the AzureCLI:
<code>az keyvault set-policy -n $KEYVAULT_NAME --key-permissions decrypt encrypt --spn &lt;YOUR APPLICATION CLIENT ID&gt;</code></p>
</td>
</tr>
<tr>
<td>
<code>keyName</code></br>
<em>
string
</em>
</td>
<td>
<p>keyName is the name of the keyvault key used for encrypt/decrypt</p>
</td>
</tr>
<tr>
<td>
<code>keyVersion</code></br>
<em>
string
</em>
</td>
<td>
<p>keyVersion contains the version of the key to use</p>
</td>
</tr>
</tbody>
</table>
###AzureKMSSpec { #hypershift.openshift.io/v1beta1.AzureKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>AzureKMSSpec defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>activeKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSKey">
AzureKMSKey
</a>
</em>
</td>
<td>
<p>activeKey defines the active key used to encrypt new secrets</p>
</td>
</tr>
<tr>
<td>
<code>backupKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSKey">
AzureKMSKey
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>backupKey defines the old key during the rotation process so previously created
secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>
</td>
</tr>
<tr>
<td>
<code>kms</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>kms is a pre-existing managed identity used to authenticate with Azure KMS.</p>
</td>
</tr>
</tbody>
</table>
###AzureMarketplaceImage { #hypershift.openshift.io/v1beta1.AzureMarketplaceImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">AzureVMImage</a>)
</p>
<p>
<p>AzureMarketplaceImage specifies the information needed to create an Azure VM from an Azure Marketplace image.
This struct supports two usage patterns:
1. Specify only imageGeneration to use marketplace defaults from the release payload (HyperShift will select the appropriate image)
2. Specify publisher, offer, sku, and version to use an explicit marketplace image (with optional imageGeneration)</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>imageGeneration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImageGeneration">
AzureVMImageGeneration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageGeneration specifies the Hyper-V generation of the Azure Marketplace image to use for the nodes.
This field is used by HyperShift to select the appropriate marketplace image (Gen1 or Gen2)
from the release payload metadata when publisher, offer, sku, and version are not explicitly provided.
It is not passed to CAPZ (Cluster API Provider Azure); the generation information is
encoded into the SKU field that CAPZ uses.
Valid values are Gen1 and Gen2. If unspecified, defaults to Gen2.</p>
</td>
</tr>
<tr>
<td>
<code>publisher</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>publisher is the name of the organization that created the image.
It must be between 3 and 50 characters in length, and consist of only lowercase letters, numbers, and hyphens (-) and underscores (_).
It must start with a lowercase letter or a number.
TODO: Can we explain where a user might find this value, or provide an example of one they might want to use</p>
</td>
</tr>
<tr>
<td>
<code>offer</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>offer specifies the name of a group of related images created by the publisher.
TODO: What is the valid character set for this field? What about minimum and maximum lengths?</p>
</td>
</tr>
<tr>
<td>
<code>sku</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>sku specifies an instance of an offer, such as a major release of a distribution.
For example, 22<em>04-lts-gen2, 8-lvm-gen2.
The value must consist only of lowercase letters, numbers, and hyphens (-) and underscores (</em>).
TODO: What about length limits?</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>version specifies the version of an image sku. The allowed formats are Major.Minor.Build or &lsquo;latest&rsquo;. Major,
Minor, and Build are decimal numbers, e.g. &lsquo;1.2.0&rsquo;. Specify &lsquo;latest&rsquo; to use the latest version of an image available at
deployment time. Even if you use &lsquo;latest&rsquo;, the VM image will not automatically update after deploy time even if a
new version becomes available.</p>
</td>
</tr>
</tbody>
</table>
###AzureNodePoolOSDisk { #hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>sizeGiB</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>sizeGiB is the size in GiB (1024^3 bytes) to assign to the OS disk.
This should be between 16 and 32,767.
When not set, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.
The current default is 30.</p>
</td>
</tr>
<tr>
<td>
<code>diskStorageAccountType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiskStorageAccountType">
AzureDiskStorageAccountType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>diskStorageAccountType is the disk storage account type to use.
Valid values are Premium_LRS, PremiumV2_LRS, Standard_LRS, StandardSSD_LRS.
Note that Standard means a HDD.
UltraSSD_LRS is not supported for OS disks (see <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison">https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison</a>).
The disk performance is tied to the disk type, please refer to the Azure documentation for further details
<a href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison">https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison</a>.
When omitted this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.
The current default is Premium SSD LRS.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionSetID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>encryptionSetID is the ID of the DiskEncryptionSet resource to use to encrypt the OS disks for the VMs.
Configuring a DiskEncyptionSet allows greater control over the encryption of the VM OS disk at rest.
Can be used with either platform (Azure) managed, or customer managed encryption keys.
This needs to exist in the same subscription id listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.SubscriptionID.
DiskEncryptionSetID should also exist in a resource group under the same subscription id and the same location
listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.Location.
The encryptionSetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Copmute/diskEncryptionSets/{resourceName}</code>.
The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12.
The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character.
The resourceName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores.
TODO: Are there other encryption related options we may want to expose, should this be in a struct as well?</p>
</td>
</tr>
<tr>
<td>
<code>persistence</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiskPersistence">
AzureDiskPersistence
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>persistence determines whether the OS disk should be persisted beyond the life of the VM.
Valid values are Persistent and Ephemeral.
When set to Ephmeral, the OS disk will not be persisted to Azure storage and implies restrictions to the VM size and caching type.
Full details can be found in the Azure documentation <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ephemeral-os-disks">https://learn.microsoft.com/en-us/azure/virtual-machines/ephemeral-os-disks</a>.
Ephmeral disks are primarily used for stateless applications, provide lower latency than Persistent disks and also incur no storage costs.
When not set, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.</p>
</td>
</tr>
</tbody>
</table>
###AzureNodePoolPlatform { #hypershift.openshift.io/v1beta1.AzureNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>AzureNodePoolPlatform is the platform specific configuration for an Azure node pool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>vmSize</code></br>
<em>
string
</em>
</td>
<td>
<p>vmSize is the Azure VM instance type to use for the nodes being created in the nodepool.
The size naming convention is documented here <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions">https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions</a>.
Size names should start with a Family name, which is represented by one of more capital letters, and then be followed by the CPU count.
This is followed by 0 or more additional features, represented by a, b, d, i, l, m, p, t, s, C, and NP, refer to the Azure documentation for an explanation of these features.
Optionally an accelerator such as a GPU can be added, prefixed by an underscore, for example A100, H100 or MI300X.
The size may also be versioned, in which case it should be suffixed with _v<version> where the version is a number.
For example, &ldquo;D32ads_v5&rdquo; would be a suitable general purpose VM size, or &ldquo;ND96_MI300X_v5&rdquo; would represent a GPU accelerated VM.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">
AzureVMImage
</a>
</em>
</td>
<td>
<p>image is used to configure the VM boot image. If unset, the default image at the location below will be used and
is expected to exist: subscription/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Compute/images/rhcos.x86_64.vhd.
The <subscriptionID> and the <resourceGroupName> are expected to be the same resource group documented in the
Hosted Cluster specification respectively, HostedCluster.Spec.Platform.Azure.SubscriptionID and
HostedCluster.Spec.Platform.Azure.ResourceGroupName.</p>
</td>
</tr>
<tr>
<td>
<code>osDisk</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk">
AzureNodePoolOSDisk
</a>
</em>
</td>
<td>
<p>osDisk provides configuration for the OS disk for the nodepool.
This can be used to configure the size, storage account type, encryption options and whether the disk is persistent or ephemeral.
When not provided, the platform will choose reasonable defaults which are subject to change over time.
Review the fields within the osDisk for more details.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>availabilityZone is the failure domain identifier where the VM should be attached to. This must not be specified
for clusters in a location that does not support AvailabilityZone because it would cause a failure from Azure API.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionAtHost</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>encryptionAtHost enables encryption at host on virtual machines. According to Microsoft documentation, this
means data stored on the VM host is encrypted at rest and flows encrypted to the Storage service. See
<a href="https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-host-based-encryption-portal?tabs=azure-powershell">https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-host-based-encryption-portal?tabs=azure-powershell</a>
for more information.</p>
</td>
</tr>
<tr>
<td>
<code>subnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>subnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a
different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must
exist in the same network, HostedCluster.Spec.Platform.Azure.VnetID, and must exist under the same subscription ID,
HostedCluster.Spec.Platform.Azure.SubscriptionID.
subnetID is immutable once set.
The subnetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}</code>.
The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12.
The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character.
The vnetName should be between 2 and 64 characters, consisting only of alphanumeric characters, hyphens, underscores and periods and must not end with either a period (.) or hyphen (-) character.
The subnetName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores and must start with an alphanumeric character and must not end with a period (.) or hyphen (-) character.</p>
</td>
</tr>
<tr>
<td>
<code>diagnostics</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">
Diagnostics
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>diagnostics specifies the diagnostics settings for a virtual machine.
If not specified, then Boot diagnostics will be disabled.</p>
</td>
</tr>
</tbody>
</table>
###AzurePlatformSpec { #hypershift.openshift.io/v1beta1.AzurePlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>AzurePlatformSpec specifies configuration for clusters running on Azure. Generally, the HyperShift API assumes bring
your own (BYO) cloud infrastructure resources. For example, resources like a resource group, a subnet, or a vnet
would be pre-created and then their names would be used respectively in the ResourceGroupName, SubnetName, VnetName
fields of the Hosted Cluster CR. An existing cloud resource is expected to exist under the same SubscriptionID.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cloud</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>cloud is the cloud environment identifier, valid values could be found here: <a href="https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33">https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33</a></p>
</td>
</tr>
<tr>
<td>
<code>location</code></br>
<em>
string
</em>
</td>
<td>
<p>location is the Azure region in where all the cloud infrastructure resources will be created.</p>
<p>Example: eastus</p>
</td>
</tr>
<tr>
<td>
<code>resourceGroup</code></br>
<em>
string
</em>
</td>
<td>
<p>resourceGroup is the name of an existing resource group where all cloud resources created by the Hosted
Cluster are to be placed. The resource group is expected to exist under the same subscription as SubscriptionID.</p>
<p>In ARO HCP, this will be the managed resource group where customer cloud resources will be created.</p>
<p>Resource group naming requirements can be found here: <a href="https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/">https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/</a>.</p>
<p>Example: if your resource group ID is /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>, your
ResourceGroupName is <resourceGroupName>.</p>
</td>
</tr>
<tr>
<td>
<code>vnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>vnetID is the ID of an existing VNET to use in creating VMs. The VNET can exist in a different resource group
other than the one specified in ResourceGroupName, but it must exist under the same subscription as
SubscriptionID.</p>
<p>In ARO HCP, this will be the ID of the customer provided VNET.</p>
<p>Example: /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Network/virtualNetworks/<vnetName></p>
</td>
</tr>
<tr>
<td>
<code>subnetID</code></br>
<em>
string
</em>
</td>
<td>
<p>subnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a
different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must
exist in the same network, HostedCluster.Spec.Platform.Azure.VnetID, and must exist under the same subscription ID,
HostedCluster.Spec.Platform.Azure.SubscriptionID.
subnetID is immutable once set.
The subnetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}</code>.
The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12.
The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character.
The vnetName should be between 2 and 64 characters, consisting only of alphanumeric characters, hyphens, underscores and periods and must not end with either a period (.) or hyphen (-) character.
The subnetName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores and must start with an alphanumeric character and must not end with a period (.) or hyphen (-) character.</p>
</td>
</tr>
<tr>
<td>
<code>subscriptionID</code></br>
<em>
string
</em>
</td>
<td>
<p>subscriptionID is a unique identifier for an Azure subscription used to manage resources.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupID</code></br>
<em>
string
</em>
</td>
<td>
<p>securityGroupID is the ID of an existing security group on the SubnetID. This field is provided as part of the
configuration for the Azure cloud provider, aka Azure cloud controller manager (CCM). This security group is
expected to exist under the same subscription as SubscriptionID.</p>
</td>
</tr>
<tr>
<td>
<code>azureAuthenticationConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureAuthenticationConfiguration">
AzureAuthenticationConfiguration
</a>
</em>
</td>
<td>
<p>azureAuthenticationConfig is the type of Azure authentication configuration to use to authenticate with Azure&rsquo;s
Cloud API.</p>
</td>
</tr>
<tr>
<td>
<code>tenantID</code></br>
<em>
string
</em>
</td>
<td>
<p>tenantID is a unique identifier for the tenant where Azure resources will be created and managed in.</p>
</td>
</tr>
</tbody>
</table>
###AzureResourceManagedIdentities { #hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureAuthenticationConfiguration">AzureAuthenticationConfiguration</a>)
</p>
<p>
<p>AzureResourceManagedIdentities contains the managed identities needed for HCP control plane and data plane components
that authenticate with Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>controlPlane</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities">
ControlPlaneManagedIdentities
</a>
</em>
</td>
<td>
<p>controlPlane contains the client IDs of all the managed identities on the HCP control plane needing to
authenticate with Azure&rsquo;s API.</p>
</td>
</tr>
<tr>
<td>
<code>dataPlane</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DataPlaneManagedIdentities">
DataPlaneManagedIdentities
</a>
</em>
</td>
<td>
<p>dataPlane contains the client IDs of all the managed identities on the data plane needing to authenticate with
Azure&rsquo;s API.</p>
</td>
</tr>
</tbody>
</table>
###AzureVMImage { #hypershift.openshift.io/v1beta1.AzureVMImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
<p>AzureVMImage represents the different types of boot image sources that can be provided for an Azure VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImageType">
AzureVMImageType
</a>
</em>
</td>
<td>
<p>type is the type of image data that will be provided to the Azure VM.
Valid values are &ldquo;ImageID&rdquo; and &ldquo;AzureMarketplace&rdquo;.
ImageID means is used for legacy managed VM images. This is where the user uploads a VM image directly to their resource group.
AzureMarketplace means the VM will boot from an Azure Marketplace image.
Marketplace images are preconfigured and published by the OS vendors and may include preconfigured software for the VM.
When Type is &ldquo;AzureMarketplace&rdquo;, you can either:
1. Specify only imageGeneration to use marketplace defaults from the release payload
2. Specify publisher, offer, sku, and version to use an explicit marketplace image
3. Specify all fields (imageGeneration along with publisher, offer, sku, version)</p>
</td>
</tr>
<tr>
<td>
<code>imageID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageID is the Azure resource ID of a VHD image to use to boot the Azure VMs from.
TODO: What is the valid character set for this field? What about minimum and maximum lengths?</p>
</td>
</tr>
<tr>
<td>
<code>azureMarketplace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureMarketplaceImage">
AzureMarketplaceImage
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>azureMarketplace contains the Azure Marketplace image info to use to boot the Azure VMs from.</p>
</td>
</tr>
</tbody>
</table>
###AzureVMImageGeneration { #hypershift.openshift.io/v1beta1.AzureVMImageGeneration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureMarketplaceImage">AzureMarketplaceImage</a>)
</p>
<p>
<p>AzureVMImageGeneration represents the Hyper-V generation of an Azure VM image.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Gen1&#34;</p></td>
<td><p>Gen1 represents Hyper-V Generation 1 VMs</p>
</td>
</tr><tr><td><p>&#34;Gen2&#34;</p></td>
<td><p>Gen2 represents Hyper-V Generation 2 VMs</p>
</td>
</tr></tbody>
</table>
###AzureVMImageType { #hypershift.openshift.io/v1beta1.AzureVMImageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureVMImage">AzureVMImage</a>)
</p>
<p>
<p>AzureVMImageType is used to specify the source of the Azure VM boot image.
Valid values are ImageID and AzureMarketplace.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AzureMarketplace&#34;</p></td>
<td><p>AzureMarketplace is used to specify the Azure Marketplace image info to use to boot the Azure VMs from.</p>
</td>
</tr><tr><td><p>&#34;ImageID&#34;</p></td>
<td><p>ImageID is the used to specify that an Azure resource ID of a VHD image is used to boot the Azure VMs from.</p>
</td>
</tr></tbody>
</table>
###AzureWorkloadIdentities { #hypershift.openshift.io/v1beta1.AzureWorkloadIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureAuthenticationConfiguration">AzureAuthenticationConfiguration</a>)
</p>
<p>
<p>AzureWorkloadIdentities is a struct that contains the client IDs of all the managed identities in self-managed Azure
needing to authenticate with Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>imageRegistry</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>imageRegistry is the client ID of a federated managed identity, associated with cluster-image-registry-operator, used in
workload identity authentication.</p>
</td>
</tr>
<tr>
<td>
<code>ingress</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>ingress is the client ID of a federated managed identity, associated with cluster-ingress-operator, used in
workload identity authentication.</p>
</td>
</tr>
<tr>
<td>
<code>file</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>file is the client ID of a federated managed identity, associated with cluster-storage-operator-file,
used in workload identity authentication.</p>
</td>
</tr>
<tr>
<td>
<code>disk</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>disk is the client ID of a federated managed identity, associated with cluster-storage-operator-disk,
used in workload identity authentication.</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>nodePoolManagement is the client ID of a federated managed identity, associated with cluster-api-provider-azure, used
in workload identity authentication.</p>
</td>
</tr>
<tr>
<td>
<code>cloudProvider</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>cloudProvider is the client ID of a federated managed identity, associated with azure-cloud-provider, used in
workload identity authentication.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.WorkloadIdentity">
WorkloadIdentity
</a>
</em>
</td>
<td>
<p>network is the client ID of a federated managed identity, associated with cluster-network-operator, used in
workload identity authentication.</p>
</td>
</tr>
</tbody>
</table>
###CIDRBlock { #hypershift.openshift.io/v1beta1.CIDRBlock }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.APIServerNetworking">APIServerNetworking</a>)
</p>
<p>
</p>
###Capabilities { #hypershift.openshift.io/v1beta1.Capabilities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>capabilities allows enabling or disabling optional components at install time.
When this is not supplied, the cluster will use the DefaultCapabilitySet defined for the respective
OpenShift version, minus the baremetal capability.
Once set, it cannot be changed.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enabled</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OptionalCapability">
[]OptionalCapability
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>enabled when specified, explicitly enables the specified capabilit√≠es on the hosted cluster.
Once set, this field cannot be changed.</p>
</td>
</tr>
<tr>
<td>
<code>disabled</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OptionalCapability">
[]OptionalCapability
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>disabled when specified, explicitly disables the specified capabilit√≠es on the hosted cluster.
Once set, this field cannot be changed.</p>
<p>Note: Disabling &lsquo;openshift-samples&rsquo;,&lsquo;Insights&rsquo;, &lsquo;Console&rsquo;, &lsquo;NodeTuning&rsquo;, &lsquo;Ingress&rsquo; are only supported in OpenShift versions 4.20 and above.</p>
</td>
</tr>
</tbody>
</table>
###CapacityReservationOptions { #hypershift.openshift.io/v1beta1.CapacityReservationOptions }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlacementOptions">PlacementOptions</a>)
</p>
<p>
<p>CapacityReservationOptions specifies Capacity Reservation options for the NodePool instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>id specifies the target Capacity Reservation into which the EC2 instances should be launched.
Must follow the format: cr- followed by 17 lowercase hexadecimal characters. For example: cr-0123456789abcdef0
When empty, no specific Capacity Reservation is targeted.</p>
<p>When specified, preference cannot be set to &lsquo;None&rsquo; or &lsquo;Open&rsquo; as these
are mutually exclusive with targeting a specific reservation. Use preference &lsquo;CapacityReservationsOnly&rsquo;
or omit preference field when targeting a specific reservation.</p>
</td>
</tr>
<tr>
<td>
<code>marketType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MarketType">
MarketType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>marketType specifies the market type of the CapacityReservation for the EC2 instances. Valid values are OnDemand, CapacityBlocks and omitted:
- &ldquo;OnDemand&rdquo;: EC2 instances run as standard On-Demand instances.
- &ldquo;CapacityBlocks&rdquo;: scheduled pre-purchased compute capacity. Capacity Blocks is recommended when GPUs are needed to support ML workloads.
When omitted, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.
The current default value is CapacityBlocks.</p>
<p>When set to &lsquo;CapacityBlocks&rsquo;, a specific Capacity Reservation ID must be provided.</p>
</td>
</tr>
<tr>
<td>
<code>preference</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CapacityReservationPreference">
CapacityReservationPreference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>preference specifies the preference for use of Capacity Reservations by the instance. Valid values include:
- &ldquo;&rdquo;: No preference (platform default)
- &ldquo;Open&rdquo;: The instance may make use of open Capacity Reservations that match its AZ and InstanceType
- &ldquo;None&rdquo;: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads
- &ldquo;CapacityReservationsOnly&rdquo;: The instance will only run if matched or targeted to a Capacity Reservation</p>
<p>Cannot be set to &lsquo;None&rsquo; or &lsquo;Open&rsquo; when a specific Capacity Reservation ID is provided,
as targeting a specific reservation is mutually exclusive with these general preference settings.</p>
</td>
</tr>
</tbody>
</table>
###CapacityReservationPreference { #hypershift.openshift.io/v1beta1.CapacityReservationPreference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CapacityReservationOptions">CapacityReservationOptions</a>)
</p>
<p>
<p>CapacityReservationPreference describes the preferred use of capacity reservations
of an instance</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;None&#34;</p></td>
<td><p>CapacityReservationPreferenceNone the instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads</p>
</td>
</tr><tr><td><p>&#34;CapacityReservationsOnly&#34;</p></td>
<td><p>CapacityReservationPreferenceOnly the instance will only run if matched or targeted to a Capacity Reservation</p>
</td>
</tr><tr><td><p>&#34;Open&#34;</p></td>
<td><p>CapacityReservationPreferenceOpen the instance may make use of open Capacity Reservations that match its AZ and InstanceType.</p>
</td>
</tr></tbody>
</table>
###CertificateSigningRequestApprovalSpec { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval">CertificateSigningRequestApproval</a>)
</p>
<p>
<p>CertificateSigningRequestApprovalSpec defines the desired state of CertificateSigningRequestApproval</p>
</p>
###CertificateSigningRequestApprovalStatus { #hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval">CertificateSigningRequestApproval</a>)
</p>
<p>
<p>CertificateSigningRequestApprovalStatus defines the observed state of CertificateSigningRequestApproval</p>
</p>
###ClusterAutoscaling { #hypershift.openshift.io/v1beta1.ClusterAutoscaling }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterAutoscaling specifies auto-scaling behavior that applies to all
NodePools associated with a control plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>scaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ScalingType">
ScalingType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>scaling defines the scaling behavior for the cluster autoscaler.
ScaleUpOnly means the autoscaler will only scale up nodes, never scale down.
ScaleUpAndScaleDown means the autoscaler will both scale up and scale down nodes.
When set to ScaleUpAndScaleDown, the scaleDown field can be used to configure scale down behavior.</p>
<p>Note: This field is only supported in OpenShift versions 4.19 and above.</p>
</td>
</tr>
<tr>
<td>
<code>scaleDown</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ScaleDownConfig">
ScaleDownConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>scaleDown configures the behavior of the Cluster Autoscaler scale down operation.
This field is only valid when scaling is set to ScaleUpAndScaleDown.</p>
</td>
</tr>
<tr>
<td>
<code>balancingIgnoredLabels</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>balancingIgnoredLabels sets &ldquo;&ndash;balancing-ignore-label <label name>&rdquo; flag on cluster-autoscaler for each listed label.
This option specifies labels that cluster autoscaler should ignore when considering node group similarity.
For example, if you have nodes with &ldquo;topology.ebs.csi.aws.com/zone&rdquo; label, you can add name of this label here
to prevent cluster autoscaler from splitting nodes into different node groups based on its value.</p>
<p>HyperShift automatically appends platform-specific balancing ignore labels:
- AWS: &ldquo;lifecycle&rdquo;, &ldquo;k8s.amazonaws.com/eniConfig&rdquo;, &ldquo;topology.k8s.aws/zone-id&rdquo;
- Azure: &ldquo;agentpool&rdquo;, &ldquo;kubernetes.azure.com/agentpool&rdquo;
- Common:
- &ldquo;hypershift.openshift.io/nodePool&rdquo;
- &ldquo;topology.ebs.csi.aws.com/zone&rdquo;
- &ldquo;topology.disk.csi.azure.com/zone&rdquo;
- &ldquo;ibm-cloud.kubernetes.io/worker-id&rdquo;
- &ldquo;vpc-block-csi-driver-labels&rdquo;
These labels are added by default and do not need to be manually specified.</p>
</td>
</tr>
<tr>
<td>
<code>maxNodesTotal</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxNodesTotal is the maximum allowable number of nodes for the Autoscaler scale out to be operational.
The autoscaler will not grow the cluster beyond this number.
If omitted, the autoscaler will not have a maximum limit.
number.</p>
</td>
</tr>
<tr>
<td>
<code>maxPodGracePeriod</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxPodGracePeriod is the maximum seconds to wait for graceful pod
termination before scaling down a NodePool. The default is 600 seconds.</p>
</td>
</tr>
<tr>
<td>
<code>maxNodeProvisionTime</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxNodeProvisionTime is the maximum time to wait for node provisioning
before considering the provisioning to be unsuccessful, expressed as a Go
duration string. The default is 15 minutes.</p>
</td>
</tr>
<tr>
<td>
<code>maxFreeDifferenceRatioPercent</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxFreeDifferenceRatioPercent sets the maximum difference ratio for free resources between similar node groups. This parameter controls how strict the similarity check is when comparing node groups for load balancing.
The value represents a percentage from 0 to 100.
When set to 0, this means node groups must have exactly the same free resources to be considered similar (no difference allowed).
When set to 100, this means node groups will be considered similar regardless of their free resource differences (any difference allowed).
A value between 0 and 100 represents the maximum allowed difference ratio for free resources between node groups to be considered similar.
When omitted, the autoscaler defaults to 10%.
This affects the &ldquo;&ndash;max-free-difference-ratio&rdquo; flag on cluster-autoscaler.</p>
</td>
</tr>
<tr>
<td>
<code>podPriorityThreshold</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>podPriorityThreshold enables users to schedule &ldquo;best-effort&rdquo; pods, which
shouldn&rsquo;t trigger autoscaler actions, but only run when there are spare
resources available. The default is -10.</p>
<p>See the following for more details:
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption</a></p>
</td>
</tr>
<tr>
<td>
<code>expanders</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ExpanderString">
[]ExpanderString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>expanders guide the autoscaler in choosing node groups during scale-out.
Sets the order of expanders for scaling out node groups.
Options include:
* LeastWaste - selects the group with minimal idle CPU and memory after scaling.
* Priority - selects the group with the highest user-defined priority.
* Random - selects a group randomly.
If not specified, <code>[Priority, LeastWaste]</code> is the default.
Maximum of 3 expanders can be specified.</p>
</td>
</tr>
</tbody>
</table>
###ClusterConfiguration { #hypershift.openshift.io/v1beta1.ClusterConfiguration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ClusterConfiguration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
<p>The API for individual configuration items is at:
<a href="https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html">https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiServer</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.APIServerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>apiServer holds configuration (like serving certificates, client CA and CORS domains)
shared by all API servers in the system, among them especially kube-apiserver
and openshift-apiserver.</p>
</td>
</tr>
<tr>
<td>
<code>authentication</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.AuthenticationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>authentication specifies cluster-wide settings for authentication (like OAuth and
webhook token authenticators).</p>
</td>
</tr>
<tr>
<td>
<code>featureGate</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.FeatureGateSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>featureGate holds cluster-wide information about feature gates.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.ImageSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>image governs policies related to imagestream imports and runtime configuration
for external registries. It allows cluster admins to configure which registries
OpenShift is allowed to import images from, extra CA trust bundles for external
registries, and policies to block or allow registry hostnames.
When exposing OpenShift&rsquo;s image registry to the public, this also lets cluster
admins specify the external hostname.
This input will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>ingress</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.IngressSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ingress holds cluster-wide information about ingress, including the default ingress domain
used for routes.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.NetworkSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>network holds cluster-wide information about the network. It is used to configure the desired network configuration, such as: IP address pools for services/pod IPs, network plugin, etc.
Please view network.spec for an explanation on what applies when configuring this resource.
TODO (csrwng): Add validation here to exclude changes that conflict with networking settings in the HostedCluster.Spec.Networking field.</p>
</td>
</tr>
<tr>
<td>
<code>oauth</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.OAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>oauth holds cluster-wide information about OAuth.
It is used to configure the integrated OAuth server.
This configuration is only honored when the top level Authentication config has type set to IntegratedOAuth.</p>
</td>
</tr>
<tr>
<td>
<code>operatorhub</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.OperatorHubSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorhub specifies the configuration for the Operator Lifecycle Manager in the HostedCluster. This is only configured at deployment time but the controller are not reconcilling over it.
The OperatorHub configuration will be constantly reconciled if catalog placement is management, but only on cluster creation otherwise.</p>
</td>
</tr>
<tr>
<td>
<code>scheduler</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.SchedulerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>scheduler holds cluster-wide config information to run the Kubernetes Scheduler
and influence its placement decisions. The canonical name for this config is <code>cluster</code>.</p>
</td>
</tr>
<tr>
<td>
<code>proxy</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.ProxySpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>proxy holds cluster-wide information on how to configure default proxies for the cluster.
This affects traffic flowing from the hosted cluster data plane.
The controllers will generate a machineConfig with the proxy config for the cluster.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworkEntry { #hypershift.openshift.io/v1beta1.ClusterNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>ClusterNetworkEntry is a single IP address block for pod IP blocks. IP blocks
are allocated with size 2^HostSubnetLength.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>cidr is the IP block address pool.</p>
</td>
</tr>
<tr>
<td>
<code>hostPrefix</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>hostPrefix is the prefix size to allocate to each node from the CIDR.
For example, 24 would allocate 2^(32-24)=2^8=256 addresses to each node. If this
field is not used by the plugin, it can be left unset.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworkOperatorSpec { #hypershift.openshift.io/v1beta1.ClusterNetworkOperatorSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">OperatorConfiguration</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disableMultiNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>disableMultiNetwork when set to true disables the Multus CNI plugin and related components
in the hosted cluster. This prevents the installation of multus daemon sets in the
guest cluster and the multus-admission-controller in the management cluster.
Default is false (Multus is enabled).
This field is immutable.
This field can only be set to true when NetworkType is &ldquo;Other&rdquo;. Setting it to true
with any other NetworkType will result in a validation error during cluster creation.</p>
</td>
</tr>
<tr>
<td>
<code>ovnKubernetesConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OVNKubernetesConfig">
OVNKubernetesConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ovnKubernetesConfig holds OVN-Kubernetes specific configuration.
This is only consumed when NetworkType is OVNKubernetes.</p>
</td>
</tr>
</tbody>
</table>
###ClusterNetworking { #hypershift.openshift.io/v1beta1.ClusterNetworking }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>clusterNetworking specifies network configuration for a cluster.
All CIDRs must be unique. Additional validation to check for CIDRs overlap and consistent network stack is performed by the controllers.
Failing that validation will result in the HostedCluster being degraded and the validConfiguration condition being false.
TODO this is available in vanilla kube from 1.31 API servers and in Openshift from 4.16.
TODO(alberto): Use CEL cidr library for all these validation when all management clusters are &gt;= 1.31.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>machineNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MachineNetworkEntry">
[]MachineNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>machineNetwork is the list of IP address pools for machines.
This might be used among other things to generate appropriate networking security groups in some clouds providers.
Currently only one entry or two for dual stack is supported.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>clusterNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworkEntry">
[]ClusterNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterNetwork is the list of IP address pools for pods.
Defaults to cidr: &ldquo;10.132.0.0/14&rdquo;.
Currently only one entry is supported.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>serviceNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServiceNetworkEntry">
[]ServiceNetworkEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceNetwork is the list of IP address pools for services.
Defaults to cidr: &ldquo;172.31.0.0/16&rdquo;.
Currently only one entry is supported.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>networkType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkType">
NetworkType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>networkType specifies the SDN provider used for cluster networking.
Defaults to OVNKubernetes.
This field is required and immutable.
kubebuilder:validation:XValidation:rule=&ldquo;self == oldSelf&rdquo;, message=&ldquo;networkType is immutable&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>apiServer</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIServerNetworking">
APIServerNetworking
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>apiServer contains advanced network settings for the API server that affect
how the APIServer is exposed inside a hosted cluster node.</p>
</td>
</tr>
<tr>
<td>
<code>allocateNodeCIDRs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AllocateNodeCIDRsMode">
AllocateNodeCIDRsMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>allocateNodeCIDRs controls whether the kube-controller-manager manages node CIDR allocation.
When using networkType=Other, it is recommended to set this field to &ldquo;Enabled&rdquo;
if Flannel is used as the CNI, as it relies on this behavior.
Default is &ldquo;Disabled&rdquo;.
This field can only be set to &ldquo;Enabled&rdquo; when NetworkType is &ldquo;Other&rdquo;. Setting it to &ldquo;Enabled&rdquo;
with any other NetworkType will result in a validation error during cluster creation.</p>
</td>
</tr>
</tbody>
</table>
###ClusterVersionOperatorSpec { #hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">OperatorConfiguration</a>)
</p>
<p>
<p>ClusterVersionOperatorSpec is the specification of the desired behavior of the Cluster Version Operator.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>operatorLogLevel</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.LogLevel">
LogLevel
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorLogLevel is an intent based logging for the operator itself. It does not give fine-grained control,
but it is a simple way to manage coarse grained logging choices that operators have to interpret for themselves.</p>
<p>Valid values are: &ldquo;Normal&rdquo;, &ldquo;Debug&rdquo;, &ldquo;Trace&rdquo;, &ldquo;TraceAll&rdquo;.
Defaults to &ldquo;Normal&rdquo;.</p>
</td>
</tr>
</tbody>
</table>
###ClusterVersionStatus { #hypershift.openshift.io/v1beta1.ClusterVersionStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>ClusterVersionStatus reports the status of the cluster versioning,
including any upgrades that are in progress. The current field will
be set to whichever version the cluster is reconciling to, and the
conditions array will report whether the update succeeded, is in
progress, or is failing.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>desired</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.Release
</a>
</em>
</td>
<td>
<p>desired is the version that the cluster is reconciling towards.
If the cluster is not yet fully initialized desired will be set
with the information available, which may be an image or a tag.</p>
</td>
</tr>
<tr>
<td>
<code>history</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.UpdateHistory
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>history contains a list of the most recent versions applied to the cluster.
This value may be empty during cluster startup, and then will be updated
when a new update is being applied. The newest update is first in the
list and it is ordered by recency. Updates in the history have state
Completed if the rollout completed - if an update was failing or halfway
applied the state will be Partial. Only a limited amount of update history
is preserved.</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code></br>
<em>
int64
</em>
</td>
<td>
<p>observedGeneration reports which version of the spec is being synced.
If this value is not equal to metadata.generation, then the desired
and conditions fields may represent a previous version.</p>
</td>
</tr>
<tr>
<td>
<code>availableUpdates</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.Release
</a>
</em>
</td>
<td>
<p>availableUpdates contains updates recommended for this
cluster. Updates which appear in conditionalUpdates but not in
availableUpdates may expose this cluster to known issues. This list
may be empty if no updates are recommended, if the update service
is unavailable, or if an invalid channel has been specified.</p>
</td>
</tr>
<tr>
<td>
<code>conditionalUpdates</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
[]github.com/openshift/api/config/v1.ConditionalUpdate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditionalUpdates contains the list of updates that may be
recommended for this cluster if it meets specific required
conditions. Consumers interested in the set of updates that are
actually recommended for this cluster should use
availableUpdates. This list may be empty if no updates are
recommended, if the update service is unavailable, or if an empty
or invalid channel has been specified.</p>
</td>
</tr>
</tbody>
</table>
###ComponentResource { #hypershift.openshift.io/v1beta1.ComponentResource }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus">ControlPlaneComponentStatus</a>)
</p>
<p>
<p>ComponentResource defines a resource reconciled by a ControlPlaneComponent.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kind</code></br>
<em>
string
</em>
</td>
<td>
<p>kind is the name of the resource schema.</p>
</td>
</tr>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<p>group is the API group for this resource type.</p>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of this resource.</p>
</td>
</tr>
</tbody>
</table>
###ConditionType { #hypershift.openshift.io/v1beta1.ConditionType }
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWSDefaultSecurityGroupCreated&#34;</p></td>
<td><p>AWSDefaultSecurityGroupCreated indicates whether the default security group
for AWS workers has been created.
A failure here indicates that NodePools without a security group will be
blocked from creating machines.</p>
</td>
</tr><tr><td><p>&#34;AWSDefaultSecurityGroupDeleted&#34;</p></td>
<td><p>AWSDefaultSecurityGroupDeleted indicates whether the default security group
for AWS workers has been deleted.
A failure here indicates that the Security Group has some dependencies that
there are still pending cloud resources to be deleted that are using that SG.</p>
</td>
</tr><tr><td><p>&#34;AWSEndpointAvailable&#34;</p></td>
<td><p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint has been
created in the guest VPC</p>
</td>
</tr><tr><td><p>&#34;AWSEndpointServiceAvailable&#34;</p></td>
<td><p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint Service
has been created for the specified NLB in the management VPC</p>
</td>
</tr><tr><td><p>&#34;CVOScaledDown&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;CloudResourcesDestroyed&#34;</p></td>
<td><p>CloudResourcesDestroyed bubbles up the same condition from HCP. It signals if the cloud provider infrastructure created by Kubernetes
in the consumer cloud provider account was destroyed.
A failure here may require external user intervention to resolve. E.g. cloud provider perms were corrupted. E.g. the guest cluster was broken
and kube resource deletion that affects cloud infra like service type load balancer can&rsquo;t succeed.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionAvailable&#34;</p></td>
<td><p>ClusterVersionAvailable bubbles up Failing configv1.OperatorAvailable from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionFailing&#34;</p></td>
<td><p>ClusterVersionFailing bubbles up Failing from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionProgressing&#34;</p></td>
<td><p>ClusterVersionProgressing bubbles up configv1.OperatorProgressing from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionReleaseAccepted&#34;</p></td>
<td><p>ClusterVersionReleaseAccepted bubbles up Failing ReleaseAccepted from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionRetrievedUpdates&#34;</p></td>
<td><p>ClusterVersionRetrievedUpdates bubbles up RetrievedUpdates from the CVO.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionSucceeding&#34;</p></td>
<td><p>ClusterVersionSucceeding indicates the current status of the desired release
version of the HostedCluster as indicated by the Failing condition in the
underlying cluster&rsquo;s ClusterVersion.</p>
</td>
</tr><tr><td><p>&#34;ClusterVersionUpgradeable&#34;</p></td>
<td><p>ClusterVersionUpgradeable indicates the Upgradeable condition in the
underlying cluster&rsquo;s ClusterVersion.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td><p>ControlPlaneComponentAvailable indicates whether the ControlPlaneComponent is available.</p>
</td>
</tr><tr><td><p>&#34;RolloutComplete&#34;</p></td>
<td><p>ControlPlaneComponentRolloutComplete indicates whether the ControlPlaneComponent has completed its rollout.</p>
</td>
</tr><tr><td><p>&#34;DataPlaneConnectionAvailable&#34;</p></td>
<td><p>DataPlaneConnectionAvailable indicates whether the control plane has a successful
network connection to the data plane components.
<strong>True</strong> means the control plane can successfully reach the data plane nodes.
<strong>False</strong> means there are network connection issues preventing the control plane from reaching the data plane.
A failure here suggests potential issues such as: network policy restrictions,
firewall rules, missing data plane nodes, or problems with infrastructure
components like the konnectivity-agent workload.</p>
</td>
</tr><tr><td><p>&#34;EtcdAvailable&#34;</p></td>
<td><p>EtcdAvailable bubbles up the same condition from HCP. It signals if etcd is available.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;EtcdRecoveryActive&#34;</p></td>
<td><p>EtcdRecoveryActive indicates that the Etcd cluster is failing and the
recovery job was triggered.</p>
</td>
</tr><tr><td><p>&#34;EtcdSnapshotRestored&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ExternalDNSReachable&#34;</p></td>
<td><p>ExternalDNSReachable bubbles up the same condition from HCP. It signals if the configured external DNS is reachable.
A failure here requires external user intervention to resolve. E.g. changing the external DNS domain or making sure the domain is created
and registered correctly.</p>
</td>
</tr><tr><td><p>&#34;GCPDNSAvailable&#34;</p></td>
<td><p>GCPDNSAvailable indicates whether the DNS configuration has been
created in the customer VPC</p>
</td>
</tr><tr><td><p>&#34;GCPEndpointAvailable&#34;</p></td>
<td><p>GCPEndpointAvailable indicates whether the GCP PSC Endpoint has been
created in the customer VPC</p>
</td>
</tr><tr><td><p>&#34;GCPPrivateServiceConnectAvailable&#34;</p></td>
<td><p>GCPPrivateServiceConnectAvailable indicates overall PSC infrastructure availability</p>
</td>
</tr><tr><td><p>&#34;GCPServiceAttachmentAvailable&#34;</p></td>
<td><p>GCPServiceAttachmentAvailable indicates whether the GCP Service Attachment
has been created for the specified Internal Load Balancer in the management VPC</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td><p>HostedClusterAvailable indicates whether the HostedCluster has a healthy
control plane.
When this is false for too long and there&rsquo;s no clear indication in the &ldquo;Reason&rdquo;, please check the remaining more granular conditions.</p>
</td>
</tr><tr><td><p>&#34;Degraded&#34;</p></td>
<td><p>HostedClusterDegraded indicates whether the HostedCluster is encountering
an error that may require user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;HostedClusterDestroyed&#34;</p></td>
<td><p>HostedClusterDestroyed indicates that a hosted has finished destroying and that it is waiting for a destroy grace period to go away.
The grace period is determined by the hypershift.openshift.io/destroy-grace-period annotation in the HostedCluster if present.</p>
</td>
</tr><tr><td><p>&#34;Progressing&#34;</p></td>
<td><p>HostedClusterProgressing indicates whether the HostedCluster is attempting
an initial deployment or upgrade.
When this is false for too long and there&rsquo;s no clear indication in the &ldquo;Reason&rdquo;, please check the remaining more granular conditions.</p>
</td>
</tr><tr><td><p>&#34;HostedClusterRestoredFromBackup&#34;</p></td>
<td><p>HostedClusterRestoredFromBackup indicates that the HostedCluster was restored from backup.
This condition is set to true when the HostedCluster is restored from backup and the recovery process is complete.
This condition is used to track the status of the recovery process and to determine if the HostedCluster
is ready to be used after restoration.</p>
</td>
</tr><tr><td><p>&#34;Available&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Degraded&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;IgnitionEndpointAvailable&#34;</p></td>
<td><p>IgnitionEndpointAvailable indicates whether the ignition server for the
HostedCluster is available to handle ignition requests.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;IgnitionServerValidReleaseInfo&#34;</p></td>
<td><p>IgnitionServerValidReleaseInfo indicates if the release contains all the images used by the local ignition provider
and reports missing images if any.</p>
</td>
</tr><tr><td><p>&#34;InfrastructureReady&#34;</p></td>
<td><p>InfrastructureReady bubbles up the same condition from HCP. It signals if the infrastructure for a control plane to be operational,
e.g. load balancers were created successfully.
A failure here may require external user intervention to resolve. E.g. hitting quotas on the cloud provider.</p>
</td>
</tr><tr><td><p>&#34;KubeAPIServerAvailable&#34;</p></td>
<td><p>KubeAPIServerAvailable bubbles up the same condition from HCP. It signals if the kube API server is available.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;KubeVirtNodesLiveMigratable&#34;</p></td>
<td><p>KubeVirtNodesLiveMigratable indicates if all nodes (VirtualMachines) of the kubevirt
hosted cluster can be live migrated without experiencing a node restart</p>
</td>
</tr><tr><td><p>&#34;PlatformCredentialsFound&#34;</p></td>
<td><p>PlatformCredentialsFound indicates that credentials required for the
desired platform are valid.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ReconciliationActive&#34;</p></td>
<td><p>ReconciliationActive indicates if reconciliation of the HostedCluster is
active or paused hostedCluster.spec.pausedUntil.</p>
</td>
</tr><tr><td><p>&#34;ReconciliationSucceeded&#34;</p></td>
<td><p>ReconciliationSucceeded indicates if the HostedCluster reconciliation
succeeded.
A failure here often means a software bug or a non-stable cluster.</p>
</td>
</tr><tr><td><p>&#34;SupportedHostedCluster&#34;</p></td>
<td><p>SupportedHostedCluster indicates whether a HostedCluster is supported by
the current configuration of the hypershift-operator.
e.g. If HostedCluster requests endpointAcess Private but the hypershift-operator
is running on a management cluster outside AWS or is not configured with AWS
credentials, the HostedCluster is not supported.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;UnmanagedEtcdAvailable&#34;</p></td>
<td><p>UnmanagedEtcdAvailable indicates whether a user-managed etcd cluster is
healthy.</p>
</td>
</tr><tr><td><p>&#34;ValidAWSIdentityProvider&#34;</p></td>
<td><p>ValidAWSIdentityProvider indicates if the Identity Provider referenced
in the cloud credentials is healthy. E.g. for AWS the idp ARN is referenced in the iam roles.
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]</p>
<p>A failure here may require external user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;ValidAWSKMSConfig&#34;</p></td>
<td><p>ValidAWSKMSConfig indicates whether the AWS KMS role and encryption key are valid and operational
A failure here indicates that the role or the key are invalid, or the role doesn&rsquo;t have access to use the key.</p>
</td>
</tr><tr><td><p>&#34;ValidAzureKMSConfig&#34;</p></td>
<td><p>ValidAzureKMSConfig indicates whether the given KMS input for the Azure platform is valid and operational
A failure here indicates that the input is invalid, or permissions are missing to use the encryption key.</p>
</td>
</tr><tr><td><p>&#34;ValidGCPCredentials&#34;</p></td>
<td><p>ValidGCPCredentials indicates if GCP credentials are valid and operational
for the HostedCluster. This includes service account authentication and
proper IAM permissions for CAPG controllers.
A failure here may require external user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;ValidGCPWorkloadIdentity&#34;</p></td>
<td><p>ValidGCPWorkloadIdentity indicates if GCP Workload Identity Federation
is properly configured and operational for the cluster.
A failure here may require external user intervention to resolve.</p>
</td>
</tr><tr><td><p>&#34;ValidConfiguration&#34;</p></td>
<td><p>ValidHostedClusterConfiguration signals if the hostedCluster input is valid and
supported by the underlying management cluster.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidHostedControlPlaneConfiguration&#34;</p></td>
<td><p>ValidHostedControlPlaneConfiguration bubbles up the same condition from HCP. It signals if the hostedControlPlane input is valid and
supported by the underlying management cluster.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidIDPConfiguration&#34;</p></td>
<td><p>ValidIDPConfiguration indicates if the Identity Provider configuration is valid.
A failure here may require external user intervention to resolve
e.g. the user-provided IDP configuration provided is invalid or the IDP is not reachable.</p>
</td>
</tr><tr><td><p>&#34;ValidKubeVirtInfraNetworkMTU&#34;</p></td>
<td><p>ValidKubeVirtInfraNetworkMTU indicates if the MTU configured on an infra cluster
hosting a guest cluster utilizing kubevirt platform is a sufficient value that will avoid
performance degradation due to fragmentation of the double encapsulation in ovn-kubernetes</p>
</td>
</tr><tr><td><p>&#34;ValidOIDCConfiguration&#34;</p></td>
<td><p>ValidOIDCConfiguration indicates if an AWS cluster&rsquo;s OIDC condition is
detected as invalid.
A failure here may require external user intervention to resolve. E.g. oidc was deleted out of band.</p>
</td>
</tr><tr><td><p>&#34;ValidProxyConfiguration&#34;</p></td>
<td><p>ValidProxyConfiguration indicates if the proxy CA bundle is valid.
A failure here may require external user intervention to resolve. E.g. certificates in the CA bundle have expired.</p>
</td>
</tr><tr><td><p>&#34;ValidReleaseImage&#34;</p></td>
<td><p>ValidReleaseImage indicates if the release image set in the spec is valid
for the HostedCluster. For example, this can be set false if the
HostedCluster itself attempts an unsupported version before 4.9 or an
unsupported upgrade e.g y-stream upgrade before 4.11.
A failure here is unlikely to resolve without the changing user input.</p>
</td>
</tr><tr><td><p>&#34;ValidReleaseInfo&#34;</p></td>
<td><p>ValidReleaseInfo bubbles up the same condition from HCP. It indicates if the release contains all the images used by hypershift
and reports missing images if any.</p>
</td>
</tr></tbody>
</table>
###ConfigurationStatus { #hypershift.openshift.io/v1beta1.ConfigurationStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>ConfigurationStatus contains the status of HostedCluster configuration</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>authentication</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.AuthenticationStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>authentication contains the observed authentication configuration status from the hosted cluster.
This field reflects the current state of the cluster authentication including OAuth metadata,
OIDC client status, and other authentication-related configurations.</p>
</td>
</tr>
</tbody>
</table>
###ControlPlaneComponent { #hypershift.openshift.io/v1beta1.ControlPlaneComponent }
<p>
<p>ControlPlaneComponent specifies the state of a ControlPlane Component</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>metadata is the metadata for the ControlPlaneComponent.</p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponentSpec">
ControlPlaneComponentSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>spec is the specification for the ControlPlaneComponent.</p>
<br/>
<br/>
<table>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus">
ControlPlaneComponentStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>status is the status of the ControlPlaneComponent.</p>
</td>
</tr>
</tbody>
</table>
###ControlPlaneComponentSpec { #hypershift.openshift.io/v1beta1.ControlPlaneComponentSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponent">ControlPlaneComponent</a>)
</p>
<p>
<p>ControlPlaneComponentSpec defines the desired state of ControlPlaneComponent</p>
</p>
###ControlPlaneComponentStatus { #hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneComponent">ControlPlaneComponent</a>)
</p>
<p>
<p>ControlPlaneComponentStatus defines the observed state of ControlPlaneComponent</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditions contains details for the current state of the ControlPlane Component.
If there is an error, then the Available condition will be false.</p>
<p>Current condition types are: &ldquo;Available&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>version reports the current version of this component.</p>
</td>
</tr>
<tr>
<td>
<code>resources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ComponentResource">
[]ComponentResource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>resources is a list of the resources reconciled by this component.</p>
</td>
</tr>
</tbody>
</table>
###ControlPlaneManagedIdentities { #hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities">AzureResourceManagedIdentities</a>)
</p>
<p>
<p>ControlPlaneManagedIdentities contains the managed identities on the HCP control plane needing to authenticate with
Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>managedIdentitiesKeyVault</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedAzureKeyVault">
ManagedAzureKeyVault
</a>
</em>
</td>
<td>
<p>managedIdentitiesKeyVault contains information on the management cluster&rsquo;s managed identities Azure Key Vault.
This Key Vault is where the managed identities certificates are stored. These certificates are pulled out of the
Key Vault by the Secrets Store CSI driver and mounted into a volume on control plane pods requiring
authentication with Azure API.</p>
<p>More information on how the Secrets Store CSI driver works to do this can be found here:
<a href="https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver">https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver</a>.</p>
</td>
</tr>
<tr>
<td>
<code>cloudProvider</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>cloudProvider is a pre-existing managed identity associated with the azure cloud provider, aka cloud controller
manager.</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>nodePoolManagement is a pre-existing managed identity associated with the operator managing the NodePools.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneOperator</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>controlPlaneOperator is a pre-existing managed identity associated with the control plane operator.</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistry</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageRegistry is a pre-existing managed identity associated with the cluster-image-registry-operator.</p>
</td>
</tr>
<tr>
<td>
<code>ingress</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>ingress is a pre-existing managed identity associated with the cluster-ingress-operator.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>network is a pre-existing managed identity associated with the cluster-network-operator.</p>
</td>
</tr>
<tr>
<td>
<code>disk</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>disk is a pre-existing managed identity associated with the azure-disk-controller.</p>
</td>
</tr>
<tr>
<td>
<code>file</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">
ManagedIdentity
</a>
</em>
</td>
<td>
<p>file is a pre-existing managed identity associated with the azure-disk-controller.</p>
</td>
</tr>
</tbody>
</table>
###DNSSpec { #hypershift.openshift.io/v1beta1.DNSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>DNSSpec specifies the DNS configuration for the hosted cluster ingress.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>baseDomain</code></br>
<em>
string
</em>
</td>
<td>
<p>baseDomain is the base domain of the hosted cluster.
It will be used to configure ingress in the hosted cluster through the subdomain baseDomainPrefix.baseDomain.
If baseDomainPrefix is omitted, the hostedCluster.name will be used as the subdomain.
Once set, this field is immutable.
When the value is the empty string &ldquo;&rdquo;, the controller might default to a value depending on the platform.</p>
</td>
</tr>
<tr>
<td>
<code>baseDomainPrefix</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>baseDomainPrefix is the base domain prefix for the hosted cluster ingress.
It will be used to configure ingress in the hosted cluster through the subdomain baseDomainPrefix.baseDomain.
If baseDomainPrefix is omitted, the hostedCluster.name will be used as the subdomain.
Set baseDomainPrefix to an empty string &ldquo;&rdquo;, if you don&rsquo;t want a prefix at all (not even hostedCluster.name) to be prepended to baseDomain.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>publicZoneID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>publicZoneID is the Hosted Zone ID where all the DNS records that are publicly accessible to the internet exist.
This field is optional and mainly leveraged in cloud environments where the DNS records for the .baseDomain are created by controllers in this zone.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>privateZoneID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>privateZoneID is the Hosted Zone ID where all the DNS records that are only available internally to the cluster exist.
This field is optional and mainly leveraged in cloud environments where the DNS records for the .baseDomain are created by controllers in this zone.
Once set, this value is immutable.</p>
</td>
</tr>
</tbody>
</table>
###DNSZoneStatus { #hypershift.openshift.io/v1beta1.DNSZoneStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectStatus">GCPPrivateServiceConnectStatus</a>)
</p>
<p>
<p>DNSZoneStatus represents a single DNS zone and its records</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the DNS zone name</p>
</td>
</tr>
<tr>
<td>
<code>records</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>records lists the DNS records created in this zone</p>
</td>
</tr>
</tbody>
</table>
###DataPlaneManagedIdentities { #hypershift.openshift.io/v1beta1.DataPlaneManagedIdentities }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities">AzureResourceManagedIdentities</a>)
</p>
<p>
<p>DataPlaneManagedIdentities contains the client IDs of all the managed identities on the data plane needing to
authenticate with Azure&rsquo;s API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>imageRegistryMSIClientID</code></br>
<em>
string
</em>
</td>
<td>
<p>imageRegistryMSIClientID is the client ID of a pre-existing managed identity ID associated with the image
registry controller.</p>
</td>
</tr>
<tr>
<td>
<code>diskMSIClientID</code></br>
<em>
string
</em>
</td>
<td>
<p>diskMSIClientID is the client ID of a pre-existing managed identity ID associated with the CSI Disk driver.</p>
</td>
</tr>
<tr>
<td>
<code>fileMSIClientID</code></br>
<em>
string
</em>
</td>
<td>
<p>fileMSIClientID is the client ID of a pre-existing managed identity ID associated with the CSI File driver.</p>
</td>
</tr>
</tbody>
</table>
###Diagnostics { #hypershift.openshift.io/v1beta1.Diagnostics }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">AzureNodePoolPlatform</a>)
</p>
<p>
<p>Diagnostics specifies the diagnostics settings for a virtual machine.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageAccountType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType">
AzureDiagnosticsStorageAccountType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageAccountType determines if the storage account for storing the diagnostics data
should be disabled (Disabled), provisioned by Azure (Managed) or by the user (UserManaged).</p>
</td>
</tr>
<tr>
<td>
<code>userManaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UserManagedDiagnostics">
UserManagedDiagnostics
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>userManaged specifies the diagnostics settings for a virtual machine when the storage account is managed by the user.</p>
</td>
</tr>
</tbody>
</table>
###EtcdManagementType { #hypershift.openshift.io/v1beta1.EtcdManagementType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>EtcdManagementType is a enum specifying the strategy for managing the cluster&rsquo;s etcd instance</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>Managed means HyperShift should provision and operator the etcd cluster
automatically.</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>Unmanaged means HyperShift will not provision or manage the etcd cluster,
and the user is responsible for doing so.</p>
</td>
</tr></tbody>
</table>
###EtcdSpec { #hypershift.openshift.io/v1beta1.EtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>EtcdSpec specifies configuration for a control plane etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>managementType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdManagementType">
EtcdManagementType
</a>
</em>
</td>
<td>
<p>managementType defines how the etcd cluster is managed.
This can be either Managed or Unmanaged.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>managed</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdSpec">
ManagedEtcdSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>managed specifies the behavior of an etcd cluster managed by HyperShift.</p>
</td>
</tr>
<tr>
<td>
<code>unmanaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec">
UnmanagedEtcdSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>unmanaged specifies configuration which enables the control plane to
integrate with an externally managed etcd cluster.</p>
</td>
</tr>
</tbody>
</table>
###EtcdTLSConfig { #hypershift.openshift.io/v1beta1.EtcdTLSConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec">UnmanagedEtcdSpec</a>)
</p>
<p>
<p>EtcdTLSConfig specifies TLS configuration for HTTPS etcd client endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>clientSecret refers to a secret for client mTLS authentication with the etcd cluster. It
may have the following key/value pairs:</p>
<pre><code>etcd-client-ca.crt: Certificate Authority value
etcd-client.crt: Client certificate value
etcd-client.key: Client certificate key value
</code></pre>
</td>
</tr>
</tbody>
</table>
###ExpanderString { #hypershift.openshift.io/v1beta1.ExpanderString }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">ClusterAutoscaling</a>)
</p>
<p>
<p>ExpanderString contains the name of an expander to be used by the cluster autoscaler.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;LeastWaste&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Priority&#34;</p></td>
<td><p>Selects the node group with the least idle resources.</p>
</td>
</tr><tr><td><p>&#34;Random&#34;</p></td>
<td><p>Selects the node group with the highest priority.</p>
</td>
</tr></tbody>
</table>
###Filter { #hypershift.openshift.io/v1beta1.Filter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSResourceReference">AWSResourceReference</a>)
</p>
<p>
<p>Filter is a filter used to identify an AWS resource</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of the filter.</p>
</td>
</tr>
<tr>
<td>
<code>values</code></br>
<em>
[]string
</em>
</td>
<td>
<p>values is a list of values for the filter.</p>
</td>
</tr>
</tbody>
</table>
###FilterByNeutronTags { #hypershift.openshift.io/v1beta1.FilterByNeutronTags }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NetworkFilter">NetworkFilter</a>, 
<a href="#hypershift.openshift.io/v1beta1.RouterFilter">RouterFilter</a>, 
<a href="#hypershift.openshift.io/v1beta1.SubnetFilter">SubnetFilter</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>tags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tags is a list of tags to filter by. If specified, the resource must
have all of the tags specified to be included in the result.</p>
</td>
</tr>
<tr>
<td>
<code>tagsAny</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tagsAny is a list of tags to filter by. If specified, the resource
must have at least one of the tags specified to be included in the
result.</p>
</td>
</tr>
<tr>
<td>
<code>notTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>notTags is a list of tags to filter by. If specified, resources which
contain all of the given tags will be excluded from the result.</p>
</td>
</tr>
<tr>
<td>
<code>notTagsAny</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NeutronTag">
[]NeutronTag
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>notTagsAny is a list of tags to filter by. If specified, resources
which contain any of the given tags will be excluded from the result.</p>
</td>
</tr>
</tbody>
</table>
###GCPBootDisk { #hypershift.openshift.io/v1beta1.GCPBootDisk }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPNodePoolPlatform">GCPNodePoolPlatform</a>)
</p>
<p>
<p>GCPBootDisk specifies configuration for the boot disk of GCP node instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>diskSizeGB</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>diskSizeGB specifies the size of the boot disk in gigabytes.
Must be at least 20 GB for RHCOS images.</p>
</td>
</tr>
<tr>
<td>
<code>diskType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>diskType specifies the disk type for the boot disk.
Valid values include:
- &ldquo;pd-standard&rdquo; - Standard persistent disk (magnetic)
- &ldquo;pd-ssd&rdquo; - SSD persistent disk
- &ldquo;pd-balanced&rdquo; - Balanced persistent disk (recommended)
If not specified, defaults to &ldquo;pd-balanced&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionKey</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPDiskEncryptionKey">
GCPDiskEncryptionKey
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>encryptionKey specifies customer-managed encryption key (CMEK) configuration.
If not specified, Google-managed encryption keys are used.</p>
</td>
</tr>
</tbody>
</table>
###GCPDiskEncryptionKey { #hypershift.openshift.io/v1beta1.GCPDiskEncryptionKey }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPBootDisk">GCPBootDisk</a>)
</p>
<p>
<p>GCPDiskEncryptionKey specifies configuration for customer-managed encryption keys.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kmsKeyName</code></br>
<em>
string
</em>
</td>
<td>
<p>kmsKeyName is the resource name of the Cloud KMS key used for disk encryption.
Format: projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{key}</p>
</td>
</tr>
</tbody>
</table>
###GCPEndpointAccessType { #hypershift.openshift.io/v1beta1.GCPEndpointAccessType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPPlatformSpec">GCPPlatformSpec</a>)
</p>
<p>
<p>GCPEndpointAccessType defines the endpoint access type for GCP clusters.
Equivalent to AWS EndpointAccessType but adapted for GCP networking model.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Private&#34;</p></td>
<td><p>GCPEndpointAccessPrivate endpoint access allows only private API server access and private
node communication with the control plane via Private Service Connect.</p>
</td>
</tr><tr><td><p>&#34;PublicAndPrivate&#34;</p></td>
<td><p>GCPEndpointAccessPublicAndPrivate endpoint access allows public API server access and
private node communication with the control plane via Private Service Connect.</p>
</td>
</tr></tbody>
</table>
###GCPNetworkConfig { #hypershift.openshift.io/v1beta1.GCPNetworkConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPPlatformSpec">GCPPlatformSpec</a>)
</p>
<p>
<p>GCPNetworkConfig specifies VPC configuration for GCP clusters and Private Service Connect endpoint creation.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPResourceReference">
GCPResourceReference
</a>
</em>
</td>
<td>
<p>network is the VPC network name</p>
</td>
</tr>
<tr>
<td>
<code>privateServiceConnectSubnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPResourceReference">
GCPResourceReference
</a>
</em>
</td>
<td>
<p>privateServiceConnectSubnet is the subnet for Private Service Connect endpoints</p>
</td>
</tr>
</tbody>
</table>
###GCPNodePoolPlatform { #hypershift.openshift.io/v1beta1.GCPNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>GCPNodePoolPlatform specifies the configuration of a NodePool when operating on GCP.
This follows the AWS and Azure patterns for platform-specific NodePool configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>machineType</code></br>
<em>
string
</em>
</td>
<td>
<p>machineType is the GCP machine type for node instances (e.g. n2-standard-4).
Must follow GCP machine type naming conventions as documented at:
<a href="https://cloud.google.com/compute/docs/machine-resource#machine_type_comparison">https://cloud.google.com/compute/docs/machine-resource#machine_type_comparison</a></p>
<p>Valid machine type formats:
- predefined: n1-standard-1, n2-highmem-4, c2-standard-8, etc.
- custom: custom-{cpus}-{memory} (e.g. custom-4-8192)
- custom with extended memory: custom-{cpus}-{memory}-ext (e.g. custom-2-13312-ext)</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<p>zone is the GCP zone where node instances will be created.
Must be a valid zone within the cluster&rsquo;s region.
Format: {region}-{zone} (e.g. us-central1-a, europe-west2-b)
See <a href="https://cloud.google.com/compute/docs/regions-zones">https://cloud.google.com/compute/docs/regions-zones</a> for available zones.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
string
</em>
</td>
<td>
<p>subnet is the name of the subnet where node instances will be created.
Must be a subnet within the VPC network specified in the HostedCluster&rsquo;s
networkConfig and located in the same region as the zone.
The subnet must have enough IP addresses available for the expected number of nodes.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>image specifies the boot image for node instances.
If unspecified, the default RHCOS image will be used based on the NodePool release payload.
Can be:
- A family name: projects/rhel-cloud/global/images/family/rhel-8
- A specific image: projects/rhel-cloud/global/images/rhel-8-v20231010
- A full resource URL: <a href="https://www.googleapis.com/compute/v1/projects/rhel-cloud/global/images/rhel-8-v20231010">https://www.googleapis.com/compute/v1/projects/rhel-cloud/global/images/rhel-8-v20231010</a></p>
</td>
</tr>
<tr>
<td>
<code>bootDisk</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPBootDisk">
GCPBootDisk
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>bootDisk specifies the configuration for the boot disk of node instances.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccount</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPNodeServiceAccount">
GCPNodeServiceAccount
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAccount configures the Google Service Account attached to node instances.
If not specified, uses the default compute service account for the project.</p>
</td>
</tr>
<tr>
<td>
<code>resourceLabels</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPResourceLabel">
[]GCPResourceLabel
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>resourceLabels is an optional list of additional labels to apply to GCP node
instances and their associated resources (disks, etc.).
Labels will be merged with cluster-level resource labels, with NodePool labels
taking precedence in case of conflicts.</p>
<p>Keys and values must conform to GCP labeling requirements:
- Keys: 1‚Äì63 chars, must start with a lowercase letter; allowed [a-z0-9<em>-]
- Values: empty or 1‚Äì63 chars; allowed [a-z0-9</em>-]
- Maximum 60 user labels per resource (GCP limit is 64 total, with ~4 reserved)</p>
</td>
</tr>
<tr>
<td>
<code>networkTags</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>networkTags is an optional list of network tags to apply to node instances.
These tags are used by GCP firewall rules to control network access.
Tags must conform to GCP naming conventions:
- 1-63 characters
- Lowercase letters, numbers, and hyphens only
- Must start with lowercase letter
- Cannot end with hyphen</p>
</td>
</tr>
<tr>
<td>
<code>provisioningModel</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPProvisioningModel">
GCPProvisioningModel
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>provisioningModel specifies the provisioning model for node instances.
Spot and Preemptible instances cost less but can be terminated by GCP with 30 seconds notice.
Spot instances are recommended over Preemptible as they have no maximum runtime limit.
Standard instances are regular VMs that run until explicitly stopped.
If not specified, defaults to &ldquo;Standard&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>onHostMaintenance</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>onHostMaintenance specifies the behavior when host maintenance occurs.
For Spot and Preemptible instances, this must be &ldquo;TERMINATE&rdquo;.
For Standard instances, can be &ldquo;MIGRATE&rdquo; (live migrate) or &ldquo;TERMINATE&rdquo;.
If not specified, defaults to &ldquo;MIGRATE&rdquo; for Standard instances and &ldquo;TERMINATE&rdquo; for Spot/Preemptible.</p>
</td>
</tr>
</tbody>
</table>
###GCPNodeServiceAccount { #hypershift.openshift.io/v1beta1.GCPNodeServiceAccount }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPNodePoolPlatform">GCPNodePoolPlatform</a>)
</p>
<p>
<p>GCPNodeServiceAccount specifies the Google Service Account configuration for node instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>email</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>email specifies the email address of the Google Service Account to use for node instances.
If not specified, uses the default compute service account for the project.
The service account must have the necessary permissions for the node to function:
- Logging writer
- Monitoring metric writer
- Storage object viewer (for pulling container images)</p>
</td>
</tr>
<tr>
<td>
<code>scopes</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>scopes specifies the access scopes for the service account.
If not specified, defaults to standard compute scopes.
Common scopes include:
- &ldquo;<a href="https://www.googleapis.com/auth/devstorage.read_only&quot;">https://www.googleapis.com/auth/devstorage.read_only&rdquo;</a> - Storage read-only
- &ldquo;<a href="https://www.googleapis.com/auth/logging.write&quot;">https://www.googleapis.com/auth/logging.write&rdquo;</a> - Logging write
- &ldquo;<a href="https://www.googleapis.com/auth/monitoring.write&quot;">https://www.googleapis.com/auth/monitoring.write&rdquo;</a> - Monitoring write
- &ldquo;<a href="https://www.googleapis.com/auth/cloud-platform&quot;">https://www.googleapis.com/auth/cloud-platform&rdquo;</a> - Full access (use with caution)</p>
</td>
</tr>
</tbody>
</table>
###GCPOnHostMaintenance { #hypershift.openshift.io/v1beta1.GCPOnHostMaintenance }
<p>
<p>GCPOnHostMaintenance defines the behavior when a host maintenance event occurs.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;MIGRATE&#34;</p></td>
<td><p>GCPOnHostMaintenanceMigrate causes Compute Engine to live migrate an instance during host maintenance.</p>
</td>
</tr><tr><td><p>&#34;TERMINATE&#34;</p></td>
<td><p>GCPOnHostMaintenanceTerminate causes Compute Engine to stop an instance during host maintenance.</p>
</td>
</tr></tbody>
</table>
###GCPPlatformSpec { #hypershift.openshift.io/v1beta1.GCPPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>GCPPlatformSpec specifies configuration for clusters running on Google Cloud Platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>project</code></br>
<em>
string
</em>
</td>
<td>
<p>project is the GCP project ID.
A valid project ID must satisfy the following rules:
length: Must be between 6 and 30 characters, inclusive
characters: Only lowercase letters (<code>a-z</code>), digits (<code>0-9</code>), and hyphens (<code>-</code>) are allowed
start and end: Must begin with a lowercase letter and must not end with a hyphen
valid examples: &ldquo;my-project&rdquo;, &ldquo;my-project-1&rdquo;, &ldquo;my-project-123&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>region is the GCP region in which the cluster resides.
Must be in the form of <geographic-area>-<location><number> (e.g., us-central1, europe-west12).
Must contain exactly one hyphen separating the geographic area from the location.
Must end with one or more digits.
Valid examples: &ldquo;us-central1&rdquo;, &ldquo;europe-west2&rdquo;, &ldquo;europe-west12&rdquo;, &ldquo;northamerica-northeast1&rdquo;
Invalid examples: &ldquo;us1&rdquo; (no hyphen), &ldquo;us-central&rdquo; (no trailing digits), &ldquo;us-central1-a&rdquo; (zone suffix)
For a full list of valid regions, see: <a href="https://cloud.google.com/compute/docs/regions-zones">https://cloud.google.com/compute/docs/regions-zones</a>.</p>
</td>
</tr>
<tr>
<td>
<code>networkConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPNetworkConfig">
GCPNetworkConfig
</a>
</em>
</td>
<td>
<p>networkConfig specifies VPC configuration for Private Service Connect.
Required for VPC configuration in Private Service Connect deployments.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPEndpointAccessType">
GCPEndpointAccessType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>endpointAccess controls API endpoint accessibility for the HostedControlPlane on GCP.
Allowed values: &ldquo;Private&rdquo;, &ldquo;PublicAndPrivate&rdquo;. Defaults to &ldquo;Private&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>resourceLabels</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPResourceLabel">
[]GCPResourceLabel
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>resourceLabels are applied to all GCP resources created for the cluster.
Labels are key-value pairs used for organizing and managing GCP resources.
Changes to this field will be propagated in-place to GCP resources where supported.
GCP supports a maximum of 64 labels per resource. HyperShift reserves approximately 4 labels for system use.
For GCP labeling guidance, see <a href="https://cloud.google.com/compute/docs/labeling-resources">https://cloud.google.com/compute/docs/labeling-resources</a></p>
</td>
</tr>
<tr>
<td>
<code>workloadIdentity,omitzero</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPWorkloadIdentityConfig">
GCPWorkloadIdentityConfig
</a>
</em>
</td>
<td>
<p>workloadIdentity configures Workload Identity Federation for the cluster.
This enables secure, short-lived token-based authentication without storing
long-term service account keys. These fields are immutable after cluster creation
to prevent breaking the authentication chain.</p>
<p>Prerequisites for WIF setup:
- Workload Identity Pool and Provider must exist in the GCP project
- Provider must be configured with audience mapping for OpenShift SA tokens
- Target Google Service Account must have roles/iam.workloadIdentityUser
granted to the workload pool principal (e.g., principal://iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/subject/system:serviceaccount:kube-system:capi-gcp-controller-manager)
- Attribute mappings on the provider should include google.subject for token subject verification</p>
</td>
</tr>
</tbody>
</table>
###GCPPrivateServiceConnectSpec { #hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnect">GCPPrivateServiceConnect</a>)
</p>
<p>
<p>GCPPrivateServiceConnectSpec defines the desired state of PSC infrastructure</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>loadBalancerIP</code></br>
<em>
string
</em>
</td>
<td>
<p>loadBalancerIP is the IP address of the Internal Load Balancer
Populated by the observer from service status
This value must be a valid IPv4 or IPv6 address.</p>
</td>
</tr>
<tr>
<td>
<code>forwardingRuleName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>forwardingRuleName is the name of the Internal Load Balancer forwarding rule
Populated by the reconciler via GCP API lookup</p>
</td>
</tr>
<tr>
<td>
<code>consumerAcceptList</code></br>
<em>
[]string
</em>
</td>
<td>
<p>consumerAcceptList specifies which customer projects can connect
Accepts both project IDs (e.g. &ldquo;my-project-123&rdquo;) and project numbers (e.g. &ldquo;123456789012&rdquo;)</p>
</td>
</tr>
<tr>
<td>
<code>natSubnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>natSubnet is the subnet used for NAT by the Service Attachment
Auto-populated by the HyperShift Operator</p>
</td>
</tr>
</tbody>
</table>
###GCPPrivateServiceConnectStatus { #hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnect">GCPPrivateServiceConnect</a>)
</p>
<p>
<p>GCPPrivateServiceConnectStatus defines the observed state of PSC infrastructure</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditions represent the current state of PSC infrastructure
Current condition types are: &ldquo;GCPPrivateServiceConnectAvailable&rdquo;, &ldquo;GCPServiceAttachmentAvailable&rdquo;, &ldquo;GCPEndpointAvailable&rdquo;, &ldquo;GCPDNSAvailable&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>serviceAttachmentName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAttachmentName is the name of the created Service Attachment</p>
</td>
</tr>
<tr>
<td>
<code>serviceAttachmentURI</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAttachmentURI is the URI customers use to connect
Format: projects/{project}/regions/{region}/serviceAttachments/{name}</p>
</td>
</tr>
<tr>
<td>
<code>endpointIP</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>endpointIP is the reserved IP address for the PSC endpoint
This value must be a valid IPv4 or IPv6 address.</p>
</td>
</tr>
<tr>
<td>
<code>dnsZones</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSZoneStatus">
[]DNSZoneStatus
</a>
</em>
</td>
<td>
<p>dnsZones contains DNS zone information created for this cluster</p>
</td>
</tr>
</tbody>
</table>
###GCPProvisioningModel { #hypershift.openshift.io/v1beta1.GCPProvisioningModel }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPNodePoolPlatform">GCPNodePoolPlatform</a>)
</p>
<p>
<p>GCPProvisioningModel defines the provisioning model for GCP node instances.
Follows GCP&rsquo;s provisioning model terminology for compute instances.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Preemptible&#34;</p></td>
<td><p>GCPProvisioningModelPreemptible specifies preemptible instances (legacy).
Preemptible instances are lower-cost instances that can be terminated by GCP
with 30 seconds notice when capacity is needed elsewhere.
Note: Preemptible instances have a maximum runtime of 24 hours.
Consider using Spot instances instead, which have no maximum runtime limit.</p>
</td>
</tr><tr><td><p>&#34;Spot&#34;</p></td>
<td><p>GCPProvisioningModelSpot specifies Spot instances.
Spot instances are lower-cost instances that can be terminated by GCP
with 30 seconds notice when capacity is needed elsewhere.
Unlike preemptible instances, Spot instances have no maximum runtime limit.
This is the recommended option for cost-effective, interruptible workloads.</p>
</td>
</tr><tr><td><p>&#34;Standard&#34;</p></td>
<td><p>GCPProvisioningModelStandard specifies standard (non-preemptible) instances.
Standard instances run until explicitly stopped and are not subject to automatic termination.</p>
</td>
</tr></tbody>
</table>
###GCPResourceLabel { #hypershift.openshift.io/v1beta1.GCPResourceLabel }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPNodePoolPlatform">GCPNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.GCPPlatformSpec">GCPPlatformSpec</a>)
</p>
<p>
<p>GCPResourceLabel is a label to apply to GCP resources created for the cluster.
Labels are key-value pairs used for organizing and managing GCP resources.
See <a href="https://cloud.google.com/compute/docs/labeling-resources">https://cloud.google.com/compute/docs/labeling-resources</a> for GCP labeling guidance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>key is the key part of the label. A label key can have a maximum of 63 characters and cannot be empty.
For Compute Engine resources (VMs, disks, networks created by CAPG), keys must:
- Start with a lowercase letter
- Contain only lowercase letters, digits, underscores, or hyphens
- End with a lowercase letter or digit (not a hyphen or underscore)
- Be 1-63 characters long
GCP reserves the &lsquo;goog&rsquo; prefix for system labels.
See <a href="https://cloud.google.com/compute/docs/labeling-resources">https://cloud.google.com/compute/docs/labeling-resources</a> for Compute Engine label requirements.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>value is the value part of the label. A label value can have a maximum of 63 characters.
Empty values are allowed by GCP. If non-empty, it must start with a lowercase letter,
contain only lowercase letters, digits, underscores, or hyphens, and end with a lowercase letter or digit.
See <a href="https://cloud.google.com/compute/docs/labeling-resources">https://cloud.google.com/compute/docs/labeling-resources</a> for Compute Engine label requirements.</p>
</td>
</tr>
</tbody>
</table>
###GCPResourceReference { #hypershift.openshift.io/v1beta1.GCPResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPNetworkConfig">GCPNetworkConfig</a>)
</p>
<p>
<p>GCPResourceReference represents a reference to a GCP resource by name.
Follows GCP naming patterns (name-based APIs, not ID-based like AWS).
See <a href="https://google.aip.dev/122">https://google.aip.dev/122</a> for GCP resource name standards.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of the GCP resource.
Must conform to GCP resource naming standards: lowercase letters, numbers, and hyphens only.
Must start with a lowercase letter and end with a lowercase letter or number, max 63 characters.
Pattern: &ldquo;^<a href="[-a-z0-9]*[a-z0-9]">a-z</a>?$&rdquo; (max 63 chars), per GCP naming requirements.
See <a href="https://cloud.google.com/compute/docs/naming-resources">https://cloud.google.com/compute/docs/naming-resources</a> for details.</p>
</td>
</tr>
</tbody>
</table>
###GCPServiceAccountsEmails { #hypershift.openshift.io/v1beta1.GCPServiceAccountsEmails }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPWorkloadIdentityConfig">GCPWorkloadIdentityConfig</a>)
</p>
<p>
<p>GCPServiceAccountsEmails contains email addresses of Google Service Accounts for different controllers.
Each service account should have the appropriate IAM permissions for its specific role.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>nodePool</code></br>
<em>
string
</em>
</td>
<td>
<p>nodePool is the Google Service Account email for CAPG controllers
that manage NodePool infrastructure (VMs, networks, disks, etc.).
This GSA requires the following IAM roles:
- roles/compute.instanceAdmin.v1 (Compute Instance Admin v1)
- roles/compute.networkAdmin (Compute Network Admin)
- roles/iam.serviceAccountUser (Service Account User - to attach service accounts to VMs)
See cmd/infra/gcp/iam-bindings.json for the authoritative role definitions.
Format: service-account-name@project-id.iam.gserviceaccount.com</p>
<p>This is a user-provided value referencing a pre-created Google Service Account.
Typically obtained from the output of <code>hypershift infra create gcp</code> which creates
the required service accounts with appropriate IAM roles and WIF bindings.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlane</code></br>
<em>
string
</em>
</td>
<td>
<p>controlPlane is the Google Service Account email for the Control Plane Operator
that manages control plane infrastructure and resources.
This GSA requires the following IAM roles:
- roles/dns.admin (DNS Admin - for managing DNS records)
- roles/compute.networkAdmin (Compute Network Admin - for network management)
- roles/compute.viewer (Compute Viewer - for CCM to read instance metadata)
See cmd/infra/gcp/iam-bindings.json for the authoritative role definitions.
Format: service-account-name@project-id.iam.gserviceaccount.com</p>
<p>This is a user-provided value referencing a pre-created Google Service Account.
Typically obtained from the output of <code>hypershift infra create gcp</code> which creates
the required service accounts with appropriate IAM roles and WIF bindings.</p>
</td>
</tr>
<tr>
<td>
<code>cloudController</code></br>
<em>
string
</em>
</td>
<td>
<p>cloudController is the Google Service Account email for the Cloud Controller Manager
that manages LoadBalancer services and node lifecycle in the hosted cluster.
This GSA requires the following IAM roles:
- roles/compute.loadBalancerAdmin (Load Balancer Admin - for provisioning GCP load balancers)
- roles/compute.securityAdmin (Security Admin - for managing firewall rules)
- roles/compute.viewer (Compute Viewer - for reading instance metadata for node management)
See cmd/infra/gcp/iam-bindings.json for the authoritative role definitions.
Format: service-account-name@project-id.iam.gserviceaccount.com</p>
<p>This is a user-provided value referencing a pre-created Google Service Account.
Typically obtained from the output of <code>hypershift infra create gcp</code> which creates
the required service accounts with appropriate IAM roles and WIF bindings.</p>
</td>
</tr>
<tr>
<td>
<code>storage</code></br>
<em>
string
</em>
</td>
<td>
<p>storage is the Google Service Account email for the GCP PD CSI Driver
that manages Persistent Disk storage operations (create, attach, delete volumes).
This GSA requires the following IAM roles:
- roles/compute.storageAdmin (Compute Storage Admin - for managing persistent disks)
- roles/compute.instanceAdmin.v1 (Compute Instance Admin - for attaching disks to VMs)
- roles/iam.serviceAccountUser (Service Account User - for impersonation)
- roles/resourcemanager.tagUser (Tag User - for applying resource tags to disks)
See cmd/infra/gcp/iam-bindings.json for the authoritative role definitions.
Format: service-account-name@project-id.iam.gserviceaccount.com</p>
<p>This is a user-provided value referencing a pre-created Google Service Account.
Typically obtained from the output of <code>hypershift infra create gcp</code> which creates
the required service accounts with appropriate IAM roles and WIF bindings.</p>
</td>
</tr>
</tbody>
</table>
###GCPWorkloadIdentityConfig { #hypershift.openshift.io/v1beta1.GCPWorkloadIdentityConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.GCPPlatformSpec">GCPPlatformSpec</a>)
</p>
<p>
<p>GCPWorkloadIdentityConfig configures Workload Identity Federation for GCP clusters.
This enables secure, short-lived token-based authentication without storing
long-term service account keys.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>projectNumber</code></br>
<em>
string
</em>
</td>
<td>
<p>projectNumber is the numeric GCP project identifier for WIF configuration.
This differs from the project ID and is required for workload identity pools.
Must be a numeric string representing the GCP project number.</p>
<p>This is a user-provided value obtained from GCP (found in GCP Console or via <code>gcloud projects describe PROJECT_ID</code>).
Also available in the output of <code>hypershift infra create gcp</code>.</p>
</td>
</tr>
<tr>
<td>
<code>poolID</code></br>
<em>
string
</em>
</td>
<td>
<p>poolID is the workload identity pool identifier within the project.
This pool is used to manage external identity mappings.
Must be 4-32 characters and start with a lowercase letter.
Allowed characters: lowercase letters (a-z), digits (0-9), hyphens (-).
Cannot start or end with a hyphen.
The prefix &ldquo;gcp-&rdquo; is reserved by Google and cannot be used.</p>
<p>This is a user-provided value referencing a pre-created Workload Identity Pool.
Typically obtained from the output of <code>hypershift infra create gcp</code> which creates
the WIF infrastructure and generates appropriate pool IDs.</p>
</td>
</tr>
<tr>
<td>
<code>providerID</code></br>
<em>
string
</em>
</td>
<td>
<p>providerID is the workload identity provider identifier within the pool.
This provider handles the token exchange between external and GCP identities.
Must be 4-32 characters and start with a lowercase letter.
Allowed characters: lowercase letters (a-z), digits (0-9), hyphens (-).
Cannot start or end with a hyphen.
The prefix &ldquo;gcp-&rdquo; is reserved by Google and cannot be used.</p>
<p>This is a user-provided value referencing a pre-created OIDC Provider within the WIF Pool.
Typically obtained from the output of <code>hypershift infra create gcp</code>.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountsEmails,omitzero</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPServiceAccountsEmails">
GCPServiceAccountsEmails
</a>
</em>
</td>
<td>
<p>serviceAccountsEmails contains email addresses of various Google Service Accounts
required to enable integrations for different controllers and operators.
This follows the AWS pattern of having different roles for different purposes.</p>
</td>
</tr>
</tbody>
</table>
###HostedClusterSpec { #hypershift.openshift.io/v1beta1.HostedClusterSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedCluster">HostedCluster</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the desired OCP release payload for all the hosted cluster components.
This includes those components running management side like the Kube API Server and the CVO but also the operands which land in the hosted cluster data plane like the ingress controller, ovn agents, etc.
The maximum and minimum supported release versions are determined by the running Hypersfhit Operator.
Attempting to use an unsupported version will result in the HostedCluster being degraded and the validateReleaseImage condition being false.
Attempting to use a release with a skew against a NodePool release bigger than N-2 for the y-stream will result in leaving the NodePool in an unsupported state.
Changing this field will trigger a rollout of the control plane components.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneRelease</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneRelease is like spec.release but only for the components running on the management cluster.
This excludes any operand which will land in the hosted cluster data plane.
It is useful when you need to apply patch management side like a CVE, transparently for the hosted cluster.
Version input for this field is free, no validation is performed against spec.release or maximum and minimum is performed.
If defined, it will dicate the version of the components running management side, while spec.release will dictate the version of the components landing in the hosted cluster data plane.
If not defined, spec.release is used for both.
Changing this field will trigger a rollout of the control plane.
The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterID uniquely identifies this cluster. This is expected to be an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in hexadecimal digits).
As with a Kubernetes metadata.uid, this ID uniquely identifies this cluster in space and time.
This value identifies the cluster in metrics pushed to telemetry and metrics produced by the control plane operators.
If a value is not specified, a random clusterID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>infraID is a globally unique identifier for the cluster.
It must consist of lowercase alphanumeric characters and hyphens (&lsquo;-&rsquo;) only, and start and end with an alphanumeric character.
It must be no more than 253 characters in length.
This identifier will be used to associate various cloud resources with the HostedCluster and its associated NodePools.
infraID is used to compute and tag created resources with &ldquo;kubernetes.io/cluster/&rdquo;+hcluster.Spec.InfraID which has contractual meaning for the cloud provider implementations.
If a value is not specified, a random infraID will be generated and set by the controller.
Once set, this value is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
If omitted we will use the appropriate update service for the cluster and region.
This is used by the control plane operator to determine and signal the appropriate available upgrades in the hostedCluster.status.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster.
If omitted no particular upgrades are suggested.
TODO(alberto): Consider the backend to use the default channel by default. Default channel will contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>kubeAPIServerDNSName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeAPIServerDNSName specifies a desired DNS name to resolve to the KAS.
When set, the controller will automatically generate a secret with kubeconfig and expose it in the hostedCluster Status.customKubeconfig field.
If it&rsquo;s set or removed day 2, the kubeconfig generated secret will be created, recreated or deleted.
The DNS entries should be resolvable from the cluster, so this should be manually configured in the DNS provider.
This field works in conjunction with configuration.APIServer.ServingCerts.NamedCertificates to enable
access to the API server via a custom domain name. The NamedCertificates provide the TLS certificates
for the custom domain, while this field triggers the generation of a kubeconfig that uses those certificates.
This API endpoint only works in OCP version 4.19 or later. Older versions will result in a no-op.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components like the Kube API Server.
Possible values are HighlyAvailable and SingleReplica. The default value is HighlyAvailable.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on the hosted cluster data plane like the ingress controller and image registry controller.
Possible values are HighlyAvailable and SingleReplica. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>dns specifies the DNS configuration for the hosted cluster ingress.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<p>networking specifies network configuration for the hosted cluster.
Defaults to OVNKubernetes with a cluster network of cidr: &ldquo;10.132.0.0/14&rdquo; and a service network of cidr: &ldquo;172.31.0.0/16&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with this HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoNode specifies the configuration for the autoNode feature.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>etcd specifies configuration for the control plane etcd cluster. The
default managementType is Managed. Once set, the managementType cannot be
changed.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>services specifies how individual control plane services endpoints are published for consumption.
This requires APIServer;OAuthServer;Konnectivity;Ignition.
This field is immutable for all platforms but IBMCloud.
Max is 6 to account for OIDC;OVNSbDb for backward compatibility though they are no-op.</p>
<p>-kubebuilder:validation:XValidation:rule=&ldquo;self.all(s, !(s.service == &lsquo;APIServer&rsquo; &amp;&amp; s.servicePublishingStrategy.type == &lsquo;Route&rsquo;) || has(s.servicePublishingStrategy.route.hostname))&rdquo;,message=&ldquo;If serviceType is &lsquo;APIServer&rsquo; and publishing strategy is &lsquo;Route&rsquo;, then hostname must be set&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.platform.type == &lsquo;IBMCloud&rsquo; ? [&lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;].all(requiredType, self.exists(s, s.service == requiredType))&rdquo;,message=&ldquo;Services list must contain at least &lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, and &lsquo;Konnectivity&rsquo; service types&rdquo; : [&lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, &lsquo;Ignition&rsquo;].all(requiredType, self.exists(s, s.service == requiredType))&ldquo;,message=&ldquo;Services list must contain at least &lsquo;APIServer&rsquo;, &lsquo;OAuthServer&rsquo;, &lsquo;Konnectivity&rsquo;, and &lsquo;Ignition&rsquo; service types&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; has(s.servicePublishingStrategy.route) &amp;&amp; has(s.servicePublishingStrategy.route.hostname)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;Route&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.route) &amp;&amp; has(y.servicePublishingStrategy.route.hostname) &amp;&amp; y.servicePublishingStrategy.route.hostname == x.servicePublishingStrategy.route.hostname)).size() &lt;= 1)&rdquo;,message=&ldquo;Each route publishingStrategy &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
-kubebuilder:validation:XValidation:rule=&ldquo;self.filter(s, s.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; has(s.servicePublishingStrategy.nodePort) &amp;&amp; has(s.servicePublishingStrategy.nodePort.address) &amp;&amp; has(s.servicePublishingStrategy.nodePort.port)).all(x, self.filter(y, y.servicePublishingStrategy.type == &lsquo;NodePort&rsquo; &amp;&amp; (has(y.servicePublishingStrategy.nodePort) &amp;&amp; has(y.servicePublishingStrategy.nodePort.address) &amp;&amp; y.servicePublishingStrategy.nodePort.address == x.servicePublishingStrategy.nodePort.address &amp;&amp; has(y.servicePublishingStrategy.nodePort.port) &amp;&amp; y.servicePublishingStrategy.nodePort.port == x.servicePublishingStrategy.nodePort.port )).size() &lt;= 1)&rdquo;,message=&ldquo;Each nodePort publishingStrategy &lsquo;nodePort&rsquo; and &lsquo;hostname&rsquo; must be unique within the Services list.&rdquo;
TODO(alberto): this breaks the cost budget for &lt; 4.17. We should figure why and enable it back. And If not fixable, consider imposing a minimum version on the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>pullSecret is a local reference to a Secret that must have a &ldquo;.dockerconfigjson&rdquo; key whose content must be a valid Openshift pull secret JSON.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This pull secret will be part of every payload generated by the controllers for any NodePool of the HostedCluster
and it will be injected into the container runtime of all NodePools.
Changing this value will trigger a rollout for all existing NodePools in the cluster.
Changing the content of the secret inplace will not trigger a rollout and might result in unpredictable behaviour.
TODO(alberto): have our own local reference type to include our opinions and avoid transparent changes.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>sshKey is a local reference to a Secret that must have a &ldquo;id_rsa.pub&rdquo; key whose content must be the public part of 1..N SSH keys.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
When sshKey is set, the controllers will generate a machineConfig with the sshAuthorizedKeys <a href="https://coreos.github.io/ignition/configuration-v3_2/">https://coreos.github.io/ignition/configuration-v3_2/</a> populated with this value.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>issuerURL is an OIDC issuer URL which will be used as the issuer in all
ServiceAccount tokens generated by the control plane API server via &ndash;service-account-issuer kube api server flag.
<a href="https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/">https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/</a>
<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection</a>
The default value is kubernetes.default.svc, which only works for in-cluster
validation.
If the platform is AWS and this value is set, the controller will update an s3 object with the appropriate OIDC documents (using the serviceAccountSigningKey info) into that issuerURL.
The expectation is for this s3 url to be backed by an OIDC provider in the AWS IAM.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAccountSigningKey is a local reference to a secret that must have a &ldquo;key&rdquo; key whose content must be the private key
used by the service account token issuer.
If not specified, a service account signing key will
be generated automatically for the cluster.
When specifying a service account signing key, an IssuerURL must also be specified.
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.</p>
<p>For key rotation, the secret may optionally contain an &ldquo;old-key.pub&rdquo; key whose content is the PEM-encoded
public key of the previous signing key. When present, the kube-apiserver will accept tokens signed by
both the current and previous keys, allowing for graceful key rotation without invalidating existing tokens.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>configuration specifies configuration for individual OCP components in the
cluster, represented as embedded resources that correspond to the openshift
configuration API.</p>
</td>
</tr>
<tr>
<td>
<code>operatorConfiguration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">
OperatorConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>auditWebhook contains metadata for configuring an audit webhook endpoint
for a cluster to process cluster audit events. It references a secret that
contains the webhook information for the audit webhook endpoint. It is a
secret because if the endpoint has mTLS the kubeconfig will contain client
keys. The kubeconfig needs to be stored in the secret with a secret key
name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageContentSources specifies image mirrors that can be used by cluster
nodes to pull content.
When imageContentSources is set, the controllers will generate a machineConfig.
This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalTrustBundle is a local reference to a ConfigMap that must have a &ldquo;ca-bundle.crt&rdquo; key
whose content must be a PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes
If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state.
TODO(alberto): Signal this in a condition.
This will be part of every payload generated by the controllers for any NodePool of the HostedCluster.
Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>secretEncryption specifies a Kubernetes secret encryption strategy for the
control plane.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>fips indicates whether this cluster&rsquo;s nodes will be running in FIPS mode.
If set to true, the control plane&rsquo;s ignition server will be configured to
expect that nodes joining the cluster will be FIPS-enabled.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the HostedCluster controller, resulting in any change to the HostedCluster being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>olmCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeSelector when specified, is propagated to all control plane Deployments and Stateful sets running management side.
It must be satisfied by the management Nodes for the pods to be scheduled. Otherwise the HostedCluster will enter a degraded state.
Changes to this field will propagate to existing Deployments and StatefulSets.
TODO(alberto): add additional validation for the map key/values.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tolerations when specified, define what custom tolerations are added to the hcp pods.</p>
</td>
</tr>
<tr>
<td>
<code>labels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>labels when specified, define what custom labels are added to the hcp pods.
Changing this day 2 will cause a rollout of all hcp pods.
Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
<p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=&ldquo;label key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (<em>), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)&rdquo;
-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=&ldquo;label value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores (</em>) or dots (.), and begin and end with an alphanumeric character&rdquo;
TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p>
</td>
</tr>
<tr>
<td>
<code>capabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">
Capabilities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capabilities allows for disabling optional components at cluster install time.
This field is optional and once set cannot be changed.</p>
</td>
</tr>
</tbody>
</table>
###HostedClusterStatus { #hypershift.openshift.io/v1beta1.HostedClusterStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedCluster">HostedCluster</a>)
</p>
<p>
<p>HostedClusterStatus is the latest observed status of a HostedCluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditions represents the latest available observations of a control
plane&rsquo;s current state.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionStatus">
ClusterVersionStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>version is the status of the release version applied to the
HostedCluster.</p>
</td>
</tr>
<tr>
<td>
<code>kubeconfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeconfig is a reference to the secret containing the default kubeconfig
for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>customKubeconfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>customKubeconfig is a local secret reference to the external custom kubeconfig.
Once the hypershift operator sets this status field, it will generate a secret with the specified name containing a kubeconfig within the <code>HostedCluster</code> namespace.</p>
</td>
</tr>
<tr>
<td>
<code>kubeadminPassword</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeadminPassword is a reference to the secret that contains the initial
kubeadmin user password for the guest cluster.</p>
</td>
</tr>
<tr>
<td>
<code>ignitionEndpoint</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ignitionEndpoint is the endpoint injected in the ign config userdata.
It exposes the config for instances to become kubernetes nodes.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIEndpoint">
APIEndpoint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneEndpoint contains the endpoint information by which
external clients can access the control plane. This is populated
after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>oauthCallbackURLTemplate</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>oauthCallbackURLTemplate contains a template for the URL to use as a callback
for identity providers. The [identity-provider-name] placeholder must be replaced
with the name of an identity provider defined on the HostedCluster.
This is populated after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>payloadArch</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PayloadArchType">
PayloadArchType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>payloadArch represents the CPU architecture type of the HostedCluster.Spec.Release.Image. The valid values are:
Multi, ARM64, AMD64, S390X, or PPC64LE.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">
PlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>platform contains platform-specific status of the HostedCluster</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ConfigurationStatus">
ConfigurationStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>configuration contains the cluster configuration status of the HostedCluster</p>
</td>
</tr>
</tbody>
</table>
###HostedControlPlaneSpec { #hypershift.openshift.io/v1beta1.HostedControlPlaneSpec }
<p>
<p>HostedControlPlaneSpec defines the desired state of HostedControlPlane</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>releaseImage</code></br>
<em>
string
</em>
</td>
<td>
<p>releaseImage is the release image applied to the hosted control plane.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneReleaseImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneReleaseImage specifies the desired OCP release payload for
control plane components running on the management cluster.
If not defined, ReleaseImage is used</p>
</td>
</tr>
<tr>
<td>
<code>updateService</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.URL
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>updateService may be used to specify the preferred upstream update service.
By default it will use the appropriate update service for the cluster and region.</p>
</td>
</tr>
<tr>
<td>
<code>channel</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>channel is an identifier for explicitly requesting that a non-default
set of updates be applied to this cluster. The default channel will be
contain stable updates that are appropriate for production clusters.</p>
</td>
</tr>
<tr>
<td>
<code>pullSecret</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>pullSecret is a reference to a secret containing the pull secret for the hosted control plane.</p>
</td>
</tr>
<tr>
<td>
<code>issuerURL</code></br>
<em>
string
</em>
</td>
<td>
<p>issuerURL is an OIDC issuer URL which is used as the issuer in all
ServiceAccount tokens generated by the control plane API server. The
default value is kubernetes.default.svc, which only works for in-cluster
validation.</p>
</td>
</tr>
<tr>
<td>
<code>networking</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">
ClusterNetworking
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>networking specifies network configuration for the cluster.
Temporarily optional for backward compatibility, required in future releases.</p>
</td>
</tr>
<tr>
<td>
<code>sshKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>sshKey is a reference to a secret containing the SSH key for the hosted control plane.</p>
</td>
</tr>
<tr>
<td>
<code>clusterID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterID is the unique id that identifies the cluster externally.
Making it optional here allows us to keep compatibility with previous
versions of the control-plane-operator that have no knowledge of this
field.</p>
</td>
</tr>
<tr>
<td>
<code>infraID</code></br>
<em>
string
</em>
</td>
<td>
<p>infraID is the unique id that identifies the cluster internally.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">
PlatformSpec
</a>
</em>
</td>
<td>
<p>platform is the platform configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dns</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.DNSSpec">
DNSSpec
</a>
</em>
</td>
<td>
<p>dns is the DNS configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountSigningKey</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>serviceAccountSigningKey is a reference to a secret containing the private key
used by the service account token issuer. The secret is expected to contain
a single key named &ldquo;key&rdquo;. If not specified, a service account signing key will
be generated automatically for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>controllerAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controllerAvailabilityPolicy specifies the availability policy applied to
critical control plane components. The default value is SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureAvailabilityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AvailabilityPolicy">
AvailabilityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>infrastructureAvailabilityPolicy specifies the availability policy applied
to infrastructure services which run on cluster nodes. The default value is
SingleReplica.</p>
</td>
</tr>
<tr>
<td>
<code>fips</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>fips specifies if the nodes for the cluster will be running in FIPS mode</p>
</td>
</tr>
<tr>
<td>
<code>kubeconfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeconfig specifies the name and key for the kubeconfig secret</p>
</td>
</tr>
<tr>
<td>
<code>kubeAPIServerDNSName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeAPIServerDNSName specifies a desired DNS name to resolve to the KAS.
When set, the controller will automatically generate a secret with kubeconfig and expose it in the hostedCluster Status.customKubeconfig field.
If it&rsquo;s set or removed day 2, the kubeconfig generated secret will be created, recreated or deleted.
The DNS entries should be resolvable from the cluster, so this should be manually configured in the DNS provider.
This field works in conjunction with configuration.APIServer.ServingCerts.NamedCertificates to enable
access to the API server via a custom domain name. The NamedCertificates provide the TLS certificates
for the custom domain, while this field triggers the generation of a kubeconfig that uses those certificates.</p>
</td>
</tr>
<tr>
<td>
<code>services</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">
[]ServicePublishingStrategyMapping
</a>
</em>
</td>
<td>
<p>services defines metadata about how control plane services are published
in the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>auditWebhook</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>auditWebhook contains metadata for configuring an audit webhook
endpoint for a cluster to process cluster audit events. It references
a secret that contains the webhook information for the audit webhook endpoint.
It is a secret because if the endpoint has MTLS the kubeconfig will contain client
keys. This is currently only supported in IBM Cloud. The kubeconfig needs to be stored
in the secret with a secret key name that corresponds to the constant AuditWebhookKubeconfigKey.</p>
</td>
</tr>
<tr>
<td>
<code>etcd</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">
EtcdSpec
</a>
</em>
</td>
<td>
<p>etcd contains metadata about the etcd cluster the hypershift managed Openshift control plane components
use to store data.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterConfiguration">
ClusterConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>configuration embeds resources that correspond to the openshift configuration API:
<a href="https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html">https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</a></p>
</td>
</tr>
<tr>
<td>
<code>operatorConfiguration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">
OperatorConfiguration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>imageContentSources</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ImageContentSource">
[]ImageContentSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageContentSources lists sources/repositories for the release-image content.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTrustBundle</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalTrustBundle references a ConfigMap containing a PEM-encoded X.509 certificate bundle</p>
</td>
</tr>
<tr>
<td>
<code>secretEncryption</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">
SecretEncryptionSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>secretEncryption contains metadata about the kubernetes secret encryption strategy being used for the
cluster when applicable.</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on a resource.
Either a date can be provided in RFC3339 format or a boolean. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>olmCatalogPlacement</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OLMCatalogPlacement">
OLMCatalogPlacement
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>olmCatalogPlacement specifies the placement of OLM catalog components. By default,
this is set to management and OLM catalog components are deployed onto the management
cluster. If set to guest, the OLM catalog components will be deployed onto the guest
cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">
ClusterAutoscaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoscaling specifies auto-scaling behavior that applies to all NodePools
associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoNode specifies the configuration for the autoNode feature.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeSelector when specified, must be true for the pods managed by the HostedCluster to be scheduled.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tolerations when specified, define what custom tolerations are added to the hcp pods.</p>
</td>
</tr>
<tr>
<td>
<code>labels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>labels when specified, define what custom labels are added to the hcp pods.
Changing this day 2 will cause a rollout of all hcp pods.
Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
<p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=&ldquo;label key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (<em>), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)&rdquo;
-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=&ldquo;label value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores (</em>) or dots (.), and begin and end with an alphanumeric character&rdquo;
TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p>
</td>
</tr>
<tr>
<td>
<code>capabilities</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">
Capabilities
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capabilities allows for disabling optional components at cluster install time.
This field is optional and once set cannot be changed.</p>
</td>
</tr>
</tbody>
</table>
###HostedControlPlaneStatus { #hypershift.openshift.io/v1beta1.HostedControlPlaneStatus }
<p>
<p>HostedControlPlaneStatus defines the observed state of HostedControlPlane</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#condition-v1-meta">
[]Kubernetes meta/v1.Condition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditions contains details for one aspect of the current state of the HostedControlPlane.
Current condition types are: &ldquo;Available&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>ready</code></br>
<em>
bool
</em>
</td>
<td>
<p>ready denotes that the HostedControlPlane API Server is ready to
receive requests
This satisfies CAPI contract <a href="https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230">https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230</a></p>
</td>
</tr>
<tr>
<td>
<code>initialized</code></br>
<em>
bool
</em>
</td>
<td>
<p>initialized denotes whether or not the control plane has
provided a kubeadm-config.
Once this condition is marked true, its value is never changed. See the Ready condition for an indication of
the current readiness of the cluster&rsquo;s control plane.
This satisfies CAPI contract <a href="https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252">https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252</a></p>
</td>
</tr>
<tr>
<td>
<code>externalManagedControlPlane</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>externalManagedControlPlane indicates to cluster-api that the control plane
is managed by an external service.
<a href="https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468">https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468</a></p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.APIEndpoint">
APIEndpoint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>controlPlaneEndpoint contains the endpoint information by which
external clients can access the control plane.  This is populated
after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>oauthCallbackURLTemplate</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>oauthCallbackURLTemplate contains a template for the URL to use as a callback
for identity providers. The [identity-provider-name] placeholder must be replaced
with the name of an identity provider defined on the HostedCluster.
This is populated after the infrastructure is ready.</p>
</td>
</tr>
<tr>
<td>
<code>versionStatus</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionStatus">
ClusterVersionStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>versionStatus is the status of the release version applied by the
hosted control plane operator.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>version is the semantic version of the release applied by
the hosted control plane operator</p>
<p>Deprecated: Use versionStatus.desired.version instead.</p>
</td>
</tr>
<tr>
<td>
<code>releaseImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>releaseImage is the release image applied to the hosted control plane.</p>
<p>Deprecated: Use versionStatus.desired.image instead.</p>
</td>
</tr>
<tr>
<td>
<code>lastReleaseImageTransitionTime</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>lastReleaseImageTransitionTime is the time of the last update to the current
releaseImage property.</p>
<p>Deprecated: Use versionStatus.history[0].startedTime instead.</p>
</td>
</tr>
<tr>
<td>
<code>kubeConfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeConfig is a reference to the secret containing the default kubeconfig
for this control plane.</p>
</td>
</tr>
<tr>
<td>
<code>customKubeconfig</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>customKubeconfig references an external custom kubeconfig secret.
This field is populated in the status when a custom kubeconfig secret has been generated
for the hosted cluster. It contains the name and key of the secret located in the
hostedCluster namespace. This field is only populated when kubeApiExternalName is set.
If this field is removed during a day 2 operation, the referenced secret will be deleted
and this field will be removed from the hostedCluster status.</p>
</td>
</tr>
<tr>
<td>
<code>kubeadminPassword</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeadminPassword is a reference to the secret containing the initial kubeadmin password
for the guest cluster.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformStatus">
PlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>platform contains platform-specific status of the HostedCluster</p>
</td>
</tr>
<tr>
<td>
<code>nodeCount</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeCount tracks the number of nodes in the HostedControlPlane.</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ConfigurationStatus">
ConfigurationStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>configuration contains the cluster configuration status of the HostedCluster</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">IBMCloudKMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSAuthSpec defines metadata for how authentication is done with IBM Cloud KMS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType">
IBMCloudKMSAuthType
</a>
</em>
</td>
<td>
<p>type defines the IBM Cloud KMS authentication strategy</p>
</td>
</tr>
<tr>
<td>
<code>unmanaged</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec">
IBMCloudKMSUnmanagedAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>unmanaged defines the auth metadata the customer provides to interact with IBM Cloud KMS</p>
</td>
</tr>
<tr>
<td>
<code>managed</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec">
IBMCloudKMSManagedAuthSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>managed defines metadata around the service to service authentication strategy for the IBM Cloud
KMS system (all provider managed).</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSAuthType { #hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSAuthType defines the IBM Cloud KMS authentication strategy</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>IBMCloudKMSManagedAuth defines the KMS authentication strategy where the IKS/ROKS platform uses
service to service auth to call IBM Cloud KMS APIs (no customer credentials required)</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>IBMCloudKMSUnmanagedAuth defines the KMS authentication strategy where a customer supplies IBM Cloud
authentication to interact with IBM Cloud KMS APIs</p>
</td>
</tr></tbody>
</table>
###IBMCloudKMSKeyEntry { #hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">IBMCloudKMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSKeyEntry defines metadata for an IBM Cloud KMS encryption key</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>crkID</code></br>
<em>
string
</em>
</td>
<td>
<p>crkID is the customer rook key id</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code></br>
<em>
string
</em>
</td>
<td>
<p>instanceID is the id for the key protect instance</p>
</td>
</tr>
<tr>
<td>
<code>correlationID</code></br>
<em>
string
</em>
</td>
<td>
<p>correlationID is an identifier used to track all api call usage from hypershift</p>
</td>
</tr>
<tr>
<td>
<code>url</code></br>
<em>
string
</em>
</td>
<td>
<p>url is the url to call key protect apis over</p>
</td>
</tr>
<tr>
<td>
<code>keyVersion</code></br>
<em>
int
</em>
</td>
<td>
<p>keyVersion is a unique number associated with the key. The number increments whenever a new
key is enabled for data encryption.</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSManagedAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSManagedAuthSpec defines metadata around the service to service authentication strategy for the IBM Cloud
KMS system (all provider managed).</p>
</p>
###IBMCloudKMSSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>IBMCloudKMSSpec defines metadata for the IBM Cloud KMS encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>region is the IBM Cloud region</p>
</td>
</tr>
<tr>
<td>
<code>auth</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">
IBMCloudKMSAuthSpec
</a>
</em>
</td>
<td>
<p>auth defines metadata for how authentication is done with IBM Cloud KMS</p>
</td>
</tr>
<tr>
<td>
<code>keyList</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry">
[]IBMCloudKMSKeyEntry
</a>
</em>
</td>
<td>
<p>keyList defines the list of keys used for data encryption</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudKMSUnmanagedAuthSpec { #hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec">IBMCloudKMSAuthSpec</a>)
</p>
<p>
<p>IBMCloudKMSUnmanagedAuthSpec defines the auth metadata the customer provides to interact with IBM Cloud KMS</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>credentials should reference a secret with a key field of IBMCloudIAMAPIKeySecretKey that contains a apikey to
call IBM Cloud KMS APIs</p>
</td>
</tr>
</tbody>
</table>
###IBMCloudPlatformSpec { #hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>IBMCloudPlatformSpec defines IBMCloud specific settings for components</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerType</code></br>
<em>
<a href="https://docs.openshift.com/container-platform/4.10/rest_api/config_apis/config-apis-index.html">
github.com/openshift/api/config/v1.IBMCloudProviderType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>providerType is a specific supported infrastructure provider within IBM Cloud.</p>
</td>
</tr>
</tbody>
</table>
###ImageContentSource { #hypershift.openshift.io/v1beta1.ImageContentSource }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ImageContentSource specifies image mirrors that can be used by cluster nodes
to pull content. For cluster workloads, if a container image registry host of
the pullspec matches Source then one of the Mirrors are substituted as hosts
in the pullspec and tried in order to fetch the image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>source</code></br>
<em>
string
</em>
</td>
<td>
<p>source is the repository that users refer to, e.g. in image pull
specifications.</p>
</td>
</tr>
<tr>
<td>
<code>mirrors</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>mirrors are one or more repositories that may also contain the same images.</p>
</td>
</tr>
</tbody>
</table>
###ImageType { #hypershift.openshift.io/v1beta1.ImageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>ImageType specifies the type of image to use for node instances.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Linux&#34;</p></td>
<td><p>ImageTypeLinux represents the default image type (Linux/RHCOS).
This is used when ImageType is empty or unspecified.</p>
</td>
</tr><tr><td><p>&#34;Windows&#34;</p></td>
<td><p>ImageTypeWindows represents a Windows-based image type.
When set, the controller will automatically populate the AMI field
with a Windows-compatible AMI based on the region and OpenShift version.</p>
</td>
</tr></tbody>
</table>
###InPlaceUpgrade { #hypershift.openshift.io/v1beta1.InPlaceUpgrade }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>InPlaceUpgrade specifies an upgrade strategy which upgrades nodes in-place
without any new nodes being created or any old nodes being deleted.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxUnavailable is the maximum number of nodes that can be unavailable
during the update.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding down.</p>
<p>Defaults to 1.</p>
<p>Example: when this is set to 30%, a max of 30% of the nodes can be made
unschedulable/unavailable immediately when the update starts. Once a set
of nodes is updated, more nodes can be made unschedulable for update,
ensuring that the total number of nodes schedulable at all times during
the update is at least 70% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
###IngressOperatorSpec { #hypershift.openshift.io/v1beta1.IngressOperatorSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OperatorConfiguration">OperatorConfiguration</a>)
</p>
<p>
<p>IngressOperatorSpec is the specification of the desired behavior of the Ingress Operator.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>endpointPublishingStrategy</code></br>
<em>
github.com/openshift/api/operator/v1.EndpointPublishingStrategy
</em>
</td>
<td>
<em>(Optional)</em>
<p>endpointPublishingStrategy is used to publish the default ingress controller endpoints.</p>
<p>The endpoint publishing strategy is determined by the following precedence order:
1. User-specified endpointPublishingStrategy (highest priority) - if this field is set,
it takes precedence over all other configuration methods
2. Platform-specific defaults with annotation overrides - if no user strategy is set,
the platform type determines the default strategy, which can be further modified by:
- hypershift.openshift.io/private-ingress-controller annotation (sets PrivateStrategyType)
- hypershift.openshift.io/ingress-controller-load-balancer-scope annotation (sets LoadBalancerScope)
3. Generic LoadBalancer fallback - if the platform is not recognized, defaults to
LoadBalancerService with External scope</p>
<p>Platform-specific defaults when endpointPublishingStrategy is not set:
- AWS: LoadBalancerService with External scope (or NLB if configured)
- Azure, GCP: LoadBalancerService with External scope
- IBMCloud: LoadBalancerService with External scope (or NodePort for UPI)
- None: HostNetwork
- KubeVirt: NodePortService
- OpenStack: LoadBalancerService with External scope and optional FloatingIP
- Other platforms: LoadBalancerService with External scope</p>
<p>See the OpenShift Ingress Operator EndpointPublishingStrategy type for the full specification:
<a href="https://github.com/openshift/api/blob/master/operator/v1/types_ingress.go">https://github.com/openshift/api/blob/master/operator/v1/types_ingress.go</a></p>
</td>
</tr>
</tbody>
</table>
###InstanceType { #hypershift.openshift.io/v1beta1.InstanceType }
<p>
</p>
###KMSProvider { #hypershift.openshift.io/v1beta1.KMSProvider }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">KMSSpec</a>)
</p>
<p>
<p>KMSProvider defines the supported KMS providers</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWS&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Azure&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;IBMCloud&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###KMSSpec { #hypershift.openshift.io/v1beta1.KMSSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>KMSSpec defines metadata about the kms secret encryption strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provider</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KMSProvider">
KMSProvider
</a>
</em>
</td>
<td>
<p>provider defines the KMS provider</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec">
IBMCloudKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ibmcloud defines metadata for the IBM Cloud KMS encryption strategy</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSKMSSpec">
AWSKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aws defines metadata about the configuration of the AWS KMS Secret Encryption provider</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">
AzureKMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>azure defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>
</td>
</tr>
</tbody>
</table>
###KarpenterAWSConfig { #hypershift.openshift.io/v1beta1.KarpenterAWSConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterConfig">KarpenterConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>roleARN</code></br>
<em>
string
</em>
</td>
<td>
<p>roleARN specifies the ARN of the Karpenter provisioner.</p>
</td>
</tr>
</tbody>
</table>
###KarpenterConfig { #hypershift.openshift.io/v1beta1.KarpenterConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ProvisionerConfig">ProvisionerConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>platform specifies the platform-specific configuration for Karpenter.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterAWSConfig">
KarpenterAWSConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aws specifies the AWS-specific configuration for Karpenter.</p>
</td>
</tr>
</tbody>
</table>
###KubeVirtNodePoolStatus { #hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus">NodePoolPlatformStatus</a>)
</p>
<p>
<p>KubeVirtNodePoolStatus contains the KubeVirt platform statuses</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cacheName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>cacheName holds the name of the cache DataVolume, if exists</p>
</td>
</tr>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials">
KubevirtPlatformCredentials
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>credentials shows the client credentials used when creating KubeVirt virtual machines.
This filed is only exists when the KubeVirt virtual machines are being placed
on a cluster separate from the one hosting the Hosted Control Plane components.</p>
<p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on
the same cluster and namespace as the Hosted Control Plane.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtCachingStrategy { #hypershift.openshift.io/v1beta1.KubevirtCachingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtCachingStrategy defines the boot image caching strategy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType">
KubevirtCachingStrategyType
</a>
</em>
</td>
<td>
<p>type is the type of the caching strategy</p>
</td>
</tr>
</tbody>
</table>
###KubevirtCachingStrategyType { #hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy">KubevirtCachingStrategy</a>)
</p>
<p>
<p>KubevirtCachingStrategyType is the type of the boot image caching mechanism for the KubeVirt provider</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;None&#34;</p></td>
<td><p>KubevirtCachingStrategyNone means that hypershift will not cache the boot image</p>
</td>
</tr><tr><td><p>&#34;PVC&#34;</p></td>
<td><p>KubevirtCachingStrategyPVC means that hypershift will cache the boot image into a PVC; only relevant when using
a QCOW boot image, and is ignored when using a container image</p>
</td>
</tr></tbody>
</table>
###KubevirtCompute { #hypershift.openshift.io/v1beta1.KubevirtCompute }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtCompute contains values associated with the virtual compute hardware requested for the VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>memory</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>memory represents how much guest memory the VM should have</p>
</td>
</tr>
<tr>
<td>
<code>cores</code></br>
<em>
uint32
</em>
</td>
<td>
<em>(Optional)</em>
<p>cores is the number of CPU cores for the KubeVirt VM.</p>
</td>
</tr>
<tr>
<td>
<code>qosClass</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.QoSClass">
QoSClass
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>qosClass if set to &ldquo;Guaranteed&rdquo;, requests the scheduler to place the VirtualMachineInstance on a node with
limit memory and CPU, equal to be the requested values, to set the VMI as a Guaranteed QoS Class;
See here for more details:
<a href="https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances">https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances</a></p>
</td>
</tr>
</tbody>
</table>
###KubevirtDiskImage { #hypershift.openshift.io/v1beta1.KubevirtDiskImage }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtDiskImage contains values representing where the rhcos image is located</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>containerDiskImage</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>containerDiskImage is a string representing the container image that holds the root disk</p>
</td>
</tr>
</tbody>
</table>
###KubevirtHostDevice { #hypershift.openshift.io/v1beta1.KubevirtHostDevice }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code></br>
<em>
string
</em>
</td>
<td>
<p>deviceName is the name of the host device that is desired to be utilized in the HostedCluster&rsquo;s NodePool
The device can be any supported PCI device, including GPU, either as a passthrough or a vGPU slice.</p>
</td>
</tr>
<tr>
<td>
<code>count</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>count is the number of instances the specified host device will be attached to each of the
NodePool&rsquo;s nodes. Default is 1.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtManualStorageDriverConfig { #hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">KubevirtStorageDriverSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageClassMapping</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping">
[]KubevirtStorageClassMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageClassMapping maps StorageClasses on the infra cluster hosting
the KubeVirt VMs to StorageClasses that are made available within the
Guest Cluster.</p>
<p>NOTE: It is possible that not all capabilities of an infra cluster&rsquo;s
storageclass will be present for the corresponding guest clusters storageclass.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSnapshotClassMapping</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping">
[]KubevirtVolumeSnapshotClassMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>volumeSnapshotClassMapping maps VolumeSnapshotClasses on the infra cluster hosting
the KubeVirt VMs to VolumeSnapshotClasses that are made available within the
Guest Cluster.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtNetwork { #hypershift.openshift.io/v1beta1.KubevirtNetwork }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtNetwork specifies the configuration for a virtual machine
network interface</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name specify the network attached to the nodes
it is a value with the format &ldquo;[namespace]/[name]&rdquo; to reference the
multus network attachment definition</p>
</td>
</tr>
</tbody>
</table>
###KubevirtNodePoolPlatform { #hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>KubevirtNodePoolPlatform specifies the configuration of a NodePool when operating
on KubeVirt platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rootVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">
KubevirtRootVolume
</a>
</em>
</td>
<td>
<p>rootVolume represents values associated with the VM volume that will host rhcos</p>
</td>
</tr>
<tr>
<td>
<code>compute</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCompute">
KubevirtCompute
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>compute contains values representing the virtual hardware requested for the VM</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceMultiqueue</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.MultiQueueSetting">
MultiQueueSetting
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>networkInterfaceMultiqueue if set to &ldquo;Enable&rdquo;, virtual network interfaces configured with a virtio bus will also
enable the vhost multiqueue feature for network devices. The number of queues created depends on additional
factors of the VirtualMachineInstance, like the number of guest CPUs.</p>
</td>
</tr>
<tr>
<td>
<code>additionalNetworks</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNetwork">
[]KubevirtNetwork
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalNetworks specify the extra networks attached to the nodes</p>
</td>
</tr>
<tr>
<td>
<code>attachDefaultNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>attachDefaultNetwork specify if the default pod network should be attached to the nodes
this can only be set to false if AdditionalNetworks are configured</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeSelector is a selector which must be true for the kubevirt VirtualMachine to fit on a node.
Selector which must match a node&rsquo;s labels for the VM to be scheduled on that node. More info:
<a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></p>
</td>
</tr>
<tr>
<td>
<code>hostDevices</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtHostDevice">
[]KubevirtHostDevice
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>hostDevices specifies the host devices (e.g. GPU devices) to be passed
from the management cluster, to the nodepool nodes</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPersistentVolume { #hypershift.openshift.io/v1beta1.KubevirtPersistentVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">KubevirtVolume</a>)
</p>
<p>
<p>KubevirtPersistentVolume contains the values involved with provisioning persistent storage for a KubeVirt VM.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>size</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>size is the size of the persistent storage volume</p>
</td>
</tr>
<tr>
<td>
<code>storageClass</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageClass is the storageClass used for the underlying PVC that hosts the volume</p>
</td>
</tr>
<tr>
<td>
<code>accessModes</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode">
[]PersistentVolumeAccessMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>accessModes is an array that contains the desired Access Modes the root volume should have.
More info: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a></p>
</td>
</tr>
<tr>
<td>
<code>volumeMode</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#persistentvolumemode-v1-core">
Kubernetes core/v1.PersistentVolumeMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>volumeMode defines what type of volume is required by the claim.
Value of Filesystem is implied when not included in claim spec.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPlatformCredentials { #hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus">KubeVirtNodePoolStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">KubevirtPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>infraKubeConfigSecret</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeconfigSecretRef">
KubeconfigSecretRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>infraKubeConfigSecret is a reference to the secret containing the kubeconfig
of an external infrastructure cluster for kubevirt provider</p>
</td>
</tr>
<tr>
<td>
<code>infraNamespace</code></br>
<em>
string
</em>
</td>
<td>
<p>infraNamespace is the namespace in the external infrastructure cluster
where kubevirt resources will be created</p>
</td>
</tr>
</tbody>
</table>
###KubevirtPlatformSpec { #hypershift.openshift.io/v1beta1.KubevirtPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>KubevirtPlatformSpec specifies configuration for kubevirt guest cluster installations</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>baseDomainPassthrough</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>baseDomainPassthrough toggles whether or not an automatically
generated base domain for the guest cluster should be used that
is a subdomain of the management cluster&rsquo;s *.apps DNS.</p>
<p>For the KubeVirt platform, the basedomain can be autogenerated using
the *.apps domain of the management/infra hosting cluster
This makes the guest cluster&rsquo;s base domain a subdomain of the
hypershift infra/mgmt cluster&rsquo;s base domain.</p>
<p>Example:
Infra/Mgmt cluster&rsquo;s DNS
Base: example.com
Cluster: mgmt-cluster.example.com
Apps:    *.apps.mgmt-cluster.example.com
KubeVirt Guest cluster&rsquo;s DNS
Base: apps.mgmt-cluster.example.com
Cluster: guest.apps.mgmt-cluster.example.com
Apps: *.apps.guest.apps.mgmt-cluster.example.com</p>
<p>This is possible using OCP wildcard routes</p>
</td>
</tr>
<tr>
<td>
<code>generateID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>generateID is used to uniquely apply a name suffix to resources associated with
kubevirt infrastructure resources</p>
</td>
</tr>
<tr>
<td>
<code>credentials</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials">
KubevirtPlatformCredentials
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>credentials defines the client credentials used when creating KubeVirt virtual machines.
Defining credentials is only necessary when the KubeVirt virtual machines are being placed
on a cluster separate from the one hosting the Hosted Control Plane components.</p>
<p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on
the same cluster and namespace as the Hosted Control Plane.</p>
</td>
</tr>
<tr>
<td>
<code>storageDriver</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">
KubevirtStorageDriverSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageDriver defines how the KubeVirt CSI driver exposes StorageClasses on
the infra cluster (hosting the VMs) to the guest cluster.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtRootVolume { #hypershift.openshift.io/v1beta1.KubevirtRootVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
<p>KubevirtRootVolume represents the volume that the rhcos disk will be stored and run from.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>diskImage</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtDiskImage">
KubevirtDiskImage
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>diskImage represents what rhcos image to use for the node pool</p>
</td>
</tr>
<tr>
<td>
<code>KubevirtVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">
KubevirtVolume
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubevirtVolume</code> are embedded into this type.)
</p>
<p>kubevirtVolume represents of type of storage to run the image on</p>
</td>
</tr>
<tr>
<td>
<code>cacheStrategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy">
KubevirtCachingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>cacheStrategy defines the boot image caching strategy. Default - no caching</p>
</td>
</tr>
</tbody>
</table>
###KubevirtStorageClassMapping { #hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">KubevirtManualStorageDriverConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>group contains which group this mapping belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>infraStorageClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>infraStorageClassName is the name of the infra cluster storage class that
will be exposed to the guest.</p>
</td>
</tr>
<tr>
<td>
<code>guestStorageClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>guestStorageClassName is the name that the corresponding storageclass will
be called within the guest cluster</p>
</td>
</tr>
</tbody>
</table>
###KubevirtStorageDriverConfigType { #hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec">KubevirtStorageDriverSpec</a>)
</p>
<p>
<p>KubevirtStorageDriverConfigType defines how the kubevirt storage driver is configured.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Default&#34;</p></td>
<td><p>DefaultKubevirtStorageDriverConfigType means the kubevirt storage driver maps to the
underlying infra cluster&rsquo;s default storageclass</p>
</td>
</tr><tr><td><p>&#34;Manual&#34;</p></td>
<td><p>ManualKubevirtStorageDriverConfigType means the kubevirt storage driver mapping is
explicitly defined.</p>
</td>
</tr><tr><td><p>&#34;None&#34;</p></td>
<td><p>NoneKubevirtStorageDriverConfigType means no kubevirt storage driver is used</p>
</td>
</tr></tbody>
</table>
###KubevirtStorageDriverSpec { #hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">KubevirtPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType">
KubevirtStorageDriverConfigType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>type represents the type of kubevirt csi driver configuration to use</p>
</td>
</tr>
<tr>
<td>
<code>manual</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">
KubevirtManualStorageDriverConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>manual is used to explicitly define how the infra storageclasses are
mapped to guest storageclasses</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolume { #hypershift.openshift.io/v1beta1.KubevirtVolume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtRootVolume">KubevirtRootVolume</a>)
</p>
<p>
<p>KubevirtVolume represents what kind of storage to use for a KubeVirt VM volume</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolumeType">
KubevirtVolumeType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>type represents the type of storage to associate with the kubevirt VMs.</p>
</td>
</tr>
<tr>
<td>
<code>persistent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume">
KubevirtPersistentVolume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>persistent volume type means the VM&rsquo;s storage is backed by a PVC
VMs that use persistent volumes can survive disruption events like restart and eviction
This is the default type used when no storage type is defined.</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolumeSnapshotClassMapping { #hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig">KubevirtManualStorageDriverConfig</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>group</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>group contains which group this mapping belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>infraVolumeSnapshotClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>infraVolumeSnapshotClassName is the name of the infra cluster volume snapshot class that
will be exposed to the guest.</p>
</td>
</tr>
<tr>
<td>
<code>guestVolumeSnapshotClassName</code></br>
<em>
string
</em>
</td>
<td>
<p>guestVolumeSnapshotClassName is the name that the corresponding volumeSnapshotClass will
be called within the guest cluster</p>
</td>
</tr>
</tbody>
</table>
###KubevirtVolumeType { #hypershift.openshift.io/v1beta1.KubevirtVolumeType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtVolume">KubevirtVolume</a>)
</p>
<p>
<p>KubevirtVolumeType is a specific supported KubeVirt volumes</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Persistent&#34;</p></td>
<td><p>KubevirtVolumeTypePersistent represents persistent volume for kubevirt VMs</p>
</td>
</tr></tbody>
</table>
###LoadBalancerPublishingStrategy { #hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>LoadBalancerPublishingStrategy specifies setting used to expose a service as a LoadBalancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>hostname is the name of the DNS record that will be created pointing to the LoadBalancer and passed through to consumers of the service.
If omitted, the value will be inferred from the corev1.Service Load balancer type .status.</p>
</td>
</tr>
</tbody>
</table>
###LogLevel { #hypershift.openshift.io/v1beta1.LogLevel }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec">ClusterVersionOperatorSpec</a>)
</p>
<p>
</p>
###MachineNetworkEntry { #hypershift.openshift.io/v1beta1.MachineNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>MachineNetworkEntry is a single IP address block for node IP blocks.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>cidr is the IP block address pool for machines within the cluster.</p>
</td>
</tr>
</tbody>
</table>
###ManagedAzureKeyVault { #hypershift.openshift.io/v1beta1.ManagedAzureKeyVault }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities">ControlPlaneManagedIdentities</a>)
</p>
<p>
<p>ManagedAzureKeyVault is an Azure Key Vault on the management cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of the Azure Key Vault on the management cluster.</p>
</td>
</tr>
<tr>
<td>
<code>tenantID</code></br>
<em>
string
</em>
</td>
<td>
<p>tenantID is the tenant ID of the Azure Key Vault on the management cluster.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdSpec { #hypershift.openshift.io/v1beta1.ManagedEtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>ManagedEtcdSpec specifies the behavior of an etcd cluster managed by
HyperShift.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storage</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">
ManagedEtcdStorageSpec
</a>
</em>
</td>
<td>
<p>storage specifies how etcd data is persisted.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdStorageSpec { #hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdSpec">ManagedEtcdSpec</a>)
</p>
<p>
<p>ManagedEtcdStorageSpec describes the storage configuration for etcd data.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageType">
ManagedEtcdStorageType
</a>
</em>
</td>
<td>
<p>type is the kind of persistent storage implementation to use for etcd.
Only PersistentVolume is supported at the moment.</p>
</td>
</tr>
<tr>
<td>
<code>persistentVolume</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec">
PersistentVolumeEtcdStorageSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>persistentVolume is the configuration for PersistentVolume etcd storage.
With this implementation, a PersistentVolume will be allocated for every
etcd member (either 1 or 3 depending on the HostedCluster control plane
availability configuration).</p>
</td>
</tr>
<tr>
<td>
<code>restoreSnapshotURL</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>restoreSnapshotURL allows an optional URL to be provided where
an etcd snapshot can be downloaded, for example a pre-signed URL
referencing a storage service.
This snapshot will be restored on initial startup, only when the etcd PV
is empty.</p>
</td>
</tr>
</tbody>
</table>
###ManagedEtcdStorageType { #hypershift.openshift.io/v1beta1.ManagedEtcdStorageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">ManagedEtcdStorageSpec</a>)
</p>
<p>
<p>ManagedEtcdStorageType is a storage type for an etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;PersistentVolume&#34;</p></td>
<td><p>PersistentVolumeEtcdStorage uses PersistentVolumes for etcd storage.</p>
</td>
</tr></tbody>
</table>
###ManagedIdentity { #hypershift.openshift.io/v1beta1.ManagedIdentity }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureKMSSpec">AzureKMSSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities">ControlPlaneManagedIdentities</a>)
</p>
<p>
<p>ManagedIdentity contains the client ID, and its certificate name, of a managed identity. This managed identity is
used, by an HCP component, to authenticate with the Azure API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientID</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureClientID">
AzureClientID
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>clientID is the client ID of a managed identity associated with CredentialsSecretName. This field is optional and
mainly used for CI purposes.</p>
</td>
</tr>
<tr>
<td>
<code>objectEncoding</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ObjectEncodingFormat">
ObjectEncodingFormat
</a>
</em>
</td>
<td>
<p>objectEncoding represents the encoding for the Azure Key Vault secret containing the certificate related to
the managed identity. objectEncoding needs to match the encoding format used when the certificate was stored in the
Azure Key Vault. If objectEncoding doesn&rsquo;t match the encoding format of the certificate, the certificate will
unsuccessfully be read by the Secrets CSI driver and an error will occur. This error will only be visible on the
SecretProviderClass custom resource related to the managed identity.</p>
<p>The default value is utf-8.</p>
<p>See this for more info - <a href="https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md">https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md</a></p>
</td>
</tr>
<tr>
<td>
<code>credentialsSecretName</code></br>
<em>
string
</em>
</td>
<td>
<p>credentialsSecretName is the name of an Azure Key Vault secret. This field assumes the secret contains the JSON
format of a UserAssignedIdentityCredentials struct. At a minimum, the secret needs to contain the ClientId,
ClientSecret, AuthenticationEndpoint, NotBefore, and NotAfter, and TenantId.</p>
<p>More info on this struct can be found here - <a href="https://github.com/Azure/msi-dataplane/blob/63fb37d3a1aaac130120624674df795d2e088083/pkg/dataplane/internal/generated_client.go#L156">https://github.com/Azure/msi-dataplane/blob/63fb37d3a1aaac130120624674df795d2e088083/pkg/dataplane/internal/generated_client.go#L156</a>.</p>
<p>credentialsSecretName must be between 1 and 127 characters and use only alphanumeric characters and hyphens.
credentialsSecretName must also be unique within the Azure Key Vault. See more details here - <a href="https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.KeyVault.SecretName/">https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.KeyVault.SecretName/</a>.</p>
</td>
</tr>
</tbody>
</table>
###MarketType { #hypershift.openshift.io/v1beta1.MarketType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.CapacityReservationOptions">CapacityReservationOptions</a>)
</p>
<p>
<p>MarketType describes the market type of the CapacityReservation for an Instance.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;CapacityBlocks&#34;</p></td>
<td><p>MarketTypeCapacityBlock is a MarketType enum value</p>
</td>
</tr><tr><td><p>&#34;OnDemand&#34;</p></td>
<td><p>MarketTypeOnDemand is a MarketType enum value</p>
</td>
</tr></tbody>
</table>
###MultiQueueSetting { #hypershift.openshift.io/v1beta1.MultiQueueSetting }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">KubevirtNodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disable&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Enable&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###NetworkFilter { #hypershift.openshift.io/v1beta1.NetworkFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">NetworkParam</a>)
</p>
<p>
<p>NetworkFilter specifies a query to select an OpenStack network. At least one property must be set.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>name is the name of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>description is the description of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>projectID is the project ID of the network to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###NetworkParam { #hypershift.openshift.io/v1beta1.NetworkParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.PortSpec">PortSpec</a>)
</p>
<p>
<p>NetworkParam specifies an OpenStack network. It may be specified by either ID or Filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>id is the ID of the network to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkFilter">
NetworkFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>filter specifies a filter to select an OpenStack network. If provided, cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###NetworkType { #hypershift.openshift.io/v1beta1.NetworkType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>NetworkType specifies the SDN provider used for cluster networking.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Calico&#34;</p></td>
<td><p>Calico specifies Calico as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;OVNKubernetes&#34;</p></td>
<td><p>OVNKubernetes specifies OVN as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;OpenShiftSDN&#34;</p></td>
<td><p>OpenShiftSDN specifies OpenShiftSDN as the SDN provider</p>
</td>
</tr><tr><td><p>&#34;Other&#34;</p></td>
<td><p>Other specifies an undefined SDN provider</p>
</td>
</tr></tbody>
</table>
###NeutronTag { #hypershift.openshift.io/v1beta1.NeutronTag }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">FilterByNeutronTags</a>)
</p>
<p>
<p>NeutronTag represents a tag on a Neutron resource.
It may not be empty and may not contain commas.</p>
</p>
###NodePoolAutoScaling { #hypershift.openshift.io/v1beta1.NodePoolAutoScaling }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolAutoScaling specifies auto-scaling behavior for a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>min</code></br>
<em>
int32
</em>
</td>
<td>
<p>min is the minimum number of nodes to maintain in the pool.
Can be set to 0 for scale-from-zero for AWS platform.
Must be &gt;= 0 and &lt;= .Max.</p>
</td>
</tr>
<tr>
<td>
<code>max</code></br>
<em>
int32
</em>
</td>
<td>
<p>max is the maximum number of nodes allowed in the pool. Must be &gt;= 1 and &gt;= Min.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolCondition { #hypershift.openshift.io/v1beta1.NodePoolCondition }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">NodePoolStatus</a>)
</p>
<p>
<p>We define our own condition type since metav1.Condition has validation
for Reason that might be broken by what we bubble up from CAPI.
NodePoolCondition defines an observation of NodePool resource operational state.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>type of condition in CamelCase or in foo.example.com/CamelCase.
Many .condition.type values are consistent across resources like Available, but because arbitrary conditions
can be useful (see .node.status.conditions), the ability to deconflict is important.</p>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#conditionstatus-v1-core">
Kubernetes core/v1.ConditionStatus
</a>
</em>
</td>
<td>
<p>status of the condition, one of True, False, Unknown.</p>
</td>
</tr>
<tr>
<td>
<code>severity</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>severity provides an explicit classification of Reason code, so the users or machines can immediately
understand the current situation and act accordingly.
The Severity field MUST be set only when Status=False.</p>
</td>
</tr>
<tr>
<td>
<code>lastTransitionTime</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>lastTransitionTime is the last time the condition transitioned from one status to another.
This should be when the underlying condition changed. If that is not known, then using the time when
the API field changed is acceptable.</p>
</td>
</tr>
<tr>
<td>
<code>reason</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>reason for the condition&rsquo;s last transition in CamelCase.
The specific API may choose whether or not this field is considered a guaranteed API.
This field may not be empty.</p>
</td>
</tr>
<tr>
<td>
<code>message</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>message is a human readable message indicating details about the transition.
This field may be empty.</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>observedGeneration represents the .metadata.generation that the condition was set based upon.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolManagement { #hypershift.openshift.io/v1beta1.NodePoolManagement }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolManagement specifies behavior for managing nodes in a NodePool, such
as upgrade strategies and auto-repair behaviors.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>upgradeType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UpgradeType">
UpgradeType
</a>
</em>
</td>
<td>
<p>upgradeType specifies the type of strategy for handling upgrades.
This can be either &ldquo;Replace&rdquo; or &ldquo;InPlace&rdquo;.
&ldquo;Replace&rdquo; will update Nodes by recreating the underlying instances.
&ldquo;InPlace&rdquo; will update Nodes by applying changes to the existing instances. This might or might not result in a reboot.</p>
</td>
</tr>
<tr>
<td>
<code>replace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">
ReplaceUpgrade
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>replace is the configuration for rolling upgrades.
It defaults to a RollingUpdate strategy with maxSurge of 1 and maxUnavailable of 0.</p>
</td>
</tr>
<tr>
<td>
<code>inPlace</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.InPlaceUpgrade">
InPlaceUpgrade
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>inPlace is the configuration for in-place upgrades.</p>
</td>
</tr>
<tr>
<td>
<code>autoRepair</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoRepair specifies whether health checks should be enabled for machines in the NodePool. The default is false.
Enabling this feature will cause the controller to automatically delete unhealthy machines.
The unhealthy criteria is reserved for the controller implementation and subject to change.
But generally it&rsquo;s determined by checking the Node ready condition is true and a timeout that might vary depending on the platform provider.
AutoRepair will no-op when more than 2 Nodes are unhealthy at the same time. Giving time for the cluster to stabilize or to the user to manually intervene.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolPlatform { #hypershift.openshift.io/v1beta1.NodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>NodePoolPlatform specifies the underlying infrastructure provider for the
NodePool and is used to configure platform specific behavior.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>type specifies the platform name.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">
AWSNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aws specifies the configuration used when operating on AWS.</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec">
IBMCloudPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ibmcloud defines IBMCloud specific settings for components</p>
</td>
</tr>
<tr>
<td>
<code>kubevirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform">
KubevirtNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubevirt specifies the configuration used when operating on KubeVirt platform.</p>
</td>
</tr>
<tr>
<td>
<code>agent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AgentNodePoolPlatform">
AgentNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>agent specifies the configuration used when using Agent platform.</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform">
AzureNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>azure specifies the configuration used when using Azure platform.</p>
</td>
</tr>
<tr>
<td>
<code>powervs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">
PowerVSNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>powervs specifies the configuration used when using IBMCloud PowerVS platform.</p>
</td>
</tr>
<tr>
<td>
<code>openstack</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform">
OpenStackNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>openstack specifies the configuration used when using OpenStack platform.</p>
</td>
</tr>
<tr>
<td>
<code>gcp</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPNodePoolPlatform">
GCPNodePoolPlatform
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>gcp specifies the configuration used when operating on GCP.</p>
</td>
</tr>
</tbody>
</table>
###NodePoolPlatformStatus { #hypershift.openshift.io/v1beta1.NodePoolPlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolStatus">NodePoolStatus</a>)
</p>
<p>
<p>NodePoolPlatformStatus struct contains platform-specific status information.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kubeVirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus">
KubeVirtNodePoolStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubeVirt contains the KubeVirt platform statuses</p>
</td>
</tr>
</tbody>
</table>
###NodePoolSpec { #hypershift.openshift.io/v1beta1.NodePoolSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePool">NodePool</a>)
</p>
<p>
<p>NodePoolSpec is the desired behavior of a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterName</code></br>
<em>
string
</em>
</td>
<td>
<p>clusterName is the name of the HostedCluster this NodePool belongs to.
If a HostedCluster with this name doesn&rsquo;t exist, the controller will no-op until it exists.</p>
</td>
</tr>
<tr>
<td>
<code>release</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Release">
Release
</a>
</em>
</td>
<td>
<p>release specifies the OCP release used for this NodePool. It drives the machine ignition configuration (including
the kubelet version) and other platform-specific properties (e.g. an AMI on AWS).</p>
<p>Version-skew rules and effects:
- The minor-version skew relative to the control-plane release must be &lt;= N-2.
This is not currently enforced, but exceeding this limit is unsupported and
may lead to unpredictable behavior.
- If the specified release is higher than the HostedCluster&rsquo;s release, the
NodePool will be degraded and the ValidReleaseImage condition will be false.
- If the specified release is lower than the NodePool&rsquo;s current y-stream,
the NodePool will be degraded and the ValidReleaseImage condition will be false.</p>
<p>Changing this field triggers a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">
NodePoolPlatform
</a>
</em>
</td>
<td>
<p>platform specifies the underlying infrastructure provider for the NodePool
and is used to configure platform specific behavior.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>replicas is the desired number of nodes the pool should maintain. If unset, the controller default value is 0.
replicas is mutually exclusive with autoscaling. If autoscaling is configured, replicas must be omitted and autoscaling will control the NodePool size internally.</p>
</td>
</tr>
<tr>
<td>
<code>management</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">
NodePoolManagement
</a>
</em>
</td>
<td>
<p>management specifies behavior for managing nodes in the pool, such as
upgrade strategies and auto-repair behaviors.</p>
</td>
</tr>
<tr>
<td>
<code>autoScaling</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolAutoScaling">
NodePoolAutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoScaling specifies auto-scaling behavior for the NodePool.
autoScaling is mutually exclusive with replicas. If replicas is set, this field must be omitted.</p>
</td>
</tr>
<tr>
<td>
<code>config</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>config is a list of references to ConfigMaps containing serialized
MachineConfig resources to be injected into the ignition configurations of
nodes in the NodePool. The MachineConfig API schema is defined here:</p>
<p><a href="https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185">https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</a></p>
<p>Each ConfigMap must have a single key named &ldquo;config&rdquo; whose value is the YML
with one or more serialized machineconfiguration.openshift.io resources:</p>
<ul>
<li>KubeletConfig</li>
<li>ContainerRuntimeConfig</li>
<li>MachineConfig</li>
<li>ClusterImagePolicy</li>
<li>ImageContentSourcePolicy</li>
<li>ImageDigestMirrorSet</li>
</ul>
<p>This is validated in the backend and signaled back via validMachineConfig condition.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeDrainTimeout is the maximum amount of time that the controller will spend on retrying to drain a node until it succeeds.
The default value is 0, meaning that the node can retry drain without any time limitations.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeVolumeDetachTimeout</code></br>
<em>
<a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node.
The default value is 0, meaning that the volumes will be detached from the node without any time limitations.
After the timeout, any remaining attached volumes will be ignored and the removal of the machine will continue.
Changing this field propagate inplace into existing Nodes.</p>
</td>
</tr>
<tr>
<td>
<code>nodeLabels</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodeLabels propagates a list of labels to Nodes, only once on creation.
Valid values are those in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</a></p>
</td>
</tr>
<tr>
<td>
<code>taints</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Taint">
[]Taint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>taints if specified, propagates a list of taints to Nodes, only once on creation.
These taints are additive to the ones applied by other controllers</p>
</td>
</tr>
<tr>
<td>
<code>pausedUntil</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>pausedUntil is a field that can be used to pause reconciliation on the NodePool controller. Resulting in any change to the NodePool being ignored.
Either a date can be provided in RFC3339 format or a boolean as in &lsquo;true&rsquo;, &lsquo;false&rsquo;, &lsquo;True&rsquo;, &lsquo;False&rsquo;. If a date is
provided: reconciliation is paused on the resource until that date. If the boolean true is
provided: reconciliation is paused on the resource until the field is removed.</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfig</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tuningConfig is a list of references to ConfigMaps containing serialized
Tuned or PerformanceProfile resources to define the tuning configuration to be applied to
nodes in the NodePool. The Tuned API is defined here:</p>
<p><a href="https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go">https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</a></p>
<p>The PerformanceProfile API is defined here:
<a href="https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2">https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</a></p>
<p>Each ConfigMap must have a single key named &ldquo;tuning&rdquo; whose value is the
JSON or YAML of a serialized Tuned or PerformanceProfile.
Changing this field will trigger a NodePool rollout.</p>
</td>
</tr>
<tr>
<td>
<code>arch</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>arch is the preferred processor architecture for the NodePool. Different platforms might have different supported architectures.
TODO: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery:
<a href="https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215">https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</a></p>
</td>
</tr>
</tbody>
</table>
###NodePoolStatus { #hypershift.openshift.io/v1beta1.NodePoolStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePool">NodePool</a>)
</p>
<p>
<p>NodePoolStatus is the latest observed status of a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>replicas</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>replicas is the latest observed number of nodes in the pool.</p>
</td>
</tr>
<tr>
<td>
<code>version</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>version is the semantic version of the latest applied release specified by
the NodePool.</p>
</td>
</tr>
<tr>
<td>
<code>platform</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus">
NodePoolPlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>platform holds the specific statuses</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolCondition">
[]NodePoolCondition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>conditions represents the latest available observations of the node pool&rsquo;s
current state.</p>
</td>
</tr>
</tbody>
</table>
###NodePortPublishingStrategy { #hypershift.openshift.io/v1beta1.NodePortPublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>NodePortPublishingStrategy specifies a NodePort used to expose a service.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>address</code></br>
<em>
string
</em>
</td>
<td>
<p>address is the host/ip that the NodePort service is exposed over.</p>
</td>
</tr>
<tr>
<td>
<code>port</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>port is the port of the NodePort service. If &lt;=0, the port is dynamically
assigned when the service is created.</p>
</td>
</tr>
</tbody>
</table>
###OLMCatalogPlacement { #hypershift.openshift.io/v1beta1.OLMCatalogPlacement }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>OLMCatalogPlacement is an enum specifying the placement of OLM catalog components.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;guest&#34;</p></td>
<td><p>GuestOLMCatalogPlacement indicates OLM catalog components will be placed in
the guest cluster.</p>
</td>
</tr><tr><td><p>&#34;management&#34;</p></td>
<td><p>ManagementOLMCatalogPlacement indicates OLM catalog components will be placed in
the management cluster.</p>
</td>
</tr></tbody>
</table>
###OVNIPv4Config { #hypershift.openshift.io/v1beta1.OVNIPv4Config }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OVNKubernetesConfig">OVNKubernetesConfig</a>)
</p>
<p>
<p>OVNIPv4Config contains IPv4-specific configuration options for OVN-Kubernetes.
<a href="https://github.com/openshift/api/blob/6d3c4e25a8d3aeb57ad61649d80c38cbd27d1cc8/operator/v1/types_network.go#L473-L503">https://github.com/openshift/api/blob/6d3c4e25a8d3aeb57ad61649d80c38cbd27d1cc8/operator/v1/types_network.go#L473-L503</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>internalTransitSwitchSubnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>internalTransitSwitchSubnet is a v4 subnet in IPV4 CIDR format used internally
by OVN-Kubernetes for the distributed transit switch in the OVN Interconnect
architecture that connects the cluster routers on each node together to enable
east west traffic. The subnet chosen should not overlap with other networks
specified for OVN-Kubernetes as well as other networks used on the host.
When omitted, this means no opinion and the platform is left to choose a reasonable
default which is subject to change over time.
The current default subnet is 100.88.0.0/16
The subnet must be large enough to accommodate one IP per node in your cluster
The value must be in proper IPV4 CIDR format</p>
</td>
</tr>
<tr>
<td>
<code>internalJoinSubnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>internalJoinSubnet is a v4 subnet used internally by ovn-kubernetes in case the
default one is being already used by something else. It must not overlap with
any other subnet being used by OpenShift or by the node network. The size of the
subnet must be larger than the number of nodes.
The current default value is 100.64.0.0/16
The subnet must be large enough to accommodate one IP per node in your cluster
The value must be in proper IPV4 CIDR format</p>
</td>
</tr>
</tbody>
</table>
###OVNKubernetesConfig { #hypershift.openshift.io/v1beta1.OVNKubernetesConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworkOperatorSpec">ClusterNetworkOperatorSpec</a>)
</p>
<p>
<p>OVNKubernetesConfig contains OVN-Kubernetes specific configuration options.
<a href="https://github.com/openshift/api/blob/6d3c4e25a8d3aeb57ad61649d80c38cbd27d1cc8/operator/v1/types_network.go#L400-L471">https://github.com/openshift/api/blob/6d3c4e25a8d3aeb57ad61649d80c38cbd27d1cc8/operator/v1/types_network.go#L400-L471</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ipv4</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OVNIPv4Config">
OVNIPv4Config
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ipv4 allows users to configure IP settings for IPv4 connections. When omitted,
this means no opinions and the default configuration is used. Check individual
fields within ipv4 for details of default values.</p>
</td>
</tr>
</tbody>
</table>
###ObjectEncodingFormat { #hypershift.openshift.io/v1beta1.ObjectEncodingFormat }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedIdentity">ManagedIdentity</a>)
</p>
<p>
<p>objectEncoding represents the encoding for the Azure Key Vault secret containing the certificate related to
CertificateName. objectEncoding needs to match the encoding format used when the certificate was stored in the
Azure Key Vault. If objectEncoding doesn&rsquo;t match the encoding format of the certificate, the certificate will
unsuccessfully be read by the Secrets CSI driver and an error will occur. This error will only be visible on the
SecretProviderClass custom resource related to the managed identity.</p>
<p>The default value is utf-8.</p>
<p>See this for more info - <a href="https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md">https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md</a></p>
</p>
###OpenStackIdentityReference { #hypershift.openshift.io/v1beta1.OpenStackIdentityReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>OpenStackIdentityReference is a reference to an infrastructure
provider identity to be used to provision cluster resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name is the name of a secret in the same namespace as the resource being provisioned.
The secret must contain a key named <code>clouds.yaml</code> which contains an OpenStack clouds.yaml file.
The secret may optionally contain a key named <code>cacert</code> containing a PEM-encoded CA certificate.</p>
</td>
</tr>
<tr>
<td>
<code>cloudName</code></br>
<em>
string
</em>
</td>
<td>
<p>cloudName specifies the name of the entry in the clouds.yaml file to use.</p>
</td>
</tr>
</tbody>
</table>
###OpenStackNodePoolPlatform { #hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>flavor</code></br>
<em>
string
</em>
</td>
<td>
<p>flavor is the OpenStack flavor to use for the node instances.</p>
</td>
</tr>
<tr>
<td>
<code>imageName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageName is the OpenStack Glance image name to use for node instances. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>availabilityZone is the nova availability zone in which the provider will create the VM.
If not specified, the VM will be created in the default availability zone specified in the nova configuration.
Availability zone names must NOT contain : since it is used by admin users to specify hosts where instances
are launched in server creation. Also, it must not contain spaces otherwise it will lead to node that belongs
to this availability zone register failure, see kubernetes/cloud-provider-openstack#1379 for further information.
The maximum length of availability zone name is 63 as per labels limits.</p>
</td>
</tr>
<tr>
<td>
<code>additionalPorts</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PortSpec">
[]PortSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>additionalPorts is a list of additional ports to create on the node instances.</p>
</td>
</tr>
</tbody>
</table>
###OpenStackPlatformSpec { #hypershift.openshift.io/v1beta1.OpenStackPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>OpenStackPlatformSpec specifies configuration for clusters running on OpenStack.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>identityRef</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackIdentityReference">
OpenStackIdentityReference
</a>
</em>
</td>
<td>
<p>identityRef is a reference to a secret holding OpenStack credentials
to be used when reconciling the hosted cluster.</p>
</td>
</tr>
<tr>
<td>
<code>managedSubnets</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetSpec">
[]SubnetSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>managedSubnets describe the OpenStack Subnet to be created. Cluster actuator will create a network,
and a subnet with the defined DNSNameservers, AllocationPools and the CIDR defined in the HostedCluster
MachineNetwork, and a router connected to the subnet. Currently only one IPv4
subnet is supported.</p>
</td>
</tr>
<tr>
<td>
<code>router</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RouterParam">
RouterParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>router specifies an existing router to be used if ManagedSubnets are
specified. If specified, no new router will be created.</p>
</td>
</tr>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>network specifies an existing network to use if no ManagedSubnets
are specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetParam">
[]SubnetParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>subnets specifies existing subnets to use if not ManagedSubnets are
specified. All subnets must be in the network specified by Network.
There can be zero, one, or two subnets. If no subnets are specified,
all subnets in Network will be used. If 2 subnets are specified, one
must be IPv4 and the other IPv6.</p>
</td>
</tr>
<tr>
<td>
<code>networkMTU</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>networkMTU is the MTU for the network.</p>
</td>
</tr>
<tr>
<td>
<code>externalNetwork</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>externalNetwork is the OpenStack Network to be used to get public internet to the VMs.
This option is ignored if DisableExternalNetwork is set to true.</p>
<p>If ExternalNetwork is defined it must refer to exactly one external network.</p>
<p>If ExternalNetwork is not defined or is empty the controller will use any
existing external network as long as there is only one. It is an
error if ExternalNetwork is not defined and there are multiple
external networks unless DisableExternalNetwork is also set.</p>
<p>If ExternalNetwork is not defined and there are no external networks
the controller will proceed as though DisableExternalNetwork was set.</p>
</td>
</tr>
<tr>
<td>
<code>disableExternalNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>disableExternalNetwork specifies whether or not to attempt to connect the cluster
to an external network. This allows for the creation of clusters when connecting
to an external network is not possible or desirable, e.g. if using a provider network.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>tags to set on all resources in cluster which support tags</p>
</td>
</tr>
<tr>
<td>
<code>ingressFloatingIP</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ingressFloatingIP is an available floating IP in your OpenStack cluster that will
be associated with the OpenShift ingress port.
When not specified, an IP address will be assigned randomly by the OpenStack cloud provider.
When specified, the floating IP has to be pre-created.  If the
specified value is not a floating IP or is already claimed, the
OpenStack cloud provider won&rsquo;t be able to provision the load
balancer.
This value must be a valid IPv4 or IPv6 address.</p>
</td>
</tr>
</tbody>
</table>
###OperatorConfiguration { #hypershift.openshift.io/v1beta1.OperatorConfiguration }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>OperatorConfiguration specifies configuration for individual OCP operators in the cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterVersionOperator</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec">
ClusterVersionOperatorSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterVersionOperator specifies the configuration for the Cluster Version Operator in the hosted cluster.</p>
</td>
</tr>
<tr>
<td>
<code>clusterNetworkOperator</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworkOperatorSpec">
ClusterNetworkOperatorSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>clusterNetworkOperator specifies the configuration for the Cluster Network Operator in the hosted cluster.</p>
</td>
</tr>
<tr>
<td>
<code>ingressOperator</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IngressOperatorSpec">
IngressOperatorSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ingressOperator specifies the configuration for the Ingress Operator in the hosted cluster.
This allows configuring how the default ingress controller endpoints are published.</p>
</td>
</tr>
</tbody>
</table>
###OptionalCapability { #hypershift.openshift.io/v1beta1.OptionalCapability }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Capabilities">Capabilities</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;baremetal&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Console&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ImageRegistry&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Ingress&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Insights&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;NodeTuning&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;openshift-samples&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###PayloadArchType { #hypershift.openshift.io/v1beta1.PayloadArchType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AMD64&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;ARM64&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Multi&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;PPC64LE&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;S390X&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###PersistentVolumeAccessMode { #hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume">KubevirtPersistentVolume</a>)
</p>
<p>
</p>
###PersistentVolumeEtcdStorageSpec { #hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec">ManagedEtcdStorageSpec</a>)
</p>
<p>
<p>PersistentVolumeEtcdStorageSpec is the configuration for PersistentVolume
etcd storage.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageClassName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageClassName is the StorageClass of the data volume for each etcd member.
See <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a>.
TODO(alberto): This shouldn&rsquo;t really be a pointer. There&rsquo;s no real different semantic for nil and empty string. Revisit all pointer vs non-pointer choices.</p>
</td>
</tr>
<tr>
<td>
<code>size</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#quantity-resource-api">
k8s.io/apimachinery/pkg/api/resource.Quantity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>size is the minimum size of the data volume for each etcd member.
Default is 8Gi.
This field is immutable</p>
</td>
</tr>
</tbody>
</table>
###PlacementOptions { #hypershift.openshift.io/v1beta1.PlacementOptions }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>PlacementOptions specifies the placement options for the EC2 instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>tenancy</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>tenancy indicates if instance should run on shared or single-tenant hardware.</p>
<p>Possible values:
- &ldquo;default&rdquo;: NodePool instances run on shared hardware.
- &ldquo;dedicated&rdquo;: Each NodePool instance runs on single-tenant hardware (Dedicated Instances).
- &ldquo;host&rdquo;: NodePool instances run on user&rsquo;s pre-allocated dedicated hosts (Dedicated Hosts).</p>
<p>When tenancy is set to &ldquo;host&rdquo;, capacityReservation cannot be specified
as AWS does not support Capacity Reservations with Dedicated Hosts.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservation</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.CapacityReservationOptions">
CapacityReservationOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>capacityReservation specifies Capacity Reservation options for the NodePool instances.</p>
<p>Cannot be specified when tenancy is set to &ldquo;host&rdquo; as Dedicated Hosts
do not support Capacity Reservations. Compatible with &ldquo;default&rdquo; and &ldquo;dedicated&rdquo; tenancy.</p>
</td>
</tr>
</tbody>
</table>
###PlatformSpec { #hypershift.openshift.io/v1beta1.PlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>PlatformSpec specifies the underlying infrastructure provider for the cluster
and is used to configure platform specific behavior.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PlatformType">
PlatformType
</a>
</em>
</td>
<td>
<p>type is the type of infrastructure provider for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformSpec">
AWSPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aws specifies configuration for clusters running on Amazon Web Services.</p>
</td>
</tr>
<tr>
<td>
<code>agent</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AgentPlatformSpec">
AgentPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>agent specifies configuration for agent-based installations.</p>
</td>
</tr>
<tr>
<td>
<code>ibmcloud</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec">
IBMCloudPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ibmcloud defines IBMCloud specific settings for components</p>
</td>
</tr>
<tr>
<td>
<code>azure</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzurePlatformSpec">
AzurePlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>azure defines azure specific settings</p>
</td>
</tr>
<tr>
<td>
<code>powervs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">
PowerVSPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>powervs specifies configuration for clusters running on IBMCloud Power VS Service.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>kubevirt</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec">
KubevirtPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kubevirt defines KubeVirt specific settings for cluster components.</p>
</td>
</tr>
<tr>
<td>
<code>openstack</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">
OpenStackPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>openstack specifies configuration for clusters running on OpenStack.</p>
</td>
</tr>
<tr>
<td>
<code>gcp</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.GCPPlatformSpec">
GCPPlatformSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>gcp specifies configuration for clusters running on Google Cloud Platform.</p>
</td>
</tr>
</tbody>
</table>
###PlatformStatus { #hypershift.openshift.io/v1beta1.PlatformStatus }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterStatus">HostedClusterStatus</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus">HostedControlPlaneStatus</a>)
</p>
<p>
<p>PlatformStatus contains platform-specific status</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>aws</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AWSPlatformStatus">
AWSPlatformStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aws contains platform-specific status for AWS</p>
</td>
</tr>
</tbody>
</table>
###PlatformType { #hypershift.openshift.io/v1beta1.PlatformType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterConfig">KarpenterConfig</a>, 
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>PlatformType is a specific supported infrastructure provider.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AWS&#34;</p></td>
<td><p>AWSPlatform represents Amazon Web Services infrastructure.</p>
</td>
</tr><tr><td><p>&#34;Agent&#34;</p></td>
<td><p>AgentPlatform represents user supplied insfrastructure booted with agents.</p>
</td>
</tr><tr><td><p>&#34;Azure&#34;</p></td>
<td><p>AzurePlatform represents Azure infrastructure.</p>
</td>
</tr><tr><td><p>&#34;GCP&#34;</p></td>
<td><p>GCPPlatform represents Google Cloud Platform infrastructure.</p>
</td>
</tr><tr><td><p>&#34;IBMCloud&#34;</p></td>
<td><p>IBMCloudPlatform represents IBM Cloud infrastructure.</p>
</td>
</tr><tr><td><p>&#34;KubeVirt&#34;</p></td>
<td><p>KubevirtPlatform represents Kubevirt infrastructure.</p>
</td>
</tr><tr><td><p>&#34;None&#34;</p></td>
<td><p>NonePlatform represents user supplied (e.g. bare metal) infrastructure.</p>
</td>
</tr><tr><td><p>&#34;OpenStack&#34;</p></td>
<td><p>OpenStackPlatform represents OpenStack infrastructure.</p>
</td>
</tr><tr><td><p>&#34;PowerVS&#34;</p></td>
<td><p>PowerVSPlatform represents PowerVS infrastructure.</p>
</td>
</tr></tbody>
</table>
###PortSecurityPolicy { #hypershift.openshift.io/v1beta1.PortSecurityPolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PortSpec">PortSpec</a>)
</p>
<p>
<p>PortSecurityPolicy defines whether or not to enable port security on a port.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;&#34;</p></td>
<td><p>PortSecurityDefault uses the default port security policy.</p>
</td>
</tr><tr><td><p>&#34;Disabled&#34;</p></td>
<td><p>PortSecurityDisabled disables port security on a port.</p>
</td>
</tr><tr><td><p>&#34;Enabled&#34;</p></td>
<td><p>PortSecurityEnabled enables port security on a port.</p>
</td>
</tr></tbody>
</table>
###PortSpec { #hypershift.openshift.io/v1beta1.PortSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform">OpenStackNodePoolPlatform</a>)
</p>
<p>
<p>PortSpec specifies the options for creating a port.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>network</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NetworkParam">
NetworkParam
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>network is a query for an openstack network that the port will be created or discovered on.
This will fail if the query returns more than one network.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>description is a human-readable description for the port.</p>
</td>
</tr>
<tr>
<td>
<code>allowedAddressPairs</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AddressPair">
[]AddressPair
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>allowedAddressPairs is a list of address pairs which Neutron will
allow the port to send traffic from in addition to the port&rsquo;s
addresses. If not specified, the MAC Address will be the MAC Address
of the port. Depending on the configuration of Neutron, it may be
supported to specify a CIDR instead of a specific IP address.</p>
</td>
</tr>
<tr>
<td>
<code>vnicType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>vnicType specifies the type of vNIC which this port should be
attached to. This is used to determine which mechanism driver(s) to
be used to bind the port. The valid values are normal, macvtap,
direct, baremetal, direct-physical, virtio-forwarder, smart-nic and
remote-managed, although these values will not be validated in this
API to ensure compatibility with future neutron changes or custom
implementations. What type of vNIC is actually available depends on
deployments. If not specified, the Neutron default value is used.</p>
</td>
</tr>
<tr>
<td>
<code>portSecurityPolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PortSecurityPolicy">
PortSecurityPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>portSecurityPolicy specifies whether or not to enable port security on the port.
Allowed values are &ldquo;Enabled&rdquo;, &ldquo;Disabled&rdquo; and omitted.
When not set, it takes the value of the corresponding field at the network level.</p>
</td>
</tr>
</tbody>
</table>
###PowerVSNodePoolImageDeletePolicy { #hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolImageDeletePolicy defines image delete policy to be used for PowerVSNodePoolPlatform</p>
</p>
###PowerVSNodePoolPlatform { #hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolPlatform">NodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolPlatform specifies the configuration of a NodePool when operating
on IBMCloud PowerVS platform.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>systemType</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>systemType is the System type used to host the instance.
systemType determines the number of cores and memory that is available.
Few of the supported SystemTypes are s922,e880,e980.
e880 systemType available only in Dallas Datacenters.
e980 systemType available in Datacenters except Dallas and Washington.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The current default is s922 which is generally available.</p>
</td>
</tr>
<tr>
<td>
<code>processorType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType">
PowerVSNodePoolProcType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>processorType is the VM instance processor type.
It must be set to one of the following values: Dedicated, Capped or Shared.</p>
<p>Dedicated: resources are allocated for a specific client, The hypervisor makes a 1:1 binding of a partition&rsquo;s processor to a physical processor core.
Shared: Shared among other clients.
Capped: Shared, but resources do not expand beyond those that are requested, the amount of CPU time is Capped to the value specified for the entitlement.</p>
<p>if the processorType is selected as Dedicated, then Processors value cannot be fractional.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The current default is shared.</p>
</td>
</tr>
<tr>
<td>
<code>processors</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>processors is the number of virtual processors in a virtual machine.
when the processorType is selected as Dedicated the processors value cannot be fractional.
maximum value for the Processors depends on the selected SystemType.
when SystemType is set to e880 or e980 maximum Processors value is 143.
when SystemType is set to s922 maximum Processors value is 15.
minimum value for Processors depends on the selected ProcessorType.
when ProcessorType is set as Shared or Capped, The minimum processors is 0.5.
when ProcessorType is set as Dedicated, The minimum processors is 1.
When omitted, this means that the user has no opinion and the platform is left to choose a
reasonable default. The default is set based on the selected ProcessorType.
when ProcessorType selected as Dedicated, the default is set to 1.
when ProcessorType selected as Shared or Capped, the default is set to 0.5.</p>
</td>
</tr>
<tr>
<td>
<code>memoryGiB</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>memoryGiB is the size of a virtual machine&rsquo;s memory, in GiB.
maximum value for the MemoryGiB depends on the selected SystemType.
when SystemType is set to e880 maximum MemoryGiB value is 7463 GiB.
when SystemType is set to e980 maximum MemoryGiB value is 15307 GiB.
when SystemType is set to s922 maximum MemoryGiB value is 942 GiB.
The minimum memory is 32 GiB.</p>
<p>When omitted, this means the user has no opinion and the platform is left to choose a reasonable
default. The current default is 32.</p>
</td>
</tr>
<tr>
<td>
<code>image</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSResourceReference">
PowerVSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>image used for deploying the nodes. If unspecified, the default
is chosen based on the NodePool release payload image.</p>
</td>
</tr>
<tr>
<td>
<code>storageType</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType">
PowerVSNodePoolStorageType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>storageType for the image and nodes, this will be ignored if Image is specified.
The storage tiers in PowerVS are based on I/O operations per second (IOPS).
It means that the performance of your storage volumes is limited to the maximum number of IOPS based on volume size and storage tier.
Although, the exact numbers might change over time, the Tier 3 storage is currently set to 3 IOPS/GB, and the Tier 1 storage is currently set to 10 IOPS/GB.</p>
<p>The default is tier1</p>
</td>
</tr>
<tr>
<td>
<code>imageDeletePolicy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy">
PowerVSNodePoolImageDeletePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>imageDeletePolicy is policy for the image deletion.</p>
<p>delete: delete the image from the infrastructure.
retain: delete the image from the openshift but retain in the infrastructure.</p>
<p>The default is delete</p>
</td>
</tr>
</tbody>
</table>
###PowerVSNodePoolProcType { #hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolProcType defines processor type to be used for PowerVSNodePoolPlatform</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;capped&#34;</p></td>
<td><p>PowerVSNodePoolCappedProcType defines capped processor type</p>
</td>
</tr><tr><td><p>&#34;dedicated&#34;</p></td>
<td><p>PowerVSNodePoolDedicatedProcType defines dedicated processor type</p>
</td>
</tr><tr><td><p>&#34;shared&#34;</p></td>
<td><p>PowerVSNodePoolSharedProcType defines shared processor type</p>
</td>
</tr></tbody>
</table>
###PowerVSNodePoolStorageType { #hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>)
</p>
<p>
<p>PowerVSNodePoolStorageType defines storage type to be used for PowerVSNodePoolPlatform</p>
</p>
###PowerVSPlatformSpec { #hypershift.openshift.io/v1beta1.PowerVSPlatformSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PlatformSpec">PlatformSpec</a>)
</p>
<p>
<p>PowerVSPlatformSpec defines IBMCloud PowerVS specific settings for components</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>accountID</code></br>
<em>
string
</em>
</td>
<td>
<p>accountID is the IBMCloud account id.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>cisInstanceCRN</code></br>
<em>
string
</em>
</td>
<td>
<p>cisInstanceCRN is the IBMCloud CIS Service Instance&rsquo;s Cloud Resource Name
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>resourceGroup</code></br>
<em>
string
</em>
</td>
<td>
<p>resourceGroup is the IBMCloud Resource Group in which the cluster resides.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>region is the IBMCloud region in which the cluster resides. This configures the
OCP control plane cloud integrations, and is used by NodePool to resolve
the correct boot image for a given release.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<p>zone is the availability zone where control plane cloud resources are
created.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSResourceReference">
PowerVSResourceReference
</a>
</em>
</td>
<td>
<p>subnet is the subnet to use for control plane cloud resources.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>serviceInstanceID</code></br>
<em>
string
</em>
</td>
<td>
<p>serviceInstanceID is the reference to the Power VS service on which the server instance(VM) will be created.
Power VS service is a container for all Power VS instances at a specific geographic region.
serviceInstance can be created via IBM Cloud catalog or CLI.
ServiceInstanceID is the unique identifier that can be obtained from IBM Cloud UI or IBM Cloud cli.</p>
<p>More detail about Power VS service instance.
<a href="https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server">https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server</a></p>
<p>This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>vpc</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSVPC">
PowerVSVPC
</a>
</em>
</td>
<td>
<p>vpc specifies IBM Cloud PowerVS Load Balancing configuration for the control
plane.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>kubeCloudControllerCreds is a reference to a secret containing cloud
credentials with permissions matching the cloud controller policy.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
<p>TODO(dan): document the &ldquo;cloud controller policy&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>nodePoolManagementCreds is a reference to a secret containing cloud
credentials with permissions matching the node pool management policy.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
<p>TODO(dan): document the &ldquo;node pool management policy&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>ingressOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>ingressOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for ingress operator to get authenticated with ibm cloud.</p>
</td>
</tr>
<tr>
<td>
<code>storageOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>storageOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for storage operator to get authenticated with ibm cloud.</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryOperatorCloudCreds</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<p>imageRegistryOperatorCloudCreds is a reference to a secret containing ibm cloud
credentials for image registry operator to get authenticated with ibm cloud.</p>
</td>
</tr>
</tbody>
</table>
###PowerVSResourceReference { #hypershift.openshift.io/v1beta1.PowerVSResourceReference }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform">PowerVSNodePoolPlatform</a>, 
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">PowerVSPlatformSpec</a>)
</p>
<p>
<p>PowerVSResourceReference is a reference to a specific IBMCloud PowerVS resource by ID, or Name.
Only one of ID, or Name may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>id of resource</p>
</td>
</tr>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>name of resource</p>
</td>
</tr>
</tbody>
</table>
###PowerVSVPC { #hypershift.openshift.io/v1beta1.PowerVSVPC }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec">PowerVSPlatformSpec</a>)
</p>
<p>
<p>PowerVSVPC specifies IBM Cloud PowerVS LoadBalancer configuration for the control
plane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<p>name for VPC to used for all the service load balancer.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>region</code></br>
<em>
string
</em>
</td>
<td>
<p>region is the IBMCloud region in which VPC gets created, this VPC used for all the ingress traffic
into the OCP cluster.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>zone</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>zone is the availability zone where load balancer cloud resources are
created.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>subnet is the subnet to use for load balancer.
This field is immutable. Once set, It can&rsquo;t be changed.</p>
</td>
</tr>
</tbody>
</table>
###Provisioner { #hypershift.openshift.io/v1beta1.Provisioner }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ProvisionerConfig">ProvisionerConfig</a>)
</p>
<p>
<p>provisioner is a enum specifying the strategy for auto managing Nodes.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Karpenter&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###ProvisionerConfig { #hypershift.openshift.io/v1beta1.ProvisionerConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AutoNode">AutoNode</a>)
</p>
<p>
<p>ProvisionerConfig is a enum specifying the strategy for auto managing Nodes.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.Provisioner">
Provisioner
</a>
</em>
</td>
<td>
<p>name specifies the name of the provisioner to use.</p>
</td>
</tr>
<tr>
<td>
<code>karpenter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KarpenterConfig">
KarpenterConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>karpenter specifies the configuration for the Karpenter provisioner.</p>
</td>
</tr>
</tbody>
</table>
###PublishingStrategyType { #hypershift.openshift.io/v1beta1.PublishingStrategyType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>PublishingStrategyType defines publishing strategies for services.</p>
</p>
###QoSClass { #hypershift.openshift.io/v1beta1.QoSClass }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.KubevirtCompute">KubevirtCompute</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Burstable&#34;</p></td>
<td></td>
</tr><tr><td><p>&#34;Guaranteed&#34;</p></td>
<td></td>
</tr></tbody>
</table>
###Release { #hypershift.openshift.io/v1beta1.Release }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>Release represents the metadata for an OCP release payload image.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>image</code></br>
<em>
string
</em>
</td>
<td>
<p>image is the image pullspec of an OCP release payload image.
See <a href="https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags">https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags</a> for a list of available images.</p>
</td>
</tr>
</tbody>
</table>
###ReplaceUpgrade { #hypershift.openshift.io/v1beta1.ReplaceUpgrade }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>ReplaceUpgrade specifies upgrade behavior that replaces existing nodes
according to a given strategy.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>strategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.UpgradeStrategy">
UpgradeStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>strategy is the node replacement strategy for nodes in the pool.
In can be either &ldquo;RollingUpdate&rdquo; or &ldquo;OnDelete&rdquo;. RollingUpdate will rollout Nodes honoring maxSurge and maxUnavailable.
OnDelete provide more granular control and will replace nodes as the old ones are manually deleted.</p>
</td>
</tr>
<tr>
<td>
<code>rollingUpdate</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RollingUpdate">
RollingUpdate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>rollingUpdate specifies a rolling update strategy which upgrades nodes by
creating new nodes and deleting the old ones.</p>
</td>
</tr>
</tbody>
</table>
###RetentionPolicy { #hypershift.openshift.io/v1beta1.RetentionPolicy }
<p>
<p>RetentionPolicy defines the policy for handling resources associated with a cluster when the cluster is deleted.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Orphan&#34;</p></td>
<td><p>OrphanRetentionPolicy will keep the resources associated with the cluster
when the cluster is deleted.</p>
</td>
</tr><tr><td><p>&#34;Prune&#34;</p></td>
<td><p>PruneRetentionPolicy will delete the resources associated with the cluster
when the cluster is deleted.</p>
</td>
</tr></tbody>
</table>
###RollingUpdate { #hypershift.openshift.io/v1beta1.RollingUpdate }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">ReplaceUpgrade</a>)
</p>
<p>
<p>RollingUpdate specifies a rolling update strategy which upgrades nodes by
creating new nodes and deleting the old ones.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxUnavailable is the maximum number of nodes that can be unavailable
during the update.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding down.</p>
<p>This can not be 0 if MaxSurge is 0.</p>
<p>Defaults to 0.</p>
<p>Example: when this is set to 30%, old nodes can be deleted down to 70% of
desired nodes immediately when the rolling update starts. Once new nodes
are ready, more old nodes be deleted, followed by provisioning new nodes,
ensuring that the total number of nodes available at all times during the
update is at least 70% of desired nodes.</p>
</td>
</tr>
<tr>
<td>
<code>maxSurge</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#intorstring-intstr-util">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>maxSurge is the maximum number of nodes that can be provisioned above the
desired number of nodes.</p>
<p>Value can be an absolute number (ex: 5) or a percentage of desired nodes
(ex: 10%).</p>
<p>Absolute number is calculated from percentage by rounding up.</p>
<p>This can not be 0 if MaxUnavailable is 0.</p>
<p>Defaults to 1.</p>
<p>Example: when this is set to 30%, new nodes can be provisioned immediately
when the rolling update starts, such that the total number of old and new
nodes do not exceed 130% of desired nodes. Once old nodes have been
deleted, new nodes can be provisioned, ensuring that total number of nodes
running at any time during the update is at most 130% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
###RoutePublishingStrategy { #hypershift.openshift.io/v1beta1.RoutePublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">ServicePublishingStrategy</a>)
</p>
<p>
<p>RoutePublishingStrategy specifies options for exposing a service as a Route.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>hostname is the name of the DNS record that will be created pointing to the Route and passed through to consumers of the service.
If omitted, the value will be inferred from management ingress.Spec.Domain.</p>
</td>
</tr>
</tbody>
</table>
###RouterFilter { #hypershift.openshift.io/v1beta1.RouterFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.RouterParam">RouterParam</a>)
</p>
<p>
<p>RouterFilter specifies a query to select an OpenStack router. At least one property must be set.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>name is the name of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>description is the description of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>projectID is the project ID of the router to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###RouterParam { #hypershift.openshift.io/v1beta1.RouterParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>RouterParam specifies an OpenStack router to use. It may be specified by either ID or filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>id is the ID of the router to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RouterFilter">
RouterFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>filter specifies a filter to select an OpenStack router. If provided, cannot be empty.</p>
</td>
</tr>
</tbody>
</table>
###ScaleDownConfig { #hypershift.openshift.io/v1beta1.ScaleDownConfig }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">ClusterAutoscaling</a>)
</p>
<p>
<p>Configures when and how to scale down cluster nodes.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>delayAfterAddSeconds</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>delayAfterAddSeconds sets how long after scale up the scale down evaluation resumes in seconds.
It must be between 0 and 86400 (24 hours).
When set to 0, this means scale down evaluation will resume immediately after scale up, without any delay.
When omitted, the autoscaler defaults to 600s (10 minutes).</p>
</td>
</tr>
<tr>
<td>
<code>delayAfterDeleteSeconds</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>delayAfterDeleteSeconds sets how long after node deletion, scale down evaluation resumes, defaults to scan-interval.
It must be between 0 and 86400 (24 hours).
When set to 0, this means scale down evaluation will resume immediately after node deletion, without any delay.
When omitted, the autoscaler defaults to 0s.</p>
</td>
</tr>
<tr>
<td>
<code>delayAfterFailureSeconds</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>delayAfterFailureSeconds sets how long after a scale down failure, scale down evaluation resumes.
It must be between 0 and 86400 (24 hours).
When set to 0, this means scale down evaluation will resume immediately after a scale down failure, without any delay.
When omitted, the autoscaler defaults to 180s (3 minutes).</p>
</td>
</tr>
<tr>
<td>
<code>unneededDurationSeconds</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>unneededDurationSeconds establishes how long a node should be unneeded before it is eligible for scale down in seconds.
It must be between 0 and 86400 (24 hours).
When omitted, the autoscaler defaults to 600s (10 minutes).</p>
</td>
</tr>
<tr>
<td>
<code>utilizationThresholdPercent</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>utilizationThresholdPercent determines the node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down.
The value represents a percentage from 0 to 100.
When set to 0, this means nodes will only be considered for scale down if they are completely idle (0% utilization).
When set to 100, this means nodes will be considered for scale down regardless of their utilization level.
A value between 0 and 100 represents the utilization threshold below which a node can be considered for scale down.
When omitted, the autoscaler defaults to 50%.</p>
</td>
</tr>
</tbody>
</table>
###ScalingType { #hypershift.openshift.io/v1beta1.ScalingType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterAutoscaling">ClusterAutoscaling</a>)
</p>
<p>
<p>ScalingType defines the scaling behavior for the cluster autoscaler.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;ScaleUpAndScaleDown&#34;</p></td>
<td><p>ScaleUpAndScaleDown means the autoscaler will both scale up and scale down nodes.</p>
</td>
</tr><tr><td><p>&#34;ScaleUpOnly&#34;</p></td>
<td><p>ScaleUpOnly means the autoscaler will only scale up nodes, never scale down.</p>
</td>
</tr></tbody>
</table>
###SecretEncryptionSpec { #hypershift.openshift.io/v1beta1.SecretEncryptionSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>SecretEncryptionSpec contains metadata about the kubernetes secret encryption strategy being used for the
cluster when applicable.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionType">
SecretEncryptionType
</a>
</em>
</td>
<td>
<p>type defines the type of kube secret encryption being used</p>
</td>
</tr>
<tr>
<td>
<code>kms</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.KMSSpec">
KMSSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>kms defines metadata about the kms secret encryption strategy</p>
</td>
</tr>
<tr>
<td>
<code>aescbc</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AESCBCSpec">
AESCBCSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>aescbc defines metadata about the AESCBC secret encryption strategy</p>
</td>
</tr>
</tbody>
</table>
###SecretEncryptionType { #hypershift.openshift.io/v1beta1.SecretEncryptionType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SecretEncryptionSpec">SecretEncryptionSpec</a>)
</p>
<p>
<p>SecretEncryptionType defines the type of kube secret encryption being used.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;aescbc&#34;</p></td>
<td><p>AESCBC uses AES-CBC with PKCS#7 padding to do secret encryption</p>
</td>
</tr><tr><td><p>&#34;kms&#34;</p></td>
<td><p>KMS integrates with a cloud provider&rsquo;s key management service to do secret encryption</p>
</td>
</tr></tbody>
</table>
###ServiceNetworkEntry { #hypershift.openshift.io/v1beta1.ServiceNetworkEntry }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ClusterNetworking">ClusterNetworking</a>)
</p>
<p>
<p>ServiceNetworkEntry is a single IP address block for the service network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidr</code></br>
<em>
<a href="#">
github.com/openshift/hypershift/api/util/ipnet.IPNet
</a>
</em>
</td>
<td>
<p>cidr is the IP block address pool for services within the cluster in CIDR format (e.g., 192.168.1.0/24 or 2001:0db8::/64)</p>
</td>
</tr>
</tbody>
</table>
###ServicePublishingStrategy { #hypershift.openshift.io/v1beta1.ServicePublishingStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">ServicePublishingStrategyMapping</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.PublishingStrategyType">
PublishingStrategyType
</a>
</em>
</td>
<td>
<p>type is the publishing strategy used for the service.
It can be LoadBalancer;NodePort;Route;None;S3</p>
</td>
</tr>
<tr>
<td>
<code>nodePort</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.NodePortPublishingStrategy">
NodePortPublishingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>nodePort configures exposing a service using a NodePort.</p>
</td>
</tr>
<tr>
<td>
<code>loadBalancer</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy">
LoadBalancerPublishingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>loadBalancer configures exposing a service using a dedicated LoadBalancer.</p>
</td>
</tr>
<tr>
<td>
<code>route</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.RoutePublishingStrategy">
RoutePublishingStrategy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>route configures exposing a service using a Route through and an ingress controller behind a cloud Load Balancer.
The specifics of the setup are platform dependent.</p>
</td>
</tr>
</tbody>
</table>
###ServicePublishingStrategyMapping { #hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.HostedClusterSpec">HostedClusterSpec</a>, 
<a href="#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec">HostedControlPlaneSpec</a>)
</p>
<p>
<p>ServicePublishingStrategyMapping specifies how individual control plane services endpoints are published for consumption.
This includes APIServer;OAuthServer;Konnectivity;Ignition.
If a given service is not present in this list, it will be exposed publicly by default.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>service</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServiceType">
ServiceType
</a>
</em>
</td>
<td>
<p>service identifies the type of service being published.
It can be APIServer;OAuthServer;Konnectivity;Ignition
OVNSbDb;OIDC are no-op and kept for backward compatibility.
This field is immutable.</p>
</td>
</tr>
<tr>
<td>
<code>servicePublishingStrategy</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategy">
ServicePublishingStrategy
</a>
</em>
</td>
<td>
<p>servicePublishingStrategy specifies how to publish a service endpoint.</p>
</td>
</tr>
</tbody>
</table>
###ServiceType { #hypershift.openshift.io/v1beta1.ServiceType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping">ServicePublishingStrategyMapping</a>)
</p>
<p>
<p>ServiceType defines what control plane services can be exposed from the
management control plane.</p>
</p>
###SubnetFilter { #hypershift.openshift.io/v1beta1.SubnetFilter }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.SubnetParam">SubnetParam</a>)
</p>
<p>
<p>SubnetFilter specifies a filter to select a subnet. At least one parameter must be specified.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>name is the name of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>description</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>description is the description of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>projectID</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>projectID is the project ID of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipVersion</code></br>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>ipVersion is the IP version of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>gatewayIP</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>gatewayIP is the gateway IP of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>cidr</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>cidr is the CIDR of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6AddressMode</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ipv6AddressMode is the IPv6 address mode of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6RAMode</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ipv6RAMode is the IPv6 RA mode of the subnet to filter by.</p>
</td>
</tr>
<tr>
<td>
<code>FilterByNeutronTags</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.FilterByNeutronTags">
FilterByNeutronTags
</a>
</em>
</td>
<td>
<p>
(Members of <code>FilterByNeutronTags</code> are embedded into this type.)
</p>
<em>(Optional)</em>
<p>FilterByNeutronTags specifies tags to filter by.</p>
</td>
</tr>
</tbody>
</table>
###SubnetParam { #hypershift.openshift.io/v1beta1.SubnetParam }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
<p>SubnetParam specifies an OpenStack subnet to use. It may be specified by either ID or filter, but not both.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>id is the uuid of the subnet. It will not be validated.</p>
</td>
</tr>
<tr>
<td>
<code>filter</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.SubnetFilter">
SubnetFilter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>filter specifies a filter to select the subnet. It must match exactly one subnet.</p>
</td>
</tr>
</tbody>
</table>
###SubnetSpec { #hypershift.openshift.io/v1beta1.SubnetSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec">OpenStackPlatformSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>dnsNameservers</code></br>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>dnsNameservers holds a list of DNS server addresses that will be provided when creating
the subnet. These addresses need to have the same IP version as CIDR.</p>
</td>
</tr>
<tr>
<td>
<code>allocationPools</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AllocationPool">
[]AllocationPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>allocationPools is an array of AllocationPool objects that will be applied to OpenStack Subnet being created.
If set, OpenStack will only allocate these IPs for Machines. It will still be possible to create ports from
outside of these ranges manually.</p>
</td>
</tr>
</tbody>
</table>
###Taint { #hypershift.openshift.io/v1beta1.Taint }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolSpec">NodePoolSpec</a>)
</p>
<p>
<p>taint is as v1 Core but without TimeAdded.
<a href="https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053">https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053</a>
Validation replicates the same validation as the upstream <a href="https://github.com/kubernetes/kubernetes/blob/9a2a7537f035969a68e432b4cc276dbce8ce1735/pkg/util/taints/taints.go#L273">https://github.com/kubernetes/kubernetes/blob/9a2a7537f035969a68e432b4cc276dbce8ce1735/pkg/util/taints/taints.go#L273</a>.
See also <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/">https://kubernetes.io/docs/concepts/overview/working-with-objects/names/</a>.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code></br>
<em>
string
</em>
</td>
<td>
<p>key is the taint key to be applied to a node.</p>
</td>
</tr>
<tr>
<td>
<code>value</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>value is the taint value corresponding to the taint key.</p>
</td>
</tr>
<tr>
<td>
<code>effect</code></br>
<em>
<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#tainteffect-v1-core">
Kubernetes core/v1.TaintEffect
</a>
</em>
</td>
<td>
<p>effect is the effect of the taint on pods
that do not tolerate the taint.
Valid effects are NoSchedule, PreferNoSchedule and NoExecute.</p>
</td>
</tr>
</tbody>
</table>
###UnmanagedEtcdSpec { #hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.EtcdSpec">EtcdSpec</a>)
</p>
<p>
<p>UnmanagedEtcdSpec specifies configuration which enables the control plane to
integrate with an eternally managed etcd cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>endpoint</code></br>
<em>
string
</em>
</td>
<td>
<p>endpoint is the full etcd cluster client endpoint URL. For example:</p>
<pre><code>https://etcd-client:2379
</code></pre>
<p>If the URL uses an HTTPS scheme, the TLS field is required.</p>
</td>
</tr>
<tr>
<td>
<code>tls</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.EtcdTLSConfig">
EtcdTLSConfig
</a>
</em>
</td>
<td>
<p>tls specifies TLS configuration for HTTPS etcd client endpoints.</p>
</td>
</tr>
</tbody>
</table>
###UpgradeStrategy { #hypershift.openshift.io/v1beta1.UpgradeStrategy }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.ReplaceUpgrade">ReplaceUpgrade</a>)
</p>
<p>
<p>UpgradeStrategy is a specific strategy for upgrading nodes in a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;OnDelete&#34;</p></td>
<td><p>UpgradeStrategyOnDelete replaces old nodes when the deletion of the
associated node instances are completed.</p>
</td>
</tr><tr><td><p>&#34;RollingUpdate&#34;</p></td>
<td><p>UpgradeStrategyRollingUpdate means use a rolling update for nodes.</p>
</td>
</tr></tbody>
</table>
###UpgradeType { #hypershift.openshift.io/v1beta1.UpgradeType }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.NodePoolManagement">NodePoolManagement</a>)
</p>
<p>
<p>UpgradeType is a type of high-level upgrade behavior nodes in a NodePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;InPlace&#34;</p></td>
<td><p>UpgradeTypeInPlace is a strategy which replaces nodes in-place with no
additional node capacity requirements.</p>
</td>
</tr><tr><td><p>&#34;Replace&#34;</p></td>
<td><p>UpgradeTypeReplace is a strategy which replaces nodes using surge node
capacity.</p>
</td>
</tr></tbody>
</table>
###UserManagedDiagnostics { #hypershift.openshift.io/v1beta1.UserManagedDiagnostics }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.Diagnostics">Diagnostics</a>)
</p>
<p>
<p>UserManagedDiagnostics specifies the diagnostics settings for a virtual machine when the storage account is managed by the user.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>storageAccountURI</code></br>
<em>
string
</em>
</td>
<td>
<p>storageAccountURI is the URI of the user-managed storage account.
The URI typically will be <code>https://&lt;mystorageaccountname&gt;.blob.core.windows.net/</code>
but may differ if you are using Azure DNS zone endpoints.
You can find the correct endpoint by looking for the Blob Primary Endpoint in the
endpoints tab in the Azure console or with the CLI by issuing
<code>az storage account list --query='[].{name: name, &quot;resource group&quot;: resourceGroup, &quot;blob endpoint&quot;: primaryEndpoints.blob}'</code>.</p>
</td>
</tr>
</tbody>
</table>
###Volume { #hypershift.openshift.io/v1beta1.Volume }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform">AWSNodePoolPlatform</a>)
</p>
<p>
<p>Volume specifies the configuration options for node instance storage devices.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>size</code></br>
<em>
int64
</em>
</td>
<td>
<p>size is the size of the volume in gibibytes (GiB).</p>
<p>Must be greater than the image snapshot size or 8 (whichever is greater).</p>
</td>
</tr>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>type is the type of volume to provision.</p>
</td>
</tr>
<tr>
<td>
<code>iops</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>iops is the number of IOPS requested for the disk. This is only valid
for type io1.</p>
</td>
</tr>
<tr>
<td>
<code>encrypted</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>encrypted indicates whether the EBS volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionKey</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>encryptionKey is the KMS key to use for volume encryption.</p>
</td>
</tr>
</tbody>
</table>
###WorkloadIdentity { #hypershift.openshift.io/v1beta1.WorkloadIdentity }
<p>
(<em>Appears on:</em>
<a href="#hypershift.openshift.io/v1beta1.AzureWorkloadIdentities">AzureWorkloadIdentities</a>)
</p>
<p>
<p>WorkloadIdentity is a struct that contains the client ID of a federated managed identity used in workload identity
authentication.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientID</code></br>
<em>
<a href="#hypershift.openshift.io/v1beta1.AzureClientID">
AzureClientID
</a>
</em>
</td>
<td>
<p>clientID is client ID of a federated managed identity used in workload identity authentication</p>
</td>
</tr>
</tbody>
</table>
<hr/>


---

## Source: docs/content/reference/architecture/aws/privatelink.md

---
title: AWS PrivateLink
---

# AWS PrivateLink Architecture in HyperShift

## Overview

HyperShift uses AWS PrivateLink to establish secure connectivity between worker nodes in the guest cluster VPC and the hosted control plane in the management cluster VPC. This is used when `EndpointAccess` is set to `Private` or `PublicAndPrivate`.

## Architecture Diagram

```mermaid
graph TB
    subgraph MC["Management Cluster (ROSA Service Account)"]
        subgraph HO["hypershift-operator"]
            HO_Reconciler["AWSEndpointServiceReconciler
            - Watches: NodePools, HostedClusters, AWSEndpointService
            - Collects SubnetIDs from NodePool.Spec.Platform.AWS.Subnet.ID
            - Updates AWSEndpointService.Spec.SubnetIDs"]
        end

        subgraph HCP["HCP Namespace (e.g., clusters-foo)"]
            subgraph CPO["control-plane-operator"]
                PSO["PrivateServiceObserver (x2)
                - Watches: kube-apiserver-private Service
                - Watches: private-router Service
                - Creates AWSEndpointService CR with NLB name"]
                CPO_Reconciler["AWSEndpointServiceReconciler
                - Waits for EndpointServiceName from HO
                - Modifies VPC Endpoint subnets on Spec.SubnetIDs changes"]
            end
            API_SVC["kube-apiserver-private (Svc)
            type: LoadBalancer
            ‚ö† Non-standard path"]:::optional
            ROUTER_SVC["private-router (Svc)
            type: LoadBalancer"]
        end
    end

    subgraph MGMT_AWS["Management AWS Account"]
        subgraph MGMT_VPC["Management VPC"]
            NLB_API["NLB (API Server)
            kube-apiserver-...
            ‚ö† Non-standard path"]:::optional
            NLB_ROUTER["NLB (Router)
            private-router-..."]
            VPCE_SVC["VPC Endpoint Service(s)
            Created by: hypershift-operator (HO)
            - AcceptanceRequired: false
            - AllowedPrincipals: CPO Role ARN
            - Returns: EndpointServiceName"]
        end
    end

    subgraph HC["Hosted Cluster (Customer Account)"]
        subgraph GUEST_VPC["Guest VPC"]
            VPCE["VPC Endpoint (Interface)
            Created by: control-plane-operator (CPO)
            - ServiceName: from AWSEndpointService.Status
            - SubnetIDs: from AWSEndpointService.Spec
            - SecurityGroup: allows 6443/443"]
            R53["Route53 Private Hosted Zone
            Created by: control-plane-operator (CPO)
            - api.cluster.hypershift.local ‚Üí VPCE DNS
            - *.apps.cluster.hypershift.local ‚Üí VPCE DNS"]
            subgraph SUBNETS["Worker Subnets"]
                SUB_A["Subnet A (AZ-1)
                ENI (VPCE) + Worker Nodes"]
                SUB_B["Subnet B (AZ-2)
                ENI (VPCE) + Worker Nodes"]
                SUB_C["Subnet C (AZ-3)
                ENI (VPCE) + Worker Nodes"]
            end
        end
    end

    PSO -.-> API_SVC
    PSO --> ROUTER_SVC
    API_SVC -.-> NLB_API
    ROUTER_SVC --> NLB_ROUTER
    NLB_API -.-> VPCE_SVC
    NLB_ROUTER --> VPCE_SVC
    HO_Reconciler -- "Creates VPC Endpoint Service
    Writes EndpointServiceName to Status" --> VPCE_SVC
    VPCE_SVC -- "AWS PrivateLink" --> VPCE
    CPO_Reconciler -- "Creates VPC Endpoint
    Creates Route53 records
    Writes EndpointID to Status" --> VPCE
    VPCE --> R53
    R53 --> SUBNETS

    classDef optional stroke-dasharray: 5 5
```

> **Note:** The `kube-apiserver-private` Service and its NLB (shown with dashed lines) are only created when the API server is explicitly exposed via LoadBalancer with PrivateLink configured. This is **not** the standard ROSA HCP path and its use is discouraged. In the standard ROSA HCP flow, only the `private-router` path is used for PrivateLink connectivity.

## Component Responsibilities

### AWS Resources

| AWS Resource | Created By | Description |
|--------------|------------|-------------|
| NLB (kube-apiserver-private) | control-plane-operator | Load balancer for API server traffic (non-standard path, not used in ROSA HCP) |
| NLB (private-router) | control-plane-operator | Load balancer for ingress router traffic |
| VPC Endpoint Service | hypershift-operator | Exposes NLBs via PrivateLink |
| VPC Endpoint (Interface) | control-plane-operator | Connects guest VPC to endpoint service |
| Security Group (for VPCE) | control-plane-operator | Allows 6443/443 from machine CIDRs |
| Route53 Private Zone records | control-plane-operator | DNS resolution for API and apps |

### Kubernetes Resources

| Resource | Created By | Responsibility |
|----------|------------|----------------|
| AWSEndpointService CR | CPO (PrivateServiceObserver) | Tracks NLB name |
| AWSEndpointService.Spec.SubnetIDs | hypershift-operator | Collected from NodePools |
| AWSEndpointService.Status.EndpointServiceName | hypershift-operator | AWS VPC Endpoint Service name |
| AWSEndpointService.Status.EndpointID | control-plane-operator | AWS VPC Endpoint ID |

## Data Flow

1. **CPO creates NLB Services** - `private-router` Service of type LoadBalancer (and optionally `kube-apiserver-private` when the API server is exposed via LoadBalancer, which is not the standard ROSA HCP path)
2. **CPO PrivateServiceObserver** watches Services, creates `AWSEndpointService` CR with NLB name
3. **HO collects SubnetIDs** from all NodePools and updates `AWSEndpointService.Spec.SubnetIDs`
4. **HO creates VPC Endpoint Service** attached to NLB, writes `EndpointServiceName` to status
5. **CPO creates VPC Endpoint** in guest VPC using the endpoint service name and subnet IDs
6. **CPO creates Route53 records** pointing to VPC Endpoint DNS

## EndpointAccess Modes

| Mode | Public NLB | Private NLB | VPC Endpoint | PrivateLink |
|------|------------|-------------|--------------|-------------|
| Public | Yes | No | No | No |
| PublicAndPrivate | Yes | Yes | Yes | Yes |
| Private | No | Yes | Yes | Yes |

## Code References

- **hypershift-operator AWS controller**: `hypershift-operator/controllers/platform/aws/controller.go`
- **control-plane-operator PrivateLink controller**: `control-plane-operator/controllers/awsprivatelink/awsprivatelink_controller.go`
- **AWSEndpointService API types**: `api/hypershift/v1beta1/endpointservice_types.go`


---

## Source: docs/content/reference/architecture/index.md

---
title: Diagrams
---

This section describes the architecture diagrams that is used by each platform.

---

## Source: docs/content/reference/architecture/managed-azure/azure-authentication-flow.md

# Azure Authentication Flow

This document explains the authentication flow from control plane pods to Azure APIs for managed Azure HostedClusters.

For CSI-based credential mounting details, see Secrets CSI Usage.

## Authentication Overview

Managed Azure HostedClusters use certificate-based authentication with credentials stored in Azure Key Vault:

| Aspect | Description |
|--------|-------------|
| **Credential Type** | X.509 Certificate |
| **Credential Source** | Azure Key Vault |
| **Credential Delivery** | Secrets Store CSI Driver |
| **Azure SDK Library** | `msi-dataplane` |
| **Credential Rotation** | Automatic via Key Vault |

## Authentication Flow

```
Pod Startup ‚Üí CSI Driver ‚Üí Azure Key Vault ‚Üí Credential File
                                                    ‚Üì
Azure ARM API ‚Üê Access Token ‚Üê Azure AD ‚Üê Azure SDK reads file
```

1. **CSI Driver** mounts credential from Azure Key Vault (see Secrets CSI Usage)
2. **Credential File** contains `UserAssignedIdentityCredentials` JSON with PEM certificate
3. **Azure SDK** uses `msi-dataplane.NewUserAssignedIdentityCredential()` to load credentials
4. **Azure AD** validates the client certificate and issues an access token
5. **ARM API** accepts the Bearer token for resource operations

## Credential Loading Code

```go
import "github.com/Azure/msi-dataplane/pkg/dataplane"

certPath := "/mnt/certs/" + credentialsSecretName
creds, err := dataplane.NewUserAssignedIdentityCredential(
    ctx,
    certPath,
    dataplane.WithClientOpts(azcore.ClientOptions{Cloud: cloud.AzurePublic}),
)

// Use with Azure SDK clients
client, err := armresources.NewResourceGroupsClient(subscriptionID, creds, nil)
```

The credential automatically watches the file for changes and reloads when the CSI driver refreshes secrets.

## Cloud Configuration File

```json
{
  "cloud": "AzurePublicCloud",
  "tenantId": "<tenant-id>",
  "subscriptionId": "<subscription-id>",
  "resourceGroup": "<resource-group>",
  "useManagedIdentityExtension": true,
  "aadMSIDataPlaneIdentityPath": "/mnt/certs/<credential-secret-name>"
}
```

## Credential Refresh

The `msi-dataplane` library handles credential refresh:

1. File watcher monitors credential file for changes
2. When CSI driver refreshes secrets, new credentials are loaded
3. Backstop timer reloads every 6 hours
4. Compares `notBefore` timestamps to detect newer credentials

## Troubleshooting

### Verify CSI Mount

```bash
kubectl exec -it <pod> -- ls -la /mnt/certs
kubectl exec -it <pod> -- cat /mnt/certs/<secret-name>
```

### Check SecretProviderClass

```bash
kubectl get secretproviderclass -n <namespace>
kubectl describe secretproviderclasspodstatus -n <namespace>
```

## Related Documentation

- Secrets CSI Usage - CSI driver overview


---

## Source: docs/content/reference/architecture/managed-azure/hostedcluster-identity-configuration.md

# HostedCluster CR Identity Configuration

This document provides a comprehensive reference for configuring Azure identity authentication in the HostedCluster custom resource for managed Azure HostedClusters.

## Overview

The `azureAuthenticationConfig` field in `spec.platform.azure` is a discriminated union that determines how the hosted cluster authenticates with Azure APIs. The authentication type is selected via the `azureAuthenticationConfigType` discriminator field.

## AzureAuthenticationConfiguration Union

The `AzureAuthenticationConfiguration` type controls which authentication mechanism is used for Azure API access.

### Type Reference

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `azureAuthenticationConfigType` | `string` | Yes | Discriminator that specifies the authentication type. Set to `ManagedIdentities` for managed Azure HostedClusters. |
| `managedIdentities` | `AzureResourceManagedIdentities` | Yes | Contains managed identity configuration for managed Azure HostedClusters. |

### Authentication Model

Managed Azure HostedClusters use:

- **Control Plane**: Certificate-based authentication with credentials stored in Azure Key Vault
- **Data Plane**: Federated identity credentials with OIDC for worker node components

Control plane components authenticate using certificates retrieved from Azure Key Vault via the Secrets Store CSI Driver.

## ControlPlaneManagedIdentities

The `ControlPlaneManagedIdentities` struct contains all managed identities required for control plane components in managed Azure HostedClusters. Each identity corresponds to a specific OpenShift operator or controller that needs Azure API access.

### Type Reference

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `managedIdentitiesKeyVault` | `ManagedAzureKeyVault` | Yes | Configuration for the Azure Key Vault containing managed identity certificates. |
| `cloudProvider` | `ManagedIdentity` | Yes | Identity for the Azure cloud controller manager (cloud provider). |
| `nodePoolManagement` | `ManagedIdentity` | Yes | Identity for the Cluster API Provider Azure (CAPZ) operator managing NodePools. |
| `controlPlaneOperator` | `ManagedIdentity` | Yes | Identity for the control plane operator. |
| `imageRegistry` | `ManagedIdentity` | No | Identity for the cluster-image-registry-operator. |
| `ingress` | `ManagedIdentity` | Yes | Identity for the cluster-ingress-operator. |
| `network` | `ManagedIdentity` | Yes | Identity for the cluster-network-operator. |
| `disk` | `ManagedIdentity` | Yes | Identity for the Azure Disk CSI driver controller. |
| `file` | `ManagedIdentity` | Yes | Identity for the Azure File CSI driver controller. |

### Component Identity Mapping

| Control Plane Identity | OpenShift Component | Purpose |
|------------------------|---------------------|---------|
| `cloudProvider` | azure-cloud-provider | Manages Azure load balancers, routes, and cloud infrastructure |
| `nodePoolManagement` | cluster-api-provider-azure | Creates and manages Azure VMs for worker nodes |
| `controlPlaneOperator` | control-plane-operator | Manages control plane component lifecycle |
| `imageRegistry` | cluster-image-registry-operator | Manages container image storage in Azure Blob |
| `ingress` | cluster-ingress-operator | Manages Azure DNS records for ingress |
| `network` | cluster-network-operator | Manages Azure network configuration |
| `disk` | azure-disk-csi-driver-controller | Manages Azure Disk persistent volumes |
| `file` | azure-file-csi-driver-controller | Manages Azure File persistent volumes |

### ManagedAzureKeyVault Type

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `name` | `string` | Yes | The name of the Azure Key Vault on the management cluster containing managed identity certificates. |
| `tenantID` | `string` | Yes | The tenant ID of the Azure Key Vault. |

## DataPlaneManagedIdentities

The `DataPlaneManagedIdentities` struct contains client IDs for managed identities used by data plane (worker node) components. These identities use federated credentials with OIDC for authentication.

### Type Reference

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `imageRegistryMSIClientID` | `string` | Yes | Client ID of the managed identity for the image registry controller on worker nodes. |
| `diskMSIClientID` | `string` | Yes | Client ID of the managed identity for the Azure Disk CSI driver on worker nodes. |
| `fileMSIClientID` | `string` | Yes | Client ID of the managed identity for the Azure File CSI driver on worker nodes. |

### Data Plane vs Control Plane Identities

Data plane identities differ from control plane identities in several ways:

- **Location**: Data plane components run on worker nodes, not in the control plane namespace
- **Authentication**: Use OIDC federation with projected ServiceAccount tokens
- **Configuration**: Only require client IDs (no Key Vault secret references)
- **Scope**: Limited to storage and registry operations on worker nodes

## ManagedIdentity Type

The `ManagedIdentity` type represents a single managed identity used by a control plane component.

### Type Reference

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `clientID` | `string` | No | - | The client ID (UUID) of the managed identity. Optional, primarily used for CI purposes. Must be a valid UUID in the format `8-4-4-4-12` (e.g., `12345678-1234-5678-9012-123456789012`). |
| `credentialsSecretName` | `string` | Yes | - | The name of the Azure Key Vault secret containing the managed identity credentials. Must be 1-127 characters using only alphanumeric characters and hyphens. |
| `objectEncoding` | `string` | Yes | `utf-8` | The encoding format of the certificate in Key Vault. Valid values are `utf-8`, `hex`, or `base64`. Must match the encoding used when storing the certificate. |

### Credentials Secret Format

The `credentialsSecretName` references a secret in Azure Key Vault that contains JSON-formatted credentials with the following structure:

```json
{
  "clientId": "<managed-identity-client-id>",
  "clientSecret": "<certificate-or-token>",
  "authenticationEndpoint": "<azure-auth-endpoint>",
  "tenantId": "<azure-tenant-id>",
  "notBefore": "<validity-start-time>",
  "notAfter": "<validity-end-time>"
}
```

The Secrets Store CSI Driver retrieves these secrets from Key Vault and mounts them into control plane pods.

### Object Encoding

The `objectEncoding` field specifies how the certificate is encoded in the Key Vault secret:

| Value | Description |
|-------|-------------|
| `utf-8` | Default encoding. Certificate stored as UTF-8 text. |
| `hex` | Certificate stored as hexadecimal string. |
| `base64` | Certificate stored as base64-encoded string. |

!!! warning "Encoding Mismatch"

    If `objectEncoding` does not match the actual encoding of the stored certificate, the Secrets Store CSI Driver will fail to read the secret. Errors will appear on the SecretProviderClass custom resource.

## AzureKMSSpec (Secret Encryption)

The `AzureKMSSpec` type configures Azure Key Vault-based secret encryption for etcd. This is configured separately from the control plane identities under `spec.secretEncryption.kms.azure`.

### Type Reference

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `activeKey` | `AzureKMSKey` | Yes | The active key used to encrypt new secrets. |
| `backupKey` | `AzureKMSKey` | No | The previous key used during rotation so existing secrets can still be decrypted. |
| `kms` | `ManagedIdentity` | Yes | The managed identity used to authenticate with Azure Key Vault for KMS operations. |

### AzureKMSKey Type

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `keyVaultName` | `string` | Yes | The name of the Azure Key Vault containing the encryption key. |
| `keyName` | `string` | Yes | The name of the key in the Key Vault. |
| `keyVersion` | `string` | Yes | The version of the key to use. |

### KMS Identity Mapping

| Identity | OpenShift Component | Purpose |
|----------|---------------------|---------|
| `kms` | kube-apiserver | Encrypts/decrypts etcd secrets using Azure Key Vault keys |

The KMS identity uses the same `ManagedIdentity` type as control plane identities and its credentials are mounted via the `managed-azure-kms` SecretProviderClass.

## Example: Managed Azure HostedCluster

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: my-aro-hcp-cluster
  namespace: clusters
spec:
  platform:
    type: Azure
    azure:
      location: eastus
      resourceGroup: my-managed-rg
      subscriptionID: "12345678-1234-5678-9012-123456789012"
      tenantID: "87654321-4321-8765-2109-876543210987"
      vnetID: /subscriptions/12345678-1234-5678-9012-123456789012/resourceGroups/my-network-rg/providers/Microsoft.Network/virtualNetworks/my-vnet
      subnetID: /subscriptions/12345678-1234-5678-9012-123456789012/resourceGroups/my-network-rg/providers/Microsoft.Network/virtualNetworks/my-vnet/subnets/my-subnet
      securityGroupID: /subscriptions/12345678-1234-5678-9012-123456789012/resourceGroups/my-network-rg/providers/Microsoft.Network/networkSecurityGroups/my-nsg

      # Identity configuration for managed Azure HostedClusters
      azureAuthenticationConfig:
        azureAuthenticationConfigType: ManagedIdentities
        managedIdentities:
          # Control plane managed identities (8 components)
          controlPlane:
            managedIdentitiesKeyVault:
              name: my-keyvault
              tenantID: "87654321-4321-8765-2109-876543210987"

            cloudProvider:
              clientID: "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa"
              credentialsSecretName: cloud-provider-cert
              objectEncoding: utf-8

            nodePoolManagement:
              clientID: "bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb"
              credentialsSecretName: nodepool-mgmt-cert
              objectEncoding: utf-8

            controlPlaneOperator:
              clientID: "cccccccc-cccc-cccc-cccc-cccccccccccc"
              credentialsSecretName: cpo-cert
              objectEncoding: utf-8

            imageRegistry:
              clientID: "dddddddd-dddd-dddd-dddd-dddddddddddd"
              credentialsSecretName: image-registry-cert
              objectEncoding: utf-8

            ingress:
              clientID: "eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee"
              credentialsSecretName: ingress-cert
              objectEncoding: utf-8

            network:
              clientID: "ffffffff-ffff-ffff-ffff-ffffffffffff"
              credentialsSecretName: network-cert
              objectEncoding: utf-8

            disk:
              clientID: "11111111-1111-1111-1111-111111111111"
              credentialsSecretName: disk-csi-cert
              objectEncoding: utf-8

            file:
              clientID: "22222222-2222-2222-2222-222222222222"
              credentialsSecretName: file-csi-cert
              objectEncoding: utf-8

          # Data plane managed identities (worker nodes)
          dataPlane:
            imageRegistryMSIClientID: "33333333-3333-3333-3333-333333333333"
            diskMSIClientID: "44444444-4444-4444-4444-444444444444"
            fileMSIClientID: "55555555-5555-5555-5555-555555555555"

  # Secret encryption with Azure KMS
  secretEncryption:
    type: kms
    kms:
      provider: Azure
      azure:
        activeKey:
          keyVaultName: my-kms-keyvault
          keyName: my-encryption-key
          keyVersion: "1234567890abcdef"
        kms:
          clientID: "66666666-6666-6666-6666-666666666666"
          credentialsSecretName: kms-cert
          objectEncoding: utf-8

  # ... other HostedCluster configuration
  release:
    image: quay.io/openshift-release-dev/ocp-release:4.19.0-multi
```

## Mapping to Azure Resources

The identity configuration maps to these Azure resources:

| Configuration Field | Azure Resource Type | Resource Location |
|---------------------|---------------------|-------------------|
| `managedIdentitiesKeyVault.name` | Azure Key Vault | Management cluster resource group |
| `*.credentialsSecretName` | Key Vault Secret | Inside the configured Key Vault |
| `*.clientID` | User Assigned Managed Identity | Customer subscription |
| `dataPlane.*MSIClientID` | User Assigned Managed Identity | Customer subscription (federated) |

## Validation Rules

The API enforces the following validation rules:

1. **Discriminator Validation**: The `azureAuthenticationConfigType` must be `ManagedIdentities` for managed Azure HostedClusters.

2. **Client ID Format**: All `clientID` fields must be valid UUIDs in the format `8-4-4-4-12` (e.g., `12345678-1234-5678-9012-123456789012`).

3. **Credentials Secret Name**: Must be 1-127 characters using only alphanumeric characters and hyphens (`^[a-zA-Z0-9-]+$`).

4. **Object Encoding**: Must be one of `utf-8`, `hex`, or `base64`.


---

## Source: docs/content/reference/architecture/managed-azure/identity-flow.md

# Managed Azure HostedClusters Identity Flow

This document provides a high-level overview of how Azure identities flow through the managed Azure HostedClusters architecture, from HostedCluster configuration to Azure API authentication.

The diagrams below show the key aspects of the identity flow. For detailed information on each component, see the linked documentation at the bottom of this page.

## Sequence Diagram

The following diagram shows the identity flow using the Control Plane Operator (CPO) as an example. Other control plane pods that need to authenticate with Azure follow a similar pattern, using their respective SecretProviderClass and credential file.

!!! note "ARO_HCP_KEY_VAULT_USER_CLIENT_ID"
    The `ARO_HCP_KEY_VAULT_USER_CLIENT_ID` environment variable contains the client ID of a user-assigned managed identity on the management cluster. This identity is authorized to pull secrets from the Azure Key Vault. It is set on the HyperShift Operator during installation and passed to the SecretProviderClass as `userAssignedIdentityID`.

```mermaid
sequenceDiagram
    participant HC as HostedCluster CR
    participant HO as HyperShift Operator
    participant CPO as Control Plane Operator
    participant SPC as SecretProviderClass
    participant CSI as CSI Driver
    participant KV as Azure Key Vault
    participant SDK as Azure SDK
    participant AAD as Azure AD
    participant ARM as Azure ARM API

    Note over HC,ARM: Setup Phase
    HC->>HO: HostedCluster created with ManagedIdentities config
    HO->>SPC: Creates SecretProviderClasses (managed-azure-cpo, etc.)
    HO->>CPO: Deploys Control Plane Operator

    Note over HC,ARM: Pod Startup Phase
    CPO->>CSI: Pod starts with CSI volume mount
    CSI->>SPC: Reads SecretProviderClass configuration
    CSI->>KV: Authenticates with ARO_HCP_KEY_VAULT_USER_CLIENT_ID managed identity
    KV-->>CSI: Returns UserAssignedIdentityCredentials JSON
    CSI->>CPO: Mounts credential file at /mnt/certs

    Note over HC,ARM: Runtime Authentication
    CPO->>SDK: Loads credential file
    SDK->>SDK: Parses JSON, decodes PEM certificate
    SDK->>AAD: Client certificate authentication
    AAD-->>SDK: Returns Azure Access Token
    SDK->>ARM: API call with Bearer token
    ARM-->>CPO: Azure resource operation result
```

## Component Identity Mapping

```mermaid
graph LR
    subgraph HC_CR[HostedCluster CR]
        CP[ControlPlaneManagedIdentities]
    end

    subgraph SPC_RESOURCES[SecretProviderClasses]
        SPC1[managed-azure-cloud-provider]
        SPC2[managed-azure-cpo]
        SPC3[managed-azure-nodepool-management]
        SPC4[managed-azure-ingress]
        SPC5[managed-azure-network]
        SPC6[managed-azure-disk-csi]
        SPC7[managed-azure-file-csi]
        SPC8[managed-azure-image-registry]
        SPC9[managed-azure-kms]
    end

    subgraph CP_PODS[Control Plane Pods]
        CCM[Cloud Controller Manager]
        CPO2[Control Plane Operator]
        CAPZ[CAPZ]
        ING[Ingress Operator]
        CNO[Network Operator]
        DISK[Disk CSI Driver]
        FILE[File CSI Driver]
        REG[Image Registry]
        KAS[Kube API Server]
    end

    CP -->|cloudProvider| SPC1 --> CCM
    CP -->|controlPlaneOperator| SPC2 --> CPO2
    CP -->|nodePoolManagement| SPC3 --> CAPZ
    CP -->|ingress| SPC4 --> ING
    CP -->|network| SPC5 --> CNO
    CP -->|disk| SPC6 --> DISK
    CP -->|file| SPC7 --> FILE
    CP -->|imageRegistry| SPC8 --> REG
    KMS_ID[secretEncryption.kms.azure.kms] -->|kms| SPC9 --> KAS
```

## Key Vault Secret Structure

```mermaid
graph TB
    subgraph KV_VAULT[Azure Key Vault]
        KV[(Key Vault)]
    end

    subgraph SECRET_JSON[Secret Content - JSON]
        JSON[UserAssignedIdentityCredentials]
        JSON --> F1[authentication_endpoint]
        JSON --> F2[client_id]
        JSON --> F3[client_secret]
        JSON --> F4[tenant_id]
        JSON --> F5[not_before / not_after]
    end

    subgraph PEM_BUNDLE[Decoded client_secret - PEM]
        PEM[PEM Bundle]
        PEM --> CERT[X.509 Certificate]
        PEM --> KEY[Private Key]
    end

    KV --> JSON
    F3 -->|base64 decode| PEM
```

## Detailed Documentation

Each stage of the identity flow is documented in detail:

| Stage | Documentation | Description |
|-------|---------------|-------------|
| 1. HostedCluster Configuration | HostedCluster Identity Configuration | API field reference for `AzureAuthenticationConfiguration`, `ControlPlaneManagedIdentities`, `DataPlaneManagedIdentities`, and `ManagedIdentity` types |
| 2. Azure Key Vault | Key Vault Secret Structure | Secret naming conventions, `UserAssignedIdentityCredentials` JSON schema, PEM certificate format, and `objectEncoding` options |
| 3-4. SecretProviderClass & CSI | Secrets CSI Usage | How `SecretProviderClass` CRs are created and how the Secrets Store CSI driver mounts credentials into pods |
| 5. Pod Volume Mounts | Pod Volume Mounts | Helper functions, mount paths, environment variables, and example pod specs |
| 6-8. Authentication Flow | Azure Authentication Flow | Complete authentication chain from credential loading through Azure AD to ARM API access |

### Additional References

- SecretProviderClass Configuration - Deep dive into `ReconcileManagedAzureSecretProviderClass` function and all SecretProviderClass resources created by HO/CPO


---

## Source: docs/content/reference/architecture/managed-azure/keyvault-secret-structure.md

# Azure Key Vault Secret Structure

This document describes the structure and format of secrets stored in Azure Key Vault for each managed identity used by managed Azure HostedClusters control plane components.

## Overview

Managed Azure HostedClusters use Azure Key Vault to store credentials for managed identities that authenticate control plane components with Azure APIs. These secrets are mounted into pods via the Secrets Store CSI driver and consumed by components to obtain Azure access tokens.

## Key Vault Reference Configuration

The Key Vault containing managed identity credentials is referenced in the HostedCluster specification under `managedIdentitiesKeyVault`:

```yaml
spec:
  platform:
    azure:
      azureAuthenticationConfig:
        azureAuthenticationConfigType: ManagedIdentities
        managedIdentities:
          controlPlane:
            managedIdentitiesKeyVault:
              name: "<key-vault-name>"
              tenantID: "<tenant-id>"
```

| Field | Description |
|-------|-------------|
| `name` | The name of the Azure Key Vault on the management cluster |
| `tenantID` | The Azure AD tenant ID where the Key Vault exists |

## Secret Naming Convention

Each managed identity has a corresponding secret in Azure Key Vault. The secret name is specified in the `credentialsSecretName` field of each managed identity configuration.

### Naming Pattern

Secret names follow a pattern based on the component name and typically end with `-json` suffix when stored by the setup scripts:

| Component | Typical Secret Name Pattern | Example |
|-----------|---------------------------|---------|
| Cloud Provider | `cloud-provider-<prefix>-json` | `cloud-provider-mycluster-json` |
| Control Plane Operator | `cpo-<prefix>-json` | `cpo-mycluster-json` |
| Node Pool Management | `nodepool-mgmt-<prefix>-json` | `nodepool-mgmt-mycluster-json` |
| Azure Disk CSI | `azure-disk-<prefix>-json` | `azure-disk-mycluster-json` |
| Azure File CSI | `azure-file-<prefix>-json` | `azure-file-mycluster-json` |
| Image Registry | `ciro-<prefix>-json` | `ciro-mycluster-json` |
| Ingress | `ingress-<prefix>-json` | `ingress-mycluster-json` |
| Network (CNCC) | `cncc-<prefix>-json` | `cncc-mycluster-json` |

### Naming Requirements

Secret names must adhere to Azure Key Vault naming requirements:

- Between 1 and 127 characters
- Only alphanumeric characters and hyphens (`[a-zA-Z0-9-]`)
- Must be unique within the Key Vault

## Secret Format and Encoding

### Object Type and Encoding

The secret is stored as an Azure Key Vault **secret** (not a certificate or key). When retrieved via the Secrets Store CSI driver, the following parameters are used:

```yaml
objectType: secret
objectEncoding: utf-8
```

The `objectEncoding` field specifies how the secret content is encoded:

| Encoding | Description |
|----------|-------------|
| `utf-8` | UTF-8 encoded text (default and most common) |
| `hex` | Hexadecimal encoded |
| `base64` | Base64 encoded |

!!! warning "Encoding Mismatch"

    The `objectEncoding` must match the actual encoding format used when the secret was stored in Azure Key Vault. If they don't match, the Secrets Store CSI driver will fail to correctly read the secret, and an error will be visible on the SecretProviderClass custom resource.

## Secret Content Structure

The secret content is a JSON object following the `UserAssignedIdentityCredentials` structure from the Azure MSI Dataplane library. This JSON is stored as UTF-8 encoded text.

### JSON Schema

```json
{
  "authentication_endpoint": "https://login.microsoftonline.com/",
  "client_id": "<managed-identity-client-id>",
  "client_secret": "<base64-encoded-pem-certificate>",
  "tenant_id": "<azure-ad-tenant-id>",
  "not_before": "<timestamp-rfc3339>",
  "not_after": "<timestamp-rfc3339>",
  "renew_after": "<timestamp-rfc3339>",
  "cannot_renew_after": "<timestamp-rfc3339>"
}
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `authentication_endpoint` | string | Yes | The Azure AD authentication endpoint URL (e.g., `https://login.microsoftonline.com/`) |
| `client_id` | string | Yes | The client ID (UUID) of the managed identity |
| `client_secret` | string | Yes | Base64-encoded X.509 certificate with private key in PEM format |
| `tenant_id` | string | Yes | The Azure AD tenant ID (UUID) |
| `not_before` | string | Yes | RFC3339 timestamp when the credential becomes valid |
| `not_after` | string | Yes | RFC3339 timestamp when the credential expires |
| `renew_after` | string | Optional | RFC3339 timestamp after which the credential should be renewed |
| `cannot_renew_after` | string | Optional | RFC3339 timestamp after which the credential cannot be renewed |

### Example Secret Content

```json
{
  "authentication_endpoint": "https://login.microsoftonline.com/",
  "client_id": "12345678-1234-1234-1234-123456789abc",
  "client_secret": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURxRENDQX...",
  "tenant_id": "87654321-4321-4321-4321-abcdef123456",
  "not_before": "2024-01-15T10:00:00Z",
  "not_after": "2025-01-15T10:00:00Z",
  "renew_after": "2024-07-15T10:00:00Z",
  "cannot_renew_after": "2024-12-15T10:00:00Z"
}
```

## Certificate Content (client_secret Field)

The `client_secret` field contains a **base64-encoded PEM certificate bundle** that includes both the X.509 certificate and private key.

### PEM Structure

When decoded from base64, the `client_secret` contains:

```pem
-----BEGIN CERTIFICATE-----
MIIDqDCCApCgAwIBAgIQExample...
(certificate data in base64)
-----END CERTIFICATE-----
-----BEGIN PRIVATE KEY-----
MIIEvgIBADANBgkqhkiG9w0BAQEFAASC...
(private key data in base64)
-----END PRIVATE KEY-----
```

The PEM bundle may include:

1. **X.509 Certificate**: The public certificate used for authentication
2. **Private Key**: The RSA or ECDSA private key (PKCS#8 format)
3. **Certificate Chain** (optional): Intermediate certificates if applicable

### How the Certificate is Used

The msi-dataplane library processes this certificate as follows:

1. Base64 decodes the `client_secret` value
2. Parses the PEM-encoded certificate and private key using `azidentity.ParseCertificates()`
3. Creates a `ClientCertificateCredential` for authenticating with Azure AD
4. The credential sends the certificate chain in the x5c header (required for MSI authentication)

!!! note "Certificate Format"

    The Azure SDK's `ParseCertificates` function expects PEM format. PKCS#12 format is not currently supported by the msi-dataplane library.

## Managed Identity Configuration in HostedCluster

Each managed identity in the HostedCluster specification includes the following fields:

```yaml
cloudProvider:
  clientID: "<uuid>"
  credentialsSecretName: "<secret-name>"
  objectEncoding: "utf-8"
```

| Field | Description |
|-------|-------------|
| `clientID` | The Azure AD client ID (UUID) of the managed identity |
| `credentialsSecretName` | Name of the secret in Azure Key Vault containing the credentials |
| `objectEncoding` | Encoding format of the secret content (utf-8, hex, or base64) |

## Key Vault Secret Attributes

When storing secrets in Key Vault, the following attributes are set:

| Attribute | Description |
|-----------|-------------|
| `Enabled` | Set to `true` to allow retrieval |
| `Expires` | Set to the `not_after` timestamp from the credential |
| `NotBefore` | Set to the `not_before` timestamp from the credential |

Additionally, the following tags are added:

| Tag | Description |
|-----|-------------|
| `renew_after` | RFC3339 timestamp for credential renewal |
| `cannot_renew_after` | RFC3339 timestamp after which renewal is not possible |

## Related Documentation

- Secrets CSI Usage - How the Secrets Store CSI driver mounts secrets
- Azure MSI Dataplane Library - Library used for credential handling


---

## Source: docs/content/reference/architecture/managed-azure/pod-volume-mounts.md

# Pod Volume Mount Implementation Details

This document provides implementation details for how control plane pods mount Azure credentials. For a conceptual overview, see Secrets CSI Usage.

## Helper Functions

All helper functions are in `support/azureutil/azureutil.go`:

### CreateVolumeForAzureSecretStoreProviderClass

Creates a CSI volume referencing a SecretProviderClass:

```go
func CreateVolumeForAzureSecretStoreProviderClass(secretStoreVolumeName, secretProviderClassName string) corev1.Volume {
    return corev1.Volume{
        Name: secretStoreVolumeName,
        VolumeSource: corev1.VolumeSource{
            CSI: &corev1.CSIVolumeSource{
                Driver:   "secrets-store.csi.k8s.io",
                ReadOnly: ptr.To(true),
                VolumeAttributes: map[string]string{
                    "secretProviderClass": secretProviderClassName,
                },
            },
        },
    }
}
```

### CreateVolumeMountForAzureSecretStoreProviderClass

Creates a volume mount at `/mnt/certs`:

```go
func CreateVolumeMountForAzureSecretStoreProviderClass(secretStoreVolumeName string) corev1.VolumeMount {
    return corev1.VolumeMount{
        Name:      secretStoreVolumeName,
        MountPath: "/mnt/certs",
        ReadOnly:  true,
    }
}
```

### CreateVolumeMountForKMSAzureSecretStoreProviderClass

Creates a volume mount at `/mnt/kms` (for KMS credentials):

```go
func CreateVolumeMountForKMSAzureSecretStoreProviderClass(secretStoreVolumeName string) corev1.VolumeMount {
    return corev1.VolumeMount{
        Name:      secretStoreVolumeName,
        MountPath: "/mnt/kms",
        ReadOnly:  true,
    }
}
```

### CreateEnvVarsForAzureManagedIdentity

Creates environment variable pointing to the credential file:

```go
func CreateEnvVarsForAzureManagedIdentity(azureCredentialsName string) []corev1.EnvVar {
    return []corev1.EnvVar{
        {
            Name:  "MANAGED_AZURE_HCP_CREDENTIALS_FILE_PATH",
            Value: "/mnt/certs/" + azureCredentialsName,
        },
    }
}
```

### IsAroHCP

Checks if running in a managed Azure HostedClusters environment by checking the MANAGED_SERVICE environment variable:

```go
func IsAroHCP() bool {
    return os.Getenv("MANAGED_SERVICE") == hyperv1.AroHCP
}
```

## Constants

Defined in `support/config/constants.go`:

| Constant | Value | Description |
|----------|-------|-------------|
| `ManagedAzureCredentialsFilePath` | `MANAGED_AZURE_HCP_CREDENTIALS_FILE_PATH` | Environment variable name |
| `ManagedAzureCertificateMountPath` | `/mnt/certs` | Base mount path |
| `ManagedAzureCertificatePath` | `/mnt/certs/` | Path prefix for building file paths |
| `ManagedAzureCredentialsMountPathForKMS` | `/mnt/kms` | KMS-specific mount path |
| `ManagedAzureCredentialsPathForKMS` | `/mnt/kms/` | KMS path prefix |
| `ManagedAzureSecretsStoreCSIDriver` | `secrets-store.csi.k8s.io` | CSI driver name |
| `ManagedAzureSecretProviderClass` | `secretProviderClass` | Volume attribute key |

## Component Volume Names

| Component | Volume Name | SecretProviderClass |
|-----------|-------------|---------------------|
| Control Plane Operator | `cpo-cert` | `managed-azure-cpo` |
| Cloud Provider (CCM) | `cloud-provider-cert` | `managed-azure-cloud-provider` |
| Ingress Operator | `ingress-cert` | `managed-azure-ingress` |
| Image Registry | `image-registry-cert` | `managed-azure-image-registry` |
| KMS | `kms-cert` | `managed-azure-kms` |
| NodePool Management (CAPZ) | `nodepool-management-cert` | `managed-azure-nodepool-management` |

## Credential File Usage

Components read the mounted credential file using the `msi-dataplane` library:

```go
import "github.com/Azure/msi-dataplane/pkg/dataplane"

// Build path from constant + secret name
certPath := config.ManagedAzureCertificatePath + credentialsSecretName

// Create credential with automatic file watching and reload
creds, err := dataplane.NewUserAssignedIdentityCredential(
    ctx,
    certPath,
    dataplane.WithClientOpts(azcore.ClientOptions{Cloud: cloud.AzurePublic}),
)
```

The credential automatically reloads when the file is updated by the CSI driver.

## Configuration File Usage

Some components (CCM, Azure Disk/File CSI) use a cloud config file that references the credential path:

```json
{
  "cloud": "AzurePublicCloud",
  "tenantId": "<tenant-id>",
  "subscriptionId": "<subscription-id>",
  "resourceGroup": "<resource-group>",
  "location": "<location>",
  "useManagedIdentityExtension": true,
  "aadMSIDataPlaneIdentityPath": "/mnt/certs/<credential-secret-name>"
}
```

The `aadMSIDataPlaneIdentityPath` field tells the component where to find the mounted credential file.


---

## Source: docs/content/reference/architecture/managed-azure/secretproviderclass-configuration.md

# SecretProviderClass Implementation Details

This document provides implementation details for how HyperShift creates and manages SecretProviderClass resources. For a conceptual overview, see Secrets CSI Usage.

## ReconcileManagedAzureSecretProviderClass Function

The core function that populates SecretProviderClass resources is located in `support/secretproviderclass/secretproviderclass.go`:

```go
func ReconcileManagedAzureSecretProviderClass(
    secretProviderClass *secretsstorev1.SecretProviderClass,
    hcp *hyperv1.HostedControlPlane,
    managedIdentity hyperv1.ManagedIdentity,
) {
    secretProviderClass.Spec = secretsstorev1.SecretProviderClassSpec{
        Provider: "azure",
        Parameters: map[string]string{
            "usePodIdentity":         "false",
            "useVMManagedIdentity":   "true",
            "userAssignedIdentityID": azureutil.GetKeyVaultAuthorizedUser(),
            "keyvaultName":           hcp.Spec.Platform.Azure.AzureAuthenticationConfig.ManagedIdentities.ControlPlane.ManagedIdentitiesKeyVault.Name,
            "tenantId":               hcp.Spec.Platform.Azure.AzureAuthenticationConfig.ManagedIdentities.ControlPlane.ManagedIdentitiesKeyVault.TenantID,
            "objects":                formatSecretProviderClassObject(managedIdentity.CredentialsSecretName, string(managedIdentity.ObjectEncoding)),
        },
    }
}
```

### Parameter Sources

| Parameter | Source | Description |
|-----------|--------|-------------|
| `userAssignedIdentityID` | `azureutil.GetKeyVaultAuthorizedUser()` | Reads from `ARO_HCP_KEY_VAULT_USER_CLIENT_ID` env var, set via `--aro-hcp-key-vault-users-client-id` flag during HO installation |
| `keyvaultName` | `hcp.Spec.Platform.Azure...ManagedIdentitiesKeyVault.Name` | From HostedControlPlane spec |
| `tenantId` | `hcp.Spec.Platform.Azure...ManagedIdentitiesKeyVault.TenantID` | From HostedControlPlane spec |
| `objects` | `formatSecretProviderClassObject(...)` | Generated from `ManagedIdentity.CredentialsSecretName` and `ObjectEncoding` |

## SecretProviderClass Names and Constants

### Created by HyperShift Operator

| Constant | Value | ManagedIdentity Field |
|----------|-------|----------------------|
| `ManagedAzureCPOSecretProviderClassName` | `managed-azure-cpo` | `ControlPlane.ControlPlaneOperator` |
| `ManagedAzureNodePoolMgmtSecretProviderClassName` | `managed-azure-nodepool-management` | `ControlPlane.NodePoolManagement` |
| `ManagedAzureKMSSecretProviderClassName` | `managed-azure-kms` | `SecretEncryption.KMS.Azure.KMS` |

### Created by Control Plane Operator

| Constant | Value | ManagedIdentity Field |
|----------|-------|----------------------|
| `ManagedAzureCloudProviderSecretProviderClassName` | `managed-azure-cloud-provider` | `ControlPlane.CloudProvider` |
| `ManagedAzureIngressSecretStoreProviderClassName` | `managed-azure-ingress` | `ControlPlane.Ingress` |
| `ManagedAzureNetworkSecretStoreProviderClassName` | `managed-azure-network` | `ControlPlane.Network` |
| `ManagedAzureImageRegistrySecretStoreProviderClassName` | `managed-azure-image-registry` | `ControlPlane.ImageRegistry` |
| `ManagedAzureDiskCSISecretStoreProviderClassName` | `managed-azure-disk-csi` | `ControlPlane.Disk` |
| `ManagedAzureFileCSISecretStoreProviderClassName` | `managed-azure-file-csi` | `ControlPlane.File` |

## Source Files

| File | Purpose |
|------|---------|
| `support/secretproviderclass/secretproviderclass.go` | `ReconcileManagedAzureSecretProviderClass` function |
| `support/config/constants.go` | SecretProviderClass name constants |
| `support/azureutil/azureutil.go` | `GetKeyVaultAuthorizedUser()` and volume helpers |
| `control-plane-operator/controllers/hostedcontrolplane/manifests/secretproviderclass.go` | Manifest factory |

## Component Adapter Files

Each component has an adapter that calls `ReconcileManagedAzureSecretProviderClass`:

| Component | Adapter File |
|-----------|--------------|
| Cloud Controller Manager | `control-plane-operator/controllers/hostedcontrolplane/v2/cloud_controller_manager/azure/config.go` |
| Cluster Network Operator | `control-plane-operator/controllers/hostedcontrolplane/v2/cno/azure.go` |
| Ingress Operator | `control-plane-operator/controllers/hostedcontrolplane/v2/ingressoperator/azure.go` |
| Image Registry Operator | `control-plane-operator/controllers/hostedcontrolplane/v2/registryoperator/azure.go` |
| Storage Operator (Disk/File) | `control-plane-operator/controllers/hostedcontrolplane/v2/storage/azure.go` |
| KMS Provider | `control-plane-operator/controllers/hostedcontrolplane/v2/kas/kms/azure.go` |


---

## Source: docs/content/reference/architecture/managed-azure/secrets-csi.md

# Secrets CSI Usage
The Secrets CSI driver is used in HyperShift's managed Azure architecture in order to read secrets from Azure Key Vault 
and mount them as files in a pod. This allows for the secure storage of sensitive information such as credentials and 
certificates.

More information on Secrets CSI driver can be found in the official documentation.

## Overview
A single managed identity is used to pull any secrets or certificates from Azure Key Vault. The managed identity is 
created when the AKS cluster is created. For example, this happens when the flag 
`enable-addons azure-keyvault-secrets-provider` is provided when creating the AKS cluster using the Azure CLI. 

!!! important

    The created managed identity is expected to have the `Key Vault Secrets User` role assigned to it so that it can read 
    secrets and credentials from the Azure Key Vault.

!!! important

    This managed identity will be used by any HostedClusters managed by the HO to read secrets from the Azure Key Vault.

This managed identity is passed in as a client ID to the HyperShift Operator during installation through the flag 
`aro-hcp-key-vault-users-client-id`. This client ID will be passed in to every created SecretProviderClass CR and used 
in the field called `userAssignedIdentityID`.

## SecretsProviderClass CRs
The HyperShift Operator creates a SecretProviderClass CR for:

- the control plane operator (CPO)
- the nodepool management provider (CAPZ)

The CPO creates a SecretProviderClass CR for:

- key management service (KMS)
- cloud-network-config-controller (CNCC)
- cloud provider (CP, aka CCM)
- ingress
- image registry
- Azure disk CSI driver
- Azure file CSI driver

Here is an example of a SecretProviderClass CR for the CPO:

```yaml
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  annotations:
    hypershift.openshift.io/cluster: e2e-clusters-example/example-cluster
spec:
  parameters:
    keyvaultName: aks-e2e
    objects: |2

      array:
        - |
          objectName: cpo-secret
          objectEncoding: utf-8
          objectType: secret
    tenantId: f47a1c3d-89ab-4def-b456-123456789abc
    usePodIdentity: "false"
    useVMManagedIdentity: "true"
    userAssignedIdentityID: 92bd5e7a-3cfe-41a2-9f88-df0123456789
  provider: azure
```

## How SecretProviderClass CR is Used
The SecretProviderClass CR is then used by the Secrets CSI driver to mount the secret, `objectName` in the above 
example, into the pod as a file in a volume mount. Here is an example of a pod spec that mounts the secret:

```yaml
  containers:
...
    name: control-plane-operator
...
    volumeMounts:
      - mountPath: /mnt/certs
        name: cpo-cert
        readOnly: true
...
    volumes:
      - csi:
          driver: secrets-store.csi.k8s.io
          readOnly: true
          volumeAttributes:
            secretProviderClass: managed-azure-cpo
        name: cpo-cert
```

The mounted secret can be viewed in the pod by navigating to the `/mnt/certs` directory and catting the file. In this 
example, something like:

```bash
k exec -it control-plane-operator -- /bin/bash
cat /mnt/certs/cpo-secret
```

## How Secret Information is Used By Components
All the different components using the Secrets CSI driver will have their own way of consuming the secret.

### Consumed directly in the operator/controller
The following components directly use the secret file mounted in the pod to authenticate with Azure cloud:

- CPO
- CNCC
- ingress
- image registry

For example:
```go
	certPath := config.ManagedAzureCertificatePath + hcp.Spec.Platform.Azure.ManagedIdentities.ControlPlane.ControlPlaneOperator.CredentialsSecretName
	creds, err := dataplane.NewUserAssignedIdentityCredential(ctx, certPath, dataplane.WithClientOpts(azcore.ClientOptions{Cloud: cloud.AzurePublic}))
	if err != nil {
		return fmt.Errorf("failed to create azure creds to verify resource group locations: %v", err)
	}
```

### Consumed by a configuration file
The following components use a configuration file in order to know where to find the secret mounted in the pod:

- CP/CCM
- Azure disk CSI driver
- Azure file CSI driver

For an example, see the official documentation.

### Consumed through a CR
Finally, the nodepool management provider (CAPZ) uses a CR, AzureClusterIdentity, to identify where the secret is 
mounted in the pod.

For an example, see the official documentation.


---

## Source: docs/content/reference/architecture/managed-azure/shared-ingress.md

# Shared Ingress
## Motivation

Today, exposing the apiserver of an hosted cluster to its nodes is possible only with two options: LoadBalancer and NodePort.
The downside of the LoadBalancer method is that it would require a separate LB for every hosted cluster configured on the mgmt cluster. This can incur additional costs at the cloud provider and an additional, even if it small, spin up time to provision the LB service.
The downside of the NodePort method is that it is bound to one of the nodes‚Äô IP of the mgmt cluster (typically the 1st one). Once this node is down, all of the hosted clusters‚Äô apiservers which are based on NodePort connectivity are no longer reachable.
Shared Ingress presents another option, in which there would be a single LoadBalacer on the mgmt cluster, that will serve all of the hosted clusters that are configured to use this method.
Connections to the hosted kube-apiservers from outside of the cluster will be made available through a Route.


## Overview

Generally in kubernetes and openshift clusters, pods who wish to communicate with their cluster‚Äôs api-server, are doing it through the kubernetes.default.svc.cluster.local master service, which is mapped to the cluster‚Äôs kube-apiserver service.
In HyperShift guest clusters, the master service (whose IP is 172.31.0.1) is mapped to a kube-apiserver-proxy pod, running on every node at the host level, which proxies the request to the apiserver on the HCP namespace at the mgmt cluster, 1:1 flat translation, in TLS passthrough.

Shared Ingress (single LB) solution is able to differentiate between various hcp api-servers hosted on the mgmt cluster using the PROXY Protocol. The PROXY protocol enables adding additional info to a TLS connection, outside of the encrypted message, when both ends support this protocol.
The fields that are added with the PROXY protocol are mainly:
- Source IP
- Source Port
- Destination IP
- Destination Port
- TLVs (key-value pairs that carry arbitrary data)

### Guest Intermediate Proxy

On the guest cluster, the kube-apiserver-proxy (HAProxy) instance, will use the PROXY protocol when sending requests and includes a custom TLV which contains the `ClusterID` so the central proxy on the mgmt can see `ClusterID` and forward it to the respective kube-apiserver on the hcp.
The default backend of this proxy will be the LB address of the central proxy.

### MGMT Cluster Central Proxy

On the management cluster, we setup a single proxy server (HAProxy) in the `hypershift-sharedingress` namespace. This central proxy will accept connections through a LoadBalancer service, and with PROXY protocol.
It will then examine the custom TLV field of the PROXY protocol, extract the attached `ClusterID` and using an ACL, will forward the connection to the respective hcp kube-apiserver.
The destination IP in this case will be exactly the same as the ClusterIP of the hcp kube-apiserver.

Note: the kube-apiserver will no longer be exposed through a dedicated LB service.


## Reference Diagram




---

## Source: docs/content/reference/architecture/mce-and-agent.md

**Introduction**

This section elucidates the collaboration between Multicluster Engine and Agent to facilitate in-house deployments. Detailed documentation for each of the network stacks can be found in the *Self-Managed Laboratories* section. If you intend to set up a self-managed environment, please proceed to that section and follow the provided steps.

**High-Level Overview**



The diagram above provides an overview of the environment and how the workflow functions, along with labeled components for reference:

1. **(Applicable in Disconnected Environments Only)**: Create a ConfigMap with a specified name in the `openshift-config` namespace. In this example, we'll name it `registry-config`. The content of this ConfigMap should be the Registry CA certificate.
2. **(Applicable in Disconnected Environments Only)**: Modify the `images.config.openshift.io` Custom Resource (CR) and add a new field in the spec named `additionalTrustedCA`, with a value of `name: registry-config`.
3. **(Applicable in Disconnected Environments Only)**: Create a ConfigMap with a specified name containing two data fields. One field should contain the `registries.conf` file in RAW format, and the other should be named `ca-bundle.crt`, which should contain the Registry CA.
4. Create the `multiclusterengine` CR, enabling both the Agent and Hypershift addons.
5. Create the HostedCluster Objects. This involves several components, including:
    1. **Secrets**: These contain the *PullSecret*, *SSHKey*, and *ETCD Encryption Key*.
    2. **ConfigMap** **(Applicable in Disconnected Environments Only)**: This ConfigMap contains the CA certificate of the private registry.
    3. **HostedCluster**: Defines the configuration of the cluster you intend to create.
    4. **NodePool**: Identifies the pool that references the Machines to be used for the data plane.

6. Following the creation of HostedCluster Objects, the Hypershift Operator will establish the __HostedControlPlane Namespace__ to accommodate ControlPlane pods. This namespace will also host components like **Agents**, **BareMetalHosts**, **Infraenv**, and more. Subsequently, you need to create the **InfraEnv** and, after ISO creation, the BareMetalHosts along with their secrets containing BMC credentials.

7. The Metal3 operator, within the `openshift-machine-api` namespace, will inspect the newly created BareMetalHosts. It will then attempt to connect to the BMCs to boot them up using the configured __LiveISO__ and __RootFS__ specified through the **AgentServiceConfig** CR in the MCE namespace.

8. Once the worker nodes of the HostedCluster have successfully booted up, an __Agent__ container will be initiated. This __Agent__ will establish contact with the Assisted Service, which will orchestrate the necessary actions to complete the deployment. Initially, you will need to scale the NodePool to the desired number of worker nodes for your HostedCluster, after which the AssistedService will manage the remaining tasks.

9. At this point, you need to patiently await the completion of the deployment process.


---

## Source: docs/content/reference/concepts-and-personas.md

# Concepts and Personas

## Concepts

**Management Cluster**

An OpenShift cluster which hosts HyperShift and zero to many Hosted Clusters.

**Hosted Cluster**

An OpenShift API endpoint managed by HyperShift.

**Hosted Control Plane**

An OpenShift control plane running on the Management Cluster which is exposed by a Hosted Cluster's API endpoint. The component parts of a control plane include etcd, apiserver, kube-controller-manager, vpn, etc.

**Hosted Control Plane Infrastructure**

Resources on the Management Cluster or external cloud provider which are prerequisites to running Hosted Control Plane processes.

**Management Cluster Infrastructure**

Network, compute, storage, etc. of the Management Cluster.

**Hosted Cluster Infrastructure**

Network, compute, storage, etc. that exist in customer cloud account.

## Personas

**Cluster Service Provider**

The user hosting cluster control planes, responsible for up-time. UI for fleet wide alerts, configuring AWS account to host control planes in, user provisioned infra (host awareness of available compute), where to pull VMs from. Has cluster admin management.

**Cluster Service Consumer**

The user empowered to request control planes, request workers, and drive upgrades or modify externalized configuration. Likely not empowered to manage or access cloud credentials or infrastructure encryption keys.

**Cluster Instance Admin**

The user with cluster-admin role in the provisioned cluster, but may have no power over when/how cluster is upgraded or configured. May see some configuration projected into the cluster in read-only fashion.

**Cluster (Instance) User**

Maps to a developer today in standalone OCP. They will not have a view or insight into OperatorHub, Machines, etc.

---

## Source: docs/content/reference/controller-architecture.md

# Controller architecture

## High Level Overview

### Physical layout and operating model

Legend:

- Yellow box: namespace
- Rounded box: processes
- Rectangle: CR instances

```mermaid
flowchart LR
  subgraph hypershift
    cluster-operator([HyperShift Operator])
  end

  subgraph user-clusters
    HostedClusterA
    NodePoolA
  end

  subgraph cluster-a
    control-plane-operator([Control Plane Operator])

    capi-manager([CAPI Manager])
    capi-provider([CAPI Provider])

    HostedControlPlane
    ExternalInfraCluster

    cp-components([Control Plane Components])

    capi-cluster[CAPICluster]

    capi-machine-template[CAPIInfrastructureMachineTemplate]
    capi-machineset[CAPI MachineSet]
    capi-machine[CAPI Machine]
    capi-provider-machine[CAPIInfrastructureMachine]
  end

  cluster-operator-->|reconciles|HostedClusterA
  cluster-operator-->|operates|control-plane-operator
  cluster-operator-->|operates|capi-manager
  cluster-operator-->|operates|capi-provider
  cluster-operator-->|creates|HostedControlPlane
  cluster-operator-->|creates|capi-cluster
  cluster-operator-->|creates|ExternalInfraCluster
  cluster-operator-->|reconciles|NodePoolA
  cluster-operator-->|creates|capi-machine-template
  cluster-operator-->|creates|capi-machineset

  control-plane-operator-->|operates|cp-components

  control-plane-operator-->|reconciles|HostedControlPlane

  capi-manager-->|reconciles|capi-cluster
  capi-manager-->|reconciles|capi-machineset
  capi-manager-->|creates|capi-machine

  capi-provider-->|reconciles|capi-machine
  capi-provider-->|creates|capi-provider-machine
```

TODO:
1. How do we (or should we) represent an input/output or "consumes" relationship (e.g. the hypershift operator creates and syncs machine templates, and the CAPI provider _reads_ the template, but nothing actively watches templates and does work in reaction to them directly)



## Major Components

### HyperShift Operator

The **HyperShift Operator** is a singleton within the management cluster that manages the lifecycle of hosted clusters represented by `HostedCluster` resources.

A single version of the the HyperShift Operator knows how to manage multiple hosted OCP versions.

The HyperShift Operator is responsible for:

- Processing `HostedCluster` and `NodePool` resources and managing Control Plane Operator and Cluster API (CAPI) deployments which do the actual work of installing a control plane.
- Managing the lifecycle of the hosted cluster by handling rollouts of new Control Plane Operator and CAPI deployments based on version changes to `HostedCluster` and `NodePool` resources.
- Aggregating and surfacing information about clusters.


#### HostedCluster Controller

```mermaid

graph TD
  hosted-cluster-controller[HostedCluster Controller] --> reconcile([Reconcile HostedCluster])
  reconcile --> is-deleted{{Deleted?}}
  is-deleted -->|Yes| teardown([Teardown])
  is-deleted -->|No| sync([Sync])
  
  teardown -->teardown-complete{{Teardown complete?}}
  teardown-complete -->|Yes| return
  teardown-complete -->|No| reconcile
  
  sync --> create-namespace([Create Namespace])
  create-namespace --> deploy-cp-operator([Deploy Control Plane Operator])
  deploy-cp-operator --> deploy-capi-manager([Deploy CAPI Manager])
  deploy-capi-manager --> deploy-capi-provider([Deploy CAPI Provider])
  deploy-capi-provider --> create-capi-cluster([Create CAPICluster])
  create-capi-cluster --> create-hosted-control-plane([Create HostedControlPlane])
  create-hosted-control-plane --> create-external-infra-cluster([Create ExternalInfraCluster])
  create-external-infra-cluster -->has-initial-nodes{{HostedCluster has initial nodes?}}
  has-initial-nodes -->|Yes| create-node-pool([Create NodePool])
  has-initial-nodes -->|No| return
  create-node-pool --> return
  
  return([End])
  
```

#### NodePool Controller

```mermaid
graph TD
  nodepool-controller[NodePool Controller] --> reconcile([Reconcile NodePool])
  reconcile --> is-deleted{{Deleted?}}
  is-deleted -->|Yes| teardown([Teardown])
  is-deleted -->|No| sync([Sync])
  
  sync --> create-capi-machineset([Create CAPIMachineSet])
  create-capi-machineset --> create-capi-infra-machine-template([Create CAPIInfrastructureMachineTemplate])
  
  create-capi-infra-machine-template --> return
  
  teardown -->teardown-complete{{Teardown complete?}}
  teardown-complete -->|Yes| return
  teardown-complete -->|No| reconcile
  
  return([End])
```

#### ExternalInfraCluster Controller

```mermaid

graph TD
  external-infra-cluster-controller[ExternalInfraCluster Controller] --> reconcile([Reconcile ExternalInfraCluster])
  reconcile --> is-deleted{{Deleted?}}
  is-deleted -->|Yes| teardown([Teardown])
  is-deleted -->|No| sync([Sync])
  
  teardown -->teardown-complete{{Teardown complete?}}
  teardown-complete -->|Yes| return
  teardown-complete -->|No| reconcile
  
  sync --> get-hosted-control-plane([Get HostedControlPlane])
  get-hosted-control-plane -->is-hcp-ready{{Is HostedControlPlane ready?}}
  is-hcp-ready -->|No| reconcile
  is-hcp-ready -->|Yes| update-infra-status([Update ExternalInfraCluster status])
  update-infra-status --> return
  
  return([End])
  
```

### Control Plane Operator

The **Control Plane Operator** is deployed by the HyperShift Operator into a hosted control plane namespace and manages the rollout of a single version of the the hosted cluster's control plane.

The Control Plane Operator is versioned in lockstep with a specific OCP version and is decoupled from the management cluster's version.

The Control Plane Operator is responsible for:

- Provisioning all the infrastructure required to host a control plane (whether this means creating or adopting existing infrastructure). This infrastructure may be management cluster resources, external cloud provider resources, etc.
- Deploying an OCP control plane configured to run in the context of the provisioned infrastructure.
- Implementing any versioned behavior necessary to rollout the new version (e.g. version specific changes at layers above OCP itself, like configuration or infrastructure changes).


#### HostedControlPlane Controller

```mermaid

graph TD
  hosted-control-plane-controller[HostedControlPlane Controller] --> reconcile([Reconcile HostedControlPlane])
  reconcile --> is-deleted{{Deleted?}}
  is-deleted -->|Yes| teardown([Teardown])
  is-deleted -->|No| sync([Sync])
  
  teardown -->teardown-complete{{Teardown complete?}}
  teardown-complete -->|Yes| return
  teardown-complete -->|No| reconcile
  
  sync --> create-infra([Deploy Control Plane<br>Components])
  create-infra --> create-config-operator([Deploy Hosted Cluster<br>Config Operator])
  create-config-operator -->is-infra-ready{{Infra ready?}}
  is-infra-ready -->|Yes| update-hosted-controlplane-ready([Update HostedControlPlane status])
  is-infra-ready -->|No| reconcile
  update-hosted-controlplane-ready --> return
  
  return([End])
  
```

### Hosted Cluster Config Operator

The **Hosted Cluster Config Operator** is a control plane component maintained by HyperShift that's a peer to other control plane components (e.g., etcd, apiserver, controller-manager), and is managed by the Control Plane Operator in the same way as those other control plane components.

The Hosted Cluster Config Operator is versioned in lockstep with a specific OCP version and is decoupled from the management cluster's version.

The Hosted Cluster Config Operator is responsible for:

- Reading CAs from the hosted cluster to configure the kube controller manager CA bundle running in the hosted control plane
- Reconciling resources that live on the hosted cluster:
    * CRDs created by operators that are absent from the hosted cluster (RequestCount CRD created by cluster-kube-apiserver-operator)
    * Clearing any user changes to the ClusterVersion resource (all updates should be driven via HostedCluster API)
    * ClusterOperator stubs for control plane components that run outside.
    * Global Configuration that is managed via the HostedCluster API
    * Namespaces that are normally created by operators that are absent from the cluster.
    * RBAC that is normally created by operators that are absent from the cluster.
    * Registry configuration
    * Default ingress controller
    * Control Plane PKI (kubelet serving CA, control plane signer CA)
    * Konnectivity Agent
    * OpenShift APIServer resources (APIServices, Service, Endpoints)
    * OpenShift OAuth APIServer resources (APIServices, Service, Endpoints)
    * Monitoring Configuration (set node selector to non-master nodes)
    * Pull Secret
    * OAuth serving cert CA
    * OAuthClients required by the console
    * Cloud Credential Secrets (contain STS role for components that need cloud access)
    * OLM CatalogSources
    * OLM PackageServer resources (APIService, Service, Endpoints)

## Resource dependency diagram

* Dotted lines are dependencies (ownerRefs)
* Solid lines are associations (e.g. infrastructureRefs or controlPlaneRefs on specs)

```mermaid
classDiagram
  HostedCluster
  HostedControlPlane ..> CAPICluster
  ExternalInfraCluster ..> CAPICluster
  CAPICluster ..> HostedCluster
  CAPICluster --> HostedControlPlane
  CAPICluster --> ExternalInfraCluster
  CAPIMachineSet ..> CAPICluster
  CAPIMachineSet --> CAPIInfrastructureMachineTemplate
  CAPIMachine ..>CAPIMachineSet
  CAPIMachine -->CAPIInfrastructureMachine
  CAPIInfrastructureMachine ..>CAPIMachine
  CAPIInfrastructureMachineTemplate ..>CAPICluster
```

## Transformations

Trying to show how certain important resources are derived from others. These are resources created by our operators, not by CAPI.

```mermaid
classDiagram
  CAPICluster ..> HostedControlPlane
  CAPICluster ..> ExternalInfraCluster
  HostedControlPlane ..> HostedCluster
  ExternalInfraCluster ..> HostedCluster

```

```mermaid
classDiagram
  CAPIInfrastructureTemplate ..> NodePool
  CAPIInfrastructureTemplate ..> HostedCluster
  CAPIMachineSet ..> NodePool
  CAPIMachineSet ..> HostedCluster
  CAPIMachineSet ..> CAPIInfrastructureTemplate
```



---

## Source: docs/content/reference/goals-and-design-invariants.md

# Project goals

These are desired project goals which drive the design invariants stated below. Goals and scope may vary as the project evolves.

- Provide an API to express intent to create OpenShift Container Platform (OCP) clusters with a hosted control plane topology on existing infrastructure.
- Decouple control and data plane.
  - Enable segregation of ownership and responsibility for different personas.
  - Security.
  - Cost efficiency.

## Design invariants

- Communication between management cluster and a hosted cluster is unidirectional. A hosted cluster has no awareness of a management cluster.
- Communication between management cluster and a hosted cluster is only allowed from within each particular control plane namespace.
- Compute worker Nodes should not run anything beyond user workloads.
  - A hosted cluster should not expose CRDs, CRs or Pods that enable users to manipulate HyperShift owned features.
- HyperShift components should not own or manage user infrastructure platform credentials.

---

## Source: docs/content/reference/index.md

---
title: Reference
---

This section of the HyperShift documentation contains references.


---

## Source: docs/content/reference/infrastructure/agent.md

The agent platform does not create any infrastructure but does have two kinds of prerequisites:

1. Agents: An Agent represents a host booted with a discovery image and ready to be provisioned as an OpenShift node. For more information, see here.
1. DNS: The API and ingress endpoints must be routable.

You can find more details about the prerequisites in the how-to.


---

## Source: docs/content/reference/infrastructure/aws.md

In this section we want to dissect who creates what and what not. It contains 4 stages:

- Infra pre-required and unmanaged for hypershift operator in an arbitrary AWS account
- Infra pre-required and unmanaged in Hosted Cluster AWS account
- Infra managed by hypershift in Management AWS account
- Infra managed by hypershift in Hosted Cluster AWS account
- Infra managed by kubernetes in Hosted Cluster AWS account

!!! note
    The arbitrary AWS account depends on who is providing the Hypershift service.

    - **Self Managed:** It will be controlled by the Cluster Service Provider.
    - **SaaS:** In this case the AWS Account will belong to Red Hat.

#### Infra pre-required and unmanaged for hypershift operator in an arbitrary AWS account

=== "Management Cluster AWS Account"
    - 1 S3 Bucket
        - OIDC
    - Route 53 Hosted zones
        - Domain to host Private and Public entries for HostedClusters

#### Infra pre-required and unmanaged in Hosted Cluster AWS account

=== "All access Modes"
    - 1 VPC
    - 1 DHCP Options
    - 2 Subnets
        - Private subnet - internal data plane subnet
        - Public subnet - enable access to the internet from the data plane
    - 1 Internet Gateway
    - 1 Elastic IP
    - 1 NAT Gateway
    - 1 Security Group (Worker Nodes)
    - 2 Route Tables (1 Private, 1 Public)
    - 2 Route 53 Hosted Zones
    - Enough quota for:
        - 1 Ingress Service Load Balancer (for Public Hosted Clusters)
        - 1 Private Link Endpoint (for Private Hosted Clusters)

#### AWS Infra Managed by Hypershift

- **Public**

=== "Management Cluster AWS Account"
    - NLB - Load Balancer Kube API Server
        - Kubernetes creates a Security Group
    - Volumes
        - For ETCD (1 or 3 depending on HA)
        - For ovn-Kube

=== "Hosted Cluster AWS account"
    - For NodePools:
        - EC2 Instances
            - Need the Role and RolePolicy

- **Private**

=== "Management Cluster AWS Account"
    - NLB - Load Balancer Private Router
    - Endpoint Service (Private Link)

=== "Hosted Cluster AWS account"
    - Private Link Endpoints
        - 1 Endpoint per Availability Zone
    - For NodePools:
        - EC2 Instances

- **PublicAndPrivate**

=== "Management Cluster AWS Account"
    - 1 NLB - Load Balancer Public Router
    - 1 NLB - Load Balancer Private Router
    - Enpoint Service (Private Link)
    - Volumes
        - For ETCD (1 or 3 depending on HA)
        - For ovn-Kube

=== "Hosted Cluster AWS account"
    - Private Link Endpoints
        - 1 Endpoint per Availability Zone
    - For NodePools:
        - EC2 Instances

#### AWS Infra Managed by Kubernetes

=== "Hosted Cluster AWS account"
    - Network Load Balancer for default ingress
    - S3 bucket for registry

!!! note

    For the Private Link networking to work, we've observed that the Endpoint zone in the hosted cluster AWS account, must match the zone of the instance resolved by the Service Endpoint in the management cluster AWS account.

    In AWS the Zone names are just alias e.g. "us-east-2b" which do not necessarily map to the same zone in different accounts.

    Because of this for Private link to be guaranteed to work, the management cluster must have subnets/workers in all zones of its region.

## IAM

In this section we want to clarify how the AWS IAM works in Hypershift context.

Hypershift expects to have the ARN roles already created in AWS, so the entity responsible to create them lies in the consumer (CLI/OCM). Hypershift tries to enable granularity to honor principle of least privilege components, which means that every component will use their own role to operate or create AWS objects and the roles are limited to what is required for the normal functioning of the product.

As an example, our CLI can handle the creation of these roles, you can follow this article to make this happen.

The HostedCluster receives the ARN roles as input, then the CLI/OCM creates an AWS permission config for each component. That lets a component to auth via STS and preconfigured oidc idp.

The roles created are consumed by some components from Hypershift, running on Control Plane and operating at Data Plane side:

These are the different roles

- **controlPlaneOperatorARN**
- **imageRegistryARN**
- **ingressARN**
- **kubeCloudControllerARN**
- **nodePoolManagementARN**
- **storageARN**
- **networkARN**

This is a sample of how this looks like the reference to the IAM Roles from the HostedCluster perspective

```yaml
...
endpointAccess: Public
  region: us-east-2
  resourceTags:
  - key: kubernetes.io/cluster/cewong-dev-bz4j5
    value: owned
rolesRef:
    controlPlaneOperatorARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-control-plane-operator
    imageRegistryARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-openshift-image-registry
    ingressARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-openshift-ingress
    kubeCloudControllerARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-cloud-controller
    networkARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-cloud-network-config-controller
    nodePoolManagementARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-node-pool
    storageARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-aws-ebs-csi-driver-controller
type: AWS
...
```

And these are samples for each one of the roles Hypershift uses:


=== "**FullPermissionSet**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "Statement1",
                "Effect": "Allow",
                "Action": [
                    "s3:CreateBucket",
                    "s3:DeleteBucket",
                    "s3:PutBucketTagging",
                    "s3:GetBucketTagging",
                    "s3:PutBucketPublicAccessBlock",
                    "s3:GetBucketPublicAccessBlock",
                    "s3:PutEncryptionConfiguration",
                    "s3:GetEncryptionConfiguration",
                    "s3:PutLifecycleConfiguration",
                    "s3:GetLifecycleConfiguration",
                    "s3:GetBucketLocation",
                    "s3:ListBucket",
                    "s3:GetObject",
                    "s3:PutObject",
                    "s3:DeleteObject",
                    "s3:ListBucketMultipartUploads",
                    "s3:AbortMultipartUpload",
                    "s3:ListMultipartUploadParts",
                    "elasticloadbalancing:DescribeLoadBalancers",
                    "tag:GetResources",
                    "route53:ListHostedZones",
                    "ec2:AttachVolume",
                    "ec2:CreateSnapshot",
                    "ec2:CreateTags",
                    "ec2:CreateVolume",
                    "ec2:DeleteSnapshot",
                    "ec2:DeleteTags",
                    "ec2:DeleteVolume",
                    "ec2:DescribeInstances",
                    "ec2:DescribeSnapshots",
                    "ec2:DescribeTags",
                    "ec2:DescribeVolumes",
                    "ec2:DescribeVolumesModifications",
                    "ec2:DetachVolume",
                    "ec2:ModifyVolume",
                    "ec2:DescribeInstances",
                    "ec2:DescribeInstanceStatus",
                    "ec2:DescribeInstanceTypes",
                    "ec2:UnassignPrivateIpAddresses",
                    "ec2:AssignPrivateIpAddresses",
                    "ec2:UnassignIpv6Addresses",
                    "ec2:AssignIpv6Addresses",
                    "ec2:DescribeSubnets",
                    "ec2:DescribeNetworkInterfaces",
                    "ec2:DescribeInstances",
                    "ec2:DescribeImages",
                    "ec2:DescribeRegions",
                    "ec2:DescribeRouteTables",
                    "ec2:DescribeSecurityGroups",
                    "ec2:DescribeSubnets",
                    "ec2:DescribeVolumes",
                    "ec2:CreateSecurityGroup",
                    "ec2:CreateTags",
                    "ec2:CreateVolume",
                    "ec2:ModifyInstanceAttribute",
                    "ec2:ModifyVolume",
                    "ec2:AttachVolume",
                    "ec2:AuthorizeSecurityGroupIngress",
                    "ec2:CreateRoute",
                    "ec2:DeleteRoute",
                    "ec2:DeleteSecurityGroup",
                    "ec2:DeleteVolume",
                    "ec2:DetachVolume",
                    "ec2:RevokeSecurityGroupIngress",
                    "ec2:DescribeVpcs",
                    "elasticloadbalancing:AddTags",
                    "elasticloadbalancing:AttachLoadBalancerToSubnets",
                    "elasticloadbalancing:ApplySecurityGroupsToLoadBalancer",
                    "elasticloadbalancing:CreateLoadBalancer",
                    "elasticloadbalancing:CreateLoadBalancerPolicy",
                    "elasticloadbalancing:CreateLoadBalancerListeners",
                    "elasticloadbalancing:ConfigureHealthCheck",
                    "elasticloadbalancing:DeleteLoadBalancer",
                    "elasticloadbalancing:DeleteLoadBalancerListeners",
                    "elasticloadbalancing:DescribeLoadBalancers",
                    "elasticloadbalancing:DescribeLoadBalancerAttributes",
                    "elasticloadbalancing:DetachLoadBalancerFromSubnets",
                    "elasticloadbalancing:DeregisterInstancesFromLoadBalancer",
                    "elasticloadbalancing:ModifyLoadBalancerAttributes",
                    "elasticloadbalancing:RegisterInstancesWithLoadBalancer",
                    "elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer",
                    "elasticloadbalancing:AddTags",
                    "elasticloadbalancing:CreateListener",
                    "elasticloadbalancing:CreateTargetGroup",
                    "elasticloadbalancing:DeleteListener",
                    "elasticloadbalancing:DeleteTargetGroup",
                    "elasticloadbalancing:DescribeListeners",
                    "elasticloadbalancing:DescribeLoadBalancerPolicies",
                    "elasticloadbalancing:DescribeTargetGroups",
                    "elasticloadbalancing:DescribeTargetHealth",
                    "elasticloadbalancing:ModifyListener",
                    "elasticloadbalancing:ModifyTargetGroup",
                    "elasticloadbalancing:RegisterTargets",
                    "elasticloadbalancing:SetLoadBalancerPoliciesOfListener",
                    "iam:CreateServiceLinkedRole",
                    "kms:DescribeKey",
                    "ec2:AllocateAddress",
                    "ec2:AssociateRouteTable",
                    "ec2:AttachInternetGateway",
                    "ec2:AuthorizeSecurityGroupIngress",
                    "ec2:CreateInternetGateway",
                    "ec2:CreateNatGateway",
                    "ec2:CreateRoute",
                    "ec2:CreateRouteTable",
                    "ec2:CreateSecurityGroup",
                    "ec2:CreateSubnet",
                    "ec2:CreateTags",
                    "ec2:DeleteInternetGateway",
                    "ec2:DeleteNatGateway",
                    "ec2:DeleteRouteTable",
                    "ec2:DeleteSecurityGroup",
                    "ec2:DeleteSubnet",
                    "ec2:DeleteTags",
                    "ec2:DescribeAccountAttributes",
                    "ec2:DescribeAddresses",
                    "ec2:DescribeAvailabilityZones",
                    "ec2:DescribeImages",
                    "ec2:DescribeInstances",
                    "ec2:DescribeInternetGateways",
                    "ec2:DescribeNatGateways",
                    "ec2:DescribeNetworkInterfaces",
                    "ec2:DescribeNetworkInterfaceAttribute",
                    "ec2:DescribeRouteTables",
                    "ec2:DescribeSecurityGroups",
                    "ec2:DescribeSubnets",
                    "ec2:DescribeVpcs",
                    "ec2:DescribeVpcAttribute",
                    "ec2:DescribeVolumes",
                    "ec2:DetachInternetGateway",
                    "ec2:DisassociateRouteTable",
                    "ec2:DisassociateAddress",
                    "ec2:ModifyInstanceAttribute",
                    "ec2:ModifyNetworkInterfaceAttribute",
                    "ec2:ModifySubnetAttribute",
                    "ec2:ReleaseAddress",
                    "ec2:RevokeSecurityGroupIngress",
                    "ec2:RunInstances",
                    "ec2:TerminateInstances",
                    "tag:GetResources",
                    "ec2:CreateLaunchTemplate",
                    "ec2:CreateLaunchTemplateVersion",
                    "ec2:DescribeLaunchTemplates",
                    "ec2:DescribeLaunchTemplateVersions",
                    "ec2:DeleteLaunchTemplate",
                    "ec2:DeleteLaunchTemplateVersions",
                    "ec2:CreateVpcEndpoint",
                    "ec2:DescribeVpcEndpoints",
                    "ec2:ModifyVpcEndpoint",
                    "ec2:DeleteVpcEndpoints",
                    "ec2:CreateTags",
                    "route53:ListHostedZones"
                    "iam:GetRolePolicy",
                    "iam:PassRole",
                    "iam:AddRoleToInstanceProfile",
                    "iam:CreateInstanceProfile",
                    "iam:GetInstanceProfile",
                    "iam:PutRolePolicy",
                    "iam:CreateRole",
                    "iam:GetRole",
                    "iam:CreateOpenIDConnectProvider",
                    "iam:ListOpenIDConnectProviders",
                    "route53:ChangeResourceRecordSets",
                    "route53:ListResourceRecordSets",
                    "ec2:ReplaceRouteTableAssociation",
                    "ec2:AssociateDhcpOptions",
                    "ec2:CreateDhcpOptions",
                    "ec2:DescribeDhcpOptions",
                    "ec2:ModifyVpcAttribute",
                    "ec2:CreateVpc",
                    "route53:CreateHostedZone"
                ],
                "Resource": [
                    "*"
                ]
            }
        ]
    }
    ```

=== "**IngressARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "elasticloadbalancing:DescribeLoadBalancers",
                    "tag:GetResources",
                    "route53:ListHostedZones"
                ],
                "Resource": "*"
            },
            {
                "Effect": "Allow",
                "Action": [
                    "route53:ChangeResourceRecordSets"
                ],
                "Resource": [
                    "arn:aws:route53:::PUBLIC_ZONE_ID",
                    "arn:aws:route53:::PRIVATE_ZONE_ID"
                ]
            }
        ]
    }
    ```

=== "**ImageRegistryARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "s3:CreateBucket",
                    "s3:DeleteBucket",
                    "s3:PutBucketTagging",
                    "s3:GetBucketTagging",
                    "s3:PutBucketPublicAccessBlock",
                    "s3:GetBucketPublicAccessBlock",
                    "s3:PutEncryptionConfiguration",
                    "s3:GetEncryptionConfiguration",
                    "s3:PutLifecycleConfiguration",
                    "s3:GetLifecycleConfiguration",
                    "s3:GetBucketLocation",
                    "s3:ListBucket",
                    "s3:GetObject",
                    "s3:PutObject",
                    "s3:DeleteObject",
                    "s3:ListBucketMultipartUploads",
                    "s3:AbortMultipartUpload",
                    "s3:ListMultipartUploadParts"
                ],
                "Resource": "*"
            }
        ]
    }
    ```
=== "**StorageARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "ec2:AttachVolume",
                    "ec2:CreateSnapshot",
                    "ec2:CreateTags",
                    "ec2:CreateVolume",
                    "ec2:DeleteSnapshot",
                    "ec2:DeleteTags",
                    "ec2:DeleteVolume",
                    "ec2:DescribeInstances",
                    "ec2:DescribeSnapshots",
                    "ec2:DescribeTags",
                    "ec2:DescribeVolumes",
                    "ec2:DescribeVolumesModifications",
                    "ec2:DetachVolume",
                    "ec2:ModifyVolume"
                ],
                "Resource": "*"
            }
        ]
    }
    ```

=== "**NetworkARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "ec2:DescribeInstances",
                    "ec2:DescribeInstanceStatus",
                    "ec2:DescribeInstanceTypes",
                    "ec2:UnassignPrivateIpAddresses",
                    "ec2:AssignPrivateIpAddresses",
                    "ec2:UnassignIpv6Addresses",
                    "ec2:AssignIpv6Addresses",
                    "ec2:DescribeSubnets",
                    "ec2:DescribeNetworkInterfaces"
                ],
                "Resource": "*"
            }
        ]
    }
    ```

=== "**KubeCloudControllerARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": [
                    "ec2:DescribeInstances",
                    "ec2:DescribeImages",
                    "ec2:DescribeRegions",
                    "ec2:DescribeRouteTables",
                    "ec2:DescribeSecurityGroups",
                    "ec2:DescribeSubnets",
                    "ec2:DescribeVolumes",
                    "ec2:CreateSecurityGroup",
                    "ec2:CreateTags",
                    "ec2:CreateVolume",
                    "ec2:ModifyInstanceAttribute",
                    "ec2:ModifyVolume",
                    "ec2:AttachVolume",
                    "ec2:AuthorizeSecurityGroupIngress",
                    "ec2:CreateRoute",
                    "ec2:DeleteRoute",
                    "ec2:DeleteSecurityGroup",
                    "ec2:DeleteVolume",
                    "ec2:DetachVolume",
                    "ec2:RevokeSecurityGroupIngress",
                    "ec2:DescribeVpcs",
                    "elasticloadbalancing:AddTags",
                    "elasticloadbalancing:AttachLoadBalancerToSubnets",
                    "elasticloadbalancing:ApplySecurityGroupsToLoadBalancer",
                    "elasticloadbalancing:CreateLoadBalancer",
                    "elasticloadbalancing:CreateLoadBalancerPolicy",
                    "elasticloadbalancing:CreateLoadBalancerListeners",
                    "elasticloadbalancing:ConfigureHealthCheck",
                    "elasticloadbalancing:DeleteLoadBalancer",
                    "elasticloadbalancing:DeleteLoadBalancerListeners",
                    "elasticloadbalancing:DescribeLoadBalancers",
                    "elasticloadbalancing:DescribeLoadBalancerAttributes",
                    "elasticloadbalancing:DetachLoadBalancerFromSubnets",
                    "elasticloadbalancing:DeregisterInstancesFromLoadBalancer",
                    "elasticloadbalancing:ModifyLoadBalancerAttributes",
                    "elasticloadbalancing:RegisterInstancesWithLoadBalancer",
                    "elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer",
                    "elasticloadbalancing:AddTags",
                    "elasticloadbalancing:CreateListener",
                    "elasticloadbalancing:CreateTargetGroup",
                    "elasticloadbalancing:DeleteListener",
                    "elasticloadbalancing:DeleteTargetGroup",
                    "elasticloadbalancing:DescribeListeners",
                    "elasticloadbalancing:DescribeLoadBalancerPolicies",
                    "elasticloadbalancing:DescribeTargetGroups",
                    "elasticloadbalancing:DescribeTargetGroupAttributes",
                    "elasticloadbalancing:DescribeTargetHealth",
                    "elasticloadbalancing:ModifyListener",
                    "elasticloadbalancing:ModifyTargetGroup",
                    "elasticloadbalancing:ModifyTargetGroupAttributes",
                    "elasticloadbalancing:RegisterTargets",
                    "elasticloadbalancing:SetLoadBalancerPoliciesOfListener",
                    "iam:CreateServiceLinkedRole",
                    "kms:DescribeKey"
                ],
                "Resource": [
                    "*"
                ],
                "Effect": "Allow"
            }
        ]
    }
    ```

=== "**NodePoolManagementARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": [
                    "ec2:AllocateAddress",
                    "ec2:AssociateRouteTable",
                    "ec2:AttachInternetGateway",
                    "ec2:AuthorizeSecurityGroupIngress",
                    "ec2:CreateInternetGateway",
                    "ec2:CreateNatGateway",
                    "ec2:CreateRoute",
                    "ec2:CreateRouteTable",
                    "ec2:CreateSecurityGroup",
                    "ec2:CreateSubnet",
                    "ec2:CreateTags",
                    "ec2:DeleteInternetGateway",
                    "ec2:DeleteNatGateway",
                    "ec2:DeleteRouteTable",
                    "ec2:DeleteSecurityGroup",
                    "ec2:DeleteSubnet",
                    "ec2:DeleteTags",
                    "ec2:DescribeAccountAttributes",
                    "ec2:DescribeAddresses",
                    "ec2:DescribeAvailabilityZones",
                    "ec2:DescribeImages",
                    "ec2:DescribeInstances",
                    "ec2:DescribeInternetGateways",
                    "ec2:DescribeNatGateways",
                    "ec2:DescribeNetworkInterfaces",
                    "ec2:DescribeNetworkInterfaceAttribute",
                    "ec2:DescribeRouteTables",
                    "ec2:DescribeSecurityGroups",
                    "ec2:DescribeSubnets",
                    "ec2:DescribeVpcs",
                    "ec2:DescribeVpcAttribute",
                    "ec2:DescribeVolumes",
                    "ec2:DetachInternetGateway",
                    "ec2:DisassociateRouteTable",
                    "ec2:DisassociateAddress",
                    "ec2:ModifyInstanceAttribute",
                    "ec2:ModifyNetworkInterfaceAttribute",
                    "ec2:ModifySubnetAttribute",
                    "ec2:ReleaseAddress",
                    "ec2:RevokeSecurityGroupIngress",
                    "ec2:RunInstances",
                    "ec2:TerminateInstances",
                    "tag:GetResources",
                    "ec2:CreateLaunchTemplate",
                    "ec2:CreateLaunchTemplateVersion",
                    "ec2:DescribeLaunchTemplates",
                    "ec2:DescribeLaunchTemplateVersions",
                    "ec2:DeleteLaunchTemplate",
                    "ec2:DeleteLaunchTemplateVersions"
                ],
                "Resource": [
                    "*"
                ],
                "Effect": "Allow"
            },
            {
                "Condition": {
                    "StringLike": {
                        "iam:AWSServiceName": "elasticloadbalancing.amazonaws.com"
                    }
                },
                "Action": [
                    "iam:CreateServiceLinkedRole"
                ],
                "Resource": [
                    "arn:*:iam::*:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing"
                ],
                "Effect": "Allow"
            },
            {
                "Action": [
                    "iam:PassRole"
                ],
                "Resource": [
                    "arn:*:iam::*:role/*-worker-role"
                ],
                "Effect": "Allow"
            }
        ]
    }
    ```

=== "**ControlPlaneOperatorARN**"

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "ec2:CreateVpcEndpoint",
                    "ec2:DescribeVpcEndpoints",
                    "ec2:ModifyVpcEndpoint",
                    "ec2:DeleteVpcEndpoints",
                    "ec2:CreateTags",
                    "route53:ListHostedZones"
                ],
                "Resource": "*"
            },
            {
                "Effect": "Allow",
                "Action": [
                    "route53:ChangeResourceRecordSets",
                    "route53:ListResourceRecordSets"
                ],
                "Resource": "arn:aws:route53:::%s"
            }
        ]
    }
    ```


---

## Source: docs/content/reference/infrastructure/azure-aro-hcp.md

# ARO HCP (Managed Azure) Infrastructure

This section describes the infrastructure requirements for ARO HCP (Azure Red Hat OpenShift Hosted Control Planes) deployments. ARO HCP uses an AKS (Azure Kubernetes Service) management cluster to host OpenShift control planes with Azure worker nodes.

## Overview

ARO HCP deployments consist of:

1. **Management Cluster**: An AKS cluster that hosts the HyperShift operator and control planes
2. **Control Plane**: OpenShift control plane components running as pods on the AKS management cluster
3. **Data Plane**: Worker nodes running as Azure Virtual Machines in the customer's Azure subscription

The architecture uses Azure Managed Identities with certificate-based authentication for secure communication between OpenShift components and Azure services. Credentials are stored in Azure Key Vault and accessed via the Secrets Store CSI driver.

## Infrastructure Stages

### Pre-required Infrastructure (Unmanaged)

The following infrastructure must exist before creating an ARO HCP HostedCluster:

#### Required Azure Permissions

A service principal with the following permissions is required for creating infrastructure:

| Role | Scope | Purpose |
|------|-------|---------|
| `Contributor` | Subscription | Create and manage Azure resources |
| `User Access Administrator` | Subscription | Assign roles to managed identities |

Additionally, Microsoft Graph API permissions are required:

| Permission | Type | Purpose |
|------------|------|---------|
| `Application.ReadWrite.OwnedBy` | Application | Create and manage service principals |

#### Shared Resources (Per Management Cluster)

A resource group for shared resources that are reusable across HostedCluster creation and deletion cycles:

| Resource | Purpose |
|----------|---------|
| Azure Key Vault | Stores service principal certificates |
| Service Principals | Control plane component authentication |
| OIDC Issuer Storage Account | Hosts OIDC discovery documents |
| Data Plane Managed Identities | Data plane component authentication |
| Azure DNS Zones | DNS management |

!!! warning "Resource Reusability"

    These resources should be created once and **reused when recreating the same HostedCluster** to avoid Azure quota limits. They can also be shared across multiple HostedClusters if desired.

#### AKS Management Cluster

An AKS cluster that hosts the HyperShift operator and control planes. For production deployments, the following features are recommended:

| Feature | Configuration |
|---------|---------------|
| Azure Key Vault Secrets Provider | Enabled for credential mounting |
| OIDC Issuer | Enabled for workload identity |
| Managed Identity | Cluster and kubelet managed identities |
| Autoscaling | Node pool autoscaling enabled |
| Node Pools | Appropriately sized for control plane workloads |

### Control Plane Identities

ARO HCP uses service principals with certificate-based authentication for control plane components. These credentials are stored in Azure Key Vault and mounted into pods via the Secrets Store CSI driver.

| Component | Operator/Controller | Key Vault Secret Name |
|-----------|--------------------|-----------------------|
| **Control Plane Operator** | control-plane-operator | `<name>-cpo` |
| **Node Pool Management** | Cluster API Provider Azure | `<name>-nodepool-mgmt` |
| **Cloud Provider** | Azure Cloud Controller Manager | `<name>-cloud-provider` |
| **Azure Disk CSI** | Azure Disk CSI Driver | `<name>-disk` |
| **Azure File CSI** | Azure File CSI Driver | `<name>-file` |
| **Ingress** | Cluster Ingress Operator | `<name>-ingress` |
| **Network** | Cloud Network Config Controller | `<name>-network` |
| **Image Registry** | Cluster Image Registry Operator | `<name>-image-registry` |

Each control plane identity includes:

- **Client ID**: UUID identifying the service principal
- **Certificate**: X.509 certificate for authentication
- **Object Encoding**: Format of the certificate in Key Vault (utf-8, hex, or base64)

The control plane identities are provided in a JSON file with the following format:

```json
{
  "controlPlane": {
    "managedIdentitiesKeyVault": {
      "name": "<key-vault-name>",
      "tenantID": "<tenant-id>"
    },
    "cloudProvider": {
      "clientID": "<uuid>",
      "credentialsSecretName": "<secret-name>",
      "objectEncoding": "utf-8"
    },
    "nodePoolManagement": { ... },
    "controlPlaneOperator": { ... },
    "imageRegistry": { ... },
    "ingress": { ... },
    "network": { ... },
    "disk": { ... },
    "file": { ... }
  },
  "dataPlane": {
    "imageRegistryMSIClientID": "<uuid>",
    "diskMSIClientID": "<uuid>",
    "fileMSIClientID": "<uuid>"
  }
}
```

### Data Plane Identities

Data plane identities are Azure managed identities with federated credentials for components running on worker nodes. Each managed identity is assigned an Azure built-in role that grants the specific permissions required by that component:

| Component | Purpose | Azure Built-in Role |
|-----------|---------|---------------------|
| **Image Registry** | Image registry operations from worker nodes | Azure Red Hat OpenShift Image Registry Operator |
| **Azure Disk CSI** | Disk operations from worker nodes | Azure Red Hat OpenShift Azure Disk CSI Driver Operator |
| **Azure File CSI** | File operations from worker nodes | Azure Red Hat OpenShift Azure Files CSI Driver Operator |

!!! note "Azure Built-in Roles"

    These roles are Azure built-in roles created specifically for Azure Red Hat OpenShift. Each role has a unique role definition ID (GUID) that HyperShift uses when creating role assignments. For detailed permissions granted by each role, see the Azure built-in roles documentation.

The data plane identities are provided in a JSON file with the following format:

```json
{
  "imageRegistryMSIClientID": "<uuid>",
  "diskMSIClientID": "<uuid>",
  "fileMSIClientID": "<uuid>"
}
```

#### OIDC Issuer

An OIDC issuer is required for data plane workload identity federation (see Data Plane Authentication for how this works). Setting up an OIDC issuer requires:

- Azure Blob Storage account configured as OIDC issuer
- RSA key pair for service account token signing
- Created using the Cloud Credential Operator (CCO) tool

### Infrastructure Required Before HostedCluster Creation

The following Azure infrastructure must exist before creating a HostedCluster resource:

!!! tip "Creating Infrastructure with the CLI"

    When using the `hypershift` CLI for cluster creation, this infrastructure gets automatically created whether via the `hypershift create infra azure` or the `hypershift create cluster azure` command. See Create an Azure Hosted Cluster on AKS for details.

#### Resource Groups

| Resource Group | Purpose | Default Naming |
|----------------|---------|----------------|
| Managed RG | Primary cluster infrastructure (VMs, disks, load balancers) | `<name>-<infra-id>` |
| VNet RG | Virtual network resources | `<name>-<infra-id>-vnet` |
| NSG RG | Network security group | `<name>-<infra-id>-nsg` |

#### Networking Resources

| Resource | Configuration | Notes |
|----------|---------------|-------|
| **Virtual Network (VNet)** | Any valid address space | Contains the subnet for worker nodes |
| **Subnet** | Any valid address prefix within the VNet | Attached to NSG |
| **Network Security Group (NSG)** | Empty rules by default | Additional rules added by cloud provider as needed |

!!! note "CLI Defaults"

    The CLI creates a VNet with address prefix `10.0.0.0/16` and a subnet named `default` with address prefix `10.0.0.0/24`. Use `--vnet-id`, `--subnet-id`, and `--network-security-group-id` to bring your own resources.

#### DNS Resources

Internal cluster DNS resolution requires a Private DNS Zone and a Virtual Network Link connecting the worker VNet to that zone.

| Resource | Configuration | Purpose |
|----------|---------------|---------|
| **Private DNS Zone** | `<name>-azurecluster.<baseDomain>` | Hosts internal DNS records for the hosted cluster |
| **Virtual Network Link** | Links to worker VNet | Connects the VNet to the zone for DNS resolution |

!!! note "CLI Defaults"

    The CLI creates both resources with `location: global` and the Virtual Network Link with automatic VM registration disabled.

#### Load Balancer Resources

| Resource | Configuration | Notes |
|----------|---------------|-------|
| **Public IP Address** | SKU: Standard, Static allocation, IPv4 | 4-minute idle timeout |
| **Load Balancer** | SKU: Standard | For guest cluster egress |

The load balancer is configured with:

- **Frontend IP Configuration**: Associated with the public IP
- **Backend Address Pool**: For worker node NICs
- **Health Probe**: HTTP probe on port 30595, path `/healthz`, 5-second interval
- **Outbound Rule**: All protocols, 1024 allocated ports per instance, TCP reset enabled, 4-minute idle timeout

### Infrastructure Managed by Kubernetes

The following resources are created and managed by Kubernetes controllers running in the hosted cluster:

#### Networking

- **Azure Load Balancers**: Created for ingress (default router) and services of type `LoadBalancer`
- **Public IP Addresses**: Allocated for load balancer frontends
- **Security Group Rules**: Added to the NSG for service traffic

#### Storage

- **Azure Disks**: Managed disks for persistent volumes (via Azure Disk CSI driver)
- **Azure File Shares**: File shares for ReadWriteMany persistent volumes (via Azure File CSI driver)
- **Storage Accounts**: Created as needed for Azure File storage

#### Compute

- **Azure Virtual Machines**: Worker nodes managed by Cluster API Provider Azure
- **Network Interfaces**: NICs attached to worker VMs
- **OS Disks**: Managed disks for VM operating systems

## Managed Identity Authentication

ARO HCP uses a two-tier authentication model:

### Control Plane Authentication (Certificate-Based)

Control plane components authenticate using service principal certificates stored in Azure Key Vault:

```mermaid
sequenceDiagram
    participant CSI as Secrets Store<br/>CSI Driver
    participant KV as Azure Key Vault
    participant Pod as Control Plane Pod
    participant Entra as Azure Entra ID
    participant Azure as Azure APIs

    CSI->>KV: 1. Fetch certificate secret
    KV->>CSI: 2. Return certificate
    CSI->>Pod: 3. Mount certificate to volume
    Pod->>Entra: 4. Authenticate with certificate
    Entra->>Pod: 5. Issue access token
    Pod->>Azure: 6. Call APIs with access token
```

The Key Vault integration uses:

- **SecretProviderClass**: Defines which secrets to mount from Key Vault
- **CSI Volume**: Mounts secrets into pod filesystem at `/etc/kubernetes/azure/`
- **Managed Identity**: AKS cluster identity authorized to read from Key Vault

### Data Plane Authentication (Federated Identity)

Data plane components use federated managed identities similar to self-managed Azure:

```mermaid
sequenceDiagram
    participant Pod as Worker Node Pod
    participant K8s as Kubernetes API
    participant OIDC as OIDC Issuer<br/>(Blob Storage)
    participant Entra as Azure Entra ID
    participant Azure as Azure APIs

    Pod->>K8s: 1. Request ServiceAccount token
    K8s->>Pod: 2. Issue signed JWT token
    Pod->>Entra: 3. Present token for exchange
    Entra->>OIDC: 4. Fetch JWKS for validation
    OIDC->>Entra: 5. Return public keys
    Entra->>Pod: 6. Issue Azure access token
    Pod->>Azure: 7. Call APIs with access token
```

## Resource Group Strategy

ARO HCP deployments use multiple resource groups with different lifecycles:

### Persistent Resources (Reused Across Clusters)

```
Persistent Resource Group (e.g., os4-common)
‚îú‚îÄ‚îÄ Azure Key Vault
‚îÇ   ‚îî‚îÄ‚îÄ Service Principal Certificates
‚îú‚îÄ‚îÄ OIDC Issuer Storage Account
‚îú‚îÄ‚îÄ Data Plane Managed Identities
‚îÇ   ‚îú‚îÄ‚îÄ Image Registry Identity
‚îÇ   ‚îú‚îÄ‚îÄ Disk CSI Identity
‚îÇ   ‚îî‚îÄ‚îÄ File CSI Identity
‚îî‚îÄ‚îÄ DNS Zones
```

### AKS Management Cluster Resources

```
AKS Resource Group
‚îú‚îÄ‚îÄ AKS Cluster
‚îú‚îÄ‚îÄ Node Pools
‚îú‚îÄ‚îÄ Cluster Managed Identity
‚îî‚îÄ‚îÄ Kubelet Managed Identity
```

### Cluster-Specific Resources (Created/Deleted Per Cluster)

By default, HyperShift creates separate resource groups for different resource types. However, all resources can be placed in a single managed resource group if preferred.

**Default (Separate Resource Groups):**

```
Managed Resource Group (<name>-<infra-id>)
‚îú‚îÄ‚îÄ Virtual Machines (worker nodes)
‚îú‚îÄ‚îÄ Managed Disks (OS and data disks)
‚îú‚îÄ‚îÄ Network Interfaces
‚îî‚îÄ‚îÄ Load Balancers (ingress, services)

VNet Resource Group (<name>-<infra-id>-vnet)
‚îú‚îÄ‚îÄ Virtual Network
‚îú‚îÄ‚îÄ Subnet
‚îú‚îÄ‚îÄ Private DNS Zone
‚îî‚îÄ‚îÄ Private DNS Zone Link

NSG Resource Group (<name>-<infra-id>-nsg)
‚îî‚îÄ‚îÄ Network Security Group
```

**Single Resource Group (Alternative):**

You can use an existing VNet and NSG from the same resource group, which places all cluster resources in a single resource group.

## Related Documentation

- Create an Azure Hosted Cluster on AKS - Step-by-step setup guide
- Azure Hosted Cluster with Options - Advanced configuration options
- Self-Managed Azure Infrastructure - Alternative deployment model


---

## Source: docs/content/reference/infrastructure/azure-self-managed.md

# Self-Managed Azure Infrastructure

This section describes the infrastructure requirements for self-managed Azure HyperShift deployments. Self-managed Azure allows you to deploy hosted clusters with Azure worker nodes using an OpenShift management cluster that you provision and manage.

!!! note "Developer Preview in OCP 4.21"

    Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.

## Overview

Self-managed Azure HyperShift deployments consist of:

1. **Management Cluster**: An OpenShift cluster (can run on any platform - AWS, Azure, bare metal, etc.) that hosts the HyperShift operator and control planes
2. **Control Plane**: Kubernetes control plane components running as pods on the management cluster
3. **Data Plane**: Worker nodes running as Azure Virtual Machines in your Azure subscription

The architecture uses Azure Workload Identity for secure, credential-free authentication between OpenShift components and Azure services.

## Infrastructure Stages

### Pre-required Infrastructure (Unmanaged)

The following infrastructure must exist before creating a self-managed Azure HostedCluster:

#### Management Cluster Requirements

| Requirement | Description |
|-------------|-------------|
| OpenShift Cluster | Version 4.17+ with HyperShift operator installed. Can run on any platform (AWS, Azure, bare metal, etc.) |
| OpenShift CLI | `oc` or `kubectl` for cluster management |
| HyperShift CLI | `hypershift` binary for cluster creation |
| Pull Secret | Valid OpenShift pull secret |

#### Azure Subscription Requirements

| Requirement | Description |
|-------------|-------------|
| Azure Subscription | Active subscription with sufficient quota |
| Azure CLI | `az` CLI installed and configured |
| `jq` | Command-line JSON processor |
| CCO Tool | Cloud Credential Operator CLI for OIDC setup |

#### Required Azure Permissions

The service principal or user creating infrastructure requires:

| Role | Scope | Purpose |
|------|-------|---------|
| `Contributor` | Subscription | Create and manage Azure resources |
| `User Access Administrator` | Subscription | Assign roles to managed identities |

#### Persistent Resource Group (Optional)

A persistent resource group is not required, but is recommended for shared resources that can be reused across multiple HostedClusters:

| Resource | Purpose | Lifecycle |
|----------|---------|-----------|
| OIDC Issuer Storage Account | Hosts OIDC discovery documents and JWKS | Can be reused |
| Workload Identities | Managed identities with federated credentials | Can be reused |
| Azure DNS Zones | Parent DNS zone for External DNS (optional) | Can be reused |

!!! tip "Resource Reuse"

    While you can create these resources per-cluster, placing workload identities and OIDC issuer in a persistent resource group allows you to reuse them across multiple hosted clusters. This reduces setup time, avoids unnecessary resource recreation, and helps stay within Azure quota limits.

#### OIDC Issuer

An OIDC issuer is required for workload identity federation. This is an Azure Blob Storage account configured to host:

- OIDC discovery document (`.well-known/openid-configuration`)
- JSON Web Key Set (JWKS) for token validation
- RSA key pair for service account token signing

The OIDC issuer is created using the Cloud Credential Operator (CCO) tool:

```bash
ccoctl azure create-oidc-issuer \
  --name=<name> \
  --region=<azure-region> \
  --subscription-id=<subscription-id> \
  --resource-group-name=<persistent-rg> \
  --storage-account-name=<oidc-storage-account> \
  --output-dir=<output-directory>
```

### Workload Identities

Self-managed Azure requires 7 managed identities with federated credentials for OpenShift components:

| Identity | Operator | Service Accounts | Azure Role |
|----------|----------|------------------|------------|
| **Disk** | Azure Disk CSI Driver | `azure-disk-csi-driver-node-sa`, `azure-disk-csi-driver-operator`, `azure-disk-csi-driver-controller-sa` | Azure Disk Role (`5b7237c5-45e1-49d6-bc18-a1f62f400748`) |
| **File** | Azure File CSI Driver | `azure-file-csi-driver-node-sa`, `azure-file-csi-driver-operator`, `azure-file-csi-driver-controller-sa` | Azure File Role (`0d7aedc0-15fd-4a67-a412-efad370c947e`) |
| **Image Registry** | Cluster Image Registry Operator | `registry`, `cluster-image-registry-operator` | Image Registry Role (`8b32b316-c2f5-4ddf-b05b-83dacd2d08b5`) |
| **Ingress** | Cluster Ingress Operator | `ingress-operator` | Ingress Role (`0336e1d3-7a87-462b-b6db-342b63f7802c`) |
| **Cloud Provider** | Azure Cloud Provider | `azure-cloud-provider` | Cloud Provider Role (`a1f96423-95ce-4224-ab27-4e3dc72facd4`) |
| **Node Pool Management** | Cluster API Provider Azure | `capi-provider` | Contributor (`b24988ac-6180-42a0-ab88-20f7382dd24c`) |
| **Network** | Cloud Network Config Controller | `cloud-network-config-controller` | Network Role (`be7a6435-15ae-4171-8f30-4a343eff9e8f`) |

Each identity is configured with:

- **Federated Identity Credentials**: Trust relationship between Azure Entra ID and OpenShift service accounts
- **Audience**: `openshift`
- **Issuer**: Your OIDC issuer URL

#### Role Assignment Scopes

Identities receive role assignments across multiple resource groups depending on their function:

| Identity | Managed RG | VNet RG | NSG RG | DNS Zone RG |
|----------|------------|---------|--------|-------------|
| Cloud Provider | Yes | Yes | Yes | No |
| Ingress | No | Yes | No | Yes |
| Azure File | Yes | Yes | Yes | No |
| Azure Disk | Yes | No | No | No |
| Network (CNCC) | Yes | Yes | No | No |
| Image Registry | Yes | No | No | No |
| Node Pool Management | Yes | Yes | No | No |

### Infrastructure Created by HyperShift

The `hypershift create cluster azure` command (or `hypershift create infra azure`) creates the following resources:

#### Resource Groups

| Resource Group | Purpose | Default Naming |
|----------------|---------|----------------|
| Managed RG | Primary cluster infrastructure (VMs, disks, load balancers) | `<name>-<infra-id>` |
| VNet RG | Virtual network resources | `<name>-<infra-id>-vnet` |
| NSG RG | Network security group | `<name>-<infra-id>-nsg` |

#### Networking Resources

| Resource | Configuration | Notes |
|----------|---------------|-------|
| **Virtual Network (VNet)** | Address prefix: `10.0.0.0/16` | Contains the subnet for worker nodes |
| **Subnet** | Address prefix: `10.0.0.0/24` | Named `default`, attached to NSG |
| **Network Security Group (NSG)** | Empty rules by default | Additional rules added by cloud provider as needed |

#### DNS Resources

| Resource | Configuration | Notes |
|----------|---------------|-------|
| **Private DNS Zone** | `<name>-azurecluster.<baseDomain>` | For internal cluster DNS resolution |
| **Private DNS Zone Link** | Location: `global` | Links private zone to VNet, registration disabled |

#### Load Balancer Resources

| Resource | Configuration | Notes |
|----------|---------------|-------|
| **Public IP Address** | SKU: Standard, Static allocation, IPv4 | 4-minute idle timeout |
| **Load Balancer** | SKU: Standard | For guest cluster egress |

The load balancer is configured with:

- **Frontend IP Configuration**: Associated with the public IP
- **Backend Address Pool**: For worker node NICs
- **Health Probe**: HTTP probe on port 30595, path `/healthz`, 5-second interval
- **Outbound Rule**: All protocols, 1024 allocated ports per instance, TCP reset enabled, 4-minute idle timeout

### Infrastructure Managed by Kubernetes

The following resources are created and managed by Kubernetes controllers running in the hosted cluster:

#### Networking

- **Azure Load Balancers**: Created for ingress (default router) and services of type `LoadBalancer`
- **Public IP Addresses**: Allocated for load balancer frontends
- **Security Group Rules**: Added to the NSG for service traffic

#### Storage

- **Azure Disks**: Managed disks for persistent volumes (via Azure Disk CSI driver)
- **Azure File Shares**: File shares for ReadWriteMany persistent volumes (via Azure File CSI driver)
- **Storage Accounts**: Created as needed for Azure File storage

#### Compute

- **Azure Virtual Machines**: Worker nodes managed by Cluster API Provider Azure
- **Network Interfaces**: NICs attached to worker VMs
- **OS Disks**: Managed disks for VM operating systems

## Workload Identity Authentication

Self-managed Azure uses workload identity federation for secure authentication. This eliminates long-lived credentials and follows Azure's modern authentication best practices.

### Authentication Flow

```mermaid
sequenceDiagram
    participant Pod as OpenShift Pod
    participant K8s as Kubernetes API
    participant OIDC as OIDC Issuer<br/>(Blob Storage)
    participant Entra as Azure Entra ID
    participant Azure as Azure APIs

    Pod->>K8s: 1. Request ServiceAccount token
    K8s->>Pod: 2. Issue signed JWT token
    Pod->>Entra: 3. Present token for exchange
    Entra->>OIDC: 4. Fetch JWKS for validation
    OIDC->>Entra: 5. Return public keys
    Entra->>Entra: 6. Validate token signature & claims
    Entra->>Pod: 7. Issue Azure access token
    Pod->>Azure: 8. Call APIs with access token
```

### Federated Credential Configuration

Each managed identity has one or more federated identity credentials configured:

```json
{
  "issuer": "https://<storage-account>.blob.core.windows.net/<container>",
  "subject": "system:serviceaccount:<namespace>:<service-account>",
  "audiences": ["openshift"]
}
```

## Resource Group Strategy

Self-managed Azure deployments use multiple resource groups with different lifecycles:

### Persistent Resources (Reused Across Clusters)

```
Persistent Resource Group (e.g., os4-common)
‚îú‚îÄ‚îÄ OIDC Issuer Storage Account
‚îú‚îÄ‚îÄ Workload Identities (7 managed identities)
‚îú‚îÄ‚îÄ Federated Identity Credentials
‚îî‚îÄ‚îÄ DNS Zones (optional, for External DNS)
```

### Cluster-Specific Resources (Created/Deleted Per Cluster)

By default, HyperShift creates separate resource groups for different resource types. However, all resources can be placed in a single managed resource group if preferred.

**Default (Separate Resource Groups):**

```
Managed Resource Group (<name>-<infra-id>)
‚îú‚îÄ‚îÄ Virtual Machines (worker nodes)
‚îú‚îÄ‚îÄ Managed Disks (OS and data disks)
‚îú‚îÄ‚îÄ Network Interfaces
‚îî‚îÄ‚îÄ Load Balancers (ingress, services)

VNet Resource Group (<name>-<infra-id>-vnet)
‚îú‚îÄ‚îÄ Virtual Network
‚îú‚îÄ‚îÄ Subnet
‚îú‚îÄ‚îÄ Private DNS Zone
‚îî‚îÄ‚îÄ Private DNS Zone Link

NSG Resource Group (<name>-<infra-id>-nsg)
‚îî‚îÄ‚îÄ Network Security Group
```

**Single Resource Group (Alternative):**

You can use an existing VNet and NSG from the same resource group, which places all cluster resources in a single resource group.

## Related Documentation

- Azure Workload Identity Setup - Set up managed identities and OIDC federation
- Setup Azure Management Cluster for HyperShift - Install HyperShift operator
- Create a Self-Managed Azure HostedCluster - Deploy your first hosted cluster
- Self-Managed Azure Overview - Comprehensive overview


---

## Source: docs/content/reference/infrastructure/index.md

---
title: Infrastructure
---

# Infrastructure

This section describes the infrastructure that is used by each platform. This includes infrastructure that hypershift creates and manages, as well as prerequisites.


---

## Source: docs/content/reference/konnectivity.md

# Konnectivity in HyperShift

## What is Konnectivity?

The Konnectivity proxy, also known as apiserver-network-proxy, is a
component in Kubernetes that facilitates secure network communication
between the Kubernetes API server and cluster nodes. It allows the
Kubernetes API server to initiate communication to worker nodes or
kubernetes services running in the data plane.

Upstream References:

- Setup Konnectivity
- Proposal
- Reference Implementation

## Reference Network Diagram

```mermaid
flowchart LR
    subgraph hosted control plane
        subgraph kube-apiserver pod
            kube-apiserver --> konnectivity-server
        end
        subgraph openshift-apiserver pod
            openshift-apiserver --> konnectivity-https-proxy-oapi[konnectivity-https-proxy]
        end
        subgraph oauth-apiserver pod
            oauth-apiserver --> konnectivity-socks5-proxy-oauth[konnectivity-socks5-proxy]
            oauth-apiserver --> konnectivity-https-proxy-oauth[konnectivity-https-proxy]
        end
        subgraph cluster-network-operator pod
            cluster-network-operator --> konnectivity-socks5-proxy-cno[konnectivity-socks5-proxy]
        end
        subgraph ovnkube-control-plane pod
            ovnkube-control-plane --> konnectivity-socks5-proxy-ovn[konnectivity-socks5-proxy]
        end
        subgraph ingress-operator pod
            ingress-operator --> konnectivity-https-proxy-ingress[konnectivity-https-proxy]
        end
        subgraph packageserver pod
            package-server --> konnectivity-socks5-proxy-pkgsrv[konnectivity-socks5-proxy]
        end
        subgraph olm-operator pod
            olm-operator --> konnectivity-socks5-proxy-olm[konnectivity-socks5-proxy]
        end
        subgraph catalog-operator pod
            catalog-operator --> konnectivity-socks5-proxy-cat[konnectivity-socks5-proxy]
        end
        konnectivity-server -- HCP APIServices --> konnectivity-agent-cp[konnectivity-agent]
        konnectivity-agent-cp --> openshift-apiserver
        konnectivity-agent-cp --> oauth-apiserver
        konnectivity-agent-cp --> package-server
        konnectivity-agent-cp -.->|registration| konnectivity-server-service
        konnectivity-https-proxy-oapi --> konnectivity-server-local-service
        konnectivity-socks5-proxy-oauth --> konnectivity-server-local-service
        konnectivity-https-proxy-oauth --> konnectivity-server-local-service
        konnectivity-socks5-proxy-cno --> konnectivity-server-local-service
        konnectivity-socks5-proxy-ovn --> konnectivity-server-local-service
        konnectivity-https-proxy-ingress --> konnectivity-server-local-service
        konnectivity-socks5-proxy-pkgsrv --> konnectivity-server-local-service
        konnectivity-socks5-proxy-olm --> konnectivity-server-local-service
        konnectivity-socks5-proxy-cat --> konnectivity-server-local-service
        konnectivity-server-local-service --> konnectivity-server
        konnectivity-server-service -..-> konnectivity-server
    end
    subgraph guest node
        konnectivity-server -- Default Route --> konnectivity-agent-node[konnectivity-agent]
        konnectivity-agent-node --> kubelet
        konnectivity-agent-node --> guest-service-network[Guest Service Network]
    end
    konnectivity-agent-node --> guest-machine-network[Guest Machine Network/Proxy]
    konnectivity-agent-node -.->|registration| konnectivity-server-service
```

## Why is it needed?

The Kubernetes API server needs to initiate requests to the kubelet
running in worker nodes in order to:

- Obtain logs of running pods.
- Execute into running pods.
- Port forward ports of pods or services.

In addition, the Kubernetes APIServer needs to access services running
in the data plane when those services are configured as either
aggregated API services (via `APIService` resource) or webhooks (via
`ValidatingWebhookConfiguration` or `MutatingWebhookConfiguration`
resources).

In a hosted control plane architecture, the Kubernetes API server may
not have direct network access to worker nodes. For example, in AWS, the
control plane runs in the provider VPC, while customer worker nodes run
in the customer's VPC. The worker nodes are not exposed externally.
Therefore communication initiated from the Kubernetes API server must
rely on a tunnel established by Konnectivity to enable this
communication.

In addition to the communication initiated by the Kubernetes API server,
in HyperShift hosted clusters, konnectivity is used via a socks or https
proxy (added as a sidecar) by the following components:

- **OpenShift APIServer**
    - Communicates with webhook services for resources served by the OpenShift APIServer
    - Routes ImageStream connection to remote registries through the data plane.

- **Ingress Operator**
    - Uses konnectivity for route health checks (routes in data plane are not necessarily accessible from the control plane)

- **OAuth Server**
    - Enables communication with identity providers that potentially are only available to the data plane network.

- **Cluster Network Operator**
    - Performs proxy readiness requests through the data plane network

- **OVNKube Control Plane**
    - Used to enable OVN interconnect for hosted clusters

- **OLM Operator**
    - Used for GRPC communication with in-cluster catalogs

- **OLM Catalog Operator**
    - Used for GRPC communication with in-cluster catalogs

- **OLM Package Server**
    - Used for GRPC communication with in-cluster catalogs

Konnectivity is **not** needed for traffic initiated from worker nodes
to the Kubernetes API server.

## How does Konnectivity work?

Konnectivity has 2 main components:

- **Konnectivity server** - accepts requests from the Kube APIServer (or
  other proxy clients) and routes them through an agent in the data
  plane. It also accepts connections from agents in the data plane to
  establish a tunnel.

- **Konnectivity agent(s)** - run as pods on the worker nodes and
  establish a connection with the Konnectivity server so that they can
  route traffic initiated by the Kube APIServer to the appropriate
  destination in the data plane.

Konnectivity agents are deployed to the data plane as a Daemonset that
runs in every worker node. When those agents first start up, they
establish a connection to the konnectivity server(s) (this means that
the control plane must be accessible to the worker nodes). The
established connection can then be used by the konnectivity server to
tunnel requests initiated by the Kubernetes API server.

In HyperShift, the Konnectivity server runs as a sidecar (container
`konnectivity-server`) of the Kubernetes API Server. This means that
when the control plane runs in HA mode, there are 3 instances of the
Konnectivity server in the control plane.

Konnectivity agents run as the `konnectivity-agent` Daemonset in the
`kube-system` namespace of the hosted cluster.

The konnectivity server exposes 2 endpoints:

- **server endpoint** - Exposed to clients of the tunnel such
  as the Kubernetes API server, proxies running in OpenShift API
  server, OAuth server, etc.
  In a HyperShift control plane namespace, this is exposed via the
  `konnectivity-server-local` service (port 8090).

- **cluster endpoint** - Exposed to agents that establish a connection
  to the Konnectivity server. In a HyperShift control plane namespace,
  this is exposed via the `konnectivity-server` service (port 8091) and
  externally via Route or NodePort.

## Konnectivity Artifacts in HyperShift

HyperShift requires konnectivity artifacts in both the control plane
namespace as well as the hosted cluster.

### In the Control Plane Namespace

#### Deployments

-   **kube-apiserver** - Runs the konnectivity-server container as a
    sidecar.

-   **konnectivity-agent** - Runs an agent for konnectivity on the control
    plane side (See Control Plane API services for more information)

#### ConfigMaps

-   **kas-egress-selector-config** - Contains the configuration that tells
    the Kubernetes API server how and when to use konnectivity. We only
    configure konnectivity for cluster connections. This means that the
    Kubernetes API server will route any connection to a service or a
    kubelet through the konnectivity server.

-   **konnectivity-ca-bundle** - Contains the self-signed CA bundle used to
    verify connections to the Konnectivity server.

#### Secrets

-   **konnectivity-signer** - Contains the self-signed CA for the
    konnectivity server

-   **konnectivity-server** - Contains the serving certificate for the
    konnectivity server endpoint (where Kube APIServer and socks5/http
    proxies connect)

-   **konnectivity-cluster** - Contains the serving certificate for the
    konnectivity cluster endpoint (where konnectivity agents connect)

-   **konnectivity-client** - Contains certificate and key to be used by
    server clients (Kube APIServer and socks5/http proxies) to
    authenticate with the konnectivity server.

-   **konnectivity-agent** - Contains certificate and key to be used by
    agents to authenticate with the konnectivity server.

#### Services

-   **konnectivity-server** - Exposes the cluster endpoint of the
    konnectivity server. This is the service that agents use to connect
    to the konnectivity server. It must be exposed externally so that
    workers can reach it.

-   **konnectivity-server-local** - The endpoint where clients connect to
    the konnectivity server to send traffic to the data plane. This
    endpoint is only exposed to control plane components and is
    protected via network policy from access by components outside of
    its own control plane namespace.

#### Routes

-   **konnectivity-server** - Exposes the konnectivity-server service
    externally so that agents can connect to it.

### In the Hosted Cluster

#### Daemonsets

-   **konnectivity-agent** (`kube-system` namespace) - Runs a pod in every
    node that establishes a connection that allows clients in the
    control plane namespace to communicate through it.

#### ConfigMaps

-   **konnectivity-ca-bundle** (`kube-system` namespace) - Contains trust
    bundle that agents use to verify connections to the konnectivity
    server.

#### Secrets

-   **konnectivity-agent** (`kube-system` namespace) - Contains certificate
    and key used by agents to authenticate with the konnectivity server.

## Konnectivity Proxies in HyperShift

The HyperShift code base contains 2 proxy commands that make use of the
konnectivity tunnel to send traffic to the data plane. These are usually
run in sidecar containers in the deployments of components that need to
send their traffic through konnectivity.

-   **konnectivity-socks5-proxy** - Starts a socks5 proxy that allows TCP
    connections to endpoints accessible from the data plane. Cannot
    route traffic through customer-supplied HTTP/S proxy. Use cases for
    this are:

    -   Traffic only destined for the data plane without the need to
        traverse a customer-supplied proxy.

    -   Traffic that is not HTTP/S. For example GRPC traffic to OLM
        catalogs, or LDAP traffic to LDAP identity providers.

-   **konnectivity-https-proxy** - Starts a HTTP proxy that can send HTTP/S
    traffic to endpoints accessible from the data plane. Can make use of
    customer-supplied HTTP/S proxy to route traffic through that. Use
    cases for this are:

    -   Traffic that can be sent through the data plane but must reach
        external endpoints and must honor customer-supplied HTTP/S proxy
        configuration. For example, openshift-apiserver traffic to
        external registries to import ImageStreams or OAuth server
        traffic to non-LDAP identity providers that rely on HTTP/S
        protocol.

To summarize, there are 2 main use cases for these proxies:

-   Routing of traffic from the control plane with destination inside
    the data plane: This is the case for services such as webhooks and
    aggregated API servers that run in the data plane, as well as OLM
    catalogs, or network checks. For this use case, customer-configured
    HTTP/S proxies are not relevant, because traffic is only needed to
    reach inside the data plane. The socks5 proxy is adequate for this.

-   Routing of traffic from the control plane to destinations outside
    the data plane but potentially only accessible from the data plane.
    This is the case for ImageStream registries, and external identity
    providers. For this use case, customer-configured HTTP/S proxy
    configuration should be taken into consideration because traffic may
    only be able to leave the data plane through those proxies. For this
    use case, the HTTP proxy is needed.

## Konnectivity and Control Plane Aggregated API Servers

For OpenShift clusters, servers such as openshift-apiserver,
openshift-oauth-apiserver, and OLM packageserver are aggregated API
servers that handle essential OpenShift resources such as ImageStreams,
Users, and Groups.

Because we consider these servers part of the control plane, they run in
the control plane namespace along with the Kubernetes API server. They
are configured via APIService resources which tell the Kubernetes API
server how to reach them to serve up their resources. APIServices have
the limitation of only allowing a Kubernetes Service to be configured as
the endpoint of the APIService. This means that a Service resource must
exist in the data plane for every APIService we want to configure for
the HostedCluster. This Service in the data plane needs to send traffic
to the components in the control plane.

The way this is accomplished is that a Service without pod selector is
created in the default namespace. Then a corresponding Endpoints
resource is created in the same namespace that contains the IP of the
service in the control plane namespace. Whenever the Kubernetes API
Server wants to reach the aggregated API server, it ends up sending
traffic to the service in the control plane.

One problem with the way Konnectivity is configured is that when you
configure the egress selector for the cluster to be proxied through
Konnectivity, then all the traffic destined for services is sent through
Konnectivity. For these services that live in the control plane
namespace, the Konnectivity server needs to know to send traffic to the
control plane and not the data plane.

The way that the Konnectivity server can send traffic to the control
plane and not the data plane, is that we run a Konnectivity agent pod in
the control plane. This agent is configured with the --agent-identifiers
flag to only route traffic destined for the control plane aggregated
API services. This means that only traffic that is destined for those
services will be routed to the control plane through the Konnectivity agent.

## Troubleshooting Konnectivity in HyperShift clusters

### Checking whether Konnectivity is functional

To determine whether Konnectivity is functional for a particular hosted
cluster an easy test is to get the logs of a pod running in the hosted
cluster. If that works, then traffic is getting routed from the
Kubernetes API server to the kubelet running in the nodes.

### Testing the Konnectivity server with curl

It is possible to use curl to send traffic through the konnectivity
server to a node. The konnectivity server acts like a regular proxy,
however it requires authentication via client certificate/key. Curl
allows specifying a proxy CA, certificate and key (`--proxy-cacert`,
`--proxy-cert`, `--proxy-key`) in addition to the certs used for the final
endpoint. The following script shows how the appropriate certificates
can be extracted and used to make a request to the kubelet running in a
worker node. The konnectivity server is made available locally via the
port-forward command. NOTE: The konnectivity server does not resolve
names, so it can only be tested with IP addresses.

```sh
#!/bin/bash

set -euo pipefail

workdir="$(mktemp -d)"
cp_namespace="CONTROL_PLANE_NAMESPACE_HERE"

echo "work directory is: ${workdir}"

# Get the cert/CA required to use the konnectivity server as a proxy
oc get secret konnectivity-client -n ${cp_namespace} -o jsonpath='{ .data.tls\.key }' | base64 -d > "${workdir}/client.key"
oc get secret konnectivity-client -n ${cp_namespace} -o jsonpath='{ .data.tls\.crt }' | base64 -d > "${workdir}/client.crt"
oc get cm konnectivity-ca-bundle -n ${cp_namespace} -o jsonpath='{ .data.ca\.crt }' > "${workdir}/konnectivity_ca.crt"

# Get the cert/CA required to access the kubelet endpoint
oc get cm kubelet-client-ca -n ${cp_namespace} -o jsonpath='{ .data.ca\.crt }' > ${workdir}/kubelet_ca.crt
oc get secret kas-kubelet-client-crt -n ${cp_namespace} -o jsonpath='{ .data.tls\.crt }' | base64 -d > ${workdir}/kubelet_client.crt
oc get secret kas-kubelet-client-crt -n ${cp_namespace} -o jsonpath='{ .data.tls\.key }' | base64 -d > ${workdir}/kubelet_client.key

# Obtain a node IP from local machines
nodeip="$(oc get machines -n ${cp_namespace} -o json | jq -r '.items[0].status.addresses[] | select(.type=="InternalIP") | .address')"

# Forward the konnectivity server endpdoint to the local machine
oc port-forward -n ${cp_namespace} svc/konnectivity-server-local 8090:8090 &

# Allow some time for the port-forwarding to start
sleep 2

# Perform the curl command with the localhost konnectivity endpoint
curl -x "https://127.0.0.1:8090" \
  --proxy-cacert ${workdir}/konnectivity_ca.crt \
  --proxy-cert ${workdir}/client.crt \
  --proxy-key ${workdir}/client.key \
  --cacert ${workdir}/kubelet_ca.crt \
  --cert ${workdir}/kubelet_client.crt \
  --key ${workdir}/kubelet_client.key \
  "https://${nodeip}:10250/metrics"

# Kill the port-forward job
kill %1
```

## Konnectivity Q&A

**Upstream allows grpc or http-connect mode for the konnectivity protocol, why does HyperShift use http-connect?**
We need to use http-connect to allow clients other than the Kubernetes
API server to send traffic through the konnectivity server. The socks5
and http proxies can use http-connect to establish a connection through
konnectivity-server.

**Upstream allows connecting to the konnectivity server via unix domain socket or via tcp port. Why does HyperShift use tcp port?**
The konnectivity server only allows one way of serving (via UDS or TCP
port). We need to use a TCP port so we can expose it as a service to
clients that use the socks5 and http proxies.

**Why does Konnectivity server run as a sidecar of the Kubernetes API server and not in its own deployment?**
When running as a sidecar, the Kubernetes API server can communicate
with the Konnectivity server very fast, since traffic happens over the
loopback interface. It also allows the Kubernetes API server to
communicate with a konnectivity-server as soon as it comes up.

**How is the Konnectivity server secured in HyperShift?**
We use mTLS to authenticate connections to Konnectivity either from
clients like Kubernetes API server or from agents. The certificates used
for authentication are in the konnectivity-client and konnectivity-agent
secrets in the control plane.

**How does Konnectivity HA work?**
When a HyperShift control plane runs in HA mode, there are by default 3
replicas of the konnectivity server (1 sidecar per kube-apiserver pod).
Each server generates an ID when starting up and the number of total
servers is passed as a command line argument. When an agent dials the
konnectivity server endpoint, the server's response includes its own ID
and the total number of servers that the agent can expect. If greater
than one, the agent keeps dialing the konnectivity server endpoint until
it can establish connections with the same number of unique servers
(identified by their unique id).

**What breaks if Konnectivity is not working?**
The first thing that breaks when Konnectivity is not working is any
operation that requires that the Kubernetes API server access kubelets
on nodes. This includes getting logs from pods, proxying to
pods/services, port-forwarding and remote execution. The next thing that
breaks is any aggregated API servers and webhooks that use data plane
services.

---

## Source: docs/content/reference/manifests/ibmcloud/4.10.md

**HostedCluster**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  annotations:
    hypershift.openshift.io/capi-manager-image: us.icr.io/armada-master/hypershift-cluster-api@sha256:dd610ea557e88156df3c04b293698defa3a8378f922f9abb2b098bc7495564f2
    hypershift.openshift.io/control-plane-operator-image: us.icr.io/armada-master/armada-hypershift-operator@sha256:70c345da5132c5a1cb82f759a1465c5448795e3a9c570b369714d6ecac263aa7
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.10.63-x86_64
    hypershift.openshift.io/konnectivity-agent-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1
    hypershift.openshift.io/konnectivity-server-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  labels:
    clusterid: cismlo21050nmreb5nhg
  name: cismlo21050nmreb5nhg
  namespace: master
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: Default
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - ServiceLBNodePortControl
        - CSIMigrationAWS
        - CSIMigrationOpenStack
        - CSIMigrationGCE
        - CSIMigrationAzureDisk
        - CSIMigrationAzureFile
        - CSIMigrationvSphere
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
        - DownwardAPIHugePages
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: cismlo21050nmreb5nhg-pull-secret
  release:
    image: us.icr.io/armada-master/ocp-release:4.10.63-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```

**HostedControlPlane**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedControlPlane
metadata:
  annotations:
    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/konnectivity-agent-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1
    hypershift.openshift.io/konnectivity-server-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  name: cismlo21050nmreb5nhg
  namespace: master-cismlo21050nmreb5nhg
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: Default
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - ServiceLBNodePortControl
        - CSIMigrationAWS
        - CSIMigrationOpenStack
        - CSIMigrationGCE
        - CSIMigrationAzureDisk
        - CSIMigrationAzureFile
        - CSIMigrationvSphere
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
        - DownwardAPIHugePages
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          ca:
            name: ""
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: pull-secret
  releaseImage: us.icr.io/armada-master/ocp-release:4.10.63-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```


---

## Source: docs/content/reference/manifests/ibmcloud/4.11.md

**HostedCluster**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  annotations:
    hypershift.openshift.io/capi-manager-image: us.icr.io/armada-master/hypershift-cluster-api@sha256:dd610ea557e88156df3c04b293698defa3a8378f922f9abb2b098bc7495564f2
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.11.45-x86_64
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  labels:
    clusterid: cismlo21050nmreb5nhg
  name: cismlo21050nmreb5nhg
  namespace: master
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: Default
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - CSIMigrationAWS
        - CSIMigrationGCE
        - CSIMigrationAzureFile
        - CSIMigrationvSphere
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
        - DownwardAPIHugePages
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: cismlo21050nmreb5nhg-pull-secret
  release:
    image: us.icr.io/armada-master/ocp-release:4.11.45-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```

**HostedControlPlane**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedControlPlane
metadata:
  annotations:
    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  name: cismlo21050nmreb5nhg
  namespace: master-cismlo21050nmreb5nhg
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: Default
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - CSIMigrationAWS
        - CSIMigrationGCE
        - CSIMigrationAzureFile
        - CSIMigrationvSphere
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
        - DownwardAPIHugePages
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          ca:
            name: ""
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: pull-secret
  releaseImage: us.icr.io/armada-master/ocp-release:4.11.45-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```


---

## Source: docs/content/reference/manifests/ibmcloud/4.12.md

**HostedCluster**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  annotations:
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.12.25-x86_64
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  labels:
    clusterid: cismlo21050nmreb5nhg
  name: cismlo21050nmreb5nhg
  namespace: master
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: None
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - CSIMigrationAzureFile
        - CSIMigrationvSphere
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: cismlo21050nmreb5nhg-pull-secret
  release:
    image: us.icr.io/armada-master/ocp-release:4.12.25-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```

**HostedControlPlane**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedControlPlane
metadata:
  annotations:
    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  name: cismlo21050nmreb5nhg
  namespace: master-cismlo21050nmreb5nhg
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: None
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - CSIMigrationAzureFile
        - CSIMigrationvSphere
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          ca:
            name: ""
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: pull-secret
  releaseImage: us.icr.io/armada-master/ocp-release:4.12.25-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```


---

## Source: docs/content/reference/manifests/ibmcloud/4.13.md

**HostedCluster**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  annotations:
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.13.0-x86_64
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:32750
  finalizers:
  - hypershift.openshift.io/finalizer
  labels:
    clusterid: civ8kn910u3bkslbt6lg
  name: civ8kn910u3bkslbt6lg
  namespace: master
spec:
  autoscaling: {}
  clusterID: e54fe2bd-cc45-46e1-8272-f1053ffd4b71
  configuration:
    apiServer:
      audit:
        profile: None
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - RetroactiveDefaultStorageClass
        enabled:
        - RotateKubeletServerCertificate
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-civ8kn910u3bkslbt6lg-client:2379
      tls:
        clientSecret:
          name: civ8kn910u3bkslbt6lg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: civ8kn910u3bkslbt6lg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: civ8kn910u3bkslbt6lg-pull-secret
  release:
    image: us.icr.io/armada-master/ocp-release:4.13.0-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 32750
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32407
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32287
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30284
      type: NodePort
  sshKey: {}
```

**HostedControlPlane**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedControlPlane
metadata:
  annotations:
    hypershift.openshift.io/cluster: master/civ8kn910u3bkslbt6lg
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:32750
  finalizers:
  - hypershift.openshift.io/finalizer
  name: civ8kn910u3bkslbt6lg
  namespace: master-civ8kn910u3bkslbt6lg
spec:
  autoscaling: {}
  clusterID: e54fe2bd-cc45-46e1-8272-f1053ffd4b71
  configuration:
    apiServer:
      audit:
        profile: None
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - RetroactiveDefaultStorageClass
        enabled:
        - RotateKubeletServerCertificate
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          ca:
            name: ""
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-civ8kn910u3bkslbt6lg-client:2379
      tls:
        clientSecret:
          name: civ8kn910u3bkslbt6lg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: civ8kn910u3bkslbt6lg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: pull-secret
  releaseImage: us.icr.io/armada-master/ocp-release:4.13.0-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 32750
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32407
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32287
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30284
      type: NodePort
  sshKey: {}
```


---

## Source: docs/content/reference/manifests/ibmcloud/4.9.md

**HostedCluster**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  annotations:
    hypershift.openshift.io/capi-manager-image: registry.ng.bluemix.net/armada-master/hypershift-cluster-api@sha256:7bde073326d99ce2008e2260671b8962cca44319438706ccce9e8e61fdd26a93
    hypershift.openshift.io/control-plane-operator-image: registry.ng.bluemix.net/armada-master/armada-hypershift-operator@sha256:16982362b89be355f8db1e0bf59b9917f3b529e110364ef34e90f84b9e60ea10
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/force-upgrade-to: registry.ng.bluemix.net/armada-master/ocp-release:4.9.53-x86_64
    hypershift.openshift.io/konnectivity-agent-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30
    hypershift.openshift.io/konnectivity-server-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30
    hypershift.openshift.io/machine-approver-image: registry.ng.bluemix.net/armada-master/hypershift-machine-approver@sha256:19323007d6e2d2de9c3140bfc041d9d11cc3c17bcc3215eaae253dd94a06e5b7
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  labels:
    clusterid: cismlo21050nmreb5nhg
  name: cismlo21050nmreb5nhg
  namespace: master
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: Default
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - ServiceLBNodePortControl
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
        - DownwardAPIHugePages
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          ca:
            name: ""
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: cismlo21050nmreb5nhg-pull-secret
  release:
    image: registry.ng.bluemix.net/armada-master/ocp-release:4.9.53-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```

**HostedControlPlane**

```yaml
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedControlPlane
metadata:
  annotations:
    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg
    hypershift.openshift.io/disable-pki-reconciliation: "true"
    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager
    hypershift.openshift.io/konnectivity-agent-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30
    hypershift.openshift.io/konnectivity-server-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30
    idpoverrides.hypershift.openshift.io/IAM: |
      {"urls": {"authorize": "https://iam.test.cloud.ibm.com/identity/authorize", "userInfo": "https://iam.test.cloud.ibm.com/identity/userinfo", "token": "https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token"}, "claims": {"id": ["iam_id"], "email": ["email"], "name": ["name"], "preferredUsername": ["preferred_username"]}, "challenge": true}
    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446
  finalizers:
  - hypershift.openshift.io/finalizer
  name: cismlo21050nmreb5nhg
  namespace: master-cismlo21050nmreb5nhg
spec:
  autoscaling: {}
  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16
  configuration:
    apiServer:
      audit:
        profile: Default
      clientCA:
        name: ""
      encryption: {}
      servingCerts:
        namedCertificates:
        - names:
          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
          servingCertificate:
            name: ibm-named-certs
      tlsSecurityProfile:
        custom:
          ciphers:
          - ECDHE-ECDSA-AES128-GCM-SHA256
          - ECDHE-RSA-AES128-GCM-SHA256
          - ECDHE-ECDSA-AES256-GCM-SHA384
          - ECDHE-RSA-AES256-GCM-SHA384
          - ECDHE-ECDSA-CHACHA20-POLY1305
          - ECDHE-RSA-CHACHA20-POLY1305
          minTLSVersion: VersionTLS12
        type: Custom
    featureGate:
      customNoUpgrade:
        disabled:
        - ServiceLBNodePortControl
        enabled:
        - ExpandInUsePersistentVolumes
        - RotateKubeletServerCertificate
        - DownwardAPIHugePages
      featureSet: CustomNoUpgrade
    ingress:
      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
      loadBalancer:
        platform:
          type: ""
    oauth:
      identityProviders:
      - mappingMethod: lookup
        name: IAM
        openID:
          ca:
            name: ""
          claims:
            email:
            - email
            name:
            - name
            preferredUsername:
            - preferred_username
          clientID: CLIENTID
          clientSecret:
            name: hypershift-ibm-iam-clientsecret
          issuer: https://iam.test.cloud.ibm.com/identity
        type: OpenID
      templates:
        error:
          name: ""
        login:
          name: ""
        providerSelection:
          name: ""
      tokenConfig: {}
  controllerAvailabilityPolicy: HighlyAvailable
  dns:
    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud
  etcd:
    managementType: Unmanaged
    unmanaged:
      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379
      tls:
        clientSecret:
          name: cismlo21050nmreb5nhg-etcd-client-tls
  fips: false
  imageContentSources:
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - us.icr.io/armada-master/ocp-release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  infraID: cismlo21050nmreb5nhg
  infrastructureAvailabilityPolicy: HighlyAvailable
  issuerURL: https://kubernetes.default.svc
  networking:
    apiServer:
      advertiseAddress: 172.20.0.1
      port: 2040
    clusterNetwork:
    - cidr: 172.30.0.0/16
    machineNetwork:
    - cidr: 172.30.0.0/16
    networkType: Calico
    serviceNetwork:
    - cidr: 172.21.0.0/16
  olmCatalogPlacement: guest
  platform:
    ibmcloud:
      providerType: UPI
    type: IBMCloud
  pullSecret:
    name: pull-secret
  releaseImage: registry.ng.bluemix.net/armada-master/ocp-release:4.9.53-x86_64
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud
        port: 31446
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 32167
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 31938
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud
        port: 30231
      type: NodePort
  sshKey: {}
```


---

## Source: docs/content/reference/manifests/index.md

---
title: Manifests
---

# Manifests

This section describes the manifests that are used for each platform.


---

## Source: docs/content/reference/multi-platform-support.md

# Multi-platform support

A platform is an infrastructure environment where different HyperShift components can run enabling them to make a series of assumptions, e.g. AWS, Azure, Kubevirt, Agent, None. 

HyperShift provides semantics and support for platforms at different levels: HostedCluster, NodePools and management cluster.
This document outlines the support matrix that involved these three entities.

## Support level

- A HostedCluster and its NodePools must all have the same platform.
- A single management cluster might have multiple HostedClusters with different platforms each of them.
- Some HostedCluster features might dictate coupling with the management cluster, e.g. an AWS private HostedCluster requires an AWS management cluster.
- For cloud provider platforms e.g. AWS, Azure, etc. HostedClusters are only tested with the same management cluster platform or a provider-agnostic platform e.g. Kubevirt, Agent, None. Mixed cloud providers e.g AWS management cluster and Azure HostedCluster is a best effort support level.
- Non OCP management is a best effort support level. The HyperShift Operator will try to auto-discover the management clusters features it has available.


---

## Source: docs/content/reference/ocp-behaviour-deviations/index.md

---
title: OCP Standalone behaviour deviations
---

In this section, we will outline the behavioral differences between Hosted Control Planes/Hypershift and standalone OpenShift.


---

## Source: docs/content/reference/ocp-behaviour-deviations/upgrade-4.17-4.18.md

# Upgrade Hypershift NodePool from 4.17 to 4.18

This section outlines the behavior and key differences between a Hosted Control Plane and a standalone OCP environment during an upgrade from version 4.17 to 4.18.

## Runtime Migration from `runc` to `crun`

In OpenShift 4.18, the MachineConfig Operator sets `crun` as the default runtime for worker nodes. This change also affects Hosted Control Planes, requiring us to assess and determine the best course of action, considering that OpenShift has some key differences in behavior.

### Expected Behavior for OCP Standalone Upgrade to 4.18

The OpenShift team has established rules for this runtime migration, affecting both control planes and worker nodes:

- A new OCP cluster deployed with the 4.18 release will use `crun` as the default runtime.
- New nodes created in 4.18 will use `crun` by default.
- Existing nodes from 4.17 **will not automatically migrate** from `runc` to `crun` in 4.18 unless additional actions are taken by the customer.

### Expected Behavior for Hosted Control Planes Upgrade to 4.18

The Hypershift team has chosen a different approach from standalone OCP due to certain design decisions:

- A new Hosted Cluster from the 4.18 release will use `crun` as the default runtime.
- New NodePools in 4.18 will also use `crun` as the default runtime.
- Existing NodePools from 4.17 **will migrate** from `runc` to `crun` once they are upgraded to 4.18.

### Detailed Overview

#### Why

##### Context

In Hosted Control Planes, there are two different upgrade strategies: **Replace** and **InPlace**. The documentation can be found here, and these strategies operate differently at the code level.

The **Replace** strategy creates new instances with the new version while removing old nodes in a rolling fashion. Internally, the NodePool controller will receive the new payload to roll out new nodes, which will eventually replace the current nodes once they join the NodePool. These new nodes will use `crun` as the default runtime, as the payload comes directly from the new version.

The **InPlace** strategy performs updates to the operating system of existing instances. Internally, the Hosted Cluster Config Operator (HCCO) will run a pod to generate new ignition payloads, which will then be applied to reignite the current nodes. The worker nodes (usually one at a time, depending on the `maxUnavailable` and `maxSurge` settings) will be cordoned, drained, and rebooted. After reboot, these nodes will apply the new payload, load the new MCS templates, and use `crun` as the default container runtime.

##### Reasons

The rationale behind this design is as follows:

- Supporting retention of the `runc` runtime would introduce significant engineering complexity at an abstraction layer that would need to be maintained indefinitely.
- This would make it difficult to reproduce scenarios or bugs, as the state of long-running clusters would diverge significantly from newly created clusters, which presents a product management challenge.
- Extensive testing upstream and downstream within CRI-O and OpenShift has not indicated any significant risk associated with this change.


### Safest way to upgrade your HostedCluster

HyperShift allows customers to just leave their NodePools on 4.17, while creating a new NodePool on 4.18 to validate the workloads perform correctly with `crun` runtime. For that, the steps are clear:

- Upgrade your HostedCluster Control Plane to 4.18.Z. NodePools will still run 4.17.Z and runc. Workloads are not affected.
- Bring up a new small nodepool in 4.18, which will run `crun` as default runtime. Scale up the workload to spread the pods to the new nodepool.
- In the best case, all is fine and the nodepool with version 4.17.Z could be upgraded.
- In the worst case, you can report the issue as a bug to the team for the further investigation.


---

## Source: docs/content/reference/test-information-debugging/Azure/test-artifacts-directory-structure.md

# ARO HCP E2E Test Artifacts Navigation Guide

This document serves as a roadmap for understanding and navigating ARO HCP (Azure Red Hat OpenShift Hosted Control Planes) end-to-end test artifacts. Use this guide to quickly locate specific components, logs, and debugging information.

## Quick Navigation Index

### üîç Finding Hosted Control Plane Components
- **Control Plane Pod Deployments**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/apps/deployments/`
- **Control Plane Pod Logs**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/core/pods/logs/`
- **Control Plane Pod Manifests**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/core/pods/`

### üéõÔ∏è Finding HyperShift Operator Components
- **HyperShift Operator Deployment**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/apps/deployments/operator.yaml`
- **External DNS Deployment**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/apps/deployments/external-dns.yaml`
- **Operator Logs**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/core/pods/logs/operator-*-operator.log`
- **External DNS Logs**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/core/pods/logs/external-dns-*-external-dns.log`

### üéØ Key Control Plane Components to Look For

#### Core Kubernetes Control Plane (Basic cluster functionality)
These components form the foundation of any Kubernetes cluster. If these fail, the entire cluster is non-functional:

- **etcd**: `etcd-0.yaml` and `etcd-0-*.log` files
    - *Why*: Stores all cluster state and configuration. Failures here cause complete cluster outages
    - *Look for*: Connection issues, disk space, quorum problems, backup/restore errors

- **kube-apiserver**: `kube-apiserver-*.yaml` and `kube-apiserver-*-*.log` files  
    - *Why*: The central API endpoint for all cluster operations. Nothing works without it
    - *Look for*: TLS certificate issues, etcd connectivity, authentication/authorization failures

- **kube-controller-manager**: `kube-controller-manager-*.yaml` and `kube-controller-manager-*-*.log` files
     - *Why*: Runs core controllers (ReplicaSets, Deployments, etc.). Controls workload lifecycle
    - *Look for*: Resource reconciliation failures, cloud provider integration issues

- **kube-scheduler**: `kube-scheduler-*.yaml` and `kube-scheduler-*-*.log` files
    - *Why*: Places pods on nodes. Scheduling failures prevent workloads from running
    - *Look for*: Resource constraints, node affinity issues, scheduling policy problems

#### OpenShift-Specific Components (OpenShift functionality)
These add OpenShift features on top of Kubernetes. Required for OpenShift-specific capabilities:

- **OpenShift API Server**: `openshift-apiserver-*.yaml` and `openshift-apiserver-*-*.log` files
    - *Why*: Handles OpenShift-specific APIs (Routes, BuildConfigs, etc.)
    - *Look for*: Custom resource failures, OpenShift feature unavailability

- **OAuth Server**: `oauth-openshift-*.yaml` and `oauth-openshift-*-*.log` files
    - *Why*: Manages authentication and user access to the cluster
    - *Look for*: Login failures, identity provider issues, token problems

#### Node Management & Infrastructure (Node lifecycle and joining)
These components manage the infrastructure and node lifecycle. Critical for scaling and node operations:

- **Control Plane Operator**: `control-plane-operator-*.yaml` and `control-plane-operator-*-*.log` files
    - *Why*: HyperShift-specific component that manages the hosted control plane lifecycle
    - *Look for*: Control plane component deployment issues, version management problems

- **Cluster API Manager**: `cluster-api-*.yaml` and `cluster-api-*-*.log` files
    - *Why*: Core CAPI controller that manages infrastructure resources and machine lifecycle
    - *Look for*: Machine creation/deletion failures, cluster infrastructure issues

- **CAPI Provider**: `capi-provider-*.yaml` and `capi-provider-*-*.log` files
    - *Why*: Azure-specific implementation that provisions VMs, networking, and storage
     - *Look for*: Azure API failures, resource quota issues, networking problems

- **Ignition Server**: `ignition-server-*.yaml` and `ignition-server-*-*.log` files
    - *Why*: Provides initial configuration to new nodes during bootstrap process
    - *Look for*: Node configuration failures, network connectivity from nodes to control plane

- **Machine Approver**: `machine-approver-*.yaml` and `machine-approver-*-*.log` files
    - *Why*: Automatically approves Certificate Signing Requests from nodes joining the cluster
    - *Look for*: CSR approval failures, certificate issues preventing node join

### üèÅ Start Here: Primary Resources for Debugging
**Always check these HyperShift custom resources first** 

- their status sections provide high-level cluster state:
    - **HostedCluster**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/hypershift.openshift.io/hostedclusters/*.yaml`
    - **HostedControlPlane**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*-{test-name}-*/hypershift.openshift.io/hostedcontrolplanes/*.yaml`
    - **NodePool**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/hypershift.openshift.io/nodepools/*.yaml`

üí° **Why check these first**: The `status` sections contain:

- Overall cluster readiness and conditions
- Infrastructure provisioning status  
- Control plane component health
- Node pool scaling and readiness
- Error messages and failure reasons

### üìÇ Top-Level Test Execution Files
- `build-log.txt` - Main CI build execution log
- `clone-log.txt` - Git repository cloning logs  
- `finished.json` - Test completion status with success/failure details
- `prowjob.json` - Complete Prow job specification (PR info, test configuration)
- `podinfo.json` - CI pod execution information

## üìã Understanding Test Results

Each test directory (i.e. `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*`) represents a different validation scenario. 

Check the `junit.xml` files for test pass/fail status (i.e. `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/junit.xml`), and use the corresponding namespace directories to drill down into specific component failures.

The artifact structure provides comprehensive debugging capabilities from high-level test orchestration down to individual container logs within the hosted control plane components.

### üß™ Available Test Scenarios
- `TestAutoscaling/` - Cluster autoscaling validation
- `TestCreateCluster/` - Basic hosted cluster creation  
- `TestCreateClusterCustomConfig/` - Custom configuration testing
- `TestHAEtcdChaos/` - etcd high availability and chaos testing
- `TestNodePool_HostedCluster*/` - Node pool lifecycle management
- `TestUpgradeControlPlane/` - Control plane upgrade procedures

### üìà Test Results & Metrics
- `e2e-metrics-raw.prometheus` - Raw Prometheus metrics from test execution
- `junit.xml` - JUnit test results summary

### `/artifacts/release/` - Release Management
- `artifacts/release-images-n2minor` - Release image artifacts and metadata
- `build-log.txt` - Release build logs

## Directory Structure Deep Dive

### `/artifacts/` - Main Test Artifacts Directory

#### `/artifacts/build-logs/` - Component Build Logs
Contains compilation logs for the core HyperShift components:

- `hypershift-amd64.log` - Main HyperShift binary build
- `hypershift-operator-amd64.log` - HyperShift operator build  
- `hypershift-tests-amd64.log` - Test suite compilation

#### `/artifacts/build-resources/` - Build-Time Kubernetes Resources
Resource manifests generated during the build process:

- `builds.json` - OpenShift build configurations
- `events.json` - Kubernetes events during build
- `pods.json` - Pod specifications and status

#### `/artifacts/ci-operator-*` - CI Pipeline Artifacts
CI operator execution details:

- `ci-operator.log` - Main CI execution log (check here for high-level test failures)
- `ci-operator-metrics.json` - Performance metrics and timing
- `junit_operator.xml` - JUnit test results for CI integration

#### `/artifacts/e2e-aks/` - Azure E2E Test Execution

This directory contains the complete AKS-based test execution pipeline:

##### üèóÔ∏è Infrastructure Management
- `aks-provision/` - AKS management cluster creation logs
- `aks-deprovision/` - AKS cluster cleanup and teardown logs  
- `azure-deprovision-resourcegroup/` - Azure resource group cleanup

##### üîê Security and Access Setup
- `hypershift-azure-aks-attach-kv/` - Azure Key Vault integration for secrets
- `hypershift-install/` - HyperShift operator installation on management cluster
- `ipi-install-rbac/` - RBAC setup for infrastructure provisioning
- `openshift-cluster-bot-rbac/` - Service account permissions for automation

##### üß™ Main Test Execution
- `hypershift-azure-run-e2e/` - **‚≠ê PRIMARY TEST DIRECTORY** containing all hosted control plane tests

### üìä Inside `hypershift-azure-run-e2e/artifacts/`

This is where you'll find the **hosted control plane components and logs**:

#### Test Case Structure
Each `Test*/` directory represents a different test scenario and contains:

##### üìÅ Per-Test Artifacts
- `create.log` - **Hosted cluster creation logs** (start here for provisioning issues)
- `destroy.log` - Cluster cleanup logs
- `dump.log` - Complete resource dump (comprehensive debugging info)
- `infrastructure.log` - Azure infrastructure provisioning logs
- `hostedcluster.tar` - Complete hosted cluster configuration archive

##### üóÇÔ∏è Namespace Organization (`namespaces/` directory)
**Key namespace patterns to look for:**

###### `hypershift/` - **üéõÔ∏è HyperShift Operator Namespace**
This namespace contains the management cluster components that orchestrate hosted control planes:

**Core Components:**

- `apps/deployments/operator.yaml` - **HyperShift operator deployment** (manages HostedClusters)
- `apps/deployments/external-dns.yaml` - **External DNS deployment** (Azure DNS integration)

**Pod Manifests & Logs:**

- `core/pods/operator-*.yaml` - HyperShift operator pod specification
- `core/pods/external-dns-*.yaml` - External DNS pod specification
- `core/pods/logs/operator-*-operator.log` - **üî• Main HyperShift operator logs** (HostedCluster reconciliation)
- `core/pods/logs/operator-*-init-environment.log` - Operator initialization logs
- `core/pods/logs/external-dns-*-external-dns.log` - **DNS management logs** (Azure DNS record creation/updates)

**Configuration & Events:**

- `core/configmaps/` - Operator configuration (feature gates, supported versions)
- `core/events/operator-*.yaml` - Operator lifecycle events
- `core/events/external-dns-*.yaml` - DNS operation events
- `core/services/operator.yaml` - Operator service definition

üí° **Key Use Cases:**

- **HostedCluster issues**: Check `operator-*-operator.log` for cluster provisioning/reconciliation errors
- **DNS problems**: Review `external-dns-*-external-dns.log` for Azure DNS integration issues
- **Operator crashes**: Look at operator events and initialization logs

###### `hypershift-sharedingress/` - Shared Ingress Infrastructure
- Router and ingress controller components

###### `e2e-clusters-*-{test-name}-*/` - **‚≠ê HOSTED CONTROL PLANE NAMESPACE**

This is where the **actual control plane pods run**. Look here for:

- `core/pods/` - **Control plane pod manifests** (etcd, kube-apiserver, etc.)
- `core/pods/logs/` - **üî• Control plane pod logs** (primary debugging location)
- `apps/deployments/` - Control plane component deployments
- `hypershift.openshift.io/` - HyperShift custom resources (HostedControlPlane, etc.)

## üõ†Ô∏è Debugging Workflows

### Finding Control Plane Issues
1. **Start with overall test status**: Check `finished.json` for high-level failure info
2. **Check HyperShift custom resources**: Examine HostedCluster, HostedControlPlane, and NodePool status sections for error conditions
3. **Check test execution**: Look at `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/create.log` 
4. **Find control plane pods**: Navigate to `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*-{test-name}-*/core/pods/`
5. **Review specific component logs**: Check `core/pods/logs/{component-name}-*-{container}.log`

### Node Joining and Infrastructure Issues
1. **Check NodePool status**: Look for scaling, readiness, and machine creation issues
2. **CAPI components**: Review `cluster-api-*` and `capi-provider-*` logs for Azure infrastructure problems
3. **Node bootstrapping**: Check `ignition-server-*` logs for node configuration issues
4. **CSR approval**: Review `machine-approver-*` logs for certificate signing problems
5. **Control plane coordination**: Check `control-plane-operator-*` logs for component management issues

### Common Log Locations

#### Hosted Control Plane Components (in `e2e-clusters-*` namespaces)
- **etcd logs**: `etcd-0-etcd.log`, `etcd-0-healthz.log`, `etcd-0-etcd-metrics.log`
- **API Server logs**: `kube-apiserver-*-kube-apiserver.log`, `kube-apiserver-*-audit-logs.log`
- **Controller Manager**: `kube-controller-manager-*-kube-controller-manager.log`
- **Scheduler**: `kube-scheduler-*-kube-scheduler.log`

#### Management Cluster Components (in `hypershift/` namespace)
- **HyperShift Operator**: `operator-*-operator.log` (HostedCluster/HostedControlPlane reconciliation)
- **External DNS**: `external-dns-*-external-dns.log` (Azure DNS record management)
- **Operator Initialization**: `operator-*-init-environment.log` (startup and environment setup)

### Infrastructure Debugging
- **AKS provisioning**: `artifacts/e2e-aks/aks-provision/build-log.txt`
- **Azure resources**: `artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/infrastructure.log`
- **Network setup**: Look for `cloud-network-config-controller` logs in hosted control plane namespace

### HyperShift Operator Debugging
1. **HostedCluster creation issues**: Check `hypershift/core/pods/logs/operator-*-operator.log` for reconciliation errors
2. **DNS resolution problems**: Review `hypershift/core/pods/logs/external-dns-*-external-dns.log` for Azure DNS failures
3. **Operator startup issues**: Examine `hypershift/core/pods/logs/operator-*-init-environment.log` and operator events
4. **Cross-reference with control plane**: If operator reports success but control plane fails, check hosted control plane namespace logs

### External DNS Specific Issues
- **Azure DNS integration**: External DNS manages DNS records for hosted cluster API endpoints
- **Common failure patterns**: Permission issues with Azure DNS zones, rate limiting, DNS propagation delays
- **Log format**: Look for Azure API calls, DNS record operations, and error responses in external-dns logs


---

## Source: docs/content/reference/versioning-support.md

# Versioning Support
There are different components that might require independent versioning and support level:

- Management Cluster
- API `hypershift.openshift.io`
- HyperShift Operator (HO)
- Control Plane Operator (CPO)
- HyperShift CLI
- Hosted Control Plane (HCP) CLI

## Support Level

### Managed Services
Managed services, such as Red Hat OpenShift on IBM Cloud, control versioning
of all components. Refer to the managed service documentation for the latest,
authoritative support matrix.

#### Red Hat OpenShift on IBM Cloud

Red Hat OpenShift on IBM Cloud may support OCP versions beyond standard
OCP timelines. As of September 28, 2025:
- Management cluster (where the HyperShift Operator runs): OpenShift 4.15 or later or Kubernetes 1.30 or later
- HostedClusters created by HyperShift on IBM Cloud: OpenShift 4.14 or later

Information above is subject to change; check IBM Cloud documentation or
contact IBM development.

### Management Cluster
In general, the upstream HyperShift project does not place strict requirements on the OpenShift version of your 
management cluster. This may vary depending on the particular platform; for example, Kubevirt requires management 
clusters with OCP 4.14 and higher.

The HO determines what versions of OCP can be installed through the HostedCluster (HC); see the HO section for 
more details. However, different versions of the HO are thoroughly tested only on a limited set of OpenShift versions, 
and this should inform your deployment decisions.

#### Production Use Cases
For production use & support, it is required to use a downstream product which bundles a supported build of the 
HyperShift Operator. This downstream product is called Multi-Cluster Engine (MCE) and it is available through 
OpenShift's OperatorHub. 

MCE versions _do_ require specific OCP versions for the Management Cluster to remain in a supported state. 
Each version documents its own support matrix. For example, 

- MCE 2.5
- MCE 2.4

As a heuristic, a new release of MCE will run on:

- The latest, yet to be released version of OpenShift
- The latest GA version of OpenShift
- Two versions prior to the latest GA version

Versions of MCE can also be obtained with the Advanced Cluster Management (ACM) offering. If you are running ACM, refer 
to product documentation to determine the bundled MCE version.

The full list of HostedCluster OCP versions that can be installed via the HO on a Management Cluster will depend on the 
version of the installed HO. However, if you are running a tested configuration or MCE, this list will always include at 
least (a) the same OCP version as the Management Cluster and (b) Two previous minor versions relative to the Management 
Cluster. For example, if the Management Cluster is running 4.16 and a supported version of MCE, then the HO will at 
least be able to install 4.16, 4.15, and 4.14 Hosted Clusters. See the Multi-Cluster Engine section, under the expanded 
section titled "OpenShift Advanced Cluster Management" on this page for more details. 

### API
There are two user facing resources exposed by HyperShift: HostedClusters and NodePools.

The HyperShift API version policy generally aligns with the Kubernetes API versioning.

### HO
The upstream HyperShift project does not release new versions aligned with the OpenShift release cadence. New versions 
of the HO are periodically tagged from the `main` branch. These versions are tested and consumed by internal Red Hat 
managed services, and you can use these versions directly. However, for supported production use, you should use a 
supported version of MCE.

The HO is tagged at particular commits as part of merging new HO versions for Red Hat managed services; there is no 
particular tagging scheme for this effort.

A list of the tags can be found here.

Once installed, the HO creates a ConfigMap called `supported-versions` into the Hypershift namespace, which describes 
the HostedClusters supported versions that could be deployed. 

Here is an example `supported-versions` ConfigMap:
```
apiVersion: v1
data:
    server-version: 2f6cfe21a0861dea3130f3bed0d3ae5553b8c28b
    supported-versions: '{"versions":["4.17","4.16","4.15","4.14"]}'
kind: ConfigMap
metadata:
    creationTimestamp: "2024-06-20T07:12:31Z"
    labels:
        hypershift.openshift.io/supported-versions: "true"
    name: supported-versions
    namespace: hypershift
    resourceVersion: "927029"
    uid: f6336f91-33d3-472d-b747-94abae725f70
```

!!! important

        You cannot install HCs higher than what the HO supports. In the example above, HCs using images greater than 
        4.17 cannot be created.

### CPO
The CPO is released as part of each OCP payload release image. You can find those release images here:

- amd64
- arm64
- multi-arch

### HyperShift CLI
The HyperShift CLI is a helper utility used only for development and testing purposes. No compatibility policies are 
guaranteed.

It helps create required infrastructure needed for a HostedCluster CR and NodePool CR to successfully install. 

#### Showing General Version Information
Running the following command will show what the latest OCP version the CLI supports against your KUBECONFIG:
```
% hypershift version
Client Version: openshift/hypershift: 1ed535a8d27c5a1546f1ff4cc71abf32dd1a26aa. Latest supported OCP: 4.17.0
Server Version: 2f6cfe21a0861dea3130f3bed0d3ae5553b8c28b
Server Supports OCP Versions: 4.17, 4.16, 4.15, 4.14
```

#### Showing HyperShift CLI Version Information
Running the following command will show the commit sha the HyperShift CLI was built from:
```
 % hypershift version --commit-only
211a8536809aca94d6047c057866be54d96777c5
```

### HCP CLI
The HCP CLI is the productized version of the HyperShift CLI. This CLI is available through download from MCE.

Similar to the HyperShift CLI, running the `hcp version` command will show the latest OCP version the CLI supports
against your KUBECONFIG. Running the `hcp version --commit-only` will show the commit sha the HCP CLI was built from.

## HostedCluster and NodePool Version Compatibility

HyperShift validates version compatibility between HostedClusters and their NodePools. While the system reports version incompatibilities, existing NodePools with unsupported version skews will continue to operate without guarantees of stability or support.

### Version Skew Policy

The following rules apply to NodePool version compatibility:

1. **NodePool version cannot be higher than the HostedCluster version**
   - A NodePool must run the same version or an older version than its HostedCluster
   - This applies to both minor and patch versions (e.g., NodePool 4.18.6 is not allowed with HostedCluster 4.18.5)

2. **N-3 version skew support**
   - For all 4.y OpenShift versions, NodePools can run up to 3 minor versions behind the HostedCluster
   - For example, a 4.18 HostedCluster supports NodePools running 4.18, 4.17, 4.16, and 4.15

### Examples

**HostedCluster version 4.18.5:**

Supported NodePool versions:
- Same minor, same or lower patch: `4.18.5`, `4.18.4`, `4.18.3`, etc.
- N-1 minor (any patch): `4.17.z` (e.g., `4.17.0`, `4.17.10`, `4.17.25`)
- N-2 minor (any patch): `4.16.z` (e.g., `4.16.0`, `4.16.15`)
- N-3 minor (any patch): `4.15.z` (e.g., `4.15.0`, `4.15.20`)

Unsupported NodePool versions:
- Higher patch in same minor: `4.18.6`, `4.18.10` (NodePool patch cannot exceed HostedCluster patch)
- Higher minor version: `4.19.0`, `4.19.z`, `4.20.0` and above
- Beyond N-3 minor version: `4.14.z`, `4.13.z` and below 

### Version Compatibility Validation

The NodePool controller automatically validates version compatibility and reports the status through the `SupportedVersionSkew` condition:

- **Status: True** - NodePool version is compatible with the HostedCluster version
- **Status: False** - Version incompatibility detected; the condition message provides details about the specific incompatibility

**Behavior when incompatibility is detected:**

- **Existing NodePools**: Will continue to operate despite the incompatibility, but without stability or support guarantees. The condition serves as a warning to administrators.
- **CLI (`hypershift` command)**: Will prevent creation of new NodePools with incompatible versions by returning an error.

**Recommendations:**

- Monitor the `SupportedVersionSkew` condition regularly to identify version compatibility issues
- When upgrading HostedClusters, verify that all NodePools remain within the supported version skew window
- If a NodePool becomes incompatible, upgrade it to a compatible version as soon as operationally feasible
- When creating new NodePools, the CLI will enforce compatibility checks to prevent misconfiguration


---

## Source: docs/content/videos/index.md

---
title: HyperShift Demos/Videos
---

## How HostedControlPlanes Work

This is a contribution from Srishti Miglani that explains how HostedControlPlanes work. The video provides an in-depth look at the KubeVirt provider.

<iframe width="560" height="315" src="https://www.youtube.com/embed/6huUSukivd0?si=toPNKSqahjLykA7t" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Dualstack Agent Hosted Control Plane Deployment (Connected)

This video explains how to deploy a Hosted Control Plane using the Agent provider on bare metal. It uses a dual-stack configuration with IPv6 and IPv4. The IPv4 connectivity ensures that all required images are available for IPv6 deployment (this is why it works).

<iframe width="560" height="315" src="https://www.youtube.com/embed/fgiu_Rz_lvc?si=GLNhB7UsJwt84bfd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Dualstack Agent Hosted Control Plane Deployment (Disconnected)

This video demonstrates how to deploy a dual-stack, fully disconnected Hosted Control Plane using the Agent provider.

<iframe width="560" height="315" src="https://www.youtube.com/embed/xdcl_Q3LTtw?si=6a6N4VGT6wQITsw9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Hosted Control Plane Deployment Using KubeVirt Provider and Multicluster Engine UI

This video demonstrates how to deploy a Hosted Control Plane using the Multicluster Engine (MCE) UI on a KubeVirt provider.

<iframe width="560" height="315" src="https://www.youtube.com/embed/ABpeVd093LI?si=sap5QsTTygA8YFT7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## IPv6 Agent Hosted Control Plane Deployment (Disconnected)

This video demonstrates how to deploy an IPv6, fully disconnected Hosted Control Plane using the Agent provider.

<iframe width="560" height="315" src="https://www.youtube.com/embed/Dgso--20Exg?si=dMSVnfdS1_FRkuwF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

